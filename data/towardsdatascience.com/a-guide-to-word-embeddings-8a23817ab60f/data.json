{"url": "https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f", "time": 1683015690.2828531, "path": "towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f/", "webpage": {"metadata": {"title": "A Guide to Word Embedding. What are they? How are they more useful\u2026 | by Shraddha Anala | Towards Data Science", "h1": "A Guide to Word Embedding", "description": "Reading, comprehending, communicating and ultimately producing new content is something we all do regardless of who we are in our professional lives. When it comes to extracting useful features from\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate", "anchor_text": "Stack Overflow Dataset", "paragraph_index": 7}, {"url": "https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate", "anchor_text": "datasets", "paragraph_index": 13}, {"url": "https://github.com/shraddha-an/nlp/blob/main/pretrained_glove_classification.ipynb", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 32}], "all_paragraphs": ["Reading, comprehending, communicating and ultimately producing new content is something we all do regardless of who we are in our professional lives.", "When it comes to extracting useful features from a given body of text, the processes involved are fundamentally different when compared to, say a vector of continuous integers. This is because the information in a sentence or a piece of text is encoded in structured sequences, with the semantic placement of words conveying the meaning of the text.", "So this dual requirement of appropriate representation of the data along with preserving the contextual meaning of the text has led me to learn about and implement 2 different NLP models to achieve the task of text classification.", "Word Embeddings are dense representations of the individual words in a text, taking into account the context and other surrounding words that that individual word occurs with.", "The dimensions of this real-valued vector can be chosen and the semantic relationships between words are captured more effectively than a simple Bag-of-Words Model.", "Simply put, words possessing similar meanings or often occuring together in similar contexts, will have a similar vector representation, based on how \u201cclose\u201d or \u201cfar apart\u201d those words are in their meanings.", "In this article, I will be exploring two Word Embeddings \u2014", "For this case study, we will be using the Stack Overflow Dataset from Kaggle. This dataset contains 60,000 questions asked by users on the website and the main task is to categorize the quality of the questions asked into 3 classes.", "Let us now look at the actual models themselves for this multi-class NLP project.", "Before starting, however, make sure you have installed these packages/libraries.", "Access the entire code here for the first model, if you wish to skip the explanation.", "In the first model, we will be training a neural network to learn an embedding from our corpus of text. Specifically, we will supply word tokens and their indexes to an Embedding Layer in our neural network using the Keras library.", "There are some key parameters that have to be decided upon before training our network. These include the size of the vocabulary or the number of unique words in our corpus and the dimension of our embedded vectors.", "There are 2 datasets provided for training and testing in the download zip. We\u2019ll now import them and only retain the questions and the quality columns for analysis.", "I\u2019ve also changed the column names and defined a function text_clean to clean up the questions.", "If you peek at the original dataset, you\u2019ll find the questions enclosed in HTML tags like so, <p>\u2026..question </p>. Moreover, there are also words like href, https etc., peppered throughout the entire text so I\u2019m making sure to remove both sets of unwanted characters from the texts.", "Gensim\u2019s simple_preprocess method returns a list of tokens in lowercase with accent marks removed.", "Using the apply method here will iteratively run each observation/row through the preprocessing function and return the output before moving on to the next row. Go ahead and apply the text preprocessing function to both training and testing datasets.", "Since there are 3 categories in the dependent variable vector, we\u2019ll apply one-hot encoding and initialize some parameters for later use.", "Next, we\u2019ll be using the Keras Tokenizer class to convert our questions which are still composed of words into an array representing the words with their indices.", "So we\u2019ll first have to build an indexed vocabulary out of the words appearing in our dataset, with the fit_on_texts method.", "After the vocabulary has been built, we use the text_to_sequences method to convert sentences into a list of numbers representing words.", "The pad_sequences function ensures that all observations are of the same length, set to either an arbitrary number or to the length of the longest question in the dataset.", "The vocab_size parameter we initialized previously is simply the size of our vocabulary of unique words (to learn and index).", "Finally, in this part, we\u2019ll build and train our model which consists of 2 main layers, an Embedding layer that will learn from our training documents prepared above and a Dense output layer to implement the classification task.", "The embedding layer will learn the word representations, along with the neural network while training and requires a lot of text data to provide accurate predictions. In our case, the 45,000 training observations are sufficient to effectively learn the corpus and classify the quality of questions asked. As we will see from the metrics.", "All that\u2019s left is to evaluate our model\u2019s performance and also draw plots to view how the accuracy & the loss metrics of the model change with epochs.", "Our model\u2019s performance metrics are displayed in the screenshot below.", "And the code for the same is displayed below.", "Here\u2019s how the accuracy increased during training\u2026", "\u2026 and the loss decreased over 20 epochs.", "Full code here, if you just want to run the model.", "Instead of training your own embedding, an alternative option is to use pre-trained word embedding like GloVe or Word2Vec. In this part, we will be using the GloVe Word Embedding trained on Wikipedia + Gigaword 5; download it from here.", "Your dataset is composed of more \u201cgeneral\u201d language and you don\u2019t have that big of a dataset, to begin with.", "Since these embeddings have been trained on a lot of words from different sources, pre-trained models might do well if your data is generalized as well.", "Also, you will save on time and computing resources with pre-trained embeddings.", "Your data (and project) is based on a niche industry, such as medicine, finance or any other non-generic and highly specific domains.", "In such cases, a general word embedding representation might not work out for you and some words might be altogether missing from the pre-trained embeddings.", "On the downside, a lot of data is needed to ensure that the word embeddings being learned do a proper job of representing the various words and the semantic relationships between them, unique to your domain.", "Also, it takes a lot of computing resources to go through your corpus and build word embeddings.", "Ultimately the choice between training your own embedding from the data you\u2019ve got or using a pre-trained embedding will boil down to your unique project circumstances.", "Obviously, you can still experiment with both models and choose the one offering better accuracy, but the above guide was a simplified one to aid you in making a decision.", "Most of the steps required have already been taken in the previous parts and only some adjustments are needed.", "We need only build an embedding matrix of words and their vectors which will then be used to set the weights of the embedding layer.", "So if you are following along with this tutorial (are you?), leave the preprocessing, tokenization and padding steps unchanged.", "Once we\u2019ve imported the original dataset and ran it through the previous text cleaning steps, we will run the below code to build the embedding matrix.", "Decide how many dimensions you want your embedding to have (50, 100, 200) and include its name in the path variable below.", "The code to build and train the Embedding layer and the neural network should be slightly modified to allow the embedding matrix to be used as weights in the embedding layer.", "And here are the performance metrics on the test set for our pre-trained model.", "From the performance metrics of both models, training the embedding layer seems to be the better fit for this dataset.", "1) Most of the questions on Stack Overflow are related to IT & Programming, i.e, a niche domain which benefits more from a custom embedding.", "2) Large Training Dataset of 45,000 samples provided a good learning scenario for our embedding layer.", "Hope you found this tutorial helpful and were able to understand the concepts behind training your own word embedding.", "As always any suggestions for improvement are helpful and welcome.", "I\u2019ll leave below some links for further reading as this is definitely an advanced topic and you should practice further to grasp a good understanding.", "Thank you for reading and I\u2019ll see you in my next article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist | Here to write, explain and plot beautiful graphs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8a23817ab60f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shraddha.anala?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shraddha.anala?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Shraddha Anala"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe4b2b9d7772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&user=Shraddha+Anala&userId=be4b2b9d7772&source=post_page-be4b2b9d7772----8a23817ab60f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate", "anchor_text": "Stack Overflow Dataset"}, {"url": "https://github.com/shraddha-an/nlp/blob/main/word_embedding_classification.ipynb", "anchor_text": "here"}, {"url": "https://www.kaggle.com/imoore/60k-stack-overflow-questions-with-quality-rate", "anchor_text": "datasets"}, {"url": "https://github.com/shraddha-an/nlp/blob/main/pretrained_glove_classification.ipynb", "anchor_text": "here"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010", "anchor_text": "Light on Math ML: Intuitive Guide to Understanding GloVe EmbeddingsUnderstanding the theory behind GloVe and Keras implementation!towardsdatascience.com"}, {"url": "https://aylien.com/blog/overview-word-embeddings-history-word2vec-cbow-glove", "anchor_text": "An overview of word embeddings and their connection to distributional semantic modelsUnsupervised learned word embeddings have seen tremendous success in numerous NLP tasks in recent years. So much so\u2026aylien.com"}, {"url": "https://pureai.com/articles/2020/01/06/neural-word-embeddings.aspx", "anchor_text": "Understanding Neural Word Embeddings \u2014 Pure AIThe data scientists at Microsoft Research explain how word embeddings are used in natural language processing \u2014 an\u2026pureai.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8a23817ab60f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8a23817ab60f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8a23817ab60f---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/keras?source=post_page-----8a23817ab60f---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/questions?source=post_page-----8a23817ab60f---------------questions-----------------", "anchor_text": "Questions"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&user=Shraddha+Anala&userId=be4b2b9d7772&source=-----8a23817ab60f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&user=Shraddha+Anala&userId=be4b2b9d7772&source=-----8a23817ab60f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8a23817ab60f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8a23817ab60f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8a23817ab60f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8a23817ab60f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8a23817ab60f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shraddha.anala?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shraddha.anala?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shraddha Anala"}, {"url": "https://medium.com/@shraddha.anala/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "125 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe4b2b9d7772&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&user=Shraddha+Anala&userId=be4b2b9d7772&source=post_page-be4b2b9d7772--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F10fe8f1669c3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-guide-to-word-embeddings-8a23817ab60f&newsletterV3=be4b2b9d7772&newsletterV3Id=10fe8f1669c3&user=Shraddha+Anala&userId=be4b2b9d7772&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}