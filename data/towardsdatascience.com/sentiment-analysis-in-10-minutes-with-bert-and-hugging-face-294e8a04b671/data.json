{"url": "https://towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671", "time": 1683017093.8542762, "path": "towardsdatascience.com/sentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671/", "webpage": {"metadata": {"title": "Sentiment Analysis in 10 Minutes with BERT and TensorFlow | by Orhan G. Yal\u00e7\u0131n | Medium | Towards Data Science", "h1": "Sentiment Analysis in 10 Minutes with BERT and TensorFlow", "description": "Learn the basics of the pre-trained NLP model, BERT, and build a sentiment classifier using the IMDB movie reviews dataset, TensorFlow, and Hugging Face transformers"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Bidirectional Encoder Representations from Transformers", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo", "paragraph_index": 4}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.", "paragraph_index": 6}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 8}, {"url": "https://www.imdb.com/", "anchor_text": "IMDB", "paragraph_index": 15}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews", "paragraph_index": 15}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "directory", "paragraph_index": 17}, {"url": "https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model", "anchor_text": "Keras Documentation", "paragraph_index": 34}, {"url": "http://eepurl.com/hd6Xfv", "anchor_text": "Newsletter", "paragraph_index": 36}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin", "paragraph_index": 37}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "paragraph_index": 37}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai", "paragraph_index": 40}], "all_paragraphs": ["I prepared this tutorial because it is somehow very difficult to find a blog post with actual working BERT code from the beginning till the end. They are always full of bugs. So, I have dug into several articles, put together their codes, edited them, and finally have a working BERT model. So, just by running the code in this tutorial, you can actually create a BERT model and fine-tune it for sentiment analysis.", "Natural language processing (NLP) is one of the most cumbersome areas of artificial intelligence when it comes to data preprocessing. Apart from the preprocessing and tokenizing text datasets, it takes a lot of time to train successful NLP models. But today is your lucky day! We will build a sentiment classifier with a pre-trained NLP model: BERT.", "BERT stands for Bidirectional Encoder Representations from Transformers and it is a state-of-the-art machine learning model used for NLP tasks. Jacob Devlin and his colleagues developed BERT at Google in 2018. Devlin and his colleagues trained the BERT on English Wikipedia (2,500M words) and BooksCorpus (800M words) and achieved the best accuracies for some of the NLP tasks in 2018. There are two pre-trained general BERT variations: The base model is a 12-layer, 768-hidden, 12-heads, 110M parameter neural network architecture, whereas the large model is a 24-layer, 1024-hidden, 16-heads, 340M parameter neural network architecture. Figure 2 shows the visualization of the BERT network created by Devlin et al.", "So, I don\u2019t want to dive deep into BERT since we need a whole different post for that. In fact, I already scheduled a post aimed at comparing rival pre-trained NLP models. But, you will have to wait for a bit.", "Additionally, I believe I should mention that although Open AI\u2019s GPT3 outperforms BERT, the limited access to GPT3 forces us to use BERT. But rest assured, BERT is also an excellent NLP model. Here is a basic visual network comparison among rival NLP models: BERT, GPT, and ELMo:", "One of the questions that I had the most difficulty resolving was to figure out where to find the BERT model that I can use with TensorFlow. Finally, I discovered Hugging Face\u2019s Transformers library.", "Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.", "We can easily load a pre-trained BERT from the Transformers library. But, make sure you install it since it is not pre-installed in the Google Colab notebook.", "Now that we covered the basics of BERT and Hugging Face, we can dive into our tutorial. We will do the following operations to train a sentiment analysis model:", "Note that I strongly recommend you to use a Google Colab notebook. If you want to learn more about how you will create a Google Colab notebook, check out this article:", "Installing the Transformers library is fairly easy. Just run the following pip line on a Google Colab cell:", "After the installation is completed, we will load the pre-trained BERT Tokenizer and Sequence Classifier as well as InputExample and InputFeatures. Then, we will build our model with the Sequence Classifier and our tokenizer with BERT\u2019s Tokenizer.", "Let\u2019s see the summary of our BERT model:", "Here are the results. We have the main BERT model, a dropout layer to prevent overfitting, and finally a dense layer for classification task:", "Now that we have our model, let\u2019s create our input sequences from the IMDB reviews dataset:", "IMDB Reviews Dataset is a large movie review dataset collected and prepared by Andrew L. Maas from the popular movie rating service, IMDB. The IMDB Reviews dataset is used for binary sentiment classification, whether a review is positive or negative. It contains 25,000 movie reviews for training and 25,000 for testing. All these 50,000 reviews are labeled data that may be used for supervised deep learning. Besides, there is an additional 50,000 unlabeled reviews that we will not use in this case study. In this case study, we will only use the training dataset.", "We will first have two imports: TensorFlow and Pandas.", "Then, we can download the dataset from Stanford\u2019s relevant directory with tf.keras.utils.get_file function, as shown below:", "To remove the unlabeled reviews, we need the following operations. The comments below explain each operation:", "Now that we have our data cleaned and prepared, we can create text_dataset_from_directory with the following lines. I want to process the entire data in a single batch. That\u2019s why I selected a very large batch size:", "Now we have our basic train and test datasets, I want to prepare them for our BERT model. To make it more comprehensible, I will create a pandas dataframe from our TensorFlow dataset object. The following code converts our train Dataset object to train pandas dataframe:", "Here is the first 5 row of our dataset:", "I will do the same operations for the test dataset with the following lines:", "We have two pandas Dataframe objects waiting for us to convert them into suitable objects for the BERT model. We will take advantage of the InputExample function that helps us to create sequences from our dataset. The InputExample function can be called as follows:", "Now we will create two main functions:", "1 \u2014 convert_data_to_examples: This will accept our train and test datasets and convert each row into an InputExample object.", "2 \u2014 convert_examples_to_tf_dataset: This function will tokenize the InputExample objects, then create the required input format with the tokenized objects, finally, create an input dataset that we can feed to the model.", "We can call the functions we created above with the following lines:", "Our dataset containing processed input sequences are ready to be fed to the model.", "We will use Adam as our optimizer, CategoricalCrossentropy as our loss function, and SparseCategoricalAccuracy as our accuracy metric. Fine-tuning the model for 2 epochs will give us around 95% accuracy, which is great.", "Training the model might take a while, so ensure you enabled the GPU acceleration from the Notebook Settings. After our training is completed, we can move onto making sentiment predictions.", "I created a list of two reviews I created. The first one is a positive review, while the second one is clearly negative.", "We need to tokenize our reviews with our pre-trained BERT tokenizer. We will then feed these tokenized sequences to our model and run a final softmax layer to get the predictions. We can then use the argmax function to determine whether our sentiment prediction for the review is positive or negative. Finally, we will print out the results with a simple for loop. The following lines do all of these said operations:", "Also, with the code above, you can predict as many reviews as possible.", "You have successfully built a transformers network with a pre-trained BERT model and achieved ~95% accuracy on the sentiment analysis of the IMDB reviews dataset! If you are curious about saving your model, I would like to direct you to the Keras Documentation. After all, to efficiently use an API, one must learn how to read and use the documentation.", "Besides my latest content, I also share my Google Colab notebooks with my subscribers, containing full codes for every post I published.", "If you liked this post, consider subscribing to the Newsletter! \u2709\ufe0f", "Since you are reading this article, I am sure that we share similar interests and are/will be in similar industries. So let\u2019s connect via Linkedin! Please do not hesitate to send a contact request! Orhan G. Yal\u00e7\u0131n \u2014 Linkedin", "If you like this article, check out my other NLP articles:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I write about AI and data apps here building them at Vizio.ai with my team. Feel free to get in touch!"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F294e8a04b671&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a----294e8a04b671---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "\u2190\u2190 PART 1"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e", "anchor_text": "\u2190 PART 2"}, {"url": "https://unsplash.com/@hauntedeyes?utm_source=medium&utm_medium=referral", "anchor_text": "Lukas"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "Bidirectional Encoder Representations from Transformers"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT paper"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "GPT"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT paper"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone."}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "https://towardsdatascience.com/4-reasons-why-you-should-use-google-colab-for-your-next-project-b0c4aaad39ed", "anchor_text": "4 Reasons Why You Should Use Google Colab for Your Next ProjectLearn whether iPython, Jupyter Notebook, and Google Colab are Rivals or Complimentary Tools; Understand Their\u2026towardsdatascience.com"}, {"url": "https://www.imdb.com/", "anchor_text": "IMDB"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Reviews"}, {"url": "https://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "directory"}, {"url": "https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model", "anchor_text": "Keras Documentation"}, {"url": "http://eepurl.com/hd6Xfv", "anchor_text": "Newsletter"}, {"url": "http://eepurl.com/hd6Xfv", "anchor_text": "Subscribe Now"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Linkedin"}, {"url": "https://linkedin.com/in/orhangaziyalcin/", "anchor_text": "Orhan G. Yal\u00e7\u0131n \u2014 Linkedin"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-tensorflow-41e25da6aa54", "anchor_text": "Mastering Word Embeddings in 10 Minutes with TensorFlowCovering the Basics of Word Embedding, One Hot Encoding, Text Vectorization, Embedding Layers, and an Example Neural\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/mastering-word-embeddings-in-10-minutes-with-imdb-reviews-c345f83e054e", "anchor_text": "Mastering Word Embeddings in 10 Minutes with IMDB ReviewsLearn the Basics of Text Vectorization, Create a Word Embedding Model trained with a Neural Network on IMDB Reviews\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----294e8a04b671---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----294e8a04b671---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----294e8a04b671---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/nlp?source=post_page-----294e8a04b671---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----294e8a04b671---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----294e8a04b671---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=-----294e8a04b671---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F294e8a04b671&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----294e8a04b671---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----294e8a04b671--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----294e8a04b671--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----294e8a04b671--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----294e8a04b671--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----294e8a04b671--------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://blog.orhangaziyalcin.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Orhan G. Yal\u00e7\u0131n"}, {"url": "https://blog.orhangaziyalcin.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://Vizio.ai", "anchor_text": "Vizio.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fff47ab81282a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=post_page-ff47ab81282a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6340e0deb03&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsentiment-analysis-in-10-minutes-with-bert-and-hugging-face-294e8a04b671&newsletterV3=ff47ab81282a&newsletterV3Id=6340e0deb03&user=Orhan+G.+Yal%C3%A7%C4%B1n&userId=ff47ab81282a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}