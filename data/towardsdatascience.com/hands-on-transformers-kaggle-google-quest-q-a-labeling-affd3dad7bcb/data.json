{"url": "https://towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb", "time": 1683012058.920845, "path": "towardsdatascience.com/hands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb/", "webpage": {"metadata": {"title": "Hands-on Transformers (Kaggle Google QUEST Q&A Labeling). | by Sarthak Vajpayee | Towards Data Science", "h1": "Hands-on Transformers (Kaggle Google QUEST Q&A Labeling).", "description": "This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge \u2014 Google QUEST Q&A Labeling to see Transformers in action (top 4.4% on the leaderboard). In\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling", "paragraph_index": 0}, {"url": "https://crowdsource.google.com/", "anchor_text": "CrowdSource", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient", "paragraph_index": 7}, {"url": "https://www.kaggle.com/c/google-quest-challenge/data", "anchor_text": "this", "paragraph_index": 9}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047", "anchor_text": "Kaggle link", "paragraph_index": 10}, {"url": "https://www.kaggle.com/c/google-quest-challenge/data?select=train.csv", "anchor_text": "this link", "paragraph_index": 44}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Bert", "paragraph_index": 58}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet", "paragraph_index": 60}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient", "paragraph_index": 73}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842", "anchor_text": "Kaggle link", "paragraph_index": 79}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet", "anchor_text": "this link", "paragraph_index": 93}, {"url": "https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&sk=4ba3eb424ff59ce765c749819c6b5892", "anchor_text": "part 1/3", "paragraph_index": 95}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=friends_link&sk=f48ce58edfdf395fe5d86436d8102a61", "anchor_text": "part 2/3", "paragraph_index": 96}], "all_paragraphs": ["This is a 3 part series where we will be going through Transformers, BERT, and a hands-on Kaggle challenge \u2014 Google QUEST Q&A Labeling to see Transformers in action (top 4.4% on the leaderboard).In this part (3/3) we will be looking at a hands-on project from Google on Kaggle.Since this is an NLP challenge, I\u2019ve used transformers in this project. I have not covered transformers in much detail in this part but if you wish you could check out the part 1/3 of this series where I\u2019ve discussed transformers in detail.", "To make the reading easy, I\u2019ve divided the blog into different sub-topics-", "Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.", "Humans are better at addressing subjective questions that require a deeper, multidimensional understanding of context. Questions can take many forms \u2014 some have multi-sentence elaborations, others may be simple curiosity or a fully developed problem. They can have multiple intents, or seek advice and opinions. Some may be helpful and others interesting. Some are simple right or wrong.", "Unfortunately, it\u2019s hard to build better subjective question-answering algorithms because of a lack of data and predictive models. That\u2019s why the CrowdSource team at Google Research, a group dedicated to advancing NLP and other types of ML science via crowdsourcing, has collected data on a number of these quality scoring aspects.", "In this competition, we\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \u201ccommon-sense\u201d fashion. The raters received minimal guidance and training and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task.", "Demonstrating these subjective labels can be predicted reliably can shine a new light on this research area. Results from this competition will inform the way future intelligent Q&A systems will get built, hopefully contributing to them becoming more human-like.", "Evaluation metric: Submissions are evaluated on the mean column-wise Spearman\u2019s correlation coefficient. The Spearman\u2019s rank correlation is computed for each target column, and the mean of these values is calculated for the submission score.", "The data for this competition includes questions and answers from various StackExchange properties. Our task is to predict the target values of 30 labels for each question-answer pair.The list of 30 target labels is the same as the column names in the sample_submission.csv file. Target labels with the prefix question_ relate to the question_title and/or question_body features in the data. Target labels with the prefix answer_ relate to the answer feature.Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.Target labels can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.The files provided are:", "You can check out the dataset using this link.", "Check-out the notebook with in-depth EDA + Data Scraping (Kaggle link).", "The training data contains 6079 listings and each listing has 41 columns. Out of these 41 columns, the first 11 columns/features have to be used as the input and the last 30 columns/features are the target predictions.Let\u2019s take a look at the input and target labels:", "The output features are all of the float types between 0 and 1.", "Let's explore the input labels one by one.", "Question answer ID represents the id of a particular data point in the given dataset. Each data point has a unique qa_id. This feature is not to be used for training and will be used later while submitting the output to Kaggle.", "This is a string data type feature that holds the title of the question asked.For the analysis of question_title, I\u2019ll be plotting a histogram of the number of words in this feature.", "From the analysis, it is evident that:- Most of the question_title features have a word length of around 9.- The minimum question length is 2.- The maximum question length is 28.- 50% of question_title have lengths between 6 and 11.- 25% of question_title have lengths between 2 and 6.- 25% of question_title have lengths between 11 and 28.", "This is again a string data type feature that holds the detailed text of the question asked.For the analysis of question_body, I\u2019ll be plotting a histogram of the number of words in this feature.", "From the analysis, it is evident that:- Most of the question_body features have a word length of around 93.- The minimum question length is 1.- The maximum question length is 4666.- 50% of question_title have lengths between 55 and 165.- 25% of question_title have lengths between 1 and 55.- 25% of question_title have lengths between 165 and 4666.", "The distribution looks like a power-law distribution, it can be converted to a gaussian distribution using log and then used as an engineered feature.", "This is a string data type feature that denotes the name of the user who asked the question.For the analysis of question_answer, I\u2019ll be plotting a histogram of the number of words in this feature.", "I did not find this feature of much use therefore I won\u2019t be using this for modeling.", "This is a string data type feature that holds the URL to the profile page of the user who asked the question.", "On the profile page, I noticed 4 useful features that could be used and should possibly contribute to good predictions. The features are:- Reputation: Denotes the reputation of the user.- gold_score: The number of gold medals awarded.- silver_score: The number of silver medals awarded.- bronze_score: The number of bronze medals awarded.", "This is again a string data type feature that holds the detailed text of the answer to the question.For the analysis of answer, I\u2019ll be plotting a histogram of the number of words in this feature.", "From the analysis, it is evident that:- Most of the question_body features have a word length of around 143.- The minimum question length is 2.- The maximum question length is 8158.- 50% of question_title have lengths between 48 and 170.- 25% of question_title have lengths between 2 and 48.- 25% of question_title have lengths between 170 and 8158.", "This distribution also looks like a power-law distribution, it can also be converted to a gaussian distribution using log and then used as an engineered feature.", "This is a string data type feature that denotes the name of the user who answered the question.", "I did not find this feature of much use therefore I won\u2019t be using this for modeling.", "This is a string data type feature similar to the feature \u201cquestion_user_page\u201d that holds the URL to the profile page of the user who asked the question.", "I also used the URL in this feature to scrape the external data from the user\u2019s profile page, similar to what I did for the feature \u2018question_user_page\u2019.", "This feature holds the URL of the question and answers page on StackExchange or StackOverflow. Below I\u2019ve printed the first 10 url data-points from train.csv", "One thing to notice is that this feature lands us on the question-answer page, and that page may usually contain a lot more data like comments, upvotes, other answers, etc. which can be used for generating more features if the model does not perform well due to fewer data in train.csvLet\u2019s see the data is present and what additional data can be scraped from the question-answer page.", "In the snapshot attached above, Post 1 and Post 2 contain the answers, upvotes, and comments for the question asked in decreasing order of upvotes. The post with a green tick is the one containing the answer provided in the train.csv file.", "Each question may have more than one answer. We can scrape these answers and use them as additional data.", "The snapshot above defines the anatomy of a post. We can scrape useful features like upvotes and comments and use them as additional data.", "Below is the code for scraping the data from the URL page.", "There are 8 new features that I\u2019ve scraped-- upvotes: The number of upvotes on the provided answer.- comments_0: Comments to the provided answer.- answer_1: Most voted answer apart from the one provided.- comment_1: Top comment to answer_1.- answer_2: Second most voted answer.- comment_2: Top comment to answer_2.- answer_3: Third most voted answer.- comment_3: Top comment to answer_3.", "This is a categorical feature that tells the categories of question and answers pairs. Below I\u2019ve printed the first 10 category data-points from train.csv", "Below is the code for plotting a Pie chart of category.", "The chart tells us that most of the points belong to the category TECHNOLOGY and least belong to LIFE_ARTS (709 out of 6079).", "This feature holds the host or domain of the question and answers page on StackExchange or StackOverflow. Below I\u2019ve printed the first 10 host data-points from train.csv", "Below is the code for plotting a bar graph of unique hosts.", "It seems there are not many but just 63 different subdomains present in the training data. Most of the data points are from StackOverflow.com whereas least from meta.math.stackexchange.com", "Let\u2019s analyze the target values that we need to predict. But first, for the sake of a better interpretation, please check out the full dataset on kaggle using this link.", "Below is the code block displaying the statistical description of the target values. These are only the first 6 features out of all the 30 features.The values of all the features are of type float and are between 0 and 1.", "Notice the second code block which displays the unique values present in the dataset. There are just 25 unique values between 0 and 1. This could be useful later while fine-tuning the code.", "Finally, let\u2019s check the distribution of the target features and their correlation.", "Now that we know our data better through EDA, let\u2019s begin with modeling. Below are the subtopics that we\u2019ll go through in this section-", "I tried various deep neural network architectures with GRU, Conv1D, Dense layers, and with different features for the competition but, an ensemble of 8 transformers (as shown above) seems to work the best.In this part, we will be focusing on the final architecture of the ensemble used and for the other baseline models that I experimented with, you can check out my github repo.", "Remember our task was for a given question_title, question_body, and answer, we had to predict 30 target labels. Now out of these 30 target labels, the first 21 are related to the question_title and question_body and have no connection to the answer whereas the last 9 target labels are related to the answer only but out of these 9, some of them also take question_title and question_body into the picture.Eg. features like answer_relevance and answer_satisfaction can only be rated by looking at both the question and answer.", "With some experimentation, I found that the base-learner (BERT_base) performs exceptionally well in predicting the first 21 target features (related to questions only) but does not perform that well in predicting the last 9 target features. Taking note of this, I constructed 3 dedicated base-learners and 2 different datasets to train them.", "To make the architecture even more robust, I used 3 different types of base learners \u2014 BERT, RoBERTa, and XLNet.We will be going through these different transformer models later in this blog.", "In the ensemble diagram above, we can see \u2014", "In the next step, the predicted data from models dedicated to predicting the question-related features only (denoted as bert_pred_q, roberta_pred_q, xlnet_pred_q) and the predicted data from models dedicated to predicting the answer-related features only (denoted as bert_pred_a, roberta_pred_a, xlnet_pred_a) is collected and concatenated column-wise which leads to a predicted data with all the 30 features. These concatenated features are denoted as xlnet_concat, roberta_concat, and bert_concat.", "Similarly, the predicted data from models dedicated to predicting all the 30 features (denoted as bert_qa, roberta_qa) is collected. Notice that I\u2019ve not used the XLNet model here for predicting all the 30 features because the scores were not up to the mark.", "Finally, after collecting all the different predicted data \u2014 [xlnet_concat, roberta_concat, bert_concat, bert_qa, and roberta_qa], the final value is calculated by taking the average of all the different predicted values.", "Now we will take a look at the 3 different transformer models that were used as base learners.", "Bert was proposed by Google AI in late 2018 and since then it has become state-of-the-art for a wide spectrum of NLP tasks.It uses an architecture derived from transformers pre-trained over a lot of unlabeled text data to learn a language representation that can be used to fine-tune for specific machine learning tasks. BERT outperformed the NLP state-of-the-art on several challenging tasks. This performance of BERT can be ascribed to the transformer\u2019s encoder architecture, unconventional training methodology like the Masked Language Model (MLM), and Next Sentence Prediction (NSP) and the humungous amount of text data (all of Wikipedia and book corpus) that it is trained on. BERT comes in different sizes but for this challenge, I\u2019ve used bert_base_uncased.", "The architecture of bert_base_uncased consists of 12 encoder cells with 8 attention heads in each encoder cell. It takes an input of size 512 and returns 2 values by default, the output corresponding to the first input token [CLS] which has a dimension of 786 and another output corresponding to all the 512 input tokens which have a dimension of (512, 768) aka pooled_output. But apart from these, we can also access the hidden states returned by each of the 12 encoder cells by passing output_hidden_states=True as one of the parameters.BERT accepts several sets of input, for this challenge, the input I\u2019ll be using will be of 3 types:", "XLNet was proposed by Google AI Brain team and researchers at CMU in mid-2019. Its architecture is larger than BERT and uses an improved methodology for training. It is trained on larger data and shows better performance than BERT in many language tasks. The conceptual difference between BERT and XLNet is that while training BERT, the words are predicted in an order such that the previous predicted word contributes to the prediction of the next word whereas, XLNet learns to predict the words in an arbitrary order but in an autoregressive manner (not necessarily left-to-right).", "This helps the model to learn bidirectional relationships and therefore better handles dependencies and relations between words.In addition to the training methodology, XLNet uses Transformer XL based architecture and 2 main key ideas: relative positional embeddings and the recurrence mechanism which showed good performance even in the absence of permutation-based training.XLNet was trained with over 130 GB of textual data and 512 TPU chips running for 2.5 days, both of which are much larger than BERT.", "For XLNet, I\u2019ll be using only input_ids and attention_mask as input.", "RoBERTa was proposed by Facebook in mid-2019. It is a robustly optimized method for pretraining natural language processing (NLP) systems that improve on BERT\u2019s self-supervised method. RoBERTa builds on BERT\u2019s language masking strategy, wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. RoBERTa modifies key hyperparameters in BERT, including removing BERT\u2019s Next Sentence Prediction (NSP) objective, and training with much larger mini-batches and learning rates. This allows RoBERTa to improve on the masked language modeling objective compared with BERT and leads to better downstream task performance. RoBERTa was also trained on more data than BERT and for a longer amount of time. The dataset used was from existing unannotated NLP data sets as well as CC-News, a novel set drawn from public news articles.", "For RoBERTa_base, I\u2019ll be using only input_ids and attention_mask as input.", "Finally here is the comparison of BERT, XLNet, and RoBERTa:", "Now that we have gained some idea about the architecture let\u2019s see how to prepare the data for the base learners.", "Next is the code block for generating the input_ids, attention_masks, and token_type_ids. I\u2019ve used a condition that checks if the function needs to return the generated data for base learners relying on the dataset [question_title + question_body] or the dataset [question_title + question_body + answer].", "Finally, here is the function that makes use of the function initialized above and generates input_ids, attention_masks, and token_type_ids for each of the instances in the provided data.", "To make the model training easy, I also created a class that generates train and cross-validation data based on the fold while using KFlod CV with the help of the functions specified above.", "After data preprocessing, let's create the model architecture starting with base learners.", "The code below takes the model name as input, collects the pre-trained model, and its configuration information according to the input name and creates the base learner model. Notice that output_hidden_states=True is passed after adding the config data.", "The next code block is to create the ensemble architecture. The function accepts 2 parameters name that expects the name of the model that we want to train and model_type that expects the type of model we want to train. The model type can be bert-base-uncased, roberta-base or xlnet-base-cased whereas the model type can be questions, answers, or question_answers.The function create_model() takes the model_name and model_type and generates a model that can be trained on the specified data accordingly.", "Now let's create a function for calculating the evaluation metric Spearman\u2019s correlation coefficient.", "Now we need a function that can collect the base learner model, data according to the base learner model, and train the model.I\u2019ve used K-Fold cross-validation with 5 folds for training.", "Now once we have trained the models and generated the predicted values, we need a function for calculating the weighted average. Here\u2019s the code for that.*The weight\u2019s in the weighted average are all 1s.", "Before bringing everything together, there is one more function that I used for processing the final predicted values. Remember in the EDA section there was an analysis of the target values where we noticed that the target values were only 25 unique floats between 0 and 1. To make use of that information, I calculated 61 (a hyperparameter) uniformly distributed percentile values and mapped them to the 25 unique values. This created 61 bins uniformly spaced between the upper and lower range of the target values. Now to process the predicted data, I used those bins to collect the predicted values and put them in the right place/order. This trick helped in improving the score in the final submission to the leaderboard to some extent.", "Finally, to bring the data-preprocessing, model training, and post-processing together, I created the get_predictions() function that-- Collects the data.- Creates the 8 base_learners.- Prepares the data for the base_learners.- Trains the base learners and collects the predicted values from them.- Calculates the weighted average of the predicted values.- Processes the weighted average prediction.- Converts the final predicted values into a dataframe format requested by Kaggle for submission and return it.", "Once the code compiles and runs successfully, it generates an output file that can be submitted to Kaggle for score calculation. The ranking of the code on the leaderboard is generated using the score.The ensemble model got a public score of 0.43658 which makes it in the top 4.4% on the leaderboard.", "Check-out the notebook with complete post-modeling analysis (Kaggle link).", "Its time for some post-modeling analysis!", "In this section, we will go through an analysis of train data to figure out what parts of the data is the model doing well on and what parts of the data it\u2019s not.The main idea behind this step is to know the capability of the trained model and it works like a charm if applied properly for fine-tuning the model and data.But we won\u2019t get into the fine-tuning part in this section, we will just be performing some basic EDA on the train data using the predicted target values for the train data.I\u2019ll be covering the data feature by feature. Here are the top features we\u2019ll be performing analysis on-", "First, we will have to divide the data into a spectrum of good data and bad data. Good data will be the data points on which the model achieves a good score and bad data will be the data points on which the model achieves a bad score. Now for scoring, we will be comparing the actual target values of the train data with the model\u2019s predicted target values on train data. I used mean squared error (MSE) as a metric for scoring since it focuses on how close the actual and target values are. Remember the more the MSE-score is, the bad the data point will be.Calculating the MSE-score is pretty simple. Here\u2019s the code:", "Starting with the first set of features, which are all text type features, I\u2019ll be plotting word clouds using them. The plan is to segment out these features from 5 data-points that have the least scores and from another 5 data-points that have the most scores.", "Let\u2019s run the code and check what the results look like.", "Word lengths of question_title, question_body, and answer", "The next analysis is on the word lengths of question_title, question_body, and answer. For that, I\u2019ll be picking 30 data-points that have the lowest MSE-scores and 30 data-points that have the highest MSE-scores for each of the 3 features question_title, question_body, and answer. Next, I\u2019ll be calculating the word lengths of these 30 data-points for all the 3 features and plot them to see the trend.", "If we look at the number of words in question_title, question_body, and answer we can observe that the data that generates a high loss has a high number of words which means that the questions and answers are kind of thorough. So, the model does a good job when the questions and answers are concise.", "The next analysis is on the feature host. For this feature, I\u2019ll be picking 100 data-points that have the lowest MSE-scores and 100 data-points that have the highest MSE-scores and select the values in the feature host. Then I\u2019ll be plotting a histogram of this categorical feature to see the distributions.", "We can see that there are a lot of data points from the domain English, biology, sci-fi, physics that contribute to a lesser loss value whereas there are a lot of data points from drupal, programmers, tex that contribute to a higher loss.", "Let\u2019s also take a look at word-clouds of the unique host values that contribute to a low score and a high score. This analysis is again done using the top and bottom 100 data-points.", "The final analysis is on the feature category. For this feature, I\u2019ll be picking 100 data-points that have the lowest MSE-scores and 100 data-points that have the highest MSE-scores and select the values in the feature category. Then I\u2019ll be plotting a pie-chart of this categorical feature to see the proportions.", "We can notice that datapoints with category as technology make up 50% of the data that the model could not predict well whereas categories like LIFE_ARTS, SCIENCE, and CULTURE contribute much less to bad predictions.For the good predictions, all the 5 categories contribute almost the same since there is no major difference in the proportion, still, we could say that the data-points with StackOverflow as the category contribute the least.", "With this, we have come to the end of this blog and the 3 part series. Hope the read was pleasant.You can check the complete notebook on Kaggle using this link and leave an upvote if found my work useful.I would like to thank all the creators for creating the awesome content I referred to for writing this blog.", "Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects or learn some new concepts in NLP.", "In part 1/3 we covered how Transformers became state-of-the-art in various modern natural language processing tasks and their working.", "In part 2/3 we went through BERT (Bidirectional Encoder Representations from Transformers).", "Learning something new epoch by epoch."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faffd3dad7bcb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c----affd3dad7bcb---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faffd3dad7bcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----affd3dad7bcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faffd3dad7bcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=-----affd3dad7bcb---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.kaggle.com/c/google-quest-challenge/", "anchor_text": "Google QUEST Q&A Labeling"}, {"url": "https://crowdsource.google.com/", "anchor_text": "CrowdSource"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient"}, {"url": "https://www.kaggle.com/c/google-quest-challenge/data", "anchor_text": "this"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047", "anchor_text": "Kaggle link"}, {"url": "https://anime.stackexchange.com/questions/56789/if-naruto-loses-the-ability-he-used-on-kakashi-and-guy-after-kaguyas-seal-what", "anchor_text": "https://anime.stackexchange.com/questions/56789/if-naruto-loses-the-ability-he-used-on-kakashi-and-guy-after-kaguyas-seal-what"}, {"url": "https://anime.stackexchange.com/questions/3281/whos-inside-the-third-coffin-that-orochimaru-tried-to-summon?rq=1", "anchor_text": "webpage source"}, {"url": "https://anime.stackexchange.com/questions/3281/whos-inside-the-third-coffin-that-orochimaru-tried-to-summon?rq=1", "anchor_text": "webpage source"}, {"url": "https://www.kaggle.com/c/google-quest-challenge/data?select=train.csv", "anchor_text": "this link"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "Bert"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet"}, {"url": "https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fbert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8&psig=AOvVaw2cf6K31PfXF2YtrANVGKZe&ust=1596463051519000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCJDg_rrW_OoCFQAAAAAdAAAAABAD", "anchor_text": "source link"}, {"url": "https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient", "anchor_text": "Spearman\u2019s correlation coefficient"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842", "anchor_text": "Kaggle link"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet", "anchor_text": "this link"}, {"url": "https://www.appliedaicourse.com/", "anchor_text": "https://www.appliedaicourse.com/"}, {"url": "https://www.kaggle.com/c/google-quest-challenge/notebooks", "anchor_text": "https://www.kaggle.com/c/google-quest-challenge/notebooks"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "http://jalammar.github.io/illustrated-transformer/"}, {"url": "https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04", "anchor_text": "https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04"}, {"url": "https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8", "anchor_text": "https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8"}, {"url": "https://towardsdatascience.com/transformers-state-of-the-art-natural-language-processing-1d84c4c7462b?source=friends_link&sk=4ba3eb424ff59ce765c749819c6b5892", "anchor_text": "part 1/3"}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=friends_link&sk=f48ce58edfdf395fe5d86436d8102a61", "anchor_text": "part 2/3"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047", "anchor_text": "https://www.kaggle.com/sarthakvajpayee/top-4-4-in-depth-eda-feature-scraping?scriptVersionId=40263047"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet", "anchor_text": "https://www.kaggle.com/sarthakvajpayee/top-4-4-bert-roberta-xlnet"}, {"url": "https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842", "anchor_text": "https://www.kaggle.com/sarthakvajpayee/top-4-4-post-modeling-analysis?scriptVersionId=40262842"}, {"url": "http://www.linkedin.com/in/sarthak-vajpayee", "anchor_text": "www.linkedin.com/in/sarthak-vajpayee"}, {"url": "https://github.com/SarthakV7/Kaggle_google_quest_challenge", "anchor_text": "https://github.com/SarthakV7/Kaggle_google_quest_challenge"}, {"url": "https://medium.com/tag/nlp?source=post_page-----affd3dad7bcb---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----affd3dad7bcb---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/kaggle?source=post_page-----affd3dad7bcb---------------kaggle-----------------", "anchor_text": "Kaggle"}, {"url": "https://medium.com/tag/google?source=post_page-----affd3dad7bcb---------------google-----------------", "anchor_text": "Google"}, {"url": "https://medium.com/tag/education?source=post_page-----affd3dad7bcb---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faffd3dad7bcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----affd3dad7bcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faffd3dad7bcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----affd3dad7bcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faffd3dad7bcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c----affd3dad7bcb---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb72ebbfba4df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&newsletterV3=d850bda2ea8c&newsletterV3Id=b72ebbfba4df&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----affd3dad7bcb---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Written by Sarthak Vajpayee"}, {"url": "https://medium.com/@itssarthakvajpayee/followers?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "136 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd850bda2ea8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=post_page-d850bda2ea8c----affd3dad7bcb---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb72ebbfba4df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhands-on-transformers-kaggle-google-quest-q-a-labeling-affd3dad7bcb&newsletterV3=d850bda2ea8c&newsletterV3Id=b72ebbfba4df&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----affd3dad7bcb---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ai-based-indian-license-plate-detector-de9d48ca8951?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/ai-based-indian-license-plate-detector-de9d48ca8951?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "AI-based Indian license plate detector.Inspiration: The guy who hit my car and got away with it!"}, {"url": "https://towardsdatascience.com/ai-based-indian-license-plate-detector-de9d48ca8951?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "\u00b711 min read\u00b7Sep 7, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fde9d48ca8951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-based-indian-license-plate-detector-de9d48ca8951&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----de9d48ca8951----0-----------------clap_footer----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/ai-based-indian-license-plate-detector-de9d48ca8951?source=author_recirc-----affd3dad7bcb----0---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fde9d48ca8951&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-based-indian-license-plate-detector-de9d48ca8951&source=-----affd3dad7bcb----0-----------------bookmark_preview----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----affd3dad7bcb----1---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----affd3dad7bcb----1-----------------bookmark_preview----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----affd3dad7bcb----2---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----affd3dad7bcb----2-----------------bookmark_preview----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Sarthak Vajpayee"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "Understanding BERT \u2014 (Bidirectional Encoder Representations from Transformers)Part 2/3 of Transformers vs Google QUEST Q&A Labeling (Kaggle top 5%)."}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": "\u00b710 min read\u00b7Aug 6, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&user=Sarthak+Vajpayee&userId=d850bda2ea8c&source=-----45ee6cd51eef----3-----------------clap_footer----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/understanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef?source=author_recirc-----affd3dad7bcb----3---------------------1db2dca3_b8fb_4605_bacf_4ed6a2376892-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45ee6cd51eef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-bert-bidirectional-encoder-representations-from-transformers-45ee6cd51eef&source=-----affd3dad7bcb----3-----------------bookmark_preview----1db2dca3_b8fb_4605_bacf_4ed6a2376892-------", "anchor_text": ""}, {"url": "https://medium.com/@itssarthakvajpayee?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "See all from Sarthak Vajpayee"}, {"url": "https://towardsdatascience.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----affd3dad7bcb----0-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://audhiaprilliant.medium.com/?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://audhiaprilliant.medium.com/?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Audhi Aprilliant"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Part 1 \u2014 End to End Machine Learning Model Deployment Using FlaskHow to prepare the data and develop a machine learning model for loan approval prediction app"}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b713 min read\u00b7Nov 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2F1df8920da9c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fpart-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3&user=Audhi+Aprilliant&userId=140ffe7d74ee&source=-----1df8920da9c3----1-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/part-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1df8920da9c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fpart-1-end-to-end-machine-learning-model-deployment-using-flask-1df8920da9c3&source=-----affd3dad7bcb----1-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----0-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----affd3dad7bcb----0---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----affd3dad7bcb----0-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://lucianosphere.medium.com/?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "LucianoSphere"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Build ChatGPT-like Chatbots With Customized Knowledge for Your Websites, Using Simple ProgrammingLike ChatGPT but in a form that you can plug into your website and expand with any kind of tailored information by combining basic\u2026"}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b711 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&user=LucianoSphere&userId=d28939b5ab78&source=-----f393206c6626----1-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/build-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626?source=read_next_recirc-----affd3dad7bcb----1---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff393206c6626&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fbuild-chatgpt-like-chatbots-with-customized-knowledge-for-your-websites-using-simple-programming-f393206c6626&source=-----affd3dad7bcb----1-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/coders-mojo/implemented-machine-learning-ops-projects-60b9414cd8c3?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://naina0412.medium.com/?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://naina0412.medium.com/?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Naina Chaturvedi"}, {"url": "https://medium.com/coders-mojo?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Coders Mojo"}, {"url": "https://medium.com/coders-mojo/implemented-machine-learning-ops-projects-60b9414cd8c3?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Implemented Machine Learning Ops ProjectsRepo for all the projects ( vertical post)\u2026"}, {"url": "https://medium.com/coders-mojo/implemented-machine-learning-ops-projects-60b9414cd8c3?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b7116 min read\u00b7Jan 6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fcoders-mojo%2F60b9414cd8c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoders-mojo%2Fimplemented-machine-learning-ops-projects-60b9414cd8c3&user=Naina+Chaturvedi&userId=8022b1c716d6&source=-----60b9414cd8c3----2-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/coders-mojo/implemented-machine-learning-ops-projects-60b9414cd8c3?source=read_next_recirc-----affd3dad7bcb----2---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60b9414cd8c3&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fcoders-mojo%2Fimplemented-machine-learning-ops-projects-60b9414cd8c3&source=-----affd3dad7bcb----2-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----3-----------------clap_footer----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----affd3dad7bcb----3---------------------a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----affd3dad7bcb----3-----------------bookmark_preview----a91dbc5e_2086_4efb_84fb_a72d7650cfcd-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----affd3dad7bcb--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}