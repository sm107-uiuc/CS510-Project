{"url": "https://towardsdatascience.com/why-does-the-optimal-policy-exist-29f30fd51f8c", "time": 1683011132.9036531, "path": "towardsdatascience.com/why-does-the-optimal-policy-exist-29f30fd51f8c/", "webpage": {"metadata": {"title": "Why does the optimal policy exist? | by Alireza Modirshanechi | Towards Data Science", "h1": "Why does the optimal policy exist?", "description": "In a finite Markov Decision Process (MDP), the optimal policy is defined as a policy that maximizes the value of all states at the same time\u00b9. In other words, if an optimal policy exists, then the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equations", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Kronecker_delta", "anchor_text": "Kronecker delta", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Fixed_point_(mathematics)", "anchor_text": "fixed point", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Metric_space", "anchor_text": "metric space", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Metric_(mathematics)", "anchor_text": "metric", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Banach_fixed-point_theorem", "anchor_text": "Banach fixed-point theorem", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Banach_fixed-point_theorem", "anchor_text": "here", "paragraph_index": 28}, {"url": "https://en.wikipedia.org/wiki/L-infinity", "anchor_text": "infinity norm", "paragraph_index": 31}, {"url": "https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&hl=de", "anchor_text": "Johanni Brea", "paragraph_index": 38}, {"url": "https://lcnwww.epfl.ch/gerstner/", "anchor_text": "Wulfram Gerstner", "paragraph_index": 38}, {"url": "https://scholar.google.com/citations?user=Ysi38KIAAAAJ&hl=en", "anchor_text": "Berfin Simsek", "paragraph_index": 38}, {"url": "https://sites.ualberta.ca/~szepesva/rlbook.html", "anchor_text": "Algorithms for Reinforcement Learning", "paragraph_index": 38}, {"url": "http://incompleteideas.net/book/the-book.html", "anchor_text": "\u201cReinforcement Learning: An Introduction\u201d", "paragraph_index": 39}, {"url": "https://sites.ualberta.ca/~szepesva/rlbook.html", "anchor_text": "Algorithms for Reinforcement Learning", "paragraph_index": 41}, {"url": "https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&hl=de", "anchor_text": "Johanni Brea", "paragraph_index": 41}, {"url": "https://medium.com/swlh/is-correlation-distance-a-metric-5a383973978f", "anchor_text": "\"Is correlation distance a metric?\"", "paragraph_index": 43}, {"url": "https://sites.wustl.edu/nachbar/course-notes/math/analysis/", "anchor_text": "here", "paragraph_index": 45}, {"url": "https://sites.wustl.edu/nachbar/files/2019/09/rncomplete.pdf", "anchor_text": "Completeness and Compactness in R^N", "paragraph_index": 45}, {"url": "https://sites.google.com/view/modirsha", "anchor_text": "https://sites.google.com/view/modirsha", "paragraph_index": 47}], "all_paragraphs": ["In a finite Markov Decision Process (MDP), the optimal policy is defined as a policy that maximizes the value of all states at the same time\u00b9. In other words, if an optimal policy exists, then the policy that maximizes the value of state s is the same as the policy that maximizes the value of state s'.\u00b2 But why should such a policy exist?", "The famous introductory book of Sutton and Barto on reinforcement learning\u00b9 takes the existence of optimal policies for granted and let this question unanswered. I had a difficult time believing them and being able to continue reading!", "In this article, I am going to give a proof of the existence of the optimal policy in finite MDPs \u00b3.", "A finite MDP is characterized by a finite set of states (usually shown by curvy S), a finite set of actions for each state (usually shown by curvy A), and a probability distribution over the immediate reward value r and the next state s\u2019, given the current state s and the currently chosen action a, denoted as p(s\u2019,r|s,a).", "Given the current state s, a policy \u03c0 is a probability distribution over possible actions at state s, denoted as \u03c0(a|s). Then, given a policy, an agent can navigate in the environment (i.e. go from one state to another) and get rewards through each transition.", "We show random variables by capital letters and their values by small letters. Time is added to each variable with a subscript. Then, given a policy and an MDP, and given the initial state (at time t=1) s, for any T > 1, the joint distribution of states, actions, and reward values is", "Given a policy \u03c0 and a discount factor 0 \u2264 \u03b3 < 1, the value of each state is defined as", "and the value of each pair of state and action as", "It is easy to show that the values of states and action-state pairs can be written in a recursive way", "These sets of equations are known as the Bellman equations.", "We will use later the fact that", "The policy \u03c0* is an optimal policy if and only if we have", "for any state s and any other policy \u03c0.", "We show the set of all possible states by curvy S and the set of all possible actions at state s by curvy A(s). We denote the Kronecker delta by \u03b4 and start this section with the following theorem.", "Proof comment: We used Eq. 1 in the 1st line of the proof, and then repeatedly used the fact that the value of the pair of state and action (s*, a*) is greater than or equal to the value of state s*.", "Theorem 1 states that whenever there is a pair of state and action (s*, a*) with a value greater than the value of state s*, with respect to policy \u03c0, then there is another policy \u03c0\u2019 that is better than or equal to (in terms of state-values) \u03c0 in all states. As a result, if an optimal policy \u03c0* exists, its values should satisfy, for any state s,", "where curvy A(s) stands for the set of all possible actions at state s \u2014 one can easily prove this statement by contradiction. Using the Bellman equations, we can expand Equation 2 as", "This set of non-linear equations (as many as the number of states) is called the \u201cBellman optimality equations\u201d. So, if an optimal policy exists, its values should satisfy this set of equations\u2074.", "Therefore, to show that an optimal policy exists, one must prove the following two statements:", "In this section, we prove that the set of the Bellman optimality equations has a unique solution. By doing so, we prove the two aforementioned statements at the same time.", "Given a set of values over states, we define the vector of values as", "which is simply a real-value vector whose elements are equal to the values of different states. Then, we define the \u201cBellman optimality operator\u201d T as a mapping", "The operator T takes a vector of values and maps it to another vector of values. Using this new notation, it is easy to see that Equations 2 and 3 are equivalent to", "This observation means that the solutions of the Bellman optimality equations are the same as the fixed points of the Bellman optimality operator. Therefore, to prove the existence and uniqueness of the solution to the Bellman optimality equations, one can prove that the Bellman optimality operator has a unique fixed point.", "To do so, we need to introduce another concept and another theorem.", "Consider a metric space (M,d), i.e. M is a set, and d is a metric defined on this set for computing the distance of every two elements of M \u2075. A mapping T: M \u2192 M is a contraction mapping if there exists 0 \u2264 k < 1 such that for any x and y in M, we have", "Intuitively, contraction mapping makes points closer to each other. Figure 1 shows an illustration of repeatedly applying a contraction mapping on two points.", "The reason that we are interested in contraction mappings is the following famous theorem, known as Banach fixed-point theorem.", "Proof comment: The proof of the theorem is not hard, but I do not include it in this article, because the theorem is well known and the proof can easily be found elsewhere, e.g. see here.", "The whole idea behind the theorem is what is illustrated in Figure 1: All points are getting closer to each other after mapping, and hence, by repeating the mapping all points converge to one point which is the unique fixed point of T.", "As a result, to prove the existence and uniqueness of the solution of the Bellman optimality equations, it is sufficient to show that there is a metric in which the Bellman optimality operator is a contraction mapping.", "For any pair of value vectors V and V', their infinity norm is defined as", "In this section, we want to prove that the Bellman optimality operator is a contraction mapping in this norm. To do so, we first need the following Lemma.", "Proof comment: Although the Lemma is pretty non-trivial, its proof is not difficult and needs only elementary techniques. I had some fun proving it and thought it might be good to leave its proof as an exercise for interested readers\u2076.", "Now, having the lemma, we can finally go to our main theorem.", "Proof comment: To go from the 2nd to the 3rd line of the proof, we used the Lemma, and to go from the 4th to the 5th line we used the convexity of the absolute value function. The rest is straightforward.", "As a result, the Bellman optimality operator has a unique fixed point\u2077, and the Bellman optimality equations have a unique solution. It is easy to show that any greedy policy on a solution of the Bellman optimality equations has values equal to that solution. Therefore, optimal policies exist!", "We showed that (1) the values of an optimal policy should satisfy the Bellman optimality equations. We then showed that (2) the solutions to the Bellman optimality equations are the fixed point of the Bellman optimality operator. By showing that (3) the Bellman optimality operator is a contraction mapping in infinity norm and using (4) Banach fixed-point theorem, we proved that (5) the Bellman optimality operator has a unique fixed point. As a result, (6) there exist policies that maximize the values of all states at the same time.", "I am grateful to Johanni Brea and my Ph.D. adviser Wulfram Gerstner for introducing me to the topic, to Mohammad Tinati and Kian Kalhor for useful discussions and proof-reading of the article, and to Berfin Simsek for introducing me to the great book of Csaba Szepesv\u00e1ri on \u201cAlgorithms for Reinforcement Learning\u201d.", "\u00b9 \u201cReinforcement Learning: An Introduction\u201d by Sutton and Barto.", "\u00b2 As a real-life but a bit strange example, if our world was a finite MDP, and if an optimal policy exists, then, the same way of cooking dinner would make us (loosely speaking) happiest both when eating dinner and when working at the office the day after.", "\u00b3 I read most of the materials of this article first time in \u201cAlgorithms for Reinforcement Learning\u201d by Csaba Szepesv\u00e1ri and the Ph.D. thesis of Johanni Brea.", "\u2074 It is easy to show that any greedy policy on a solution of the Bellman optimality equations has values equal to that solution.", "\u2075 For a recap on the definition of metric, you can see my article on \"Is correlation distance a metric?\".", "\u2076 Hint: Without loss of generality, assume that the maximum of f\u2081 is greater than the maximum of f\u2082. Then, take the maximizer of f\u2081 and name it a\u2081, and \u2026", "\u2077 Note that for any finite n, \u211d\u207f with infinity norm is compact \u2014 see here for a proof of \u201cCompleteness and Compactness in R^N\u201d.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS PhD student in the Laboratory of Computational Neuroscience at EPFL || Personal website: https://sites.google.com/view/modirsha"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F29f30fd51f8c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://a-modirshanechi.medium.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": ""}, {"url": "https://a-modirshanechi.medium.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Alireza Modirshanechi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a535debb95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&user=Alireza+Modirshanechi&userId=6a535debb95&source=post_page-6a535debb95----29f30fd51f8c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Markov_decision_process", "anchor_text": "Markov Decision Process"}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equations"}, {"url": "https://en.wikipedia.org/wiki/Kronecker_delta", "anchor_text": "Kronecker delta"}, {"url": "https://en.wikipedia.org/wiki/Fixed_point_(mathematics)", "anchor_text": "fixed point"}, {"url": "https://en.wikipedia.org/wiki/Metric_space", "anchor_text": "metric space"}, {"url": "https://en.wikipedia.org/wiki/Metric_(mathematics)", "anchor_text": "metric"}, {"url": "https://en.wikipedia.org/wiki/Banach_fixed-point_theorem", "anchor_text": "Banach fixed-point theorem"}, {"url": "https://en.wikipedia.org/wiki/Banach_fixed-point_theorem", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/L-infinity", "anchor_text": "infinity norm"}, {"url": "https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&hl=de", "anchor_text": "Johanni Brea"}, {"url": "https://lcnwww.epfl.ch/gerstner/", "anchor_text": "Wulfram Gerstner"}, {"url": "https://scholar.google.com/citations?user=Ysi38KIAAAAJ&hl=en", "anchor_text": "Berfin Simsek"}, {"url": "https://sites.ualberta.ca/~szepesva/rlbook.html", "anchor_text": "Algorithms for Reinforcement Learning"}, {"url": "http://incompleteideas.net/book/the-book.html", "anchor_text": "\u201cReinforcement Learning: An Introduction\u201d"}, {"url": "https://sites.ualberta.ca/~szepesva/rlbook.html", "anchor_text": "Algorithms for Reinforcement Learning"}, {"url": "https://scholar.google.ch/citations?user=nZ0m0xUAAAAJ&hl=de", "anchor_text": "Johanni Brea"}, {"url": "https://medium.com/swlh/is-correlation-distance-a-metric-5a383973978f", "anchor_text": "\"Is correlation distance a metric?\""}, {"url": "https://sites.wustl.edu/nachbar/course-notes/math/analysis/", "anchor_text": "here"}, {"url": "https://sites.wustl.edu/nachbar/files/2019/09/rncomplete.pdf", "anchor_text": "Completeness and Compactness in R^N"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----29f30fd51f8c---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----29f30fd51f8c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----29f30fd51f8c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----29f30fd51f8c---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/math?source=post_page-----29f30fd51f8c---------------math-----------------", "anchor_text": "Math"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&user=Alireza+Modirshanechi&userId=6a535debb95&source=-----29f30fd51f8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&user=Alireza+Modirshanechi&userId=6a535debb95&source=-----29f30fd51f8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F29f30fd51f8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----29f30fd51f8c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----29f30fd51f8c--------------------------------", "anchor_text": ""}, {"url": "https://a-modirshanechi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://a-modirshanechi.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alireza Modirshanechi"}, {"url": "https://a-modirshanechi.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "53 Followers"}, {"url": "https://sites.google.com/view/modirsha", "anchor_text": "https://sites.google.com/view/modirsha"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a535debb95&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&user=Alireza+Modirshanechi&userId=6a535debb95&source=post_page-6a535debb95--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2b8d46879b14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-does-the-optimal-policy-exist-29f30fd51f8c&newsletterV3=6a535debb95&newsletterV3Id=2b8d46879b14&user=Alireza+Modirshanechi&userId=6a535debb95&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}