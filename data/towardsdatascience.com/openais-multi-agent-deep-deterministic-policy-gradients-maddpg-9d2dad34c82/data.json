{"url": "https://towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82", "time": 1683008153.85344, "path": "towardsdatascience.com/openais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82/", "webpage": {"metadata": {"title": "OpenAI's MADDPG Algorithm | Towards Data Science", "h1": "OpenAI\u2019s MADDPG Algorithm", "description": "An outline of OpenAI's MADDPG algorithm. Uses an actor-critic style approach with centralized planning, decentralized execution methodology."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/b24112d01863?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Chris Yoon", "paragraph_index": 6}], "all_paragraphs": ["Multi-agent reinforcement learning is an on-going, rich field of research. However, naively applying single-agent algorithms in multi-agent contexts \u201cputs us in a pickle.\u201d Learning becomes difficult due to many reasons, especially due to:", "Researchers have proposed plenty of approaches to mitigate the effects of these challenges. A large subset of these methods falls under the umbrella of \u201ccentralized planning with decentralized execution.\u201d", "Each agent only has direct access to local observations. These observations can be many things: an image of the environment, relative positions to landmarks, or even relative positions of other agents. Also, during learning, all agents are guided by a centralized module or critic.", "Even though each agent only has local information and local policies to train, there is an entity overlooking the entire system of agents, advising them on how to update their policies. This reduces the effect of non-stationarity. All agents learn with the help of a module with global information.", "Then, during testing, the centralized module is removed, leaving only the agents, their policies, and local observations. This reduces the detriments of increasing state and action space because joint policies are never explicitly learned. Instead, we hope that the central module has given enough information to guide local policy training such that it is optimal for the entire system once test time comes around.", "Researchers at OpenAI, UC Berkeley, and McGill University introduced a novel approach to multi-agent settings using Multi-Agent Deep Deterministic Policy Gradients. Inspired by its single-agent counterpart DDPG, this approach uses actor-critic style learning and has shown promising results.", "We assume familiarity with the single-agent version of MADDPG: Deep Deterministic Policy Gradients (DDPG). For a quick refresher, Chris Yoon has a fantastic article overviewing it here:", "Every agent has an observation space and continuous action space. Also, each agent has three components:", "As the critic learns the joint Q-value function over time, it sends appropriate Q-value approximations to the actor to help training. We\u2019ll see in the next section a more in-depth look at this interaction.", "Keep in mind that the critic can be a shared network between all N agents. In other words, instead of training N networks that estimate the same value, simply train one network and use it to help all of the actors learn. The same applies for the actor networks if the agents are homogenous.", "First, MADDPG uses an experience replay for efficient off-policy training. At each timestep, the agent stores the following transition:", "where we store the joint state, next joint state, joint action, and each of the agents\u2019 received rewards. Then, we sample a batch of these transitions from the experience replay to train our agent.", "To update an agent\u2019s centralized critic, we use a one-step lookahead TD-error:", "where mu denotes the actor. Keep in mind that this is a centralized critic, meaning it uses joint information to update its parameters. The primary motivation is that knowing the actions taken by all agents makes the environment stationary even when policies change.", "Notice the calculation of our target Q-value on the right. Even though we never explicitly store the next joint actions, we use each of the agent\u2019s target actor to compute this next action during the update to help in training stability. The target actor\u2019s parameters are updated periodically to match the agent\u2019s actor parameters.", "Similar to single-agent DDPG, we use the deterministic policy gradient to update each of the agent\u2019s actor parameters.", "where mu denotes an agent\u2019s actor.", "Let\u2019s dig into this update equation just a little bit. We take the gradient with respect to the actor\u2019s parameters using a central critic to guide us. The most important thing to notice is that even though the actor only has local observations and actions, we use a centralized critic during training time, providing information about the optimality of its actions for the entire system. This reduces the effects of nonstationarity while keeping policy learning at a lower state space!", "We can take decentralization one step further. In earlier critic updates, we assumed each agent automatically knew other agents\u2019 actions. However, MADDPG suggests inferring other agents\u2019 policies to make learning even more independent. In effect, each agent adds N-1 more networks to estimate the true policy of each of the other agents. We use a probabilistic network and maximize the log probability of outputting another agent\u2019s observed action.", "where we show the loss function for the ith agent estimating the jth agent\u2019s policy with an entropy regularizer. As a result, our Q-value target becomes a slightly different value as we replace agent actions with our predicted action!", "So, what exactly have we done? We\u2019ve removed any assumption that agents know each other\u2019s policies. Instead, we try to train agents to correctly predict other policies through a series of observations. In effect, each agent is trained independently by extracting global information from the environment instead of automatically having it on hand.", "There\u2019s one big issue with the approach above. In many multi-agent settings, especially in competitive ones, agents can craft policies that overfit to other agents\u2019 behaviors. This makes policies brittle, unstable, and typically suboptimal. To compensate for that, MADDPG trains a collection of K sub-policies for each agent. At each timestep, an agent randomly selects one of the sub-policies to choose an action from. Then, execute.", "The policy gradient becomes slightly modified. We average over the K sub-policies, use linearity of expectation, and propagate updates through the Q-value function.", "That outlines the entire algorithm! At this point, it\u2019s important to take a step back and internalize what exactly we\u2019ve done and intuitively understand why it works. In essence, we\u2019ve done the following things:", "Every component of the algorithm serves a specific, delegated purpose. This is what makes MADDPG a powerful algorithm: its various components are meticulously designed to overcome big obstacles multi-agent systems usually have in spades. Next, we take a look at the algorithm\u2019s performance.", "MADDPG was tested in many environments. For the full overview of its performance, feel free to check out the paper [1]. Here, we\u2019ll only discuss the cooperative communication task.", "Here, there are two agents: a speaker and a listener. During each iteration, the listener is given a colored landmark to travel to and receives a reward inversely proportional to its distance from it. Here\u2019s the catch: the listener only knows its relative position and the color of all landmarks. It doesn\u2019t know which landmark it\u2019s supposed to travel to. On the other hand, the speaker knows the color of the correct landmark for this episode. As a result, the two agents must communicate and collaborate to solve the task.", "For this task, the paper pits MADDPG against state-of-the-art single-agent methods. We can see a significant improvement with the use of MADDPG.", "It was also shown that policy inference, even though policies were not fitted perfectly, achieved the same success rates as using true policy observations. Even better, there was no significant slowing in convergence.", "Lastly, policy ensembles showed promising results. The paper [1] tests the effect of ensembles in competitive environments and demonstrated significantly better performance than agents with only one policy.", "And that\u2019s it! Here we overviewed a novel approach to multi-agent reinforcement learning problems. Of course, there\u2019s an endless sea of methods under the \u201cMARL umbrella,\u201d but MADDPG provides a strong starting point for approaches that tackle multi-agent systems\u2019 biggest problems.", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9d2dad34c82&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----9d2dad34c82---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@codestorm?utm_source=medium&utm_medium=referral", "anchor_text": "Safar Safarov"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@alinnnaaaa?utm_source=medium&utm_medium=referral", "anchor_text": "Alina Grubnyak"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/u/b24112d01863?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Chris Yoon"}, {"url": "https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b", "anchor_text": "Deep Deterministic Policy Gradients ExplainedReinforcement Learning in Continuous Action Spacestowardsdatascience.com"}, {"url": "https://unsplash.com/@sanderweeteling?utm_source=medium&utm_medium=referral", "anchor_text": "Sander Weeteling"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@timmossholder?utm_source=medium&utm_medium=referral", "anchor_text": "Tim Mossholder"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@retrosupply?utm_source=medium&utm_medium=referral", "anchor_text": "RetroSupply"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1706.02275.pdf", "anchor_text": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"}, {"url": "https://towardsdatascience.com/how-deepminds-unreal-agent-performed-9-times-better-than-experts-on-atari-9c6ee538404e", "anchor_text": "DeepMind\u2019s UNREAL Algorithm ExplainedDeep reinforcement learning at its finesttowardsdatascience.com"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----9d2dad34c82---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9d2dad34c82---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----9d2dad34c82---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9d2dad34c82---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9d2dad34c82---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9d2dad34c82---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----9d2dad34c82---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9d2dad34c82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9d2dad34c82---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9d2dad34c82--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9d2dad34c82--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9d2dad34c82--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "208 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fopenais-multi-agent-deep-deterministic-policy-gradients-maddpg-9d2dad34c82&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}