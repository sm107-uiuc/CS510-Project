{"url": "https://towardsdatascience.com/pegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce", "time": 1683009561.186912, "path": "towardsdatascience.com/pegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce/", "webpage": {"metadata": {"title": "PEGASUS: Google\u2019s State of the Art Abstractive Summarization Model | by Rohan Jagtap | Towards Data Science", "h1": "PEGASUS: Google\u2019s State of the Art Abstractive Summarization Model", "description": "Hence, summarization is a fairly significant concept in NLP. I have already covered summarization as a whole and abstractive summarization along with its implementation using Transformers in this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "this post", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1912.08777", "anchor_text": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "paragraph_index": 3}, {"url": "https://icml.cc/Conferences/2020", "anchor_text": "ICML 2020", "paragraph_index": 3}, {"url": "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "anchor_text": "BERT", "paragraph_index": 6}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet", "paragraph_index": 6}, {"url": "https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html", "anchor_text": "ALBERT", "paragraph_index": 6}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "T5", "paragraph_index": 6}, {"url": "https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html", "anchor_text": "ELECTRA", "paragraph_index": 6}, {"url": "https://www.aclweb.org/anthology/W04-1013/", "anchor_text": "ROUGE", "paragraph_index": 9}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT paper", "paragraph_index": 12}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://github.com/google-research/pegasus", "anchor_text": "here", "paragraph_index": 25}], "all_paragraphs": ["The ability to summarize evaluates one\u2019s understanding of a given piece of text or a language.", "Perhaps the best test of a man\u2019s intelligence is his capacity for making a summary", "Hence, summarization is a fairly significant concept in NLP. I have already covered summarization as a whole and abstractive summarization along with its implementation using Transformers in this post. Consider giving it a read if you are interested in getting a brief background on this task; the PEGASUS model is trained on the Transformer architecture.", "In this article, we will discuss a recent paper, \u201cPEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\u201d proposed by Google AI which is supposed to appear in ICML 2020.", "Like any other sequence transduction task, PEGASUS, too, implements the seq2seq architecture. However, the novelty of this architecture lies in its self-supervised pre-training objective.", "Self-Supervised Learning is the new cool in Deep Learning. It essentially removes the dependency of data on labeled samples and makes a huge amount of unexplored, unlabelled data accessible for training.", "Combination of Transformer-based models with self-supervised pre-training (e.g., BERT, GPT-2, RoBERTa, XLNet, ALBERT, T5, ELECTRA), has proven to be immensely effective in overall Language Modeling tasks.", "The main idea behind this objective is the assumption that, closer the pre-training self-supervised objective is to the final downstream task, the better the fine-tuning performance", "Thus, in PEGASUS, complete sentences are removed from a document (i.e. they are \u2018masked\u2019), and the model is trained to predict these sentences as shown in the figure. The authors admit that this task seems nearly impossible, even for humans for a matter of fact. But such training elicits a higher sense of understanding for the generation of sentences that have an instance of the original document; thus supporting their assumption. This task is coined as Gap Sentence Generation (GSG).", "As an addition to this, the authors assert that choosing the most important sentences from the document for masking works best. This is done by finding sentences that are the most similar to the complete document according to a metric called ROUGE (which is usually used to evaluate the quality of a summary in summarization tasks).", "Although the main contribution of PEGASUS is the GSG (discussed in the previous section), its base architecture consists of an encoder and a decoder; hence, it makes sense to pre-train the encoder as a masked language model.", "In this task, we randomly mask words from the sequences and use other words from the sequence to predict these masked words. The GSG task can be construed as a document-level MLM and is derived from this very concept.", "So, as suggested in the BERT paper, 15% words from the sequence are randomly masked and the model is trained to predict these masked words.", "Both the approaches discussed in the previous sections are incorporated and the Transformer is trained in a combined manner.", "Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some words are randomly masked by [MASK2] (MLM).", "The model is fine-tuned on 12 public abstractive summarization datasets. It has surpassed the previous state of the art on 6 of these datasets and surprisingly, by training on a very few number of samples.", "It is clearly seen that, with a bare minimum of 1000 training samples, PEGASUS has surpassed, and achieved the state of the art on these datasets.", "PEGASUS has also achieved human-level results on 3 datasets. The evaluation is done by rating human summaries and model generated summaries without knowing which one is which.", "\u201cWe performed the experiment with 3 different datasets and found that human raters do not consistently prefer the human summaries to those from our model\u201d", "This is another interesting result achieved by PEGASUS:", "An article from the Xsum dataset suggested the names of 4 ships viz. HMS Cumberland, HMS Campbeltown, HMS Chatham and HMS Cornwall. The model correctly abstracts this as \u201cfour Royal Navy frigates\u201d, although here is no trace of mention of the number \u201cfour\u201d in the sample.", "Considering this as a fluke, the authors have tested this by adding or removing names from the list. The model correctly abstracts the number if there are 2\u20135 names. However, it miscounted 6 ships as \u201cseven\u201d, which shows that it is able to abstract only a small number of names in a list.", "The demo can be found here.", "Fun Fact: The model has achieved better results than its peer models like T5 while using only 5% of the number of parameters of T5.", "We have discussed the working of the Google\u2019s state of the art model for abstractive summarization. We also saw how pretraining on a task which is relatively similar to the downstream task greatly enhances the model performance on fine tuning. This opens a possibility for modeling the self-supervised pre-training objective more specifically than generically.", "The code and checkpoints are open sourced and can be found here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Immensely interested in AI Research | I read papers and post my notes on Medium"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F627b1bbbc5ce&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rojagtap.medium.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c----627b1bbbc5ce---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@sdoy1995?utm_source=medium&utm_medium=referral", "anchor_text": "Sudan Ouyang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "this post"}, {"url": "https://arxiv.org/abs/1912.08777", "anchor_text": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"}, {"url": "https://icml.cc/Conferences/2020", "anchor_text": "ICML 2020"}, {"url": "https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html", "anchor_text": "BERT"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet"}, {"url": "https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html", "anchor_text": "ALBERT"}, {"url": "https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html", "anchor_text": "T5"}, {"url": "https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html", "anchor_text": "ELECTRA"}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "Google AI Blog"}, {"url": "https://www.aclweb.org/anthology/W04-1013/", "anchor_text": "ROUGE"}, {"url": "https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html", "anchor_text": "Google AI Blog"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT paper"}, {"url": "https://arxiv.org/abs/1912.08777", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1912.08777", "anchor_text": "PEGASUS paper"}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "Google AI Blog"}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "Google AI Blog"}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "here"}, {"url": "https://github.com/google-research/pegasus", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1912.08777", "anchor_text": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive SummarizationRecent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success\u2026arxiv.org"}, {"url": "https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html", "anchor_text": "PEGASUS: A State-of-the-Art Model for Abstractive Text SummarizationStudents are often tasked with reading a document and producing a summary (for example, a book report) to demonstrate\u2026ai.googleblog.com"}, {"url": "https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453", "anchor_text": "Abstractive Text Summarization Using TransformersAn exhaustive explanation of Google\u2019s Transformer model; from theory to implementationmedium.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----627b1bbbc5ce---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----627b1bbbc5ce---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----627b1bbbc5ce---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----627b1bbbc5ce---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----627b1bbbc5ce---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&user=Rohan+Jagtap&userId=39646f947a4c&source=-----627b1bbbc5ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&user=Rohan+Jagtap&userId=39646f947a4c&source=-----627b1bbbc5ce---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F627b1bbbc5ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----627b1bbbc5ce---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----627b1bbbc5ce--------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rojagtap.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rohan Jagtap"}, {"url": "https://rojagtap.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "465 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F39646f947a4c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&user=Rohan+Jagtap&userId=39646f947a4c&source=post_page-39646f947a4c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe51e2b6202c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpegasus-google-state-of-the-art-abstractive-summarization-model-627b1bbbc5ce&newsletterV3=39646f947a4c&newsletterV3Id=e51e2b6202c5&user=Rohan+Jagtap&userId=39646f947a4c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}