{"url": "https://towardsdatascience.com/operationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b", "time": 1683007720.694369, "path": "towardsdatascience.com/operationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b/", "webpage": {"metadata": {"title": "Operationalization of ML Pipelines on Apache Mesos and Hadoop using Airflow | by Mihail Vieru | Towards Data Science", "h1": "Operationalization of ML Pipelines on Apache Mesos and Hadoop using Airflow", "description": "An architecture for orchestrating machine learning pipelines in production on Apache Mesos and Hadoop using Airflow"}, "outgoing_paragraph_urls": [{"url": "https://www.newyorker.ai/operationalization-of-machine-learning-pipelines-on-apache-mesos-and-hadoop-using-airflow/", "anchor_text": "NEW YORKER Tech Blog", "paragraph_index": 0}, {"url": "https://www.newyorker.ai/a-brief-introduction-to-d2iq-dc-os/", "anchor_text": "DC/OS cluster", "paragraph_index": 8}, {"url": "https://www.newyorker.ai/anatomy-of-a-marathon-application-definition/", "anchor_text": "Marathon application", "paragraph_index": 8}, {"url": "https://www.newyorker.ai/anatomy-of-a-metronome-job-definition/", "anchor_text": "Metronome job", "paragraph_index": 8}, {"url": "https://github.com/NewYorkerData/ny-public-airflow-operators#hdfs-sensor-python-3-compatible", "anchor_text": "https://github.com/NewYorkerData/ny-public-airflow-operators", "paragraph_index": 20}, {"url": "https://github.com/Yelp/elastalert", "anchor_text": "elastalert", "paragraph_index": 22}, {"url": "https://neptune.ai/", "anchor_text": "Neptune", "paragraph_index": 22}, {"url": "https://www.newyorker.ai/anatomy-of-a-metronome-job-definition/", "anchor_text": "Metronome job definition", "paragraph_index": 24}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/security/ent/iam-api/", "anchor_text": "IAM REST API", "paragraph_index": 25}, {"url": "https://dcos.github.io/metronome/docs/generated/api.html", "anchor_text": "Metronome REST API", "paragraph_index": 25}, {"url": "https://github.com/NewYorkerData/ny-public-airflow-operators#metronome-operator", "anchor_text": "https://github.com/NewYorkerData/ny-public-airflow-operators", "paragraph_index": 26}], "all_paragraphs": ["This blog post originally appeared on the NEW YORKER Tech Blog.", "One of the highest impact use cases for machine learning in retail companies is markdown pricing. As one of Europe\u2019s largest fashion retailers, NEW YORKER sets an initial price for each item sold in our 1100 stores in 45 countries. The price is gradually marked down in a series of price reductions until the item is sold out. The goal is to predict when and how to mark down the price, so that the whole inventory is sold at the highest possible price. Thus, maximizing the company\u2019s revenue.", "Solving this use case involves two tasks: creating a model which delivers accurate predictions, and operationalizing it into a production pipeline on our current infrastructure.", "In this post we focus on the latter task, and present an architecture for orchestrating machine learning pipelines in production on Apache Mesos and Hadoop using Airflow. We start with the requirements for the machine learning model along with the ones posed by our current infrastructure and data engineering architecture. We then present an architecture which fulfills those, including containerization, pipeline orchestration with Airflow, performance considerations and the simplified DAG code. We conclude with a summary and an outlook on future work.", "Let\u2019s start with the requirements of the model we wish to operationalize, along with the ones stemming from our current infrastructure and data engineering architecture.", "The training data for the model are stored as Parquet files on HDFS in our Hadoop cluster. They are the result of extensive prior batch data processing with Spark, including data cleaning, aggregation and imputation where necessary. This legacy ETL is not orchestrated with Airflow yet and uses Luigi for the workflow management and Cron Jobs for the scheduling.", "The model is written in Python and the machine learning framework employed should be up to the choice of our Data Scientists. Currently LightGBM is used. The training and inference are done on a per country basis \u2014 a characteristic which we can leverage later for parallelization. GPU support is important, as our Data Scientists are also considering deep learning approaches, which would profit from it.", "The resulting predictions of the model need to be written to our ERP system\u2019s persistence layer \u2014 a relational database. The predictions serve as a guide to our Pricing department, which checks them via the ERP\u2019s UI for plausibility and applies the suggested price reductions where and when it deems them appropriate. As the predictions change very slowly, a refresh is required once at the beginning of each week.", "Besides the aforementioned Hadoop cluster, we employ a DC/OS cluster in our infrastructure. It provides us with container orchestration and resource scheduling based on Apache Mesos. GPUs can be used in containers via Marathon application and Metronome job definitions for long and short running applications. We have a containerized Airflow installation running on DC/OS for orchestrating most of our data pipelines.", "For fulfilling the requirements stated above, we have come up with the following architecture.", "We\u2019ve decided to containerize the model training and inference computation and use DC/OS to run it. This has two advantages. First, it preserves the independence of the machine learning framework chosen by our Data Scientists. Second, we can leverage existing GPUs on the DC/OS cluster to speed up future deep learning approaches.", "If we would have chosen to use SparkML or parallelize another ML framework with Spark on Hadoop, we would have had to migrate the existing LightGBM model to this approach. We would have then essentially locked in the ML framework employed. Furthermore, to facilitate GPU usage, we would have needed to install GPUs on the Hadoop cluster or move them from the DC/OS cluster to it. A Hadoop version upgrade to 3.x from the present 2.x would have also been required. All this would have been non trivial and very time consuming.", "The container itself has the following workflow:", "At the beginning, it downloads the training data from HDFS. Then it trains the model on it and computes the predictions for a given country. Finally, the predictions are refreshed in the ERP\u2019s underlying relational database.", "The refreshing of the predictions is done in a single transaction, i.e. it deletes old predictions for all items for a single country and it writes new ones. We don\u2019t use updates here, as the set of predictions can vary across time. In this way, the ERP\u2019s users will always see predictions in the UI for the selected set of items with no downtime.", "We\u2019ve parameterized the container, so that it can calculate and refresh predictions not only for a single country but for a specified group of countries.", "Having a parameterized container for the model training and inference in place, we can now proceed to deploy and orchestrate multiple instances of it to form a proper pipeline with the help of Airflow:", "For short-running applications, i.e. jobs, DC/OS provides Metronome as a task scheduler on top of Mesos. As our container is ephemeral in nature, we can employ Metronome to deploy it on the DC/OS cluster. We do this from Airflow using the MetronomeOperator, which we\u2019ll introduce more in-depth shortly.", "We start multiple instances of the MetronomeOperator for groups of countries so that all 45 countries are covered, thus effectively parallelizing the work.", "As the legacy ETL \u2014 which processes the needed training data on Hadoop \u2014 is not orchestrated with Airflow yet, we employ a modified HDFS sensor in order to trigger the start of the MetronomeOperator instances. The original Airflow HDFS sensor was only functioning with Python 2. We\u2019ve made some small changes so that it works with the Python 3 version that we use in our containerized Airflow installation.", "The code for it can be found here: https://github.com/NewYorkerData/ny-public-airflow-operators", "We schedule the pipeline to start simultaneously with the legacy ETL on Hadoop. The HDFS sensor will continuously poll for the success file and trigger the remaining operators in the DAG when the file is found.", "Note: As part of the production pipeline we also write the predictions to HDFS for future analysis. During the computation thereof we log all steps to Elasticsearch. We have Kibana dashboards in place and currently use elastalert to monitor and alert on anomalies, e.g. the number of predictions decreased by 10% from last run.Before promoting changes to production, we use Neptune as part of the test pipeline to keep track and evaluate the impact of the various changes to the model itself.", "No Metronome operator existed for Airflow to the best of our knowledge at the inception of the project. So we created one.", "It takes a Metronome job definition as a JSON parameter and works as depicted in the figure below:", "First, we get an authorization token from the IAM REST API to be able to deploy containers on the DC/OS cluster. Second, we check whether the job exists via the Metronome REST API. Depending on the response, we upsert the job definition. Then we start the job. Finally, we poll for the job status and, depending on the response, change the Airflow operator\u2019s status to SUCCESSFUL or FAILED.", "The code for the operator can be found here: https://github.com/NewYorkerData/ny-public-airflow-operators", "In the Metronome job definition we can specify CPU, RAM and GPU usage for the container. Therefore we can scale vertically. Regarding the horizontal scaling, we do this by parallelizing the model training and inference for the groups of countries. The degree of parallelism is however not unbounded and is limited by the available cluster resources. In order not to use up all resources, we limit the number of concurrently running containers using the Airflow resource pool feature. The resource pool name is set in all MetronomeOperator instances, as we\u2019ll see in the DAG code below. The size of the resource pool is thus equal to the degree of parallelism.", "Below you can find the simplified version of the Airflow DAG:", "We have shown an architecture which allows us to easily orchestrate machine learning pipelines in a mixed Mesos and Hadoop cluster environment using Airflow. We leverage Mesos\u2019 container orchestration and resource scheduling to scale the model training and inference both horizontally and vertically, while being able to take advantage of available hardware acceleration provided by GPUs. We employ Airflow\u2019s powerful features, such as sensors and dynamic DAGs, to manage the whole workflow effectively across the clusters.", "There are still many improvement areas we can work on in the future.For instance, migrating the legacy ETL on Hadoop from Luigi/Cron to Airflow will allow us to deprecate the HDFS sensor and simplify the DAG by using Airflow\u2019s pipeline interdependency feature.We\u2019re also aware that some computation is common across all countries. This step could be extracted in a separate stage, on which all further stages would depend on, possibly improving the performance.", "Finally, we\u2019re in the process of preparing the migration of our clusters from Mesos to Kubernetes. In the short-term we will need to adjust all our machine learning pipelines in Airflow. This should be in theory relatively easy to accomplish, e.g. use the KubernetesOperator instead of the MetronomeOperator in the DAGs. In the longer term, Kubernetes opens up a new world of possibilities in terms of machine learning operationalization approaches, such as Kubeflow.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Passionate about designing and implementing highly scalable, performant and robust data processing solutions. In the last years for Data Science use cases"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8c1d89e0ad3b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mihail.vieru1?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.vieru1?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Mihail Vieru"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8638a4022dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&user=Mihail+Vieru&userId=8638a4022dde&source=post_page-8638a4022dde----8c1d89e0ad3b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.newyorker.ai/operationalization-of-machine-learning-pipelines-on-apache-mesos-and-hadoop-using-airflow/", "anchor_text": "NEW YORKER Tech Blog"}, {"url": "https://unsplash.com/@simonmigaj?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Simon Migaj"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.newyorker.ai/a-brief-introduction-to-d2iq-dc-os/", "anchor_text": "DC/OS cluster"}, {"url": "https://www.newyorker.ai/anatomy-of-a-marathon-application-definition/", "anchor_text": "Marathon application"}, {"url": "https://www.newyorker.ai/anatomy-of-a-metronome-job-definition/", "anchor_text": "Metronome job"}, {"url": "https://github.com/NewYorkerData/ny-public-airflow-operators#hdfs-sensor-python-3-compatible", "anchor_text": "https://github.com/NewYorkerData/ny-public-airflow-operators"}, {"url": "https://github.com/Yelp/elastalert", "anchor_text": "elastalert"}, {"url": "https://neptune.ai/", "anchor_text": "Neptune"}, {"url": "https://www.newyorker.ai/anatomy-of-a-metronome-job-definition/", "anchor_text": "Metronome job definition"}, {"url": "https://docs.d2iq.com/mesosphere/dcos/1.13/security/ent/iam-api/", "anchor_text": "IAM REST API"}, {"url": "https://dcos.github.io/metronome/docs/generated/api.html", "anchor_text": "Metronome REST API"}, {"url": "https://github.com/NewYorkerData/ny-public-airflow-operators#metronome-operator", "anchor_text": "https://github.com/NewYorkerData/ny-public-airflow-operators"}, {"url": "https://medium.com/tag/mesos?source=post_page-----8c1d89e0ad3b---------------mesos-----------------", "anchor_text": "Mesos"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----8c1d89e0ad3b---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/airflow?source=post_page-----8c1d89e0ad3b---------------airflow-----------------", "anchor_text": "Airflow"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8c1d89e0ad3b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&user=Mihail+Vieru&userId=8638a4022dde&source=-----8c1d89e0ad3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&user=Mihail+Vieru&userId=8638a4022dde&source=-----8c1d89e0ad3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8c1d89e0ad3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8c1d89e0ad3b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8c1d89e0ad3b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.vieru1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mihail.vieru1?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mihail Vieru"}, {"url": "https://medium.com/@mihail.vieru1/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8638a4022dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&user=Mihail+Vieru&userId=8638a4022dde&source=post_page-8638a4022dde--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8638a4022dde%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Foperationalization-of-ml-pipelines-on-apache-mesos-and-hadoop-using-airflow-8c1d89e0ad3b&user=Mihail+Vieru&userId=8638a4022dde&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}