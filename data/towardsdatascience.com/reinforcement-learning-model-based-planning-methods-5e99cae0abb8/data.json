{"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8", "time": 1682997010.0134988, "path": "towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Model Based Planning Methods | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Model Based Planning Methods", "description": "In previous articles, we have talked about reinforcement learning methods that are all based on model-free methods, which is also one of the key advantages of RL learning, as in most cases learning a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "here", "paragraph_index": 10}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "full implementation", "paragraph_index": 20}, {"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb", "anchor_text": "post", "paragraph_index": 24}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "here", "paragraph_index": 25}], "all_paragraphs": ["In previous articles, we have talked about reinforcement learning methods that are all based on model-free methods, which is also one of the key advantages of RL learning, as in most cases learning a model of environment can be tricky and tough. But what if we want to learn a model of environment or what if we already have a model of environment and how can we leverage that to help the learning process? In this article, we will together explore RL methods with environment as a model. The following will be structured as:", "An agent starts from a state, by taking an available action in that state, the environment gives it feedback, and accordingly the agent lands into next state and receive reward if any. In this general settings, the environment gives an agent two signals, one is its next state in the setting, and the other is reward. So when we say to model an environment, we are modelling a function mapping (state, action) to (nextState, reward) . For example, consider a situation in a grid world setting, an agent bashes its head into the wall, and in response, the agent stays where it is and gets a reward of 0, then in the simplest format, the model function will be (state, action)-->(state, 0) , indicating that the agent with this specific state and action, the agent will stay at the same place and get reward 0.", "Let\u2019s now look into how a model of environment can help improve the process of Q-learning. We start by introducing the simplest form of an algorithm called Dyna-Q:", "The way Q-learning leveraging models to backup policy is simple and straight forward. Firstly, the a, b, c, d steps are exactly the same as general Q-learning steps(if you are not familiar with Q-learning, please check out my examples here). The only difference lies in step e and f , in step e , a model of the environment is recorded based on the assumption of deterministic environment(for non-deterministic and more complex environment, a more general model can be formulated based on the particular case). Step f can be simply summarised as applying the model being learnt and update the Q function n times, where n is a predefined parameter. The backup in step f is totally the same as it is in step d , and you may think it as repeating what the agent has experienced several times in order to reinforce the learning process.", "Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the \u201cfinal common path\u201d for both learning and planning.", "The graph shown above more directly displays the general structure of Dyna methods. Notice the 2 upward arrows in Policy/value functons , which in most cases are Q functions that we talked before, one of the arrow comes from direct RL update through real experience , which in this case equals the agent exploring around the environment, and the other comes from planning update through simulated experience , which, in this case, is repeating the model the agent learnt from real experience. So in each action taking, the learning process is strengthened by updating the Q function from both actual action taking and model simulation.", "I believe the best way to understand an algorithm is to implement an actual example. I will take the example from reinforcement learning an introduction, implement it in Python and compare it with general Q learning without planning steps(model simulation).", "Consider the simple maze shown inset in the Figure. In each of the 47 states there are four actions, up, down, right, and left, which take the agent deterministically to the corresponding neighbouring states, except when movement is blocked by an obstacle or the edge of the maze, in which case the agent remains where it is. Reward is zero on all transitions, except those into the goal state, on which it is +1. After reaching the goal state (G), the agent returns to the start state (S) to begin a new episode.", "The whole structure of this implementation is to have 2 classes, the first class represents the board, which is also the environment, that is able to", "And the second class represents the agent, which is able to", "The first class of the board settings are similar with many board games we talked before, you can checkout full implementation here. I will eliminate my explanation here(you can check my previous articles to see more examples), as a result, we will have a board look like this:", "The board is represented in an numpy array, where z indicates the block, * indicates the agent\u2019s current position and 0 indicates empty and available spots.", "Firstly, in the init function we will initialise all the parameters required for the algorithm.", "Besides those general Q-learning settings(learning rate, state_actions, \u2026), a model of (state, action) -> (reward, state) is also initialised as python dictionary, and the model will only be updated along with the agent\u2019s exploration in the environment. The self.steps is the number of time the model is used to update the Q function in each action taking, and self.steps_per_episode is used to record the number of steps in each episode(we will take it as a key metrics in the following algorithm comparison).", "In the chooseAction function, the agent will still take \u03f5-greedy action, where it has self.exp_rate probability to take a random action and 1 \u2014 self.exp_rate probability to take a greedy action.", "Now let\u2019s get to the key point of policy updating using models being learnt along agent\u2019s exploration.", "This implementation follows exactly as the algorithm we listed above. At each episode(game playing), after the first round of Q function update, the model will also be updated with self.model[self.state][action]=(reward, nxtState) , and then Q function will be repeatedly updated by self.steps number of times. Notice that in side the loop, the state and action are both randomly selected from the previously observations.", "When the number of steps is set to 0, the Dyna-Q method is essentially Q-learning. Let\u2019s compare the learning process with steps of 0, 5 and 50.", "The x-axis is the number of episodes and y-axis is the number of steps to reach the goal. The task is to get to the goal as fast as possible. From the learning curve, we observe that the learning curve of planning agent(with simulated model) stabilises faster than non-planing agent. Referring to the words in Sutton\u2019s book:", "Without planning (n = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the first episode, but here during the second episode an extensive policy has been developed that by the end of the episode will reach almost back to the start state", "The additional model simulation and backup further reinforced the agent\u2019s experience, thus resulted in a faster and more stable learning process.(checkout the full implementation)", "The example we explored here surely has limited use, as the state is discretised an the action is deterministic. But the idea of modelling environment to accelerate learning process has unlimited use.", "A probability model could be learnt rather than a straight forward 1 to 1 mapping we introduced above. The probability model should be constantly updated through the learning process, and during the backup stage, the (reward, nextState) could be chosen non-deterministically with a probability distribution.", "The Q function update will be slightly different(I will introduce it in further articles), and the key would be to learn a more complex and general parametric model of the environment. This process could involve general supervised learning algorithms with current state, action as input and next state and reward as output.", "In next post, we will learn further ideas to improve Dyna methods and talk about situations when the model is wrong!", "And lastly, please check out the full code here. You are welcomed to contribute, and if you have any questions or suggestions, please raise comment below!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5e99cae0abb8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----5e99cae0abb8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "full implementation"}, {"url": "https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb", "anchor_text": "post"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py", "anchor_text": "here"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5e99cae0abb8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5e99cae0abb8---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python3?source=post_page-----5e99cae0abb8---------------python3-----------------", "anchor_text": "Python3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----5e99cae0abb8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----5e99cae0abb8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5e99cae0abb8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5e99cae0abb8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5e99cae0abb8--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-model-based-planning-methods-5e99cae0abb8&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}