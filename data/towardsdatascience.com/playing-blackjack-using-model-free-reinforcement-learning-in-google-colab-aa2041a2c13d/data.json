{"url": "https://towardsdatascience.com/playing-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d", "time": 1682994647.240243, "path": "towardsdatascience.com/playing-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d/", "webpage": {"metadata": {"title": "Playing Blackjack using Model-free Reinforcement Learning in Google Colab! | by Pranav Mahajan | Towards Data Science", "h1": "Playing Blackjack using Model-free Reinforcement Learning in Google Colab!", "description": "I felt compelled to write this article because I noticed not many articles explained Monte Carlo methods in detail whereas just jumped straight to Deep Q-learning applications. Before we start I\u2019d\u2026"}, "outgoing_paragraph_urls": [{"url": "https://colab.research.google.com/drive/1zVdv5KRmWyoYZGt83QTGxPkY1Gm7WjDM", "anchor_text": "Now onward we\u2019ll dive into code while explaining the algorithms, it\u2019ll be helpful if you open the Colab notebook in another tab", "paragraph_index": 12}], "all_paragraphs": ["I felt compelled to write this article because I noticed not many articles explained Monte Carlo methods in detail whereas just jumped straight to Deep Q-learning applications.", "In this article you\u2019ll get to learn about", "Before we start I\u2019d like to let you know that this article assumes a basic knowledge of very basic concepts of Reinforcement Learning, if you don\u2019t it\u2019s alright here\u2019s a quick recap :", "Reinforcement is the strengthening of a pattern of behavior as a result of an animal receiving a stimulus in an appropriate temporal relationship with another stimulus or a response. \u2014 Pavlov\u2019s Monograph (1927)", "Policy for an agent can be thought of as a strategy the agent uses, it usually maps from perceived states of environment to actions to be taken when in those states.", "Various Model-based methods like Dynamic Programming use the Bellman Equation ( a recursive relation between V(St) and V(St+1) ) to iteratively find optimal Value functions and Q functions.", "And that\u2019s the end of recap!", "To use model-based methods we need to have complete knowledge of the environment i.e. we need to know Pss\u2019 (please refer to above picture): the transition probability we\u2019ll end up in state St+1=s\u2019 if the agent is in state St=1 and takes action At=a. For example, if a bot chooses to move forward, it might move sideways in case of slippery floor underneath it. In games like Blackjack our action space is limited like we can either choose to \u201chit\u201d or \u201cstick\u201d but we can end up in any of the many possible states of which you know none of the probabilities! In Blackjack state is determined by your sum, the dealers sum and whether you have a usable ace or not as follows:", "What to do when we don\u2019t have model of the environment? You take samples by interacting with the again and again and estimate such information from them. Model-free are basically trial and error approaches which require no explicit knowledge of environment or transition probabilities between any two states.", "Thus we see that model-free systems cannot even think bout how their environments will change in response to a certain action. This way they have reasonable advantage over more complex methods where the real bottleneck is the difficulty of constructing a sufficiently accurate environment model. (For example, we can\u2019t possibly start listing out probabilities of next card the dealer will pull out in each state of Blackjack.)", "(Side note) Perhaps the first to succinctly express the essence of trial-and-error learning as a principle of learning was Edward Thorndike in his \u201cLaw of Effect\u201d in 1911 but the idea of trial-and-error learning goes as far as the 1850s.", "With understood the motivation behind model-free methods, let\u2019s see a few algorithms!", ">> Now onward we\u2019ll dive into code while explaining the algorithms, it\u2019ll be helpful if you open the Colab notebook in another tab!", "In order to construct better policies, we need to first be able to evaluate any policy. If an agent follows a policy for many episodes, using Monte-Carlo Prediction, we can construct the Q-table (i.e. \u201cestimate\u201d the action-value function ) from the results from these episodes.", "So we can start with a stochastic policy like \u201cstick\u201d with 80% probability if sum is greater than 18 as we don\u2019t want to exceed 21. Else if sum is less than 18, we\u2019ll \u201chit\u201d with 80% probability. Following code generates episodes using the following policy and then later we\u2019ll evaluate this policy:", "(Note here an \u2018episode\u2019 i.e. the quantity that is returned is a list of (state, action, reward) tuples corresponding to every action taken in the episode )", "Now, we want to get the Q-function given a policy and it needs to learn the value functions directly from episodes of experience. Note that in Monte Carlo approaches we are getting the reward at the end of an episode where..", "We\u2019ll learn value functions from sample returns with the MDP, recollect from the recap that:", "What is the sample return? Say we played for 10 episodes using a policy and we got rewards 2,6,5,7 when we visited the same state \u2018S\u2019 for 4 out of 10 times then the sample return would be (2+6+5+7)/4 = 20/4 = 5 ~V(s). Thus sample return is the average of returns(rewards) from episodes. In what order did we end up visiting the state doesn\u2019t really matter here, estimate for each value is calculated independently!", "This way we can build either a V-table or a Q-table, in order to create a Q-table we\u2019ll need to keep track of reward we got for each we visited a (state,action) pair and also keep a track of how many times we visited the state say an N-table.", "Depending on which returns are chosen while estimating our Q-values", "For example: In an episode, S1 A1 R1, S2 A2 R2, S3 A3 R3, S1 A1 R4 \u2192End. Then first visit MC will consider rewards till R3 in calculating the return while every visit MC will consider all rewards till the end of episode.", "Here, in Blackjack it doesn\u2019t affect much whether we use first visit or every visit MC. Here\u2019s the algorithm for first-visit MC prediction", "But we\u2019ll implement every-visit MC prediction as shown below:", "We first initialize a Q-table and N-table to keep a tack of our visits to every [state][action] pair.", "Then in the generate episode function, we are using the 80\u201320 stochastic policy as we discussed above.", "This will estimate the Q-table for any policy used to generate the episodes! Once we have Q-values, getting V-values is fairly easy as V(s) = Q (s, \u03c0(s)). Let\u2019s plot the state values V(s)!", "So now we know how to estimate the action-value function for a policy, how do we improve on it? Using the \u2026", "So here\u2019s the plan in a nutshell. We start with a stochastic policy and compute the Q-table using MC prediction. So we now have the knowledge of which actions in which states are better than other i.e. they have greater Q-values. So we can improve upon our existing policy by just greedily choosing the best action at each state as per our knowledge i.e. Q-table and then recompute the Q-table and chose next policy greedily and so on! Sounds good?", "Having learnt these crucial practical changes to idea of just sampling return, here\u2019s the algorithm for first-visit MC control!", "We\u2019ll implement every-visit MC control, because it\u2019s just slightly easier to code", "We\u2019re just using 3 functions to make the code look cleaner. To generate episode just like we did for MC prediction, we need a policy. But note that we are not feeding in a stochastic policy, but instead our policy is epsilon-greedy wrt our previous policy. get_probs function gives us the action probabilities for the same i.e. \u03c0(a|s)", "And the update_Q function just updates Q-values using incremental mean and constant alpha. Finally we call all these functions in the MC control and ta-da!", "Feel free to explore the notebook comments and explanations for further clarification!", "Thus finally we have an algorithm that learns to play Blackjack, well a slightly simplified version of Blackjack at least. Let\u2019s compare the policy learnt compared to the optimal policy mentioned in RL book by Sutton and Barto.", "Wolla! There you go, we have an AI that wins most of the times when it plays Blackjack! But there\u2019s more\u2026", "Now Blackjack isn\u2019t the best environment to learn advantages of TD methods as Blackjack is an episodic game and Monte Carlo methods assume episodic environments. In MC control, at the end of each episode, we update the Q-table and update our policy. Thus we have no way of finding out which was the wrong move that led to the loss but it won\u2019t matter in a short game like Blackjack. If it were a longer game like chess, it would make more sense to use TD control methods because they boot strap , meaning it will not wait until the end of the episode to update the expected future reward estimation(V) , it will only wait until the next time step to update the value estimates.", "(Side note) TD methods are distinctive in being driven by the difference between temporally successive estimates of the same quantity. More over the origins of temporal-difference learning are in part in animal psychology, in particular, in the notion of secondary reinforcers. Secondary reinforcer is a stimulus that has been paired with a primary reinforcer (simplistic reward from environment itself) and as a result the secondary reinforcer has come to take similar properties.", "Just like in Dynamic Programming, TD uses Bellman Equations to update at each step.", "Following picture can help explain the difference between DP, MC and TD approaches.", "Thus we can think of the incremental mean in a different way as if Gt was the Target or our expectation of the return the agent would have got, but instead got the return Q(St,At) so it would make sense to push Q-values towards Gt by \u03b1 *(Gt-Q(St,At)).", "Depending on different TD targets and slightly different implementations the 3 TD control methods are:", "Which when implemented in python looks like this:", "Which when implemented in python looks like this:", "Which when implemented in python looks like this:", "NOTE that Q-table in TD control methods is updated every time-step every episode as compared to MC control where it was updated at the end of every episode.", "I know I haven\u2019t explained TD methods in as much depth as the MC methods instead analyzed in a comparative fashion, but for those who are interested all of the 3 methods are implemented in the notebook. You are welcome to explore the whole notebook for and play with functions for a better understanding!", "That\u2019s all folks! Hope you enjoyed!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ECE Undergrad @ BITS Pilani. Deep learning and reinforcement learning enthusiast. Loves to tinker with electronics and math and do things from scratch :)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Faa2041a2c13d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mahajan.pranav25?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahajan.pranav25?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Pranav Mahajan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff033c765d486&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&user=Pranav+Mahajan&userId=f033c765d486&source=post_page-f033c765d486----aa2041a2c13d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://i.stack.imgur.com/eoeSq.png", "anchor_text": "https://i.stack.imgur.com/eoeSq.png"}, {"url": "https://colab.research.google.com/drive/1zVdv5KRmWyoYZGt83QTGxPkY1Gm7WjDM", "anchor_text": "Now onward we\u2019ll dive into code while explaining the algorithms, it\u2019ll be helpful if you open the Colab notebook in another tab"}, {"url": "https://colab.research.google.com/drive/1zVdv5KRmWyoYZGt83QTGxPkY1Gm7WjDM", "anchor_text": "Google ColaboratoryEdit descriptioncolab.research.google.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----aa2041a2c13d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----aa2041a2c13d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----aa2041a2c13d---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----aa2041a2c13d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----aa2041a2c13d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&user=Pranav+Mahajan&userId=f033c765d486&source=-----aa2041a2c13d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&user=Pranav+Mahajan&userId=f033c765d486&source=-----aa2041a2c13d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Faa2041a2c13d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----aa2041a2c13d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----aa2041a2c13d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahajan.pranav25?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahajan.pranav25?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pranav Mahajan"}, {"url": "https://medium.com/@mahajan.pranav25/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "59 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff033c765d486&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&user=Pranav+Mahajan&userId=f033c765d486&source=post_page-f033c765d486--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F104682c2f391&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fplaying-blackjack-using-model-free-reinforcement-learning-in-google-colab-aa2041a2c13d&newsletterV3=f033c765d486&newsletterV3Id=104682c2f391&user=Pranav+Mahajan&userId=f033c765d486&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}