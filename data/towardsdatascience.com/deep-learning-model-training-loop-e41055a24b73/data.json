{"url": "https://towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73", "time": 1682994135.456418, "path": "towardsdatascience.com/deep-learning-model-training-loop-e41055a24b73/", "webpage": {"metadata": {"title": "Deep Learning Model Training Loop | by Ilia Zaitsev | Towards Data Science", "h1": "Deep Learning Model Training Loop", "description": "Several months ago I started exploring PyTorch \u2014 a fantastic and easy to use Deep Learning framework. In the previous post, I was describing how to implement a simple recommendation system using\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@iliazaitsev/how-to-implement-a-recommendation-system-with-deep-learning-and-pytorch-2d40476590f9", "anchor_text": "the previous post", "paragraph_index": 0}, {"url": "https://github.com/pytorch/pytorch/tree/v1.0.0", "anchor_text": "1.0 version", "paragraph_index": 2}, {"url": "https://github.com/devforfu/loop", "anchor_text": "this link", "paragraph_index": 4}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "a link to the notebook", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Software_design_pattern", "anchor_text": "software design patterns", "paragraph_index": 12}, {"url": "http://www.gameprogrammingpatterns.com/observer.html", "anchor_text": "a well-known design pattern", "paragraph_index": 12}, {"url": "https://keras.io/callbacks/", "anchor_text": "keras.io", "paragraph_index": 17}, {"url": "https://docs.fast.ai/callbacks.html", "anchor_text": "docs.fast.ai", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Linear_interpolation", "anchor_text": "linear interpolation", "paragraph_index": 19}, {"url": "https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#in-practice", "anchor_text": "this post", "paragraph_index": 20}, {"url": "https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models", "anchor_text": "many cases", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Sample_mean", "anchor_text": "sample mean", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1702.04283", "anchor_text": "one", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1708.07120", "anchor_text": "two", "paragraph_index": 25}, {"url": "https://arxiv.org/abs/1803.09820", "anchor_text": "three", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1608.03983.pdf", "anchor_text": "stochastic gradient with warm restarts", "paragraph_index": 28}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "aforementioned Jupyter notebook", "paragraph_index": 32}, {"url": "https://forums.fast.ai/t/training-metrics-as-notifications-on-mobile-using-callbacks/17330", "anchor_text": "send as a notification to your mobile phone", "paragraph_index": 33}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "the notebook", "paragraph_index": 40}, {"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "anchor_text": "should understand backprop", "paragraph_index": 42}, {"url": "https://iliazaitsev.me/", "anchor_text": "my blog", "paragraph_index": 45}], "all_paragraphs": ["Several months ago I started exploring PyTorch \u2014 a fantastic and easy to use Deep Learning framework. In the previous post, I was describing how to implement a simple recommendation system using MovieLens dataset. This time I would like to focus on the topic essential to any Machine Learning pipeline \u2014 a training loop.", "The PyTorch framework provides you with all the fundamental tools to build a machine learning model. It gives you CUDA-driven tensor computations, optimizers, neural networks layers, and so on. However, to train a model, you need to assemble all these things into a data processing pipeline.", "Recently the developers released the 1.0 version of PyTorch, and there are already a lot of great solutions helping you to train the model without a need to dig into basic operations with tensors and layers. (Briefly discussed in the next section). Nevertheless, I believe that every once in a while most of the software engineers have a strong desire to implement things \u201cfrom scratch\u201d to get a better understanding of underlying processes and to get skills that do not depend on a particular implementation or high-level library.", "In the next sections, I am going to show how one can implement a simple but useful training loop using torch and torchvision Python packages.", "TL;DR: Please follow this link to get right into the repository where you can find the source code discussed in this post. Also, here is a link to the notebook that contains the whole implementation in a single place, as well as additional information not included in the post to make it concise.", "As it was noted, there are some high-level wrappers built on top of the framework that simplify the model training process a lot. In the order of the increasing complexity, from minimalistic to very involved:", "The main benefit of high-level libraries is that instead of writing custom utils and wrappers to read and prepare the data, one can focus on the data exploration process itself \u2014 no need to find bugs in the code, hard-working maintainers improving the library and ready to help if you have issues. No need to implement custom data augmentation tools or training parameters scheduling, everything is already here.", "Using a well-maintained library is a no-doubt choice if you\u2019re developing a production-ready code, or participating in a data science competition and need to search for the best model, and not sitting with a debugger trying to figure out where this memory error comes. The same is true if you\u2019re learning new topics and would like to get some working solution faster instead of spending many days (or weeks) coding ResNets layers and writing your SGD optimizer.", "However, if you\u2019re like me then one day you\u2019ll like to test your knowledge and build something with fewer layers of abstraction. If so, let\u2019s proceed to the next section and start reinventing the wheel!", "The very basic implementation of the training loop is not that difficult. The pytorch package already includes convenience classes that allow instantiating dataset accessors and iterators. So in essence, we need to do something shown in the snippet below.", "We could stop our discussion on this section and save some time. However, usually, we need something more than simple loss computation and updating model weights. First of all, we would like to track progress using various performance metrics. Second, the initially set optimizer parameters should be tuned during the training process to improve convergence.", "A straightforward approach would be to modify the loop\u2019s code to include all these additional features. The only problem is that as time goes, we could lose the clarity of our implementation by adding more and more tricks, introduce regression bugs, and end up with spaghetti code. How can we find a tradeoff between simplicity and maintainability of the code and the efficiency of the training process?", "The answer is to use software design patterns. The observer is a well-known design pattern in object-oriented languages. It allows decoupling a sophisticated system into more maintainable fragments. We don\u2019t try to encapsulate all possible features into a single class or function, but delegate calls to subordinate modules. Each module is responsible for reacting onto received notification properly. It can also ignore the notification in case if the message intended for someone else.", "The pattern is known under different names that reflect various features of an implementation: observer, event/signal dispatcher, callback. In our case, we go with callbacks, the approach represented in Keras and (especially) fastai libraries. The solution taken by authors of ignite package is a bit different, but in essence, it boils down to the same idea. Take a look at the picture below. It shows a schematical organization of our improved training loop.", "Each colored section is a sequence of method calls delegated to the group of callbacks. Each callback has methods like epoch_started, batch_started, and so on, and usually implements only a few of them. For example, consider loss metric computation callback. It doesn\u2019t care about methods running before backward propagation, but as soon as batch_ended notification is received, it computes a batch loss.", "The next snippet shows Python implementation of that idea.", "That\u2019s all, isn\u2019t much more sophisticated than the original version, right? It is still clean and concise yet much more functional. Now the complexity of training algorithm is entirely determined with delegated calls.", "There are a lot of useful callbacks (see keras.io and docs.fast.ai for inspiration) we could implement. To keep the post concise, we\u2019re going to describe only a couple of them and move the rest few into a Jupyter notebook.", "The very first thing that comes into mind when talking about Machine Learning model training is a loss function. We use it to guide the optimization process and would like to see how it changes during the training. So let\u2019s implement a callback that would track this metric for us.", "At the end of every batch, we\u2019re computing a running loss. The computation could seem a bit involved, but the primary purpose is to smooth the loss curve which would be bumpy otherwise. The formula a*x + (1 \u2014 a)*y is a linear interpolation between old and new values.", "A denominator helps us to account a bias we have at the beginning of computations. Check this post that describes the smoothed loss computation formula in detail.", "The accuracy metric is probably one of the best-known metrics in machine learning. Though it can\u2019t give you a good estimation of your model\u2019s quality in many cases, it is very intuitive, simple to understand and implement.", "Note that the callback receives notifications at the end of each batch, and the end of training epoch. It computes the accuracy metric iteratively because otherwise, we would need to keep outputs and targets in memory during the whole training epoch.", "Due to this iterative nature of our computations, we need to account a number of samples in batch. We use this value to adjust our computations at the end of the epoch. Effectively, we\u2019re using the formula the picture below shows.", "Where b(i) is a batch size on iteration i, a(i) \u2014 accuracy computed on batch b(i), N \u2014 total number of samples. As the last formula shows, our code computes a sample mean of accuracy. Check these useful references to read more about iterative metrics computations:", "Now the most interesting stuff comes. Modern neural network training algorithms don\u2019t use fixed learning rates. The recent papers (one, two, and three) shows an educated approach to tune Deep Learning models training parameters. The idea is to use cyclic schedulers that adjust model\u2019s optimizer parameters magnitudes during single or several training epochs. Moreover, these schedulers not only decrease learning rates as a number of processed batches grows but also increase them for some number of steps or periodically.", "For example, consider the following function which is a scaled and shifted cosine:", "If we repeat this function several times doubling its period, we\u2019ll get a cosine annealing scheduler as the next picture shows.", "Multiplying the optimizer\u2019s learning rate by the values of this function, we are effectively getting a stochastic gradient with warm restarts that allows us to escape from local minima. The following snippet shows how one can implement a cosine annealing learning rate.", "There is an even more exciting scheduler though called One-Cycle Policy. The idea of this schedule is to use a single cycle of learning rate increasing-decreasing during the whole training process as the following picture shows.", "At the very beginning of the training process, the model weights are not optimal, yet so we can allow yourself use larger update steps (i.e., higher learning rates) without risk to miss optimal values. After a few training epochs, the weights become better and better tailored to our dataset, so we\u2019re slowing down the learning pace and exploring the learning surface more carefully.", "The One-Cycle Policy has a quite straightforward implementation if we use the previously shown class. We only need to add a linear segment that goes before cosine decay, as the lines 27-30 show.", "The final step is to wrap schedulers with a callback interface. An example of implementation is not shown here to make this post concise and easy to read. However, you can find a fully functional code in the aforementioned Jupyter notebook.", "The last thing we would like to add is some logging to see how well our model performs during the training process. The most simplistic approach is to print stats into the standard output stream. However, you could save it into CSV file or even send as a notification to your mobile phone instead.", "OK, finally, we\u2019re ready to start using our training loop!", "Now when the callbacks are ready, it is time to show how our training loop works. For this purpose, let\u2019s pick the ubiquitous MNIST dataset. You can easily train it even on CPU within a few minutes.", "The dataset is very simple for modern Deep Learning architectures and algorithms. Therefore, we can use a relatively shallow architecture, with a few convolution and linear layers.", "We don\u2019t use a transfer learning here but you definitely should when working on your daily tasks. It makes your network to converge much faster compared to the training from scratch.", "Next, we use torchvision package to simplify dataset loading and iterating. Also, we apply a couple of augmentation methods to improve the quality of the model. Then, we build a callbacks group that adds a bunch of features to our basic training loop. Finally, we make a couple of small preparations and call training function to optimize the model.", "You should get an output similar to the output shown below.", "Note that the code shown above includes make_phases() function that is not shown here. Please refer the notebook to see its implementation. In essence, it wraps data loaders with thin structures helping to track performance metrics during model\u2019s training.", "An ultimate goal of a Deep Learning engineer is to build a robust and accurate solution for a specific dataset and task. The best way to achieve the goal is to use proven tools and well-maintained frameworks and libraries tested in many use cases by users throughout the world.", "However, if you would like to be versed in Data Science and eventually build your custom solutions, you probably \u201cshould understand backprop\u201d. Knowing your tools well gives you the capability to tailor them for your specific needs, add new functionality and learn new instruments faster.", "I believe that keeping yourself in a balance between using proven APIs and understanding \u201clow-level\u201d details makes you a better engineer who can easily transfer obtained knowledge to new platforms, languages, and interfaces.", "Interested in Python language? Can\u2019t live without Machine Learning? Have read everything else on the Internet?", "Then probably you would be interested in my blog where I am talking about various programming topics and provide links to textbooks and guides I\u2019ve found interesting.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Developer & AI Enthusiast. Working with Machine Learning, Data Science, and Data Analytics. Writing posts every once in a while."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe41055a24b73&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e41055a24b73--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@iliazaitsev?source=post_page-----e41055a24b73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iliazaitsev?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Ilia Zaitsev"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59449609fea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&user=Ilia+Zaitsev&userId=59449609fea0&source=post_page-59449609fea0----e41055a24b73---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@iliazaitsev/how-to-implement-a-recommendation-system-with-deep-learning-and-pytorch-2d40476590f9", "anchor_text": "the previous post"}, {"url": "https://github.com/pytorch/pytorch/tree/v1.0.0", "anchor_text": "1.0 version"}, {"url": "https://github.com/devforfu/loop", "anchor_text": "this link"}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "a link to the notebook"}, {"url": "https://pytorch.org/ignite/", "anchor_text": "Ignite"}, {"url": "https://github.com/ncullen93/torchsample", "anchor_text": "Torchsample"}, {"url": "https://github.com/dnouri/skorch", "anchor_text": "Skorch"}, {"url": "https://docs.fast.ai", "anchor_text": "fastai"}, {"url": "https://en.wikipedia.org/wiki/Software_design_pattern", "anchor_text": "software design patterns"}, {"url": "http://www.gameprogrammingpatterns.com/observer.html", "anchor_text": "a well-known design pattern"}, {"url": "https://keras.io/callbacks/", "anchor_text": "keras.io"}, {"url": "https://docs.fast.ai/callbacks.html", "anchor_text": "docs.fast.ai"}, {"url": "https://en.wikipedia.org/wiki/Linear_interpolation", "anchor_text": "linear interpolation"}, {"url": "https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html#in-practice", "anchor_text": "this post"}, {"url": "https://stats.stackexchange.com/questions/312780/why-is-accuracy-not-the-best-measure-for-assessing-classification-models", "anchor_text": "many cases"}, {"url": "https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Sample_mean", "anchor_text": "sample mean"}, {"url": "https://docs.fast.ai/metrics.html#Creating-your-own-metric", "anchor_text": "Metrics as callbacks"}, {"url": "https://pytorch.org/ignite/_modules/ignite/metrics/accuracy.html#Accuracy", "anchor_text": "Accuracy metric"}, {"url": "https://arxiv.org/abs/1702.04283", "anchor_text": "one"}, {"url": "https://arxiv.org/abs/1708.07120", "anchor_text": "two"}, {"url": "https://arxiv.org/abs/1803.09820", "anchor_text": "three"}, {"url": "https://arxiv.org/pdf/1608.03983.pdf", "anchor_text": "stochastic gradient with warm restarts"}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "aforementioned Jupyter notebook"}, {"url": "https://forums.fast.ai/t/training-metrics-as-notifications-on-mobile-using-callbacks/17330", "anchor_text": "send as a notification to your mobile phone"}, {"url": "https://github.com/devforfu/pytorch_playground/blob/master/loop.ipynb", "anchor_text": "the notebook"}, {"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "anchor_text": "should understand backprop"}, {"url": "https://iliazaitsev.me/", "anchor_text": "my blog"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e41055a24b73---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e41055a24b73---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e41055a24b73---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----e41055a24b73---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/python?source=post_page-----e41055a24b73---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&user=Ilia+Zaitsev&userId=59449609fea0&source=-----e41055a24b73---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&user=Ilia+Zaitsev&userId=59449609fea0&source=-----e41055a24b73---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe41055a24b73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e41055a24b73---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e41055a24b73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e41055a24b73--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e41055a24b73--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e41055a24b73--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e41055a24b73--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iliazaitsev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@iliazaitsev?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ilia Zaitsev"}, {"url": "https://medium.com/@iliazaitsev/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "401 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F59449609fea0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&user=Ilia+Zaitsev&userId=59449609fea0&source=post_page-59449609fea0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb1fca54e4aa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-model-training-loop-e41055a24b73&newsletterV3=59449609fea0&newsletterV3Id=b1fca54e4aa9&user=Ilia+Zaitsev&userId=59449609fea0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}