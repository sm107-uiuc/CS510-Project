{"url": "https://towardsdatascience.com/demystifying-support-vector-machine-b04d202bf11e", "time": 1683005561.607187, "path": "towardsdatascience.com/demystifying-support-vector-machine-b04d202bf11e/", "webpage": {"metadata": {"title": "Demystifying Support Vector Machine | by Anuj Shrivastav | Towards Data Science", "h1": "Demystifying Support Vector Machine", "description": "Supervised learning describes a class of problems that involves using a model to learn a mapping between input examples and the target variable. The target variable could be class labels in case of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Supervised_learning", "anchor_text": "Supervised learning", "paragraph_index": 0}], "all_paragraphs": ["Supervised learning describes a class of problems that involves using a model to learn a mapping between input examples and the target variable. The target variable could be class labels in case of classification problems or regressor values in case of regression problems. Some models work both for regression as well as classification. One such model that we are going to discuss in this blog is Support Vector Machine abbreviated as SVM. I aim to provide you with the internal working of SVM simply and clearly.", "Without any further ado, let\u2019s get started.\ud83d\ude97", "Let\u2019s suppose we are dealing with a binary classification task.", "There might be infinitely many hyperplanes possible that can separate the two classes. One may wish to choose any of these. But will that hyperplane be good at predicting the class for a new query point? Don\u2019t you think the plane which is very close to one of the classes, is favoring the other class? Intuitively the best way to separate two classes would be to choose a hyperplane that is equidistant from the nearest points in either class.This is what SVM does!", "Choose that hyperplane \u03c0 that separates +ve points from -ve points as widely as possible.", "Let \u03c0 be a hyperplane separating the two classes and \u03c0\u208a and \u03c0\u208b be two hyperplanes parallel to \u03c0 such that", "\u03c0\u208a be the plane that we get when we move parallel to \u03c0 and which touches the closest +ve point to \u03c0", "and \u03c0\u208b be the plane that we get when we move parallel to \u03c0 and which touches the closest -ve point to \u03c0", "The points lying on \u03c0\u208a or \u03c0\u208b are known as support vectors.", "any point on \u03c0\u208a or away from it in the positive direction is labeled as positive", "any point on \u03c0\u208b or away from it in the negative direction is labeled as negative", "In that case, the margin would be", "Now, this may look good, however, it works only till our data is linearly separable. If it isn\u2019t, then we would not be able to solve the above optimization problem and we would not get the optimal w and b.", "Let\u2019s imagine a scenario where the data is not linearly separable:", "Because of these four points, our optimization problem will never get solved as for these points y\u1d62(w\u1d40x+b) is not \u2265 1.", "This is because our optimization problem is too strict that it only solves for linearly separable data. Hard-margin is the name given to this approach.", "So, can we modify it? Can we make it a little lenient so that it would work on almost linearly separable data?", "What we do is we make a slack variable \u03b6 \u1d62 (zeta) corresponding to each data point such that for +ve points lying in +ve region and -ve points lying in -ve region,\u03b6 \u1d62 =0.", "This leaves us with misclassified points and points that are inside the margin.", "Take for example point 4 in figure 7, \u03b6 \u1d62 = 2.5which means point 4 is 2.5 units away from its correct hyperplane(\u03c0\u208a in this case) in the opposite direction.", "Similarly, point 1 is 2.5 units ways from its correct hyperplane(\u03c0\u208b in this case) in the opposite direction.", "As \u03b6 \u1d62 increases, the point is further away from the correct hyperplane in the incorrect direction.", "We can intuitively think the optimization problem as:", "Wait! How did we get this dual form?", "Well, it\u2019s all related to optimization and if we dive into that, we will divert from our objective. Let\u2019s keep that for another blog and just continue with this dual form.", "Why do we need this dual form?", "Dual form is sometimes easier to solve and we get similar results if the duality gap is very small. In particular, for SVM: dual form is very important as it opens a new way of interpreting SVMs through kernel functions (which I\u2019ll tell you in the later part of this blog).", "NOTICE: In dual form, all x\u1d62 occur in the form of the dot product, unlike primal form where all x\u1d62 occur as standalone points.", "\u03b1\u1d62 can be thought of as a Lagrangian multiplier", "That means only points that matter are the support vectors which is the reason behind naming this model as Support Vector Machine.", "Hence, if a similarity matrix is given, we can use the dual form but not the primal form. That\u2019s the beauty of SVM. Often, this similarity(x\u1d62,x\u2c7c) is replaced by K(x\u1d62,x\u2c7c) where K is known as the kernel function.", "Replacing similarity(x\u1d62,x\u2c7c) by a kernel function K(x\u1d62,x\u2c7c) is known as kernelization, or applying the kernel trick.", "If you look at it, it is nothing but calculating the dot product of x\u1d62 and x\u2c7c. So, What\u2019s so great about it?", "Let\u2019s take the example of the following dataset.", "It is quite obvious that the two classes can\u2019t be separated with the help of a linear model.", "Did you see that? Applying proper feature transformation and increasing the dimension made our data linearly separable.", "This is what kernel SVM does. It maps the original features into a higher-dimensional space, finds the margin maximizing hyper-plane in that space and maps that hyperplane into the original dimension space to get a non-linear decision surface without actually visiting that higher-dimensional space.", "Linear SVM: finding margin maximizing hyperplane in space of x\u1d62\u2019s", "Kernel SVM: finding margin maximizing hyperplane in the transformed space of x\u1d62\u2019s", "Hence, kernel SVM is capable of solving non linearly separable datasets as well.", "Let\u2019s look at some of the kernels used in SVM-", "For the quadratic kernel, let c=1 i.e.", "This can be thought of as the product of 2 vectors x\u2081\u2019 and x\u2082\u2019 where:", "So, kernelization is the same as feature transformation but typically d\u2019 > d and the kernelization is done internally and implicitly.", "It is the most popular kernel in the sense that when you can\u2019t figure which kernel to choose, you can go with this \ud83d\ude09", "When d increases, the numerator of the exponential part decreases, or in other words, K value or similarity decreases.", "If you notice that when \u03c3 = 0.3, the graph reaches close to 0 at about x=2. It reaches close to 0 at x=4 when \u03c3 =1 and at x=11 when \u03c3=10. This indicates that when \u03c3 increases, even if two points are far away, there would be a similarity score > 0 for them.", "Let\u2019s say there are two points x\u2081 and x\u2082 at a distance of 4 units. If we apply the RBF kernel with \u03c3 = 0.3, the kernel function K value or similarity value would be 0. If we take \u03c3 = 1, the K value would be very close to 0 but if we take \u03c3 = 10, the K value would be around 0.4 or so.", "Now, the intuition behind RBF kernel -", "Remember we got the mapping function in 6 dimensions when we used the polynomial kernel (refer figure 24)? Let\u2019s now try to find out a mapping function for the RBF kernel.", "For simplicity of math, assuming the original dimensions of data to be 2 and the denominator of the exponential part to be equal to 1.In that case,", "If we try to find out the mapping function for RBF kernel, this would lead us to an infinite vector. It means that the RBF kernel takes our data into infinite-dimensional space, calculates the similarity score and returns it, and that\u2019s why if you can\u2019t figure out which kernel to choose, RBF kernel would be the safest option.\ud83d\ude0e", "So far we have looked at how to perform a classification task using SVM. But SVM is not just limited to that. It can be utilized to perform a regression task as well. How? Let\u2019s check it out!", "First, let\u2019s see the mathematical formulation of Support Vector Regressor (SVR)", "Don\u2019t worry! I\u2019ll break it up for you.", "The way SVR works is that it tries to find a hyperplane that best fits the data points while keeping a margin of tolerance= \u03b5(hyperparameter), which means all points should be within \u03b5 distance from the hyperplane on either side of it.", "Minimizing the margin here means that we want to find a hyperplane that fits the data with a low margin of tolerance.", "NOTE: Squaring is done so that the function becomes differentiable and gets suitable to optimize. One could do the same with the SVC.", "What\u2019s the problem with this formulation?", "The problem here is that we are too strict in the sense that we expect the points to lie within \u03b5 distance from the hyperplane which in real-world doesn\u2019t happen too often. In that case, we won\u2019t be able to find the required hyperplane. So, how do we go about it?", "The same way we have dealt with this problem in SVC, we\u2019ll introduce two slack variables \u03b6 (zeta) for points in the direction of normal and \u03b6* (zeta star) for points in the direction opposite to normal, to allow some points so lie outside the margin of tolerance but with a penalty.", "So the mathematical function now becomes:-", "where C determines the amount of strictness required. More the C, more penalty to points outside margin and that could lead to overfitting of data. Less C means less penalty to points outside the margin and that could lead to underfitting of data.", "Just like in SVC, figure 31 shows the primal form of SVR. It has been observed that the dual form is easier to solve and we can use kernel trick to find non-linear hyperplane as well.", "As I have already said that formulation of dual form is a bit tricky and involves knowledge of solving constraint optimization problems.We won\u2019t go into that much detail as that would divert us from SVM.", "Sorry! I didn\u2019t mean to scare you. This is obtained by using the Lagrangian multipliers to solve optimization problems.", "For a new point, the way we calculate the output value is:-", "Did you notice that x\u1d62 appears in the form of a dot product? Yes the same way we obtained this in SVC. We can replace this dot product with a similarity or kernel function which I have already mentioned above. Applying the kernel trick would help us fit non-linear data as well.", "This is done implicitly by finding the right kernel as discussed above", "For linear SVM: decision surface is just a hyperplane", "For kernel SVM: It would be a non-linear surface", "3. Similarity function / distance function", "The primal form of SVM doesn\u2019t handle the similarity function. However, the dual form can easily handle it because of the presence of x\u1d62 in dot product form.", "If the features are not collinear, then weights of features in weight vector w determine the feature importance. In case the features are collinear, one may use the forward feature selection or backward feature elimination which are the standard ways of determining feature importance for any model.", "They do affect SVM, however, the effect is less as compared to other models like Logistic Regression.", "It depends on the value of c in the dual form of SVM.", "If c is high, more weightage would be given to the error term and hence the model may overfit the data and if c is less, less weightage would be given to error term and the model may underfit the data.", "SVMs are designed to work well even in high dimensions. Refer to figure 14 which shows that a regularization term is already present in the mathematical formulation of SVM which helps in dealing with high dimensions. You may say that other models like KNN don\u2019t work well high dimensions, then what\u2019s so special about SVMs? This is because SVM only cares about finding the plane that maximizes the margin and it doesn\u2019t care about the relative distance between the points.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb04d202bf11e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@anujshrivastav97?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anujshrivastav97?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Anuj Shrivastav"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21263d8cf807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&user=Anuj+Shrivastav&userId=21263d8cf807&source=post_page-21263d8cf807----b04d202bf11e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Supervised_learning", "anchor_text": "Supervised learning"}, {"url": "https://alex.smola.org/papers/2004/SmoSch04.pdf", "anchor_text": "https://alex.smola.org/papers/2004/SmoSch04.pdf"}, {"url": "https://www.saedsayad.com/support_vector_machine_reg.htm", "anchor_text": "https://www.saedsayad.com/support_vector_machine_reg.htm"}, {"url": "https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/", "anchor_text": "https://statinfer.com/204-6-8-svm-advantages-disadvantages-applications/"}, {"url": "http://www.cs.uky.edu/~jzhang/CS689/PPDM-Chapter2.pdf", "anchor_text": "http://www.cs.uky.edu/~jzhang/CS689/PPDM-Chapter2.pdf"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b04d202bf11e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/svm?source=post_page-----b04d202bf11e---------------svm-----------------", "anchor_text": "Svm"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b04d202bf11e---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----b04d202bf11e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/classification?source=post_page-----b04d202bf11e---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&user=Anuj+Shrivastav&userId=21263d8cf807&source=-----b04d202bf11e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&user=Anuj+Shrivastav&userId=21263d8cf807&source=-----b04d202bf11e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb04d202bf11e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b04d202bf11e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b04d202bf11e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b04d202bf11e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b04d202bf11e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anujshrivastav97?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@anujshrivastav97?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Anuj Shrivastav"}, {"url": "https://medium.com/@anujshrivastav97/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "43 Followers"}, {"url": "https://www.linkedin.com/in/anuj-shrivastav/", "anchor_text": "https://www.linkedin.com/in/anuj-shrivastav/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F21263d8cf807&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&user=Anuj+Shrivastav&userId=21263d8cf807&source=post_page-21263d8cf807--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F66568b0a5f6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-support-vector-machine-b04d202bf11e&newsletterV3=21263d8cf807&newsletterV3Id=66568b0a5f6e&user=Anuj+Shrivastav&userId=21263d8cf807&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}