{"url": "https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "time": 1682994577.259773, "path": "towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677/", "webpage": {"metadata": {"title": "Qrash Course: Reinforcement Learning 101 & Deep Q Networks in 10 Minutes | by Shaked Zychlinski | Towards Data Science", "h1": "Qrash Course: Reinforcement Learning 101 & Deep Q Networks in 10 Minutes", "description": "Out of all the different types of Machine Learning fields, the one that always fascinated me the most was Reinforcement Learning. For those of you who are less familiar with this field\u2026"}, "outgoing_paragraph_urls": [{"url": "https://jeinalog.tistory.com/20", "anchor_text": "jeinalog.tistory.com", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Markov_model", "anchor_text": "Markov States", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Wikipedia page", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Circular_buffer", "anchor_text": "cyclic memory buffer", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Double Deep Q Network", "paragraph_index": 32}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "right here", "paragraph_index": 35}, {"url": "https://medium.com/@shakedzy/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8", "anchor_text": "another blogpost I wrote", "paragraph_index": 37}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz", "paragraph_index": 38}], "all_paragraphs": ["This blogpost is now available in Korean too, read it on jeinalog.tistory.com", "This article assumes no prior knowledge in Reinforcement Learning, but it does assume some basic understanding of neural networks.", "Out of all the different types of Machine Learning fields, the one fascinating me the most is Reinforcement Learning. For those who are less familiar with it \u2014 while Supervised Learning deals with predicting values or classes based on labeled data and Unsupervised Learning deals with clustering and finding relations in unlabeled data, Reinforcement Learning deals with how some arbitrary being (formally referred to as an \u201cAgent\u201d) should act and behave in a given environment. The way it is done is by giving the Agent rewards or punishments based on the actions it has performed on different scenarios.", "One of the first practical Reinforcement Learning methods I learned was Deep Q Networks, and I believe it\u2019s an excellent kickstart to this journey. So allow me to walk you through the path I walked on when attempted to learn RL \u2014including a \u201cHello World\u201d exercise, which helped me more than I can explain.", "A Reinforcement Learning task is about training an Agent which interacts with its Environment. The Agent transitions between different scenarios of the Environment, referred to as states, by performing actions. Actions, in return, yield rewards, which could be positive, negative or zero. The Agent\u2019s sole purpose is to maximize the total reward it collects over an episode, which is everything that happens between an initial state and a terminal state. Hence, we reinforce the Agent to perform certain actions by providing it with positive rewards, and to stray away from others by providing negative rewards. This is how an Agent learns to develop a strategy, or a policy.", "Take Super Mario as an example: Mario is the Agent interacting with the world (the Environment). The states are exactly what we see on the screen, and an Episode is a level: the initial state is how the level begins, and the terminal state is how the level ends, whether we completed it or perished while trying. The actions are move forward, move backwards, jump, etc. Rewards are given depending on actions outcome: when Mario collects coins or bonuses, it receives a positive reward, and when it falls or being hit by an enemy, it receives a negative reward. When Mario just wonders around, the reward it receives is zero, as if to say \u201cyou did nothing special\u201d.", "But there\u2019s a problem here: to be able to collect rewards, some \u201cnon-special\u201d actions are needed to be taken \u2014 you have to walk towards the coins before you can collect them. So an Agent must learn how to handle postponed rewards by learning to link those to the actions that really caused them. In my opinion, this is the most fascinating thing in Reinforcement Learning.", "Each state the Agent is in is a direct consequence of the previous state and the chosen action. The previous state is also a direct consequence of the one that came before it, and so on till we reach the initial state. Each one of these steps, and their order, hold information about the current state \u2014 and therefore have direct effect on which action should the Agent choose next. But there\u2019s an obvious problem here: the further we go, the more information the Agent needs to save and process at every given step it takes. This can easily reach the point where it is simply unfeasible to perform calculations.", "To tackle this, we assume all states are Markov States; that is \u2014 we assume that any state depends solely on the state that came before it, and the transition from that state to the current one (the action performed and reward given). Let\u2019s see an example \u2014 look at these two Tic Tac Toe games:", "Both games reached the same state, but in different ways. Still, in both cases, the blue player must capture the top-right cell, or he will lose. All we needed in order to determine this was the last state, nothing else.", "It\u2019s important to remember that when using the Markov assumption, data is being lost \u2014 in complex games such as Chess or Go, the order of the moves might have some implicit information on the opponent\u2019s strategy or way of thought. Still, the Markov States assumption is fundamental when attempting to calculate long-term strategies.", "Let\u2019s go ahead and develop our first strategy. Consider the simplest case: assume we already know what is the expected reward for each action on each step. How will we choose an action in this case? Quite simply \u2014 we\u2019ll choose the sequence of action that will eventually generate the highest reward. This cumulative reward we\u2019ll receive is often referred to as Q Value (an abbreviation of Quality Value), and we can formalize our strategy mathematically as:", "The above equation states that the Q Value yielded from being at state s and selecting action a, is the immediate reward received, r(s,a), plus the highest Q Value possible from state s\u2019 (which is the state we ended up in after taking action a from state s). We\u2019ll receive the highest Q Value from s\u2019 by choosing the action that maximizes the Q Value. We also introduce \u03b3, usually called the discount factor, which controls the importance of longterm rewards versus the immediate one.", "This equation is known as the Bellman Equation, and its Wikipedia page provides a comprehensive explanation of its mathematical derivation. This elegant equation is quite powerful and will be very useful to us due to two important characteristics:", "We now have a basic strategy \u2014 at any given state, perform the action that will eventually yield the highest cumulative reward. Algorithms like this are called greedy, for an obvious reason.", "How would we implement this to solve real-life challenges? One way is drawing a table to store all possible state-action combinations, and use it to save Q Values. We can then update it using the Bellman Equation as an update rule:", "Now remember some states are terminal states. When the Agent reaches one, no action or state-transition are possible. So, if the future state s\u2019 is a terminal state, we are left with:", "Not done yet \u2014 our greedy algorithm has a serious problem: if you keep selecting the same best-actions, you\u2019ll never try anything new, and you might miss a more rewarding approach just because you never tried it.", "To solve this, we use an \u03b5-greedy approach: for some 0 < \u03b5 < 1, we choose the greedy action (using our table) with a probability p = 1-\u03b5, or a random action with probability p = \u03b5. We thus give the Agent a chance to explore new opportunities.", "This algorithm is known as Q Learning (or Q-Table). Congratulations! you just learned your very first Reinforcement Learning algorithm!", "You might have asked yourself how does Q Learning scale \u2014 and if you haven\u2019t, let\u2019s ask it together: what happens when the number of states and actions becomes very large? This is actually not that rare \u2014 even a simple game such as Tic Tac Toe has has hundreds of different states (try to calculate this), and don\u2019t forget we multiply this number by 9, which is the number of possible actions. So how will we solve really complex problems?", "Enter Deep Learning! We combine Q Learning and Deep Learning, which yields Deep Q Networks. The idea is simple: we\u2019ll replace the the Q Learning\u2019s table with a neural network that tries to approximate Q Values. It is usually referred to as the approximator or the approximating function, and denoted as Q(s,a; \u03b8), where \u03b8 represents the trainable weights of the network.", "Now it only makes sense to use the Bellman Equation as the cost function \u2014 but what exactly will we minimize? Let\u2019s take another look at it:", "The \u201c=\u201d sign marks assignment, but is there any condition which will also satisfy an equality? Well, yes \u2014 when the Q Value reached its converged and final value. And this is exactly our goal \u2014 so we can minimize the difference between the left-hand side and the right-hand side \u2014 and, viola! Our cost function:", "Does this looks familiar? Probably \u2014 it\u2019s the Mean Square Error function, where the current Q Value is the prediction (y), and the immediate and future rewards are the target (y\u2019):", "This is why Q(s\u2019,a; \u03b8) is usually referred to as Q-target.", "Moving on: Training. In Reinforcement Learning, the training set is created as we go; we ask the Agent to try and select the best action using the current network \u2014 and we record the state, action, reward and the next state it ended up at. We decide on a batch size b, and after every time b new records were recorded, we select b records at random (!!) from the memory, and train the network. The memory buffers used are usually referred to as Experience Replay. Several types of such memories exist \u2014 one very common is a cyclic memory buffer. This makes sure the Agent keeps training over its new behavior rather than things that might no longer be relevant.", "Things are getting real, so let\u2019s talk architecture: if imitating a table, the network should receive as input the state and action, and should output a Q Value:", "While correct, this architecture is very inefficient from a technical point of view. Note that the cost function requires the maximal future Q Value, so we\u2019ll need several network predictions for a single cost calculation. So instead, we can use the following architecture:", "Here we provide the network only the state s as input, and receive Q Values for all possible actions at-once. Much better.", "And well, what do you know \u2014 that\u2019s pretty much it. Congratulations again! You just learned how to design a Deep Q Network!", "Before we wrap it up, here\u2019s something extra: a few paragraphs ago we compared the Deep Q Network cost function to Mean Square Error. But MSE compares predictions y to the true labels y\u2019 \u2014 and the true labels are constant throughout the entire training procedure. Obviously, this is not the case in Deep Q Networks: both y and y\u2019 are predicted by the network itself, and therefore might vary at every iteration. The impact is clear.", "Introducing: Double Deep Q Network, which uses semi-constant labels during training. How? We keep two copies of the Q Network, but only one is being updated \u2014 the other one remains still. Every once in a while though, we replace the constant network with a copy of the trained Q Network, hence the reason we call it \u201csemi-constant\u201d. And so:", "Here, \u03d1 represents the semi-constant weights, so Q(s\u2019,a; \u03d1) means the Q Value predicted by the semi-constant network. That\u2019s it, you got it.", "I personally believe the best way to grasp new concepts is by trying to implement them yourself. In order to try Q Learning and Deep Q Networks, I made up a simple game: a board with 4 slots, which should be filled by the Agent. When the Agent selects an empty slot, it receives a reward of +1, and the slot is filled. If it selects a non-vacant slot, it receives a reward of -1. The game ends when the entire board is full.", "Give it a shot and try to implement an Agent that learns to master this game using both methods. You can find my attempts right here.", "Good luck, and for the third time today \u2014 Congratulations!", "Ready to try some Reinforcement Learning yourself? Here\u2019s another blogpost I wrote with some practical tips.", "Head of Recommendations at Lightricks. Lives in Tel-Aviv, Israel. See me on shakedzy.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1bbda41d3677&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://shakedzy.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----1bbda41d3677---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bbda41d3677&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=-----1bbda41d3677---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bbda41d3677&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=-----1bbda41d3677---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://jeinalog.tistory.com/20", "anchor_text": "jeinalog.tistory.com"}, {"url": "https://medium.com/@shakedzy/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c", "anchor_text": "Policy Gradients and Actor-Critic"}, {"url": "https://en.wikipedia.org/wiki/Markov_model", "anchor_text": "Markov States"}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Wikipedia page"}, {"url": "https://en.wikipedia.org/wiki/Circular_buffer", "anchor_text": "cyclic memory buffer"}, {"url": "https://arxiv.org/abs/1509.06461", "anchor_text": "Double Deep Q Network"}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "right here"}, {"url": "https://medium.com/@shakedzy/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8", "anchor_text": "another blogpost I wrote"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1bbda41d3677---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1bbda41d3677---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1bbda41d3677---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----1bbda41d3677---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1bbda41d3677---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bbda41d3677&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=-----1bbda41d3677---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1bbda41d3677&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=-----1bbda41d3677---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1bbda41d3677&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----1bbda41d3677---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=-----1bbda41d3677---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Written by Shaked Zychlinski"}, {"url": "https://shakedzy.medium.com/followers?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "1.3K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----1bbda41d3677---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fqrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=-----1bbda41d3677---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/6-papers-every-modern-data-scientist-must-read-1d0e708becd?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/6-papers-every-modern-data-scientist-must-read-1d0e708becd?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "6 Papers Every Modern Data Scientist Must ReadA list of some of the most important modern fundamentals of Deep Learning everyone in the field show be familiar with"}, {"url": "https://towardsdatascience.com/6-papers-every-modern-data-scientist-must-read-1d0e708becd?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "8 min read\u00b7Jul 31, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d0e708becd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-papers-every-modern-data-scientist-must-read-1d0e708becd&user=Shaked+Zychlinski&userId=43218078e688&source=-----1d0e708becd----0-----------------clap_footer----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/6-papers-every-modern-data-scientist-must-read-1d0e708becd?source=author_recirc-----1bbda41d3677----0---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d0e708becd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F6-papers-every-modern-data-scientist-must-read-1d0e708becd&source=-----1bbda41d3677----0-----------------bookmark_preview----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1bbda41d3677----1---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1bbda41d3677----1-----------------bookmark_preview----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----1bbda41d3677----2---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----1bbda41d3677----2-----------------bookmark_preview----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "The Search for Categorical CorrelationExploring the uncharted territories where Perason's R no longer works"}, {"url": "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": "6 min read\u00b7Feb 24, 2018"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa1cf7f1888c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-search-for-categorical-correlation-a1cf7f1888c9&user=Shaked+Zychlinski&userId=43218078e688&source=-----a1cf7f1888c9----3-----------------clap_footer----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9?source=author_recirc-----1bbda41d3677----3---------------------5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "48"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa1cf7f1888c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-search-for-categorical-correlation-a1cf7f1888c9&source=-----1bbda41d3677----3-----------------bookmark_preview----5ae58dd7_20a1_42b7_9227_7ae6e810c13c-------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "See all from Shaked Zychlinski"}, {"url": "https://towardsdatascience.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----1bbda41d3677----0-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----1-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----1bbda41d3677----1-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----0-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----1bbda41d3677----0---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----1bbda41d3677----0-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&user=Bruce+Yang+ByFinTech&userId=a878fc45fb3f&source=-----7af8e747c4bd----1-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----1bbda41d3677----1---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7af8e747c4bd&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2Ffinrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd&source=-----1bbda41d3677----1-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&user=Andrew+Austin&userId=42d388912d13&source=-----614846383eb7----2-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----1bbda41d3677----2---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F614846383eb7&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7&source=-----1bbda41d3677----2-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "Rainbow DQN \u2014 The Best Reinforcement Learning Has to Offer?What happens if the most successful techniques in Deep Q-Learning are combined into a single algorithm?"}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": "\u00b711 min read\u00b7Dec 8, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----166cb8ed2f86----3-----------------clap_footer----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----1bbda41d3677----3---------------------2bfc5d49_1f02_4133_b273_d31b665a8c10-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F166cb8ed2f86&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86&source=-----1bbda41d3677----3-----------------bookmark_preview----2bfc5d49_1f02_4133_b273_d31b665a8c10-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1bbda41d3677--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}