{"url": "https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d", "time": 1683000322.383188, "path": "towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d/", "webpage": {"metadata": {"title": "Document Embedding Techniques. A review of notable literature on the\u2026 | by Shay Palachy Affek | Towards Data Science", "h1": "Document Embedding Techniques", "description": "Word embedding \u2014 the mapping of words into numerical vector spaces \u2014 has proved to be an incredibly important method for natural language processing (NLP) tasks in recent years, enabling various\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.bigpanda.io/", "anchor_text": "BigPanda", "paragraph_index": 2}, {"url": "http://www.shaypalachy.com/", "anchor_text": "I consulted for several years", "paragraph_index": 2}, {"url": "https://cs.stanford.edu/~quocle/paragraph_vector.pdf", "anchor_text": "Le & Mikolov, 2014", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1507.07998.pdf", "anchor_text": "Dai et al, 2015", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016", "paragraph_index": 8}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/Main_Page", "anchor_text": "the Semantic Textual Similarity (STS) SemEval shared task", "paragraph_index": 8}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "Kiros et al, 2015", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1810.09302.pdf", "anchor_text": "Chen et al, 2018", "paragraph_index": 10}, {"url": "https://github.com/ncbi-nlp/BioSentVec", "anchor_text": "official Python implementation", "paragraph_index": 10}, {"url": "https://www.microsoft.com/en-us/research/project/dssm/", "anchor_text": "Deep Semantic Similarity Model was used by various authors", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Multiset", "anchor_text": "multiset", "paragraph_index": 20}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "term frequency\u2013inverse document frequency", "paragraph_index": 26}, {"url": "http://pmcnamee.net/744/papers/SaltonBuckley.pdf", "anchor_text": "Salton & Buckley, 1988", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution", "paragraph_index": 31}, {"url": "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization", "anchor_text": "non-negative matrix factorization (NMF)", "paragraph_index": 34}, {"url": "https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis", "anchor_text": "probabilistic latent semantic indexing (PLSI)", "paragraph_index": 34}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Bengio, 2003", "paragraph_index": 36}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "well-written two-part tutorial of word2vec by Chris McCormick", "paragraph_index": 37}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "part 2", "paragraph_index": 37}, {"url": "http://www.scholarpedia.org/article/Neural_net_language_models", "anchor_text": "the Scholarpedia article on neural net language models by Prof. Joshua Bengio", "paragraph_index": 37}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "anchor_text": "Hunter Heidenreich\u2019s post for a more general and concise overview of word embeddings in general", "paragraph_index": 37}, {"url": "http://alexminnaar.com/2015/05/18/word2vec-tutorial-continuousbow.html", "anchor_text": "Alex Minnar\u2019s two part post", "paragraph_index": 37}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Bengio, 2003", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Mikolov et al, 2013a", "paragraph_index": 37}, {"url": "https://www.aclweb.org/anthology/D14-1162", "anchor_text": "Pennington et al, 2014", "paragraph_index": 37}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis", "anchor_text": "Wikipedia", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/w/index.php?title=Semantic_theory&action=edit&redlink=1", "anchor_text": "semantic theory", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/J._R._Firth", "anchor_text": "Firth", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/Statistical_semantics", "anchor_text": "statistical semantics", "paragraph_index": 39}, {"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "Mikolov et al, 2013b", "paragraph_index": 42}, {"url": "https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf", "anchor_text": "a wonderful practical review of this approach", "paragraph_index": 43}, {"url": "https://arxiv.org/pdf/1606.04640.pdf", "anchor_text": "Kenter et al, 2016", "paragraph_index": 47}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016", "paragraph_index": 47}, {"url": "http://wwwusers.di.uniroma1.it/~navigli/pubs/KBS_Sinoaraetal_2019.pdf", "anchor_text": "Sinoara et al, 2019", "paragraph_index": 47}, {"url": "https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf", "anchor_text": "Arora et al, 2016", "paragraph_index": 48}, {"url": "https://github.com/peter3125/sentence2vec", "anchor_text": "Python implementation", "paragraph_index": 48}, {"url": "https://aclweb.org/anthology/N18-1049", "anchor_text": "Pagliardini et al, 2017", "paragraph_index": 50}, {"url": "https://www.aclweb.org/anthology/N19-1098", "anchor_text": "Gupta et al, 2019", "paragraph_index": 50}, {"url": "https://github.com/epfml/sent2vec", "anchor_text": "an official C++-based Python implementation", "paragraph_index": 50}, {"url": "https://link.springer.com/article/10.1186/s12859-018-2496-4", "anchor_text": "Agibetov et al, 2018", "paragraph_index": 52}, {"url": "https://cs.stanford.edu/~quocle/paragraph_vector.pdf", "anchor_text": "Le & Mikolov, 2014", "paragraph_index": 53}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "its Gensim implementation", "paragraph_index": 60}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016", "paragraph_index": 60}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/Main_Page", "anchor_text": "Semantic Textual Similarity (STS) SemEval", "paragraph_index": 62}, {"url": "https://github.com/jhlau/doc2vec", "anchor_text": "including code", "paragraph_index": 62}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "a Python implementation, as part of the gensim package", "paragraph_index": 63}, {"url": "https://github.com/inejc/paragraph-vectors", "anchor_text": "a PyTorch implementation", "paragraph_index": 63}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016", "paragraph_index": 63}, {"url": "https://github.com/jhlau/doc2vec", "anchor_text": "supplied the code used for their examination", "paragraph_index": 63}, {"url": "https://arxiv.org/abs/1512.08183", "anchor_text": "Li et al, 2016", "paragraph_index": 64}, {"url": "https://github.com/tanthongtan/dv-cosine", "anchor_text": "a Java implementation", "paragraph_index": 64}, {"url": "https://arxiv.org/pdf/1707.02377.pdf", "anchor_text": "Chen, 2017", "paragraph_index": 65}, {"url": "https://github.com/mchen24/iclr2017", "anchor_text": "a public Github repository", "paragraph_index": 68}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016", "paragraph_index": 69}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "Kiros et al, 2015", "paragraph_index": 70}, {"url": "https://github.com/ryankiros/skip-thoughts", "anchor_text": "an official pure Python implementation", "paragraph_index": 70}, {"url": "https://github.com/sanyam5/skip-thoughts", "anchor_text": "PyTorch", "paragraph_index": 70}, {"url": "https://github.com/tensorflow/models/tree/master/research/skip_thoughts", "anchor_text": "TensorFlow", "paragraph_index": 70}, {"url": "http://arno.uvt.nl/show.cgi?fid=146003", "anchor_text": "Broere, 2017", "paragraph_index": 74}, {"url": "https://arxiv.org/abs/1706.03146", "anchor_text": "Tang et al, 2017a", "paragraph_index": 75}, {"url": "https://www.groundai.com/project/trimming-and-improving-skip-thought-vectors/1", "anchor_text": "Tang et al, 2017b", "paragraph_index": 75}, {"url": "https://arxiv.org/pdf/1611.07897.pdf", "anchor_text": "Gan et al, 2016", "paragraph_index": 75}, {"url": "https://openreview.net/pdf?id=H1a37GWCZ", "anchor_text": "Lee & Park, 2018", "paragraph_index": 76}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016", "paragraph_index": 76}, {"url": "http://sanyam5.github.io/my-thoughts-on-skip-thoughts/", "anchor_text": "Sanyam Agarwa gives a great detailed overview of the method on his blog", "paragraph_index": 77}, {"url": "https://sourcediving.com/building-recipe-skill-representations-using-skip-thought-vectors-8a6e4c38ae6c", "anchor_text": "Ammar Zaher demonstrate its use to construct an embedding space for cooking recipes", "paragraph_index": 77}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016", "paragraph_index": 78}, {"url": "https://en.wikipedia.org/wiki/%E2%88%91", "anchor_text": "\u2211", "paragraph_index": 78}, {"url": "https://en.wikipedia.org/wiki/%E2%88%91", "anchor_text": "\u2211", "paragraph_index": 78}, {"url": "https://github.com/fh295/SentenceRepresentation", "anchor_text": "an official Python implementation", "paragraph_index": 78}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "Logeswaran & Lee, 2018", "paragraph_index": 79}, {"url": "https://github.com/lajanugen/S2V", "anchor_text": "an official Python implementation", "paragraph_index": 81}, {"url": "https://arxiv.org/pdf/1811.01713v1.pdf", "anchor_text": "Wu et al, 2018b", "paragraph_index": 82}, {"url": "https://github.com/IBM/WordMoversEmbeddings", "anchor_text": "An official C-based, Python-wrapped implementation is provided", "paragraph_index": 82}, {"url": "http://proceedings.mlr.press/v37/kusnerb15.pdf", "anchor_text": "Kushner et al, 2015", "paragraph_index": 83}, {"url": "https://arxiv.org/pdf/1802.04956.pdf", "anchor_text": "Wu et al, 2018a", "paragraph_index": 83}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017", "paragraph_index": 88}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Reimers & Gurevych, 2019", "paragraph_index": 89}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Reimers & Gurevych, 2019", "paragraph_index": 90}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "a Python implementation", "paragraph_index": 90}, {"url": "https://www.aclweb.org/anthology/D14-1179", "anchor_text": "Cho et al, 2014a", "paragraph_index": 94}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al, 2014", "paragraph_index": 94}, {"url": "https://arxiv.org/pdf/1511.08198.pdf", "anchor_text": "Wieting et al, 2015", "paragraph_index": 94}, {"url": "http://paraphrase.org/#/", "anchor_text": "the PPDB dataset", "paragraph_index": 94}, {"url": "https://arxiv.org/pdf/1504.00548.pdf", "anchor_text": "Hill et al, 2015", "paragraph_index": 94}, {"url": "https://www.aclweb.org/anthology/D17-1070.pdf", "anchor_text": "Conneau et al, 2017", "paragraph_index": 94}, {"url": "https://www.aclweb.org/anthology/P16-1036", "anchor_text": "Das et al, 2016", "paragraph_index": 95}, {"url": "https://www.aclweb.org/anthology/K17-1027", "anchor_text": "Nicosia & Moschitti, 2017", "paragraph_index": 96}, {"url": "https://github.com/epfl-dlab/Cr5", "anchor_text": "An official Python implementation", "paragraph_index": 97}, {"url": "https://arxiv.org/pdf/1511.08198.pdf", "anchor_text": "Wieting et al, 2015", "paragraph_index": 99}, {"url": "https://arxiv.org/pdf/1506.06726.pdf", "anchor_text": "Kiros et al, 2015", "paragraph_index": 100}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Radford et al, 2018", "paragraph_index": 102}, {"url": "https://openai.com/blog/language-unsupervised/", "anchor_text": "presented the generative pre-training (GPT) approach", "paragraph_index": 102}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "accompanied by a Python implementation", "paragraph_index": 102}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017", "paragraph_index": 102}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "They later presented GPT-2", "paragraph_index": 102}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Radford et al, 2019", "paragraph_index": 102}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "releasing an official Python implementation", "paragraph_index": 102}, {"url": "https://www.microsoft.com/en-us/research/project/dssm/", "anchor_text": "A Microsoft Research project", "paragraph_index": 103}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf", "anchor_text": "Huang et al, 2013", "paragraph_index": 104}, {"url": "https://kishorepv.github.io/DSSM/", "anchor_text": "TensorFlow", "paragraph_index": 105}, {"url": "https://github.com/airalcorn2/Deep-Semantic-Similarity-Model", "anchor_text": "Keras", "paragraph_index": 105}, {"url": "https://github.com/nishnik/Deep-Semantic-Similarity-Model-PyTorch", "anchor_text": "two PyTorch", "paragraph_index": 105}, {"url": "https://github.com/moinnadeem/CDSSM", "anchor_text": "variations", "paragraph_index": 105}, {"url": "https://arxiv.org/pdf/1810.00681v1.pdf", "anchor_text": "Ahmad et al, 2018", "paragraph_index": 106}, {"url": "https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0", "anchor_text": "Yu & Jiang, 2016", "paragraph_index": 107}, {"url": "https://arxiv.org/pdf/1803.11175.pdf", "anchor_text": "Cer et al, 2018a", "paragraph_index": 108}, {"url": "https://www.aclweb.org/anthology/D18-2029/", "anchor_text": "Cer et al, 2018b", "paragraph_index": 108}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "a TensorFlow implementation", "paragraph_index": 108}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "extended to address multilingual settings", "paragraph_index": 108}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017", "paragraph_index": 109}, {"url": "https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf", "anchor_text": "Iyyer et al, 2015", "paragraph_index": 111}, {"url": "https://arxiv.org/pdf/1804.00079.pdf", "anchor_text": "Subramanian et al, 2018", "paragraph_index": 112}, {"url": "https://github.com/Maluuba/gensen", "anchor_text": "an official Python implementation", "paragraph_index": 112}, {"url": "https://github.com/Maluuba/gensen", "anchor_text": "An official Python implementation was published", "paragraph_index": 112}, {"url": "http://www.shaypalachy.com/", "anchor_text": "contacting me directly", "paragraph_index": 114}, {"url": "https://medium.com/u/d12e803b524e?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Adam Bali", "paragraph_index": 115}, {"url": "https://medium.com/u/4dde5994e6c1?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Ori Cohen", "paragraph_index": 115}, {"url": "https://paperswithcode.com/task/document-embedding", "anchor_text": "a task dedicated to document embedding", "paragraph_index": 116}, {"url": "https://github.com/facebookresearch/SentEval", "anchor_text": "SentEval, an evaluation toolkit for sentence representations", "paragraph_index": 116}, {"url": "https://arxiv.org/pdf/1803.05449.pdf", "anchor_text": "Conneau & Kiela, 2018", "paragraph_index": 116}, {"url": "https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf", "anchor_text": "A simple but tough-to-beat baseline for sentence embeddings", "paragraph_index": 118}, {"url": "https://github.com/peter3125/sentence2vec", "anchor_text": "unofficial implementation", "paragraph_index": 118}, {"url": "http://arno.uvt.nl/show.cgi?fid=146003", "anchor_text": "Syntactic properties of skip-thought vectors", "paragraph_index": 119}, {"url": "https://www.aclweb.org/anthology/P16-1036", "anchor_text": "Together we stand: Siamese networks for similar question retrieval", "paragraph_index": 120}, {"url": "https://arxiv.org/pdf/1504.00548.pdf", "anchor_text": "Learning to understand phrases by embedding the dictionary", "paragraph_index": 121}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf", "anchor_text": "Learning deep structured semantic models for web search using clickthrough data", "paragraph_index": 122}, {"url": "https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf", "anchor_text": "Deep unordered composition rivals syntactic methods for text classification", "paragraph_index": 123}, {"url": "https://dl.acm.org/citation.cfm?id=3291023", "anchor_text": "Crosslingual Document Embedding as Reduced-Rank Ridge Regression", "paragraph_index": 124}, {"url": "https://openreview.net/forum?id=H1a37GWCZ", "anchor_text": "UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT", "paragraph_index": 125}, {"url": "https://www.aclweb.org/anthology/K17-1027", "anchor_text": "Learning contextual embeddings for structural semantic similarity using categorical information", "paragraph_index": 127}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Improving language understanding with unsupervised learning", "paragraph_index": 128}, {"url": "https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0", "anchor_text": "Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification", "paragraph_index": 130}, {"url": "http://www.shaypalachy.com", "anchor_text": "www.shaypalachy.com", "paragraph_index": 133}], "all_paragraphs": ["Word embedding \u2014 the mapping of words into numerical vector spaces \u2014 has proved to be an incredibly important method for natural language processing (NLP) tasks in recent years, enabling various machine learning models that rely on vector representation as input to enjoy richer representations of text input. These representations preserve more semantic and syntactic information on words, leading to improved performance in almost every imaginable NLP task.", "Both the novel idea itself and its tremendous impact have led researchers to consider the problem of how to provide this boon of richer vector representations to larger units of texts \u2014 from sentences to books. This effort has resulted in a slew of new methods to produce these mappings, with various innovative solutions to the problem and some notable breakthroughs.", "This post, written while I was introducing myself to the topic (as part of a project in BigPanda, where I consulted for several years \u2764\ufe0f\ud83d\udc3c), is meant to present the different ways practitioners have come up with to produce document embeddings.", "Note: I use the word document here to refer to any sequence of words, ranging from sentences and paragraphs through social media posts all way up to articles, books and more complexly structured text documents (e.g. forms).", "In this post, I will touch upon not only approaches which are direct extensions of word embedding techniques (e.g. in the way doc2vec extends word2vec), but also other notable techniques that produce \u2014 sometimes among other outputs \u2014 a mapping of documents to vectors in \u211d\u207f.", "I will also try to provide links and references to both the original papers and code implementations of the reviewed methods whenever possible.", "Note: This topic is somewhat related, but not equivalent, to the problem of learning structured text representations (e.g. [Liu & Lapata, 2018]).", "The ability to map documents to informative vector representations has a wide range of applications. What follows is only a partial list.", "[Le & Mikolov, 2014] demonstrated the capabilities of their paragraph vectors method on several text classification and sentiment analysis tasks, while [Dai et al, 2015] examined it in the context of document similarity tasks and [Lau & Baldwin, 2016] benchmarked it against a forum question duplication task and the Semantic Textual Similarity (STS) SemEval shared task.", "[Kiros et al, 2015] has demonstrated the use of their skip-thought vectors for semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and four sentiment and subjectivity datasets. [Broere, 2017] used them to predict POS tags and dependency relations.", "[Chen et al, 2018] showed BioSentVec, their set of sentence embeddings trained on biomedical texts, to perform well on sentence pair similarity tasks (official Python implementation).", "Finally, the Deep Semantic Similarity Model was used by various authors for information retrieval and web search ranking, ad selection/relevance, contextual entity search and interestingness tasks, question answering, knowledge inference, image captioning, and machine translation tasks.", "I\u2019m writing this part last, having dedicated a lot of time to think on how to structure this article, how the various techniques covered in the following sections can be grouped into prominent approaches and what trends emerge when examining how the different works in the field relate to each other and the way the followed one another.", "Do note, however, that while the problem of document embedding is old, many of the currently influential solutions are young, and this field has seen a resurgence very recently (around 2014), directly following the success of contemporary encoder-decoder-based word embedding techniques, so this is very much still early days. Having said that, I hope this part can put the following sections into a wider context, and frame them in a meaningful manner.", "A possible way to map the field is into the following four prominent approaches:", "There are a couple of unsupervised approaches that don\u2019t fit any of the above groups (specifically, quick-thought and Word Mover\u2019s Distance come to mind), but I think most techniques do fall into one of these four broad categories.", "Note: While it is tempting to point out the classic bag-of-words technique as suffering from a unique absence of order information, this is actually the rule rather then the exception. The main information gained by most of the newer methods reviewed here is extending the distributional hypothesis to larger units of texts. Neural network-based sequence models are the exception.", "There are several broad trends arising when examining both research and application of document embedding techniques as a whole, as well as several challenges one might identify.", "Note: If you found this part a bit out of context, I suggest you revisit it after going through a good portion of the techniques covered in this post.", "This section briefly covers two established techniques for document embedding: bag-of-words and latent Dirichlet allocation. Feel free to skip it.", "Presented in [Harris, 1954], this method represents text as the bag (multiset) of its words (losing grammar and ordering information). This is done by deciding on a set of n words that will form the vocabulary supported by the mapping, and assigning each word in the vocabulary a unique index. Then, each document is represented by a vector of length n, in which the i-th entry contains the number of occurrences of the word i in the document.", "For example, the sentence \u201cdog eat dog world, baby!\u201d (after cleaning punctuation) might be represented by a 550-length vector v (assuming a vocabulary of 550 words was chosen), which is zero everywhere except the following entries:", "Despite its tremendous simplicity, the fact that all information besides word occurrence frequency is lost, and the tendency of representation size to grow quickly to support rich vocabularies, this technique was used almost exclusively and with great success on a huge range of NLP tasks for decades. Even with the significant progress in vector representation for text in recent years, common slight variations of this method \u2014 covered below \u2014 are still used today, and not always as only the first baseline to be quickly surpassed.", "Bag-of-n-gramsTo gain back some of the word order information lost by the bag-of-words approach, the frequency of short word sequences (of length two, three, etc.) can be used (additionally or instead) to construct word vectors. Naturally, bag-of-words is a private case of this method, for n=1.", "For the sentence of \u201cdog eat dog world, baby!\u201d, the word pairs are \u201cdog eat\u201d, \u201ceat dog\u201d, \u201cdog world\u201d and \u201cworld baby\u201d (and sometimes also \u201c<START> dog\u201d and \u201cbaby <END>\u201d), and the vocabulary is made of (or enhanced with) all successive word pairs in the input corpus.", "One major downside of this approach is the non-linear dependency of the vocabulary size on the number of unique words, which can be very large for large corpora. Filtering techniques are commonly used to reduce the vocabulary size.", "tf-idf weightingA final related technique worth mentioning in the context of bag-of-words is term frequency\u2013inverse document frequency, commonly denoted as tf-idf. This method re-weights the above word (or n-gram) frequency vectors with the inverse document frequency (IDF) of each word. The IDF of a word is simply the logarithm of the number of documents in the corpus divided by the number of documents in which that word appears in.", "In short, the TF term grows as the word appears more often, while the IDF term increases with the word\u2019s rarity. This is meant to adjust the frequency scores for the fact that some words appear more (or less) frequently in general. See [Salton & Buckley, 1988] for a thorough overview of term-weighting approaches.", "LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word\u2019s presence is attributable to one of the document\u2019s topics.", "To connect this back to bag-of-words, the former approach can be thought of as a simplistic probabilistic model of documents as distributions over words. The bag-of-words vector then represents the best approximation we have for the unnormalized distribution-of-words in each document; but document here is the basic probabilistic unit, each a single sample of its unique distribution.", "The crux of the matter, then, is to move from this simple probabilistic model of documents as distributions over words to a more complex one by adding a latent (hidden) intermediate layer of K topics.", "Topics are now characterized by distributions over words, while documents are distributions over topics. This probabilistic model of a document corresponds to a generative model of documents; To generate a set of M documents of lengths {N\u1d62}, assuming a predetermined number of K topics, where Dir() denotes a Dirichlet distribution:", "Given this model and a corpus of documents, the problem becomes one of inference, and approximations of the various distributions mentioned above are found in the inference process. Among these are \u03b8\u1d62, the topic distribution for each document i, vectors of dimension K.", "So in the process of inferring the model, a vector space of dimension K was inferred, one which captures in some way the topics or themes in our corpus and the way they are shared between documents in it. This, of course, can be taken as an embedding space for these documents, and \u2014 depending on the choice of K \u2014 it can be of a significantly smaller dimension than vocabulary-based ones.", "Indeed, while a main use case for LDA is unsupervised topic/community discovery, other cases include the use of the resulting latent topic space as an embedding space for the document corpus. Also, note that other topic modeling techniques \u2014 such as non-negative matrix factorization (NMF) and probabilistic latent semantic indexing (PLSI) \u2014 can be used in a similar manner to learn document embedding spaces.", "Note: A main issue practitioners have with probabilistic topic models is with their stability. Since training a topic model requires sampling of probability distributions, models of the same corpus can be expected to differ as seeds of the random number generator vary. This issue is compounded by the sensitivity of topic models to relatively small corpus changes.", "Many of the methods presented in this section are inspired by prominent word embedding techniques, chief among them word2vec, and they are sometimes even direct generalizations of these methods. These word embedding techniques are sometime also called Neural Probabilistic Language Models; these are not identical terms, as a probabilistic language model is a probability distribution over word sequences, but as this approach was introduced as a way to learn language models in [Bengio, 2003], they are closely associated.", "As such, a basic understanding of word embedding techniques is essential for understanding this section. If you are not familiar with the topic, the well-written two-part tutorial of word2vec by Chris McCormick is an excellent starting point (part 2), as is the Scholarpedia article on neural net language models by Prof. Joshua Bengio (also see Hunter Heidenreich\u2019s post for a more general and concise overview of word embeddings in general, and Alex Minnar\u2019s two part post for a more in-depth mathematical deep dive). However, for a profound grasp of the details I urge you to read the seminal papers by [Bengio, 2003], [Mikolov et al, 2013a] and [Pennington et al, 2014] on the topic, which in many ways shaped this sub-field.", "Even assuming your familiarity with word2vec, I still wish to note an important assumption this model makes, and which is carried forward by perhaps each and every one of the models reviewed here: The Distributional Hypothesis. Here is a brief description from Wikipedia:", "The distributional hypothesis in linguistics is derived from the semantic theory of language usage, i.e. words that are used and occur in the same contexts tend to purport similar meanings. The underlying idea that \u201ca word is characterized by the company it keeps\u201d was popularized by Firth. The distributional hypothesis is the basis for statistical semantics.", "Indeed, it is easy to see that word2vec, and other self-supervised methods for learning word representations, rely heavily on this hypothesis; the crux of the model, after all, is that word representations learned while learning to predict the context of a word from the word itself (or vice versa) represent a vector space capturing deep semantic and syntactic concepts and phenomena. Meaning, learning from the context of a word can teach us about both its meaning and its syntactic role.", "In this section, covering self-supervised document representation learning, you will see that all such methods both maintain this assumption for words, and extend it in some way to larger units of texts.", "[Mikolov et al, 2013b] extended word2vec\u2019s skip-gram model to handle short phrases by identifying a large number of short phrases \u2014 the authors focus on two- and three-word phrases \u2014 using a data-driven approach, and then treating the phrases as individual tokens during the training of the word2vec model. Naturally, this is less suitable for learning longer phrases \u2014 as the size of the vocabulary explodes when increasing phrase length \u2014 and is bound to not generalize to unseen phrases as well as the methods that follow it.", "Moshe Hazoom wrote a wonderful practical review of this approach, used by his employer for a search engine focused on the finance domain.", "There is a very intuitive way to construct document embeddings from meaningful word embeddings: Given a document, perform some vector arithmetics on all the vectors corresponding to the words of the document to summarize them into a single vector in the same embedding space; two such common summarization operators are average and sum.", "Building upon this, you can perhaps already imagine that extending the encoder-decoder architecture of word2vec and its relatives to learn how to combine word vectors into document embeddings can be interesting; the methods that follow this one fall into this category.", "A second possibility is to use a fixed (unlearnable) operator for vector summarization \u2014 e.g. averaging \u2014 and learn word embeddings in a preceding layer, using a learning target that is aimed at producing rich document embeddings; a common example is using a sentence to predict context sentences. Thus the main advantage here is that word embeddings are optimized for averaging into document representations.", "[Kenter et al, 2016] did exactly that, using a simple neural network over an averaging of word vectors, learning the word embeddings by predicting, given a sentence representation, its surrounding sentences. They compare the results to both averaged word2vec vectors and to skip-thoughts vectors (see the appropriate sub-section below). [Hill et al, 2016] compare a plethora of methods, including training CBOW and skip-gram word embeddings while optimizing for sentence representation (here using element-wise addition of word vectors). [Sinoara et al, 2019] also propose a straightforward composition of word embedding vectors and other knowledge sources (like word-sense vectors) into their centroid to represent documents.", "Finally, [Arora et al, 2016] have further showed this approach to be a simple but tough-to-beat baseline when augmented with two small variations: (1) using a smooth inverse frequency weighting scheme, and (2) removing the common discourse component from word vectors; this component is found using PCA, and it is used as a correction term for the most frequent discourse, presumably related to syntax. The authors provide a Python implementation.", "Note: Another demonstration of the power of correctly-averaged word \u201cembeddings\u201d can perhaps be found when looking at attention-based machine translation models. The one-directional decoder RNN gets the previous translated word as input, plus not just the \u201cembedding\u201d (i.e. the bi-directional activations from the encoder RNN) of the current word to translate, but also those of words around it; these are averaged in a weighted manner into a context vector. It is teaching that this weighted averaging is able to maintain the complex compositional and order-dependent information from the encoder network\u2019s activations (recall, these are not isolated embeddings like in our case; each is infused with the context of previous/following words).", "Presented in [Pagliardini et al, 2017] and [Gupta et al, 2019] (including an official C++-based Python implementation), this technique is very much a combination of the two above approaches: The classic CBOW model of word2vec is both extended to include word n-grams and adapted to optimize the word (and n-grams) embeddings for the purpose of averaging them to yield document vectors.", "In addition, the process of input subsampling is removed, considering the entire sentence as context instead. This means both that (a) the use of frequent word subsampling is discarded \u2014 so as not to prevent the generation of n-grams features \u2014 and (b) the dynamic context windows used by word2vec are made away with: the entire sentence is considered as the context window, instead of sampling the context window size for each subsampled word uniformly between 1 and the length of the current sentence.", "Another way to think of sent2vec is as an unsupervised version of fastText (see Figure 6), where the entire sentence is the context and possible class labels are all vocabulary words. Coincidentally, [Agibetov et al, 2018] compare the performance of a multi-layer perceptron using sent2vec vectors as features to that of fastText, against the task of biomedical sentence classification.", "Sometimes referred to as doc2vec, this method, presented in [Le & Mikolov, 2014] is perhaps the first attempt to generalize word2vec to work with word sequences. The authors introduce two variants of the paragraph vectors model: Distributed Memory and Distributed Bag-of-Words.", "Paragraph Vectors: Distributed Memory (PV-DM)The PV-DM model augments the standard encoder-decoder model by adding a memory vector, aimed at capturing the topic of the paragraph, or context from the input. The training task here is quite similar to that of continuous bag of words; a single word is to be predicted from its context. In this case, the context words are the preceding words, not the surrounding words, as is the paragraph.", "To achieve this, every paragraph is mapped to a unique vector, represented by a column in a matrix (denoted by D), as is each word in the vocabulary. The contexts are fixed-length and sampled from a sliding window over the paragraph. The paragraph vector is shared across all contexts generated from the same paragraph but not across paragraphs. Naturally, word embeddings are global, and pre-trained word embeddings can be used (see implementations and enhancements below).", "As in word2vec, vectors must be summarized in some way into a single vector; but unlike word2vec, the authors use concatenation in their experiments. Notice that this preserves order information. Similar to word2vec, a simple softmax classifier (in this case, actually hierarchical softmax) is used over this summarized vector representation to predict the task output. Training is done the standard way, using stochastic gradient descent and obtaining the gradient via backpropagation.", "Notice that only the paragraphs in the training corpus have a column vector from D associated with them. At prediction time, one needs to perform an inference step to compute the paragraph vector for a new paragraph: The document vector is initialized randomly. Then, repeatedly, a random word is selected from the new document, and gradient descent is used to adjust input-to-hidden-layer weights such that softmax probability is maximized for the selected word, while hidden-to-softmax-output weights are fixed. This results in a representation of the new document as a mixture of training corpus document vectors (i.e. columns of D), naturally residing in the document embedding space.", "Paragraph Vectors: Distributed Bag of Words (PV-DBOW)The second variant of paragraph vectors, despite its name, is perhaps the parallel of word2vec\u2019s skip-gram architecture; the classification task is to predict a single context word using only the paragraph vector. At each iteration of stochastic gradient descent, a text window is sampled, then a single random word is sampled from that window, forming the below classification task.", "Training is otherwise similar, except for the fact that word vectors are not jointly learned along with paragraph vectors. This makes both memory and runtime performance of the PV-DBOW variant much better.", "Note: In its Gensim implementation, PV-DBOW uses randomly initialized word embeddings by default; if dbow_words is set to 1, a single step of skip-gram is ran to update word embeddings before running dbow. [Lau & Baldwin, 2016] argue that even though dbow can in theory work with randomized word embeddings, this degrades performance severely in the tasks they have examined.", "An intuitive explanation can be traced back to the model\u2019s objective function, which is to maximize the dot product between the document embedding and its constituent word embeddings: if word embeddings are randomly distributed, it becomes more difficult to optimize the document embedding to be close to its more critical content words.", "Applications, implementations and enhancements[Le & Mikolov, 2014] demonstrated the use of paragraph vectors on several text classification and sentiment analysis tasks, while [Dai et al, 2015] examined it in the context of document similarity tasks and [Lau & Baldwin, 2016] benchmarked it against a forum question duplication task and the Semantic Textual Similarity (STS) SemEval shared task. Both later papers present an extended evaluation of the method (the former focusing on the PV-DBOW variant), comparing it to several other methods, and also giving practical advice (the later including code).", "The method has a Python implementation, as part of the gensim package, and a PyTorch implementation. Again, [Lau & Baldwin, 2016] also supplied the code used for their examination.", "Finally, various enhancements to the method have been proposed. For example, [Li et al, 2016] extend the method to also incorporate n-gram features, while [Thongtan & Phienthrakul, 2019] suggest using cosine similarity instead of dot product when computing the embedding projection (also providing a Java implementation).", "[Chen, 2017] presented an interesting approach inspired by both the distributed memory model of the paragraph vectors approach (PV-DM) and approaches that average word embeddings to represent documents.", "Similar to paragraph vectors, Doc2VecC (an acronym of documentvector through corruption) consists of an input layer, a projection layer and an output layer to predict the target word (\u201cceremony\u201d in the above example). The embeddings of neighboring words (e.g. \u201copening\u201d, \u201cfor\u201d, \u201cthe\u201d) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to paragraph vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (e.g. \u201cperformance\u201d at position p, \u201cpraised\u201d at position q, and \u201cbrazil\u201d at position r).", "Additionally, the authors choose to corrupt the original document by randomly removing a significant portion of words, representing the document by averaging only the embeddings of the remaining words. This corruption mechanism allows a speedup during training as it significantly reduces the number of parameters to update in back propagation. The authors also show how it introduces a special form of regularization, which they believe results in the observed performance improvement, benchmarked on a sentiment analysis task, a document classification task and a semantic relatedness task versus a plethora of state-of-the-art document embedding techniques.", "An open source C-based implementation of the method and code to reproduce the experiments in the paper can be found in a public Github repository.", "The general idea of corrupting, or adding noise, to the document embedding learning process to produce a more robust embedding space has also been applied by [Hill et al, 2016] to the skip-thought model (see the following sub-section) to create their sequential denoising autoencoder (SDAE) model.", "Presented in [Kiros et al, 2015], this is another early attempt to generalize word2vec, and was published with an official pure Python implementation (and recently also boasting implementations for PyTorch and TensorFlow).", "This, however, extends word2vec \u2014 specifically the skip-gram architecture \u2014 in another intuitive way: the base unit is now sentences, and an encoded sentence is used to predict the sentences around it. The vector representations are learned using an encoder-decoder model trained on the above task; the authors use an RNN encoder with GRU activations and RNN decoders with a conditional GRU. Two different decoders are trained for previous and next sentences.", "Vocabulary expansion in skip-thoughtThe skip-thought encoder uses a word embedding layer that converts each word in the input sentence to its corresponding word embedding, effectively converting the input sentence into a sequence of word embeddings. This embedding layer is also shared with both of the decoders.", "However, the authors only use a small vocabulary of 20,000 words, and as a result many unseen words might be encountered during use in various tasks. To overcome this, a mapping is learned from a word embedding space trained on a much larger vocabulary (e.g. word2vec) to the word embedding space of the skip-thoughts model, by solving an un-regularized L2 linear regression loss for the matrix W parameterizing this mapping.", "Applications, enhancements and further readingThe authors demonstrate the use of skip-thought vectors for semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and four sentiment and subjectivity datasets. [Broere, 2017] further investigates the syntactic properties of skip-thought sentence representations by training logistic regression on them to predict POS tags and dependency relations.", "[Tang et al, 2017a] propose a neighborhood approach to skip-thought, dropping ordering information and predicting both the previous and next sentence using a single decoder. [Tang et al, 2017b] expand this examination to propose three enhancements to the model they claim to provide comparable performance using a faster and lighter-weight model: (1) only learning to decode the next sentence, (2) adding an avg+max connection layer between encoder and decoder (as a way to allow non-linear non-parametric feature engineering), and (3) performing good word embedding initialization. Finally, [Gan et al, 2016] apply the same approach using a hierarchical CNN-LSTM-based encoder instead of an RNN-only one, across a broad range of applications.", "Another variation, presented in [Lee & Park, 2018], learns sentence embeddings by choosing, for each target sentence, influential sentences in the entire document based on document structure, thus identifying dependency structures of sentences using metadata or text styles. Additionally, [Hill et al, 2016] suggest the sequential denoising autoencoder (SDAE) model, a variant of skip-thought where input data is corrupted according to some noise function, and the model is trained to recover the original data from the corrupted data.", "For further non-academic reading on the skip-thought model, Sanyam Agarwa gives a great detailed overview of the method on his blog, and Ammar Zaher demonstrate its use to construct an embedding space for cooking recipes.", "[Hill et al, 2016] propose a significantly simpler variation on the skip-thoughts model; FastSent is a simple additive (log-bilinear) sentence model designed to exploit the same signal, but at a much lower computational expense. Given a BOW representation of some context sentence, the model simply predicts adjacent sentences (also represented as BOW). More formally, FastSent learns a source u\u1d42 and target v\u1d42 embedding for each word w in the model vocabulary. For training example S\u1d62\u208b\u2081,S\u1d62,S\u1d62\u208a\u2081 of consecutive sentences, S\u1d62 is represented as the sum of its source embeddings s\u1d62=\u2211u\u1d42 over w\u2208S\u1d62. The cost of the example is then simply \u2211 \ud835\udf19(s\u1d62,v\u1d42) over w\u2208S\u1d62\u208b\u2081\u222aS\u1d62\u208a\u2081, where \ud835\udf19 is the softmax function. The paper is accompanied by an official Python implementation.", "[Logeswaran & Lee, 2018] reformulate the document embedding task \u2014 the problem of predicting the context in which a sentence appears \u2014 as a supervised classification problem (see Figure 12b) rather than the prediction task of previous approaches (see Figure 12a).", "The gist is to use the meaning of the current sentence to predict the meanings of adjacent sentences, where meaning is represented by an embedding of the sentence computed from an encoding function; notice two encoders are learned here: f for the input sentence and g for candidates. Given an input sentence, it is encoded by an encoder (RNNs, in this case), but instead of generating the target sentence, the model chooses the correct target sentence from a set of candidate sentences; the candidate set is built from both valid context sentences (ground truth) and many other non-context sentences. Finally, the constructed training objective maximizes the probability of identifying the correct context sentences for each sentence in the training data. Viewing the former sentence prediction formulation as choosing a sentence from all possible sentences, this new approach can be seen as a discriminative approximation to the prediction problem.", "The authors evaluate their approach on various text classification, paraphrase identification and semantic relatedness tasks, and also provide an official Python implementation.", "A very recent method, coming out of IBM research, is Word Mover\u2019s Embedding (WME), presented in [Wu et al, 2018b]. An official C-based, Python-wrapped implementation is provided.", "[Kushner et al, 2015] presented Word Mover\u2019s Distance (WMD); this measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to \u201ctravel\u201d in the embedding space to reach the embedded words of another document (see Figure 13a). Additionally, [Wu et al, 2018a] proposed D2KE (distances to kernels and embeddings), a general methodology for the derivation of a positive-definite kernel from a given distance function.", "WME builds on three components to learn continuous vector representations for texts of varying lengths:", "Using these three components, the following approach is applied:", "This framework is extensible, since its two building blocks, word2vec and WMD, can be replaced by other techniques such as GloVe (for word embeddings) or S-WMD (for translation of the word embedding space into a document distance metric).", "The authors evaluate WME on 9 real-world text classification tasks and 22 textual similarity tasks, and demonstrate that it consistently matches, and sometimes even outperforms, other state-of-the art techniques.", "2018 in NLP was marked by the rise of the transformers (see Figure 14), state-of-the-art neural language models inspired by the transformer model presented in [Vaswani et al 2017] \u2014 a sequence model that dispenses of both convolutions and recurrence and uses attention instead to incorporate sequential information into sequence representation. This booming family includes BERT (and its extensions), GPT (1 and 2) and the XL-flavored transformers.", "These models generate contextual embeddings of input tokens (commonly sub-word units), each infused with information of its neighborhood, but are not aimed at generating a rich embedding space for input sequences. BERT even has a special [CLS] token whose output embedding is used for classification tasks, but still turns out to be a poor embedding of the input sequence for other tasks. [Reimers & Gurevych, 2019]", "Sentence-BERT, presented in [Reimers & Gurevych, 2019] and accompanied by a Python implementation, aims to adapt the BERT architecture by using siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity (see Figure 15).", "The unsupervised methods covered in the previous section allowed us to learn useful representations from large unlabelled corpora. Not unique to natural language processing, this approach focuses on learning representations by designing learning objectives that exploit labels that are freely available within the data. The strength and robustness of these methods thus depend heavily not only on the learning framework, but also on how well the artificially designed learning objective requires or brings about the learning of meaningful features or knowledge that would prove useful in various downstream tasks. For example, we expect both semantic and syntactic information to be captured well by word and document embedding spaces.", "The contrasting approach to learning meaningful representations of data \u2014 in our case word sequences \u2014 is to utilize explicit labels (almost always generated by human annotators in some way). Here, the relevancy to various tasks depends on how close are the explicit task and labels used to the final application, and again, how well does this task brings about the learning of generalizable features and knowledge.", "We\u2019ll see that supervised approaches range from those that directly utilize a specific labeled task to learn representations, to those that restructure tasks or extract new labeled tasks from them to elicit better representations.", "There have been various attempts to use labeled or structured data to learn sentence representations. Specifically, [Cho et al, 2014a] and [Sutskever et al, 2014] are perhaps the first attempts to apply the encoder-decoder approach to explicitly learn sentence/phrase embeddings with labelled data; the first using using Europarl, a parallel corpus of phrases for statistical machine translation, the second using the English to French translation task from the WMT-14 dataset. Another such notable attempt is presented in [Wieting et al, 2015] and [Wieting & Gimpel, 2017], where both word embeddings and their mapping into document embeddings are jointly learned to minimize cosine similarity between pairs of paraphrases (from the PPDB dataset). [Hill et al, 2015] trained neural language models to map dictionary definitions to pre-trained word embeddings of the words defined by those definitions. Finally, [Conneau et al, 2017] trained NN encoders of various architectures on the Stanford Natural Language Inference task (see Figure 16).", "Contextual embeddings for document similarityA specific case of the above approach is one driven by document similarity. [Das et al, 2016] showcase document embeddings learned to maximize similarity between two documents via a siamese network for community Q/A. (see Figure 17)", "Similarly, [Nicosia & Moschitti, 2017] use siamese networks to produce word representations while learning binary text similarity, considering examples in the same category as similar. (see Figure 18)", "Crosslingual reduced-rank ridge regression (Cr5)[Josifoski et al, 2019] introduce a method for embedding documents written in any language into a single, language-independent vector space. This is done by training a ridge-regression-based classifier that uses language-specific bag-of-word features in order to predict the concept that a given document is about. When constraining the learned weight matrix to be of low rank, the authors show it can be factored to obtain the desired mappings from language-specific bags-of-words to language-independent embeddings. An official Python implementation is provided.", "A common supervised method to produce document embeddings uses various neural network architectures, learning composition operators that map word vectors to document vectors; these are passed to a supervised task and depend on a class label in order to back-propagate through the composition weights (see Figure 19).", "Therefore, almost all hidden layers of the network can be considered to produce a vector embedding of an input document, with the prefix of the network up to that layer being the learned mapping from word vectors to the embedding space. A rigorous examination of the different ways to learn sentence vectors based on word vectors and a supervised learning task can be found in [Wieting et al, 2015].", "Note that while the word embeddings used can be pre-generated and task-agnostic (to a degree, at least), the mapping learned from them to document embedding is task specific. While these can be useful for related tasks, this method is bound to be less robust and generalized than unsupervised ones, at least in theory. [Kiros et al, 2015]", "Notable uses include sentiment classification using RNNs [Socher et al, 2013], various text classification tasks using CNNs [Kalchbrenner et al, 2014] [Kim, 2014] and both machine translation and text classification tasks using recursive-convolutional neural networks [Cho et al, 2014a, 2014b] [Zhao et al, 2015].", "GPT[Radford et al, 2018] presented the generative pre-training (GPT) approach (accompanied by a Python implementation), combining unsupervised and supervised representation learning, using the transformer model presented in [Vaswani et al 2017] to learn an unsupervised language model on unlabeled corpora, and then fine-tuning its use for each task separately using supervised data. They later presented GPT-2 in [Radford et al, 2019], focusing on bolstering the unsupervised learning portion of their work, an again releasing an official Python implementation.", "Deep Semantic Similarity Model (DSSM)A Microsoft Research project, DSSM is a deep neural network modeling technique for representing text strings in a continuous semantic space and modeling semantic similarity between two text strings (see Figure 20).", "DSSM was used, among other applications, to develop latent semantic models that project entities of different types (e.g., queries and documents) into a common low-dimensional semantic space for a variety of machine learning tasks such as ranking and classification. For example, [Huang et al, 2013] use it project queries and documents into a common low-dimensional space where the relevance of a document given query is computed as the distance between them.", "Implementations include TensorFlow, Keras and two PyTorch variations.", "[Ahmad et al, 2018] suggest that jointly learning sentence representations from multiple text classification tasks and combining them with pre-trained word-level and sentence-level encoders results in robust sentence representations that are useful for transfer learning", "[Yu & Jiang, 2016] similarly show that using two auxiliary tasks to help induce a sentence embedding supposedly works well across domains for sentiment classification, jointly learning this sentence embedding together with the sentiment classifier itself (Figure 21).", "Universal Sentence EncoderPresented in [Cer et al, 2018a] and [Cer et al, 2018b], and accompanied by a TensorFlow implementation, this method actually includes two possible models for sentence representation learning: The Transformer model and the Deep Averaging Network (DAN) model (see Figure 22). Both are designed to allow multi-task learning, with supported tasks including (1) a skip-thought like task for unsupervised learning; (2) a conversational input-response task for the inclusion of parsed conversational data; and (3) classification tasks for training on supervised data (see the previous sub-section). The authors focus on experiments with transfer learning tasks, and benchmark their models versus simple CNN and DAN baselines. The method was later extended to address multilingual settings.", "The transformer model is directly based on the transformer model presented in [Vaswani et al 2017], the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention (see Figure 22a).", "The model constructs sentence embeddings using the encoding sub-graph of the transformer architecture. The encoder uses attention to compute context-aware representations of words in a sentence that take into account both the ordering and identity of other words. The context-aware word representations are averaged together to obtain a sentence-level embedding.", "Conversely, in the DAN model, presented in [Iyyer et al, 2015], input embeddings for words and bi-grams are first averaged together and then passed through a feedforward deep neural network (DNN) to produce sentence embeddings (see Figure 22b).", "GenSenMuch like the Universal Sentence Encoder, the GenSen approach, presented in [Subramanian et al, 2018] together with an official Python implementation, combines multiple supervised and unsupervised learning task to train an RNN w/ GRU based encoder-decoder model from which the embedding is extracted. The four supported tasks are: (1) Skip-thought vectors, (2) neural machine translation, (3) constituency parsing, and (4) natural language inference (a 3-way classification problem; given a premise and a hypothesis sentence, the objective is to classify their relationship as either entailment, contradiction, or neutral). An official Python implementation was published.", "I have no easy answers here, but here are a few possible takeaways:", "That\u2019s it! As always, I\u2019m sure that the posts I write are not complete, so feel free to suggest corrections and additions to the above overview, either by commenting here or contacting me directly. :)", "I also want to thank both Adam Bali and Ori Cohen, who have provided very valuable feedback. Go read their posts!", "Finally, I found it worthwhile mentioning that Papers With Code has a task dedicated to document embedding, and that Facebook Research has open-sourced SentEval, an evaluation toolkit for sentence representations presented in [Conneau & Kiela, 2018].", "Now sit back, and let the references overwhelm you. \ud83d\udcd5 \ud83d\udcd7 \ud83d\udcd8 \ud83d\udcd9", "Arora, S., Liang, Y., & Ma, T. (2016). A simple but tough-to-beat baseline for sentence embeddings. [unofficial implementation]", "B. Broere, (2017). Syntactic properties of skip-thought vectors. Master\u2019s thesis, Tilburg University.", "Das, A., Yenala, H., Chinnakotla, M., & Shrivastava, M. (2016, August). Together we stand: Siamese networks for similar question retrieval. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 378\u2013387).", "Hill, F., Cho, K., Korhonen, A., & Bengio, Y. (2015). Learning to understand phrases by embedding the dictionary. Transactions of the Association for Computational Linguistics, 4, 17\u201330.", "Huang, P. S., He, X., Gao, J., Deng, L., Acero, A., & Heck, L. (2013, October). Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management (pp. 2333\u20132338). ACM.", "Iyyer, M., Manjunatha, V., Boyd-Graber, J., & Daum\u00e9 III, H. (2015). Deep unordered composition rivals syntactic methods for text classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (Vol. 1, pp. 1681\u20131691).", "Josifoski, M., Paskov, I. S., Paskov, H. S., Jaggi, M., & West, R. (2019, January). Crosslingual Document Embedding as Reduced-Rank Ridge Regression. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (pp. 744\u2013752). ACM.", "Lee, T., & Park, Y. (2018). UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT.", "Liu, Y., & Lapata, M. (2018). Learning structured text representations. Transactions of the Association for Computational Linguistics, 6, 63\u201375.", "Nicosia, M., & Moschitti, A. (2017, August). Learning contextual embeddings for structural semantic similarity using categorical information. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)(pp. 260\u2013270).", "Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understanding with unsupervised learning. Technical report, OpenAI.", "Thongtan, T., & Phienthrakul, T. (2019, July). Sentiment Classification using Document Embeddings trained with Cosine Similarity. In Proceedings of the 57th Conference of the Association for Computational Linguistics: Student Research Workshop (pp. 407\u2013414).", "Yu, J., & Jiang, J. (2016, November). Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification. In Proceedings of the 2016 conference on empirical methods in natural language processing (pp. 236\u2013246).", "Zhao, H., Lu, Z., & Poupart, P. (2015, June). Self-adaptive hierarchical sentence model. In Twenty-Fourth International Joint Conference on Artificial Intelligence.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Science Consultant. Teacher @ Tel Aviv University's business school. CEO @ Datahack nonprofit. www.shaypalachy.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffed3e7a6a25d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shay-palachy.medium.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": ""}, {"url": "https://shay-palachy.medium.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Shay Palachy Affek"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed0e9ae905e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&user=Shay+Palachy+Affek&userId=ed0e9ae905e3&source=post_page-ed0e9ae905e3----fed3e7a6a25d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.bigpanda.io/", "anchor_text": "BigPanda"}, {"url": "http://www.shaypalachy.com/", "anchor_text": "I consulted for several years"}, {"url": "https://cs.stanford.edu/~quocle/paragraph_vector.pdf", "anchor_text": "Le & Mikolov, 2014"}, {"url": "https://arxiv.org/pdf/1507.07998.pdf", "anchor_text": "Dai et al, 2015"}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016"}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/Main_Page", "anchor_text": "the Semantic Textual Similarity (STS) SemEval shared task"}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "Kiros et al, 2015"}, {"url": "https://arxiv.org/pdf/1810.09302.pdf", "anchor_text": "Chen et al, 2018"}, {"url": "https://github.com/ncbi-nlp/BioSentVec", "anchor_text": "official Python implementation"}, {"url": "https://www.microsoft.com/en-us/research/project/dssm/", "anchor_text": "Deep Semantic Similarity Model was used by various authors"}, {"url": "https://en.wikipedia.org/wiki/Universal_approximation_theorem", "anchor_text": "which can approximate a wide range of mappings"}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "the GLUE leaderboard"}, {"url": "https://en.wikipedia.org/wiki/Multiset", "anchor_text": "multiset"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "term frequency\u2013inverse document frequency"}, {"url": "http://pmcnamee.net/744/papers/SaltonBuckley.pdf", "anchor_text": "Salton & Buckley, 1988"}, {"url": "https://en.wikipedia.org/wiki/Dirichlet_distribution", "anchor_text": "Dirichlet distribution"}, {"url": "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization", "anchor_text": "non-negative matrix factorization (NMF)"}, {"url": "https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis", "anchor_text": "probabilistic latent semantic indexing (PLSI)"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Bengio, 2003"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "well-written two-part tutorial of word2vec by Chris McCormick"}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "part 2"}, {"url": "http://www.scholarpedia.org/article/Neural_net_language_models", "anchor_text": "the Scholarpedia article on neural net language models by Prof. Joshua Bengio"}, {"url": "https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc", "anchor_text": "Hunter Heidenreich\u2019s post for a more general and concise overview of word embeddings in general"}, {"url": "http://alexminnaar.com/2015/05/18/word2vec-tutorial-continuousbow.html", "anchor_text": "Alex Minnar\u2019s two part post"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Bengio, 2003"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Mikolov et al, 2013a"}, {"url": "https://www.aclweb.org/anthology/D14-1162", "anchor_text": "Pennington et al, 2014"}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/w/index.php?title=Semantic_theory&action=edit&redlink=1", "anchor_text": "semantic theory"}, {"url": "https://en.wikipedia.org/wiki/J._R._Firth", "anchor_text": "Firth"}, {"url": "https://en.wikipedia.org/wiki/Statistical_semantics", "anchor_text": "statistical semantics"}, {"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "Mikolov et al, 2013b"}, {"url": "https://towardsdatascience.com/word2vec-for-phrases-learning-embeddings-for-more-than-one-word-727b6cf723cf", "anchor_text": "a wonderful practical review of this approach"}, {"url": "https://arxiv.org/pdf/1606.04640.pdf", "anchor_text": "Kenter et al, 2016"}, {"url": "https://arxiv.org/pdf/1606.04640.pdf", "anchor_text": "Kenter et al, 2016"}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016"}, {"url": "http://wwwusers.di.uniroma1.it/~navigli/pubs/KBS_Sinoaraetal_2019.pdf", "anchor_text": "Sinoara et al, 2019"}, {"url": "https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf", "anchor_text": "Arora et al, 2016"}, {"url": "https://github.com/peter3125/sentence2vec", "anchor_text": "Python implementation"}, {"url": "https://aclweb.org/anthology/N18-1049", "anchor_text": "Pagliardini et al, 2017"}, {"url": "https://www.aclweb.org/anthology/N19-1098", "anchor_text": "Gupta et al, 2019"}, {"url": "https://github.com/epfml/sent2vec", "anchor_text": "an official C++-based Python implementation"}, {"url": "https://link.springer.com/article/10.1186/s12859-018-2496-4", "anchor_text": "Agibetov et al, 2018"}, {"url": "https://cs.stanford.edu/~quocle/paragraph_vector.pdf", "anchor_text": "Le & Mikolov, 2014"}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "its Gensim implementation"}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016"}, {"url": "http://ixa2.si.ehu.es/stswiki/index.php/Main_Page", "anchor_text": "Semantic Textual Similarity (STS) SemEval"}, {"url": "https://github.com/jhlau/doc2vec", "anchor_text": "including code"}, {"url": "https://radimrehurek.com/gensim/models/doc2vec.html", "anchor_text": "a Python implementation, as part of the gensim package"}, {"url": "https://github.com/inejc/paragraph-vectors", "anchor_text": "a PyTorch implementation"}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "Lau & Baldwin, 2016"}, {"url": "https://github.com/jhlau/doc2vec", "anchor_text": "supplied the code used for their examination"}, {"url": "https://arxiv.org/abs/1512.08183", "anchor_text": "Li et al, 2016"}, {"url": "https://github.com/tanthongtan/dv-cosine", "anchor_text": "a Java implementation"}, {"url": "https://arxiv.org/pdf/1707.02377.pdf", "anchor_text": "Chen, 2017"}, {"url": "https://github.com/mchen24/iclr2017", "anchor_text": "a public Github repository"}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016"}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "Kiros et al, 2015"}, {"url": "https://github.com/ryankiros/skip-thoughts", "anchor_text": "an official pure Python implementation"}, {"url": "https://github.com/sanyam5/skip-thoughts", "anchor_text": "PyTorch"}, {"url": "https://github.com/tensorflow/models/tree/master/research/skip_thoughts", "anchor_text": "TensorFlow"}, {"url": "https://sourcediving.com/building-recipe-skill-representations-using-skip-thought-vectors-8a6e4c38ae6c", "anchor_text": "Ammar Zaher\u2019s post"}, {"url": "http://arno.uvt.nl/show.cgi?fid=146003", "anchor_text": "Broere, 2017"}, {"url": "https://arxiv.org/abs/1706.03146", "anchor_text": "Tang et al, 2017a"}, {"url": "https://www.groundai.com/project/trimming-and-improving-skip-thought-vectors/1", "anchor_text": "Tang et al, 2017b"}, {"url": "https://arxiv.org/pdf/1611.07897.pdf", "anchor_text": "Gan et al, 2016"}, {"url": "https://openreview.net/pdf?id=H1a37GWCZ", "anchor_text": "Lee & Park, 2018"}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016"}, {"url": "http://sanyam5.github.io/my-thoughts-on-skip-thoughts/", "anchor_text": "Sanyam Agarwa gives a great detailed overview of the method on his blog"}, {"url": "https://sourcediving.com/building-recipe-skill-representations-using-skip-thought-vectors-8a6e4c38ae6c", "anchor_text": "Ammar Zaher demonstrate its use to construct an embedding space for cooking recipes"}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Hill et al, 2016"}, {"url": "https://en.wikipedia.org/wiki/%E2%88%91", "anchor_text": "\u2211"}, {"url": "https://en.wikipedia.org/wiki/%E2%88%91", "anchor_text": "\u2211"}, {"url": "https://github.com/fh295/SentenceRepresentation", "anchor_text": "an official Python implementation"}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "Logeswaran & Lee, 2018"}, {"url": "https://github.com/lajanugen/S2V", "anchor_text": "an official Python implementation"}, {"url": "https://arxiv.org/pdf/1811.01713v1.pdf", "anchor_text": "Wu et al, 2018b"}, {"url": "https://github.com/IBM/WordMoversEmbeddings", "anchor_text": "An official C-based, Python-wrapped implementation is provided"}, {"url": "http://proceedings.mlr.press/v37/kusnerb15.pdf", "anchor_text": "Kushner et al, 2015"}, {"url": "https://arxiv.org/pdf/1802.04956.pdf", "anchor_text": "Wu et al, 2018a"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017"}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Reimers & Gurevych, 2019"}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Reimers & Gurevych, 2019"}, {"url": "https://github.com/UKPLab/sentence-transformers", "anchor_text": "a Python implementation"}, {"url": "https://www.aclweb.org/anthology/D14-1179", "anchor_text": "Cho et al, 2014a"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sutskever et al, 2014"}, {"url": "https://arxiv.org/pdf/1511.08198.pdf", "anchor_text": "Wieting et al, 2015"}, {"url": "http://paraphrase.org/#/", "anchor_text": "the PPDB dataset"}, {"url": "https://arxiv.org/pdf/1504.00548.pdf", "anchor_text": "Hill et al, 2015"}, {"url": "https://www.aclweb.org/anthology/D17-1070.pdf", "anchor_text": "Conneau et al, 2017"}, {"url": "https://www.aclweb.org/anthology/P16-1036", "anchor_text": "Das et al, 2016"}, {"url": "https://www.aclweb.org/anthology/K17-1027", "anchor_text": "Nicosia & Moschitti, 2017"}, {"url": "https://www.aclweb.org/anthology/K17-1027", "anchor_text": "Nicosia & Moschitti, 2017"}, {"url": "https://github.com/epfl-dlab/Cr5", "anchor_text": "An official Python implementation"}, {"url": "https://arxiv.org/pdf/1511.08198.pdf", "anchor_text": "Wieting et al, 2015"}, {"url": "https://arxiv.org/pdf/1506.06726.pdf", "anchor_text": "Kiros et al, 2015"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Radford et al, 2018"}, {"url": "https://openai.com/blog/language-unsupervised/", "anchor_text": "presented the generative pre-training (GPT) approach"}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "accompanied by a Python implementation"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "They later presented GPT-2"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Radford et al, 2019"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "releasing an official Python implementation"}, {"url": "https://www.microsoft.com/en-us/research/project/dssm/", "anchor_text": "A Microsoft Research project"}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf", "anchor_text": "Huang et al, 2013"}, {"url": "https://kishorepv.github.io/DSSM/", "anchor_text": "TensorFlow"}, {"url": "https://github.com/airalcorn2/Deep-Semantic-Similarity-Model", "anchor_text": "Keras"}, {"url": "https://github.com/nishnik/Deep-Semantic-Similarity-Model-PyTorch", "anchor_text": "two PyTorch"}, {"url": "https://github.com/moinnadeem/CDSSM", "anchor_text": "variations"}, {"url": "https://arxiv.org/pdf/1810.00681v1.pdf", "anchor_text": "Ahmad et al, 2018"}, {"url": "https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0", "anchor_text": "Yu & Jiang, 2016"}, {"url": "https://arxiv.org/pdf/1803.11175.pdf", "anchor_text": "Cer et al, 2018a"}, {"url": "https://www.aclweb.org/anthology/D18-2029/", "anchor_text": "Cer et al, 2018b"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "a TensorFlow implementation"}, {"url": "https://ai.googleblog.com/2019/07/multilingual-universal-sentence-encoder.html", "anchor_text": "extended to address multilingual settings"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Vaswani et al 2017"}, {"url": "https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf", "anchor_text": "Iyyer et al, 2015"}, {"url": "https://arxiv.org/pdf/1804.00079.pdf", "anchor_text": "Subramanian et al, 2018"}, {"url": "https://github.com/Maluuba/gensen", "anchor_text": "an official Python implementation"}, {"url": "https://github.com/Maluuba/gensen", "anchor_text": "An official Python implementation was published"}, {"url": "https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf", "anchor_text": "Arora et al, 2016"}, {"url": "https://github.com/facebookresearch/SentEval", "anchor_text": "SentEval, an evaluation toolkit for sentence representations"}, {"url": "https://arxiv.org/pdf/1803.05449.pdf", "anchor_text": "Conneau & Kiela, 2018"}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "Logeswaran & Lee, 2018"}, {"url": "https://arxiv.org/pdf/1811.01713v1.pdf", "anchor_text": "Wu et al, 2018b"}, {"url": "http://www.shaypalachy.com/", "anchor_text": "contacting me directly"}, {"url": "https://medium.com/u/d12e803b524e?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Adam Bali"}, {"url": "https://medium.com/u/4dde5994e6c1?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Ori Cohen"}, {"url": "https://paperswithcode.com/task/document-embedding", "anchor_text": "a task dedicated to document embedding"}, {"url": "https://github.com/facebookresearch/SentEval", "anchor_text": "SentEval, an evaluation toolkit for sentence representations"}, {"url": "https://arxiv.org/pdf/1803.05449.pdf", "anchor_text": "Conneau & Kiela, 2018"}, {"url": "https://link.springer.com/article/10.1186/s12859-018-2496-4", "anchor_text": "Fast and scalable neural embedding models for biomedical sentence classification"}, {"url": "https://arxiv.org/pdf/1810.00681v1.pdf", "anchor_text": "Learning Robust, Transferable Sentence Representations for Text Classification"}, {"url": "https://pdfs.semanticscholar.org/3fc9/7768dc0b36449ec377d6a4cad8827908d5b4.pdf", "anchor_text": "A simple but tough-to-beat baseline for sentence embeddings"}, {"url": "https://github.com/peter3125/sentence2vec", "anchor_text": "unofficial implementation"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "A neural probabilistic language model"}, {"url": "http://arno.uvt.nl/show.cgi?fid=146003", "anchor_text": "Syntactic properties of skip-thought vectors"}, {"url": "https://arxiv.org/pdf/1803.11175.pdf", "anchor_text": "Universal sentence encoder"}, {"url": "https://www.aclweb.org/anthology/D18-2029/", "anchor_text": "Universal sentence encoder for English"}, {"url": "https://arxiv.org/pdf/1707.02377.pdf", "anchor_text": "Efficient vector representation for documents through corruption"}, {"url": "https://arxiv.org/pdf/1810.09302.pdf", "anchor_text": "BioSentVec: creating sentence embeddings for biomedical texts"}, {"url": "https://arxiv.org/abs/1406.1078", "anchor_text": "Learning phrase representations using RNN encoder-decoder for statistical machine translation"}, {"url": "https://arxiv.org/abs/1409.1259", "anchor_text": "On the properties of neural machine translation: Encoder-decoder approaches"}, {"url": "https://www.aclweb.org/anthology/D17-1070.pdf", "anchor_text": "Supervised learning of universal sentence representations from natural language inference data"}, {"url": "https://arxiv.org/pdf/1803.05449.pdf", "anchor_text": "Senteval: An evaluation toolkit for universal sentence representations"}, {"url": "https://arxiv.org/pdf/1507.07998.pdf", "anchor_text": "Document embedding with paragraph vectors"}, {"url": "https://www.aclweb.org/anthology/P16-1036", "anchor_text": "Together we stand: Siamese networks for similar question retrieval"}, {"url": "https://arxiv.org/pdf/1611.07897.pdf", "anchor_text": "Learning generic sentence representations using convolutional neural networks"}, {"url": "https://www.aclweb.org/anthology/N19-1098", "anchor_text": "Better Word Embeddings by Disentangling Contextual n-Gram Information"}, {"url": "https://arxiv.org/pdf/1504.00548.pdf", "anchor_text": "Learning to understand phrases by embedding the dictionary"}, {"url": "https://www.aclweb.org/anthology/N16-1162", "anchor_text": "Learning distributed representations of sentences from unlabelled data"}, {"url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf", "anchor_text": "Learning deep structured semantic models for web search using clickthrough data"}, {"url": "https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf", "anchor_text": "Deep unordered composition rivals syntactic methods for text classification"}, {"url": "https://dl.acm.org/citation.cfm?id=3291023", "anchor_text": "Crosslingual Document Embedding as Reduced-Rank Ridge Regression"}, {"url": "https://arxiv.org/pdf/1606.04640.pdf", "anchor_text": "Siamese cbow: Optimizing word embeddings for sentence representations"}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "Skip-thought vectors"}, {"url": "http://proceedings.mlr.press/v37/kusnerb15.pdf", "anchor_text": "From word embeddings to document distances"}, {"url": "https://arxiv.org/pdf/1607.05368.pdf", "anchor_text": "An empirical evaluation of doc2vec with practical insights into document embedding generation"}, {"url": "https://github.com/jhlau/doc2vec", "anchor_text": "code"}, {"url": "https://cs.stanford.edu/~quocle/paragraph_vector.pdf", "anchor_text": "Distributed representations of sentences and documents"}, {"url": "https://openreview.net/forum?id=H1a37GWCZ", "anchor_text": "UNSUPERVISED SENTENCE EMBEDDING USING DOCUMENT STRUCTURE-BASED CONTEXT"}, {"url": "https://arxiv.org/pdf/1803.02893.pdf", "anchor_text": "An efficient framework for learning sentence representations"}, {"url": "https://arxiv.org/abs/1512.08183", "anchor_text": "Learning document embeddings by predicting n-grams for sentiment classification of long movie reviews"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "Efficient estimation of word representations in vector space"}, {"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "Distributed representations of words and phrases and their compositionality"}, {"url": "https://www.aclweb.org/anthology/K17-1027", "anchor_text": "Learning contextual embeddings for structural semantic similarity using categorical information"}, {"url": "https://aclweb.org/anthology/N18-1049", "anchor_text": "Unsupervised learning of sentence embeddings using compositional n-gram features"}, {"url": "https://www.aclweb.org/anthology/D14-1162", "anchor_text": "Glove: Global vectors for word representation"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "Improving language understanding with unsupervised learning"}, {"url": "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf", "anchor_text": "Language models are unsupervised multitask learners"}, {"url": "https://arxiv.org/pdf/1908.10084.pdf", "anchor_text": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"}, {"url": "http://pmcnamee.net/744/papers/SaltonBuckley.pdf", "anchor_text": "Term-weighting approaches in automatic text retrieval"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0950705118305124", "anchor_text": "Knowledge-enhanced document embeddings for text classification"}, {"url": "https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf", "anchor_text": "Recursive deep models for semantic compositionality over a sentiment treebank"}, {"url": "https://arxiv.org/pdf/1804.00079.pdf", "anchor_text": "Learning general purpose distributed sentence representations via large scale multi-task learning"}, {"url": "https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf", "anchor_text": "Sequence to sequence learning with neural networks"}, {"url": "https://arxiv.org/abs/1706.03146", "anchor_text": "Rethinking skip-thought: A neighborhood based approach"}, {"url": "https://www.groundai.com/project/trimming-and-improving-skip-thought-vectors/1", "anchor_text": "Trimming and improving skip-thought vectors"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://arxiv.org/pdf/1511.08198.pdf", "anchor_text": "Towards universal paraphrastic sentence embeddings"}, {"url": "https://arxiv.org/pdf/1802.04956.pdf", "anchor_text": "D2ke: From distance to kernel and embedding"}, {"url": "https://arxiv.org/pdf/1811.01713v1.pdf", "anchor_text": "Word Mover\u2019s Embedding: From Word2Vec to Document Embedding"}, {"url": "https://www.semanticscholar.org/paper/Learning-Sentence-Embeddings-with-Auxiliary-Tasks-Yu-Jiang/2d38f7aab07d4435b2110602db4138ef20da4cc0", "anchor_text": "Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----fed3e7a6a25d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----fed3e7a6a25d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----fed3e7a6a25d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----fed3e7a6a25d---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/data-science?source=post_page-----fed3e7a6a25d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&user=Shay+Palachy+Affek&userId=ed0e9ae905e3&source=-----fed3e7a6a25d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&user=Shay+Palachy+Affek&userId=ed0e9ae905e3&source=-----fed3e7a6a25d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffed3e7a6a25d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fed3e7a6a25d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fed3e7a6a25d--------------------------------", "anchor_text": ""}, {"url": "https://shay-palachy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shay-palachy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shay Palachy Affek"}, {"url": "https://shay-palachy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "http://www.shaypalachy.com", "anchor_text": "www.shaypalachy.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed0e9ae905e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&user=Shay+Palachy+Affek&userId=ed0e9ae905e3&source=post_page-ed0e9ae905e3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F738a5f2bbbb2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdocument-embedding-techniques-fed3e7a6a25d&newsletterV3=ed0e9ae905e3&newsletterV3Id=738a5f2bbbb2&user=Shay+Palachy+Affek&userId=ed0e9ae905e3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}