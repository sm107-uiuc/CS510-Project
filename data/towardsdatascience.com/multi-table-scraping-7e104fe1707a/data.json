{"url": "https://towardsdatascience.com/multi-table-scraping-7e104fe1707a", "time": 1683011716.870869, "path": "towardsdatascience.com/multi-table-scraping-7e104fe1707a/", "webpage": {"metadata": {"title": "Multi-Table Scraping!. What to do when you want info from\u2026 | by Kate Christensen | Towards Data Science", "h1": "Multi-Table Scraping!", "description": "For the past few weeks now, I\u2019ve been writing about how to scrape different things from a Wikipedia page. Three weeks ago, I wrote about how to scrape a single table from a Wikipedia article. Two\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/using-beautifulsoup-on-wikipedia-dd0c620d5861?source=friends_link&sk=ba9fd2d3ddd3d5dc2fab2433f6848b81", "anchor_text": "how to scrape a single table from a Wikipedia article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/scraping-from-all-over-wikipedia-4aecadcedf11?source=friends_link&sk=f5f047f39c7a4685afb39e67781cfc22", "anchor_text": "how to use one script to scrape data from multiple pages across Wikipedia", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/scraping-tables-without-text-881eb7ba12fc?source=friends_link&sk=db2c4f87fe794a17ce387de7950d707a", "anchor_text": "craping a table that uses colors instead of text to display data", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/The_Great_British_Bake_Off_(series_1)", "anchor_text": "season one of the Great British Bake Off", "paragraph_index": 1}], "all_paragraphs": ["For the past few weeks now, I\u2019ve been writing about how to scrape different things from a Wikipedia page. Three weeks ago, I wrote about how to scrape a single table from a Wikipedia article. Two weeks ago, I wrote about how to use one script to scrape data from multiple pages across Wikipedia. Finally, last week I wrote about scraping a table that uses colors instead of text to display data. This week, I\u2019m going to talk about how to scrape data from multiple tables.", "As we have for the past three weeks, we\u2019re going to be working with the article about season one of the Great British Bake Off. This week, our objective will be to access the information about how bakers placed in the technical challenge. For those of you unfamiliar with the format of the show, each episode has the bakers go through three challenges, a signature, a technical, and a showstopper. The technical challenge is where all the bakers bake the same thing from a recipe written by one of the judges, and those bakes are judged anonymously and ranked from worst to best. On the article, the technical challenge results are displayed in a table, shown below:", "Getting information from one chart is easy enough, and you can go back to my first article to refresh yourself on how that works. However, what do you do when you want to get the technical challenge information for all the episodes? That will be discussed below", "As we\u2019ve been doing, we\u2019re going to start with importing packages and accessing the website using BeautifulSoup", "Additionally, to get our name list of bakers, we\u2019re going to first isolate the elimination chart (the one we worked with last week).", "Then, we\u2019re going to grab the bakers\u2019 names from that chart by doing the following:", "I won\u2019t get too into how I got this list, since this isn\u2019t the emphasis of this article, but essentially, if an item in the elimination chart has the attribute \u201calign,\u201d then the text of that item is a contestant name. Also if \u201calign\u201d is in the style part of an item, the text is also a contestant name. These names are added to a list, and after the names have been scraped, they all go through .rstrip() to get rid of any newline characters. This leaves us with a contestant name list that looks like the following:", "Now all we have to do is grab all the technical bake results. Here is the full code that we\u2019ll be walking through:", "And here is the final output of the code above:", "Now let\u2019s get into the code above. The first three lines state three variables, \u201cep_counter,\u201d \u201ctech_results_list,\u201d and \u201cep_tech,\u201d and are a zero, empty list, and a dictionary respectively. The dictionary starts with the key as 1, and the value is a dictionary where the keys are the contestant names and the values are all zero. This is made because if you\u2019re using this data to create a prediction model, you want to get the technical rankings from the most recent previous episode, not the current one you\u2019re analyzing. Obviously, going into the first episode, the contestants haven\u2019t done a technical bake and were not ranked, therefore they\u2019ve all been assigned zeroes.", "From here, we start scraping. The first two lines (5\u20136) find all the headings that have \u201cEpisode\u201d in them. Then we create two empty lists, names and place. Names will have the contestant names from a given episode and place will have how the contestant placed in the technical bake in said episode. From here we do the following:", "Essentially what we\u2019re doing is finding the episodes that have a technical bake (line 2 above), then we take the full table and name it \u201cresult_tables\u201d and we add one to the episode counter. Finally, we create \u201cep_list\u201d, a list of numbers that starts at two and ends at the last episode (6 in this case). Then we dive into the result table:", "The way we start is by finding the column that contains the contestant names, which we do by saying, \u201cif a word in the text of an item matches an item on the contestant name list, add that item\u2019s text to the names list.\u201d Then we go two columns over to get how they placed in their technical. However, you may have noticed in the article that there is one slight issue:", "In the first episode, only the top and bottom three got ranked, so we don\u2019t know who got 4th, 5th, 6th, and 7th place. Therefore, when I append an item\u2019s text to the place list, I tell the regex split to either get the number, which might have one or two digits (in the case of the number ten), or to get \u201cN/A.\u201d", "With that in mind, how do we deal with missing values like the ones above? The approach I chose to take is by taking the average of the values above and assigning that average to the missing values, which I did using the code below:", "At this point, I would be dealing with a full list of placements, so I start by saying if there is an \u201cN/A\u201d in the place list, do the following. Then I create a number list (num_list) by isolating all the numbers in that list and making them all integers. Then, I take the average of that list (ave) and replacing all the \u201cN/A\u201d values in that place list with that average.", "Now that we have a list of names and all these names have placement numbers that correspond to them, we can put it all together using the following:", "First, we make all items in the place list floats, since they were previously strings, unless they were the average value that was substituted for the \u201cN/A.\u201d Then, we create our dictionary (episode_placement) where the keys are contestant names and the values are how they placed in the technical. Then, we add that dictionary to the tech results list we created at the very beginning.", "Finally we\u2019re able to update that ep_tech dictionary we started at the beginning. If you recall, that dictionary started with a key of one and a value of a dictionary containing all the contestants names with values of 0. Now that we have all the info for the following episodes we can do the following:", "By doing this, we take that ep_list, which counts all the episodes with a technical challenge and make the items in that list keys, while the values are the dictionaries of names and placements in each episode. This leads us to that output shown above:", "Now, say you had a preexisting dataframe that has other data about the episode, such as age and hometown info about the contestants and how they performed in previous episodes. I\u2019ve previously created this dataframe, and here is a snapshot of how it looks:", "Here is how I used that dictionary to create a technical placement column:", "Walking though the code, I start by making a dictionary where the keys are tuples with a name and an episode and the value is how that contestant placed on that episode (lines 1\u20134). Then I create an empty list, placement_list, which is what will become the technical column. Then, I create a list of tuples where each tuple is a contestant name and episode number for each row in the dataframe shown above (line 6\u20137). From there, I say for each item in that list of tuples, look up its value in the tech_tuple dictionary and add that value to the placement list. Now that I have a full list of technical placements, I just create a new column \u201ctechnical\u201d and assign it the placement list. That is how we get the following:", "In this article I\u2019ve shown how to look at many different areas of a page instead of just one chart or section of the page. Wikipedia pages are formatted in such a way that they are generally readable and easy for a user to follow, but that also means the data you may be looking for needs to be formatted and condensed. By doing this, you can take that readable information and make it processable.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7e104fe1707a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@katec125?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@katec125?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Kate Christensen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F103f2c837773&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&user=Kate+Christensen&userId=103f2c837773&source=post_page-103f2c837773----7e104fe1707a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@clintadair?utm_source=medium&utm_medium=referral", "anchor_text": "Clint Adair"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/using-beautifulsoup-on-wikipedia-dd0c620d5861?source=friends_link&sk=ba9fd2d3ddd3d5dc2fab2433f6848b81", "anchor_text": "how to scrape a single table from a Wikipedia article"}, {"url": "https://towardsdatascience.com/scraping-from-all-over-wikipedia-4aecadcedf11?source=friends_link&sk=f5f047f39c7a4685afb39e67781cfc22", "anchor_text": "how to use one script to scrape data from multiple pages across Wikipedia"}, {"url": "https://towardsdatascience.com/scraping-tables-without-text-881eb7ba12fc?source=friends_link&sk=db2c4f87fe794a17ce387de7950d707a", "anchor_text": "craping a table that uses colors instead of text to display data"}, {"url": "https://en.wikipedia.org/wiki/The_Great_British_Bake_Off_(series_1)", "anchor_text": "season one of the Great British Bake Off"}, {"url": "https://medium.com/tag/python?source=post_page-----7e104fe1707a---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----7e104fe1707a---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/technology?source=post_page-----7e104fe1707a---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7e104fe1707a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/wikipedia?source=post_page-----7e104fe1707a---------------wikipedia-----------------", "anchor_text": "Wikipedia"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&user=Kate+Christensen&userId=103f2c837773&source=-----7e104fe1707a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&user=Kate+Christensen&userId=103f2c837773&source=-----7e104fe1707a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7e104fe1707a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7e104fe1707a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7e104fe1707a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7e104fe1707a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7e104fe1707a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@katec125?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@katec125?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kate Christensen"}, {"url": "https://medium.com/@katec125/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "129 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F103f2c837773&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&user=Kate+Christensen&userId=103f2c837773&source=post_page-103f2c837773--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7f9729f0c45c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-table-scraping-7e104fe1707a&newsletterV3=103f2c837773&newsletterV3Id=7f9729f0c45c&user=Kate+Christensen&userId=103f2c837773&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}