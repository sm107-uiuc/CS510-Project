{"url": "https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d", "time": 1682994323.8952792, "path": "towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d/", "webpage": {"metadata": {"title": "Breaking the curse of small datasets in Machine Learning: Part 1 | by Jyoti Prakash Maheswari | Towards Data Science", "h1": "Breaking the curse of small datasets in Machine Learning: Part 1", "description": "This is Part 1 of Breaking the curse of small datasets in Machine Learning. In this part, I will discuss how the size of the data set impacts traditional Machine Learning algorithms and few ways to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/breaking-the-curse-of-small-data-sets-in-machine-learning-part-2-894aa45277f4", "anchor_text": "In Part 2", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "iris dataset", "paragraph_index": 14}, {"url": "https://scikit-learn.org/stable/documentation.html", "anchor_text": "Scikit-learn\u2019s", "paragraph_index": 20}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html", "anchor_text": "OneClassSVM", "paragraph_index": 21}, {"url": "https://anomaly.io/anomaly-detection-clustering/", "anchor_text": "Clustering methods", "paragraph_index": 21}, {"url": "https://www.coursera.org/lecture/machine-learning/anomaly-detection-using-the-multivariate-gaussian-distribution-DnNr9", "anchor_text": "Gaussian Anomaly detection", "paragraph_index": 21}, {"url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "anchor_text": "documentation", "paragraph_index": 21}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/", "anchor_text": "blog", "paragraph_index": 22}, {"url": "https://imbalanced-learn.org/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html", "anchor_text": "imblearn", "paragraph_index": 22}, {"url": "http://rikunert.com/SMOTE_explained", "anchor_text": "blog", "paragraph_index": 24}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/", "anchor_text": "blog", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/breaking-the-curse-of-small-data-sets-in-machine-learning-part-2-894aa45277f4", "anchor_text": "In part 2", "paragraph_index": 26}], "all_paragraphs": ["This is Part 1 of Breaking the curse of small datasets in Machine Learning. In this part, I will discuss how the size of the data set impacts traditional Machine Learning algorithms and few ways to mitigate these issues. In Part 2, I will discuss how deep learning model performance depends on data size and how to work with smaller data sets to get similar performances.", "We all are aware of how machine learning has revolutionized our world in recent years and has made a variety of complex tasks much easier to perform. The recent breakthroughs in implementing Deep learning techniques has shown that superior algorithms and complex architectures can impart human-like abilities to machines for specific tasks. But we can also observe that a large amount of training data plays a critical role in making the Deep learning models successful. ResNet, a popular image classification architecture, won 1st place in the ILSVRC 2015 classification competition with ~50% improvement in the previous state of the art.", "ResNet not only had a very complex and deep architecture but was also trained on 1.2 Mn images. It has been well established both across industry and academia that for a given problem, with large enough data, very different algorithms perform virtually the same. It is to be noted that the large data should have meaningful information and not just noise so that model can learn from it. It is also one of the major reasons for the dominance of companies like Google, Facebook, Amazon, Twitter, Baidu in AI research and product development. Although traditional Machine learning requires lesser data compared to deep learning, large data impacts the performance in a much similar way. The graph below clearly depicts how the performance of traditional Machine Learning and Deep Learning models improve with large data.", "Let us answer this question with an example. Let\u2019s say we have a ball which we are throwing with a velocity v and at a certain angle \u03b8 and we wish to predict how far the ball will land. From high school physics, we know that the ball will follow a projectile motion and we can find the range using the formulas shown in the figure. The equation above can be considered as the model/representation for the task and various terms involved in the equation can be considered as important features i.e v, \u03b8 and g(acceleration due to gravity). In situations like above, we have fewer features and we have a good understanding of their impact on our task. Hence, we were able to come up with a good mathematical model. Let\u2019s consider another situation in which we want to predict Apple\u2019s stock price on 30th December 2018. In such a task, we don\u2019t have a full understanding of how various factors can influence stock prices. In absence of a true model, we make use of the historical stock prices and various features such as S&P 500, other stocks prices, market sentiment etc. to figure out the latent relationship using a machine learning algorithm. This is an example of situations where it is difficult for humans to understand the intricate relationship between a large number of features but machines can easily capture it by exploring large amounts of data. Another similarly complex task is of marking emails as spam or not. As a human, we might have to come up with a lot of rules and heuristics which are hard to write and maintain. On the other hand, machine learning algorithms can easily pick up these relationships and can do a much better job which is easy to maintain and scale. Since we don't have to explicitly formulate a lot of these rules, but the data helps us learn the relationship, machine learning has revolutionized disparate fields and industries.", "Before we jump to how more data improves model performance, we need to understand Bias and Variance.", "Bias: Let us consider a data set which has a quadratic relationship between dependent and independent variables. However, we don't know the true relationship and approximate it as linear. In such a case, we will observe a significant difference between our prediction and actual observed data. This difference between observed value and the predicted value is called Bias. Such models are said to have less power and represent underfitting.", "Variance: In the same example, if we approximate the relationship as cubic or any higher powers, we have a case of high variance. Variance is defined as the difference in performance on the training set vs on the test set. The major issue with high variance is the model fits the training data really well but it does not generalize well on out of training datasets. This is one of the major reasons validation and test set are very important in the model building process.", "We generally want to minimize both bias and variance i.e build a model which not only fits the training data well but also generalizes well on test/validation data. There are a lot of techniques to achieve this but training with more data is one of the best ways of achieving this. Lets understand this using the figure below:", "Let\u2019s say we have a data distribution which is similar to a sinusoidal wave. Fig(5a) depicts that multiple models are equally good in fitting the data point. A lot of those models overfit and do not generalize well on the whole dataset. As we increase data, Fig(5b) illustrates a reduction in the number of models which can fit the data. As we further increase the number of data points, we successfully capture the true distribution of the data as shown in Fig(5c). This example helps us clearly understand how more data helps the model uncover the true relationship. Next, we will try to understand this phenomenon for some of the Machine learning algorithms and figure out how the model parameters get impacted by the size of the data.", "(a) Linear Regression: In linear regression, we assume a linear relationship between the predictor variables(features) and dependent variables(target) and the relationship is formulated as:", "where y is the dependent variable and x(i)\u2019s are independent variables. \u03b2(i)\u2019s are the true coefficients and \u03f5 is the error not explained by the model. For a univariate case, estimated coefficients based on the observed data are given by:", "Above equations give us the point estimate of the slope and intercept terms but there is always some uncertainty associated with these estimates which can be quantified by the variance equations:", "So, as the number of data point increases, the denominator value gets bigger and reduces the variance of our point estimates. Hence our model becomes more confident about the underlying relationship and gives robust coefficient estimates. We can see the above phenomenon in action with the help of following code:", "We have simulated a linear regression model with a slope(b)=5 and intercept(a)=10. We can clearly see a difference between the slope and intercept when we build a regression model with less data Fig 6(a) and more data Fig 6(b). In Fig 6(a) we have a linear regression model with 4.65 as slope and 8.2 as intercept compared to a slope of 5.1 and intercept of 10.2 in Fig 6(b), which is much closer to the true values.", "(b) k-nearest neighbors (k-NN): k-NN is one of the simplest yet very powerful algorithms used both for regression and classification. k-NN does not require any specific training phase and as the name suggests, predictions are made based on k-nearest neighbors to the test point. Since k-NN is a non-parametric model, model performance depends on the distribution of the data. In the example below, we are exploring iris dataset to understand how the number of data points effect k-NN performance. We are considering only two features i.e sepal length and sepal width out of the four features for easier visualization.", "Let\u2019s randomly select a point from class 1 as test data(represented by a red star) and use k=3 to predict the class of the test data using majority voting. Fig 7(a) represents the case with fewer data and we observe that the model miss-classify the test point as class 2. With larger data points, the model correctly predicts the class as 1. From the above figures, we can note that k-NN is highly influenced by the available data and more data may help in making the model much more consistent and accurate.", "(c) Decision Trees: Similar to linear regression and k-NN, decision tree performance is also impacted by the amount of data.", "Decision tree is also a non-parametric model and tries to best fit the underlying distribution of the data. The splitting is performed on features values with the aim of creating disparate classes at the child level. Since the model tries to best fit the available training data, the quantity of data directly determines the split levels and final classes. From the above figure, we can clearly observe that the split points and final class predictions get greatly influenced by the size of the dataset. More data helps in finding optimum split points and avoid overfitting.", "Above figure tries to capture the core issues faced while dealing with small data sets and possible approaches and techniques to address them. In this part we will focus on only the techniques used in traditional machine learning and the rest will be discussed in part 2 of the blog.", "a) Change the loss function: For classification problems, we often use cross-entropy loss and rarely use mean absolute error or mean squared error to train and optimize our model. In the case of unbalanced data, the model becomes more biased towards the majority class as it has a larger influence on the final loss value and our model becomes less useful. In such scenarios, we can add weights to the losses corresponding to different classes to even out this data bias. For example, if we have two classes with data in the ration 4:1, we can apply weights in the ratio 1:4 to the loss function calculation to make the data balanced. This technique helps us easily mitigate the issue of unbalanced data and improves model generalization across different classes. We can easily find libraries in both R and Python which can help in assigning weights to classes during loss calculation and optimization. Scikit-learn has a convenient utility function to calculate the weights based on class frequencies:", "We can avoid the above calculation by using class_weight=`balanced` which does the same calculations to find class_weights. We can also feed explicit class weights as per our requirements. For more details refer to Scikit-learn\u2019s documentation", "b) Anomaly/Change detection: In cases of highly imbalanced data sets like Fraud or machine failure, it is worth pondering if such examples can be considered as Anomaly or not. If the given problem meets the criterion of Anomaly, we can use models such as OneClassSVM, Clustering methods or Gaussian Anomaly detection methods. These techniques require a shift in thinking where we consider the minor class as the outliers class which might help us find new ways to separate and classify. Change detection is similar to anomaly detection except we look for a change or difference instead of an anomaly. These might be changes in the behavior of a user as observed by usage patterns or bank transactions. Please refer to the following documentation to learn how to implement anomaly detection with Scikit-Learn.", "c) Up-sample or Down-sample: Since unbalanced data inherently penalizes majority class at different weight compared to a minority class, one solution to the problem is to make the data balanced. This can be done either by increasing the frequency of minority class or by reducing the frequency of majority class through random or clustered sampling techniques. The choice of Over-sampling vs under-sampling and random vs clustered is determined by business context and data size. Generally upsampling is preferred when the overall data size is small while downsampling is useful when we have a large amount of data. Similarly, random vs clustered sampling is determined by how well the data is distributed. For detailed understanding please refer to the following blog. Resampling can be easily done with the help of imblearn package as shown below:", "d) Generate Synthetic Data: Although upsampling or downsampling helps in making the data balanced, duplicate data increases the chances of overfitting. Another approach to address this issue is to generate synthetic data with the help of minority class data. Synthetic Minority Over-sampling Technique (SMOTE) and Modified- SMOTE are two such techniques which generate synthetic data. Simply put, SMOTE takes the minority class data points and creates new data points which lie between any two nearest data points joined by a straight line. In order to do this, the algorithm calculates the distance between two data points in the feature space, multiplies the distance by a random number between 0 and 1 and places the new data point at this new distance from one of the data points used for distance calculation. Note the number of nearest neighbors considered for data generation is also a hyperparameter and can be changed based on requirement.", "M-SMOTE is a modified version of SMOTE which takes the underlying distribution of the minority class also into consideration. This algorithm classifies the samples of minority classes into 3 distinct groups \u2014 Security/Safe samples, Border samples, and latent noise samples. This is done by calculating the distances among samples of the minority class and samples of the training data. Unlike SMOTE, the algorithm randomly selects a data point from the k nearest neighbors for the security sample, select the nearest neighbor from the border samples and does nothing for latent noise. For detailed understanding refer to the blog.", "e) Ensembling Techniques: The idea of aggregating multiple weak learners/different models have shown great results while dealing with imbalanced data sets. Both Bagging and Boosting techniques have shown great results across a variety of problems and should be explored along with methods discussed above to get better results. In an attempt to limit the length of this blog, I will not discuss these techniques but for a detailed understanding of various Ensembling techniques and how to use them for imbalanced data please refer to the following blog.", "In this part, we saw that the size of the data may manifest issues relating to generalization, data imbalance, and difficulty in reaching the global optimum. We have covered a few most commonly used techniques to tackle such issues for traditional machine learning algorithms. One or many of the above techniques can be a good starting point depending on the business problem at hand. In order to keep the blog short, I have not covered a lot of these techniques in great detail but there are a lot of great resources online to which covers the above techniques in great detail. In part 2, we will discuss how small data set impedes the learning process in deep learning models and various ways to overcome this.", "About Me: Graduate Student ,Masters in Data Science from University of San Francisco", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F36f28b0c044d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jyotip?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jyotip?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Jyoti Prakash Maheswari"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbd7c81112f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&user=Jyoti+Prakash+Maheswari&userId=fbd7c81112f&source=post_page-fbd7c81112f----36f28b0c044d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/breaking-the-curse-of-small-data-sets-in-machine-learning-part-2-894aa45277f4", "anchor_text": "In Part 2"}, {"url": "https://arxiv.org/abs/1512.03385", "anchor_text": "ResNet"}, {"url": "https://www.coursera.org/learn/machine-learning", "anchor_text": "Source"}, {"url": "https://www.microsoft.com/en-us/research/people/cmbishop/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fcmbishop%2Fprml%2Fwebfigs.htm", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "iris dataset"}, {"url": "https://scikit-learn.org/stable/documentation.html", "anchor_text": "Scikit-learn\u2019s"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html", "anchor_text": "OneClassSVM"}, {"url": "https://anomaly.io/anomaly-detection-clustering/", "anchor_text": "Clustering methods"}, {"url": "https://www.coursera.org/lecture/machine-learning/anomaly-detection-using-the-multivariate-gaussian-distribution-DnNr9", "anchor_text": "Gaussian Anomaly detection"}, {"url": "https://scikit-learn.org/stable/modules/outlier_detection.html", "anchor_text": "documentation"}, {"url": "http://www.svds.com/wp-content/uploads/2016/08/ImbalancedClasses_fig5.jpg", "anchor_text": "Source"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/", "anchor_text": "blog"}, {"url": "https://imbalanced-learn.org/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html", "anchor_text": "imblearn"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0020025517310083", "anchor_text": "Source"}, {"url": "http://rikunert.com/SMOTE_explained", "anchor_text": "blog"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/breaking-the-curse-of-small-data-sets-in-machine-learning-part-2-894aa45277f4", "anchor_text": "In part 2"}, {"url": "https://www.linkedin.com/in/jyoti-prakash-maheswari-940ab766/", "anchor_text": "https://www.linkedin.com/in/jyoti-prakash-maheswari-940ab766/"}, {"url": "https://github.com/jyotipmahes", "anchor_text": "https://github.com/jyotipmahes"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/", "anchor_text": "How to handle Imbalanced Classification Problems in machine learning?"}, {"url": "https://medium.com/rants-on-machine-learning/what-to-do-with-small-data-d253254d1a89", "anchor_text": "What to do with \u201csmall\u201d data?"}, {"url": "https://elitedatascience.com/imbalanced-classes", "anchor_text": "How to Handle Imbalanced Classes in Machine Learning"}, {"url": "https://medium.com/datadriveninvestor/small-data-deep-learning-ai-a-data-reduction-framework-9772c7273992", "anchor_text": "Small Data & Deep Learning (AI) \u2014 A Data Reduction Framework"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0020025517310083", "anchor_text": "Imbalanced enterprise credit evaluation with DTE-SBD: Decision tree ensemble based on SMOTE and bagging with differentiated sampling rates"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----36f28b0c044d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----36f28b0c044d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----36f28b0c044d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----36f28b0c044d---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&user=Jyoti+Prakash+Maheswari&userId=fbd7c81112f&source=-----36f28b0c044d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&user=Jyoti+Prakash+Maheswari&userId=fbd7c81112f&source=-----36f28b0c044d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F36f28b0c044d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----36f28b0c044d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----36f28b0c044d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----36f28b0c044d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----36f28b0c044d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jyotip?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jyotip?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jyoti Prakash Maheswari"}, {"url": "https://medium.com/@jyotip/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "199 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffbd7c81112f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&user=Jyoti+Prakash+Maheswari&userId=fbd7c81112f&source=post_page-fbd7c81112f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F474a04ab464&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbreaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d&newsletterV3=fbd7c81112f&newsletterV3Id=474a04ab464&user=Jyoti+Prakash+Maheswari&userId=fbd7c81112f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}