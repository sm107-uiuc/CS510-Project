{"url": "https://towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500", "time": 1683000312.284032, "path": "towardsdatascience.com/practical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500/", "webpage": {"metadata": {"title": "Practical guide to Attention mechanism for NLU tasks | by Michel Kana, Ph.D | Towards Data Science", "h1": "Practical guide to Attention mechanism for NLU tasks", "description": "Tested hands-on strategies to tackle attention for improving sequence to sequence models"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "neural word embeddings", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "first article of this series", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Bahdanau et al., 2014", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1508.04025", "anchor_text": "Luong et al., 2015", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "previous article", "paragraph_index": 8}, {"url": "https://doi.org/10.21437/Interspeech.2016-1352", "anchor_text": "source", "paragraph_index": 12}, {"url": "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb", "anchor_text": "Tensorflow 2.0 tutorial for neural machine translation", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "previous sequence-to-sequence model for natural language understanding", "paragraph_index": 27}], "all_paragraphs": ["Chatbots, virtual assistants, augmented analytic systems typically receive user queries such as \u201cFind me an action movie by Steven Spielberg\u201d. The system should correctly detect the intent \u201cfind_movie\u201d while filling the slots \u201cgenre\u201d with value \u201caction\u201d and \u201cdirected_by\u201d with value \u201cSteven Spielberg\u201d. This is a Natural Language Understanding (NLU) task kown as Intent Classification & Slot Filling. State-of-the-art performance is typically obtained using recurrent neural network (RNN) based approaches, as well as by leveraging an encoder-decoder architecture with sequence-to-sequence models. In this article we demonstrate hands-on strategies for improving the performance even further by adding Attention mechanism.", "In 2014, after Sutskever revolutionized deep learning by discovering sequence to sequence models, it was the invention of the attention mechanism in 2015 that ultimately completed the idea and opened the doors to amazing machine translation we enjoy every day. Attention matters because it has been shown to produce state-of-the-art results in machine translation and other natural language processing tasks, when combined with neural word embeddings, and is one component of breakthrough algorithms such as BERT, GPT-2 and others, which are setting new records in accuracy in NLP. So attention is part of our best effort to date to create real natural-language understanding in machines. If that succeeds, it will have an enormous impact on society and almost every form of business.", "In the first article of this series, we introduced the machine translation task to motivate sequence-to-sequence model. We also provided a hands-on and working implementation for solving the slot filling task. Here, in this article, we will improve our solution by adding an attention mechanism, achieving state-of-the-art accuracy in natural language understanding of flight request queries. We will introduce and code the attention mechanism into our sequence-to-sequence model. We strongly recommend to read our first article below, before diving into this one.", "Our previous sequence-to-sequence model had severe issues with a long range sentences. For example, the model was not able to understand neither the flight time \u201cafter 4 pm\u201d, nor the airline name \u201ccontinental\u201d in the user query below.", "Why does the model do great on short queries, but produces nonsense prediction on longer queries? The context vector turned out to be a bottleneck for sequence-to-sequence models and it made it quite challenging to deal with long sentences. Until a solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015. The authors introduced \u201cAttention\u201d, and demonstrated how it improved the quality of machine translation systems by focusing on the relevant parts of the input sequence as needed.", "But before diving into the attention technology, let\u2019s motivate the attention mechanism with a fairy tale.", "Bafou was a young man, who believed, he was the most beautiful human in the universe. Every day he would cross the nearby forest in order to reach to the nearby lake. He would then bend over the lake and spend a lot of time admiring his beauty as his face would reflect over the water. One day, while doing this, he fell into the water, drowned himself and died. As the lake started complaining and bursting into tears, the surrounding trees from the forest asked the lake: \u201cWhy are your crying?\u201d. The lake answered: \u201cI am crying because Bafou is dead\u201d. The trees replied: \u201cYou shouldn\u2019t be crying! We, the trees, are the ones who should be in great pain, because every day, when Bafou was passing by, we wished, he would have stopped for a moment, and would have dared looking at us, so that we could also have had a chance to admire his beauty. You had him all the time at your shores!\u201d The trees continued: \u201cBafou never considered us. Now, he is dead and we will never ever admire his beauty.\u201d The lake replied: \u201cAha! Was Bafou good looking? I did not know that. I am crying because, every time he was looking at me, I had a single opportunity to watch my own beauty as reflection in his eyes.\u201d", "You are asking, why I am telling you such a story, although this article is about machine learning. Well, you are right. But, let\u2019s consider the motivation for a moment. In our story, all three actors, Bafou, the lake and the trees needed attention. Is Attention all what you need? Is it true that, social networks are all about attention? So is natural language understanding? Let\u2019s dive into the subject.", "The same as social networking is about attention; understanding sequence data can also be about attention. The sequence-to-sequence model, we built in the previous article was putting too much emphasis on words being close to one another, but also it focuses too much on upstream context over downstream context. Another limit of traditional words embeddings, as those used in our previous implementation, is the assumption that a word\u2019s meaning is relatively stable across sentences. This is not usually the case. Attention on the other side, takes two user queries, turns them into a matrix where the words of one query form the columns, and the words of another query form the rows. Therefore, Attention learns contextual relationships. This is very useful in natural language understanding because it also allows to investigate how some parts of a single query relate to others, what is called self-attention. If we can build directed arcs of a semantic dependency graph illustrating relationships across a crowded sentence, than we can understand the meaning of the sentence.", "An attention model improves the classic sequence-to-sequence model as follows. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder. If you need to recall how hidden states work, please refer to our articles below where recurrent networks are explained in details.", "Each encoder\u2019s hidden state is most associated with a certain word in the input sentence. The decoder gives each hidden states a score, then multiplies each hidden states by its softmaxed score. By this mean, the decoder amplifies hidden states with high scores, and drowning out hidden states with low scores. It repeats this for each word, thus paying attention to at each decoding step.", "When a sequence-to-sequence model pays attention, the encoder passes all the hidden states to the decoder, instead of passing only the last hidden state of the encoding stage. Moreover the model performs a scoring exercise to produce an attention vector, thus allowing the decoder to pay attention at certain part of the input sentence at each decoding step.", "Attention-based learning methods were proposed and achieved the state-of-the-art performance for intent classification and slot filling (source). We leverage the official Tensorflow 2.0 tutorial for neural machine translation by modifying the code to work with user queries from the ATIS dataset as input sequence and the intent slot fillings as target sequence, after adding the necessary start and end tokens. Below we define hyperparameters, as well as a TensorFlow Dataset iterator that will be used to batch through the data for lower memory consumption.", "Attention requires us to create the encoder as a Python class.", "Bahdanau Attention is implemented, which is described as an additive equation (4) below, where h_t is the encoder output and h_s is the hidden state.", "Bahdanau additive attention is computed in the class below.", "Now we can implement the decoder as follows. Both encoder and decoder have an embedding layer and a GRU layer with 1024 cells.", "Sparse categorical crossentropy is used as loss function. Below we define the optimizer and loss function as well as checkpoints.", "A custom training loop using teacher forcing technique, gradients calculation with Tensorflow built-in functions and backpropagation allow us to tune embedding weights, GRU cells\u2019 weigths and attention weights. A training step is executed by the function below on a batch of data. Gradients are calculated and backpropagation is performed. 64 batch size and 256-dimensional embedding are used.", "As we can see in the plot below, loss drops down to 0.0005 after 18 epochs, from the initial value 0.51.", "Inference is performed similar to the training loop, except we don\u2019t use teacher forcing here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output. We stop predicting when the model predicts the end token (EOS). We store the attention weights for every time step.", "Below we present our code for preparing the query. We also implement a function that takes a query sentence as input, as well as vocabulary variables, it creates a vector tensor, executes the encoder and decoder for inference. The function also calculates the attention weights. There is an additional function that plots the attention weights in a heatmap.", "The slot filling quality is reasonable for a toy example \u201clist all flights to boston\u201d. The generated attention plot is quite interesting. It shows which parts of the input sentence has the model\u2019s attention while predicting the user query. The intent to travel to Boston is correctly labelled as a destination city (B-toloc.city_name) by looking at the words \u201cto\u201d and \u201cboston\u201d.", "When we submit a bit more complex query to the system \u201ci want to fly next week at 8 am with continental\u201d, we are pleased to see how the model recognizes the airline \u201ccontinental\u201d, we wish to fly with, as well as the relative time \u201cnext week\u201d.", "If we ask \u201cshow me the cheapest fare in the airport mco to denver\u201d, the model interestingly uses the words \u201ccheapest\u201d and \u201cfare\u201d to recognize the intention to have a relative cost (B-cost_relative label), for the destination city \u201cdenver\u201d. And all this, in a specific airport \u201cMCO\u201d. Surprisingly, the word \u201cairport\u201d from the user query did not affect the decision to label \u201cMCO\u201d as an airport. From the training set, the model obviously learnt airport codes as well. We would have expected the vicinity of the word \u201cairport\u201d near to a 3-characters abbreviation to affect the models decision for better generalization.", "The next query is a complex one: \u201ci want to fly from detroit to washington on northwest airlines and leave around 9 am\u201d. And the model gets it all right! The Attention mechanism is amazing here.", "At last, another example of successful prediction: \u201cwhat is fare code h on a flight from cleveland to dallas\u201d.", "In this article, we improved our previous sequence-to-sequence model for natural language understanding by including a Bahdanau Attention mechanism. We demonstrated how attention significantly improves prediction on long sentences by correctly identifying the semantically meaningful information in a customer query where we should pay attention.", "Thanks to the practical implementation of few models on the ATIS dataset about flight requests, we demonstrated how attention matters, how this type of mechanism produces state-of-the-art results in machine translation and other natural language processing tasks.", "In the next articles we will combine our results with powerful words embedding from Transformer.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Husband & Dad. Mental health advocate. Top Medium Writer. 20 years in IT. AI Expert @Harvard. Empowering human-centered organizations with high-tech."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fccc47be8d500&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michel-kana.medium.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2----ccc47be8d500---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://mensline.org.au/changingforgood/blog/why-kids-just-need-your-time-and-attention/", "anchor_text": "Why kids just need your time and attention"}, {"url": "https://towardsdatascience.com/representing-text-in-natural-language-processing-1eead30e57d8", "anchor_text": "neural word embeddings"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "first article of this series"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "Natural Language Understanding with Sequence to Sequence ModelsHow to predict the intent behind a customer query. Seq2Seq models explained. Slot filling demonstrated on ATIS dataset\u2026towardsdatascience.com"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "Bahdanau et al., 2014"}, {"url": "https://arxiv.org/abs/1508.04025", "anchor_text": "Luong et al., 2015"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "previous article"}, {"url": "https://skymind.ai/wiki/attention-mechanism-memory-network", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a", "anchor_text": "Sentiment Analysis: a benchmarkRecurrent neural networks explained. Classifying Customer Reviews using FCNNs, CNNs, RNNs and Embeddings.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/lstm-based-african-language-classification-e4f644c0f29e", "anchor_text": "LSTM-based African Language ClassificationTired of German-French dataset? Look at Yemba, and stand out. Mechanics of LSTM, GRU explained and applied, with\u2026towardsdatascience.com"}, {"url": "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/", "anchor_text": "source"}, {"url": "https://doi.org/10.21437/Interspeech.2016-1352", "anchor_text": "source"}, {"url": "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb", "anchor_text": "Tensorflow 2.0 tutorial for neural machine translation"}, {"url": "https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b", "anchor_text": "previous sequence-to-sequence model for natural language understanding"}, {"url": "https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03", "anchor_text": "BERT for dummies \u2014 Step by Step TutorialDIY Practical guide on Transformer. Hands-on proven PyTorch code for Intent Classification with BERT fine-tuned.towardsdatascience.com"}, {"url": "https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f", "anchor_text": "BERT explained. Lost in Translation. Found by Transformer.Building the next chatbot? BERT, GPT-2: tackle the mystery of Transformer model.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ccc47be8d500---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ccc47be8d500---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ccc47be8d500---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----ccc47be8d500---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ccc47be8d500---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----ccc47be8d500---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=-----ccc47be8d500---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fccc47be8d500&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ccc47be8d500---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ccc47be8d500--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ccc47be8d500--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ccc47be8d500--------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michel-kana.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michel Kana, Ph.D"}, {"url": "https://michel-kana.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "5.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb0b01fe20d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=post_page-cb0b01fe20d2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F69e95067d2a1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpractical-guide-to-attention-mechanism-for-nlu-tasks-ccc47be8d500&newsletterV3=cb0b01fe20d2&newsletterV3Id=69e95067d2a1&user=Michel+Kana%2C+Ph.D&userId=cb0b01fe20d2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}