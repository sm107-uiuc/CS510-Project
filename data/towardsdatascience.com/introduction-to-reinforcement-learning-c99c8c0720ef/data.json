{"url": "https://towardsdatascience.com/introduction-to-reinforcement-learning-c99c8c0720ef", "time": 1683013747.429726, "path": "towardsdatascience.com/introduction-to-reinforcement-learning-c99c8c0720ef/", "webpage": {"metadata": {"title": "Introduction to Reinforcement Learning | by Marco Del Pra | Towards Data Science", "h1": "Introduction to Reinforcement Learning", "description": "Reinforcement Learning (RL) is an increasing subset of Machine Learning and one of the most important frontiers of Artificial Intelligence, since it has gained great popularity in the last years with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/teopir/ifqi", "anchor_text": "https://github.com/teopir/ifqi", "paragraph_index": 16}, {"url": "https://medium.com/@marcodelpra", "anchor_text": "https://medium.com/@marcodelpra", "paragraph_index": 72}], "all_paragraphs": ["Reinforcement Learning (RL) is an increasing subset of Machine Learning and one of the most important frontiers of Artificial Intelligence, since it has gained great popularity in the last years with a lot of successful real-world applications in robotics, games and many other fields. It denotes a set of algorithms that handle sequential decision-making and have the ability to take intelligent decisions depending on their local environment.", "A RL algorithm can be described as a model that indicates to an agent which set of actions it should take within a closed environment in order to to maximize a predefined overall reward. Generally speaking, the agent tries different sets of actions, evaluating the total obtained return. After many trials, the algorithm learns which actions give a greater reward and establishes a pattern of behavior. Thanks to this, it is able to tell the agent which actions to take in every condition.", "The goal of RL is to capture more complex structures and use more adaptable algorithms than classical Machine Learning, infact RL algorithms are more dynamic in their behavior compared to classical Machine Learning ones.", "Let\u2019s see some examples of applications based on RL:", "RL algorithms are based on Markov Decision Process (MDP). A Markov Decision Processes is a special stochastic time control process for decision making. The main actors of a RL algorithm are:", "The maximum of the action value function, as the policy changes, is referred as the optimal action value function Q*(s , a), and according to Bellman equation is given by", "Then the optimal policy \u03c0*(s) is given by the action that maximizes the action value function:", "The problem is that in most real cases the state transition model and the reward function are unknown, so it\u2019s necessary to learn them from sampling in order to estimate the optimal action value function and the best policy. For these reasons RL algorithms are used, in order to take the actions in the environment, observe and learn the dynamics of the model, estimate the optimal value function and the optimal policy, and improve the rewards.", "Exploration is the training on new data points, while exploitation is the use of the previously captured data. If we keep searching for the best action in every iteration we might remain stopped in a limited set of states without being able to explore the entire environment. To get out of this suboptimal set, generally it\u2019s used a strategy called \u03f5-greedy: when we select the best action, there is small a probability \u03f5 that a random action is chosen.", "There are 3 main possible approaches that we can use when we implement a RL algorithm:", "The Value Function Approximation is one of the most classical Value-based methods. Its goal is to estimate the optimal policy \u03c0*(s) by iteratively approximating the optimal action value function Q*(s , a). We start considering a parametric action value function Q^(s , a , w), where w is a vector of parameters. We initialize randomly the vector w and we iterate on every step of every episode. For every iteration, given the state s and the action a, we observe the reward R(s , a) and the new state s\u2019. According to the obtained reward we update the parameters using the gradient descent:", "In the equation, \u03b1 is the learning rate. It can be shown that this process converges, and the obtained action value function is our approximation of the optimal action value function. In most of the real cases the better choice for the parametric action value function Q^(s , a , w) is a Neural Network, and then the vector of parameters w is given by the vector of the weights of the Neural Network.", "A Deep Q-Network is a combination of Deep Learning and RL, since it is a Value Function Approximation algorithm where the parametric action value function Q^(s , a , w) is a Deep Neural Network, and in particular a Convolutional Neural Network. Moreover, a Deep Q-Network overcomes unstable learning using mainly 2 techniques", "Another popular Value-based algorithm is Fitted Q-Iteration. Consider the deterministic case, in which we have that the new state s\u2019 is uniquely determined by the state s and the action a according to some a function f , then we can write s\u2019 = f (s , a). Let L be the horizon, possibly infinite, and we recall that the horizon is the length of all the episodes. The goal of this algorithm is to estimate the optimal action value function. By the Bellman equation, the optimal action value function Q*(s , a) can be seen as the application of an operator H to the action value function Q(s , a):", "Consider now a temporal horizon N less than or equal to the horizon L, and denote by Q_N (s , a) the action value function over N steps defined by the application of the just defined operator H to the action value function Q_(N\u22121) (s , a), with", "It is possible to show that this sequence of N-step action value functions Q_N (s , a) converges to the optimal action value function Q*(s , a) as N \u2192 L. Thanks to this, it\u2019s possible to build an algorithm to approximate the optimal action value function Q*(s , a) iterating on N.", "A full implementation of Fitted Q-Iteration can be found on GitHub(https://github.com/teopir/ifqi).", "Consider a car, modeled by a point mass, that is traveling on a hill with this form:", "The control problem goal is to bring the car in a minimum time to the top of the hill while preventing the position p of the car to become smaller than -1 and its speed v to go outside the interval [-3 , 3]. The top of the hill is reached at position p = 1.", "State space - This problem has a (continuous) state space of dimension two (the position p and the speed v of the car), and we want that the absolute value of the position is less than or equal to 1, and that the absolute value of the speed is less than or equal to 3:", "Every other combination of position and speed is considered a terminal state.", "Action space - The action a acts directly on the acceleration of the car and canonly assume two extreme values (full acceleration (a = 4) or full deceleration (a -4)). Hence the action space is given by the set", "System dynamics - The time is discretized in timesteps of 0.1 seconds. Given the state (p, v) and the action a at timestep t, we are able to compute the state (p, v) at timestep t + 1 solving with a numeric method the two differential equations related to position and speed that describe the dynamic of the system:", "Of course for our purpose it\u2019s not important to understand the meaning of these equations, it\u2019s important to understand that given state and action at timestep t, the state at timestep t + 1 in uniquely determined.", "Reward function - The reward function r(s , a) is defined through this expression:", "The reward is -1 if the position is less then -1 or if the absolute value of the speed is greater than 3 because we reached a termination state but we didn\u2019t reach the top of the hill; the reward is 1 if the position is greater than 1 and the absolute value of the speed is less than 3 because we reached the top of the hill respecting the speed limits; otherwise the reward is 0.", "Discount factor - The decay factor \u03b3 has been chosen equal to 0.95.", "Initial point - At the begin the car stopped at the bottom of the hill (p , v) = (0.5, 0).", "Regressor - The regressor used is an Extra Tree Regressor.", "Performing the Fitted Q-Iteration for N = 1 to 50 it turns out that for N > 20 the mean squared error between action value functions Q^_N (s , a) and Q^_(N+1)(s , a) (computed on all the combinations of (p, v)) decreases quickly to 0 as N increases. For this reason the results are studied using the action state function Q^_20(s , a).", "In figure on the left we can see the action chosen for every combination of(p, v), according to the action value function Q^_20(s , a) (red area represents deceleration, green area represents acceleration, blu area means that the action values of deceleration and acceleration are equal).", "The optimal trajectory according to the action value function Q^_20(s , a) is represented in the figure on the right.", "Policy Gradient os the most classical Policy-based method. The goal of the Policy Gradient method is to find the vector of parameters \u03b8 that maximizes the value function V (s , \u03b8) under a parametric policy \u03c0 (a|s , \u03b8).", "We start considering a parametric policy \u03c0 (a|s , \u03b8) differentiable with respect to the vector of parameters \u03b8; in particular in this case we choose a stochastic policy (in this case the method is called Stochastic Policy Gradient, however the case with a deterministic policy is very similar).", "We initialize randomly the vector w and we iterate on every episode. For each timestep t we generate a sequence of triplets (s, a, r) choosing the action according the parametric policy \u03c0 (a|s , \u03b8). For every timestep in the resulting sequence we compute the total long term reward with discount G_t in function of the obtained rewards:", "Then the vector of parameters \u03b8_t is modified using a gradient update process", "In the equation \u03b1 > 0 is the learning rate.", "It can be shown that this process converges, and the obtained process is our approximated optimal policy.", "The most used parametric policies are Softmax Policy and Gaussian Policy.", "Softmax PolicyThe Softmax Policy consists of a softmax function that converts output to adistribution of probabilities, and is mostly used in the case discrete actions:", "In this case the explicit formula for the gradient update is given by", "where \u03c6(s , a) is the feature vector related to the state and the action.", "Gaussian PolicyThe Gaussian Policy is used in the case of a continuous action space, and is given by the Gaussian function", "where \u00b5(s) is given by \u03c6(s) T \u03b8, \u03c6(s , a) is feature vector, and \u03c3 can be fixed or parametric. Also in this case we have the explicit formula for the gradient update:", "CartPole is a game where a pole is attached by an unactuated joint to a cart, which moves along a frictionless track. The pole starts upright.", "The goal is to prevent the pole from falling over by increasing and reducing the cart\u2019s velocity.", "State space - A single state is composed of 4 elements:", "The game ends when the pole falls, which is when the pole angle is more than \u00b112\u00b0, or the cart position reaches the edge of the display.", "Action space - The agent can take only 2 actions:", "Reward - For every step taken (including the termination step), the reward is increased by 1. This is obviously because we want to achieve the greatest possible number of steps.", "The problem is solved with Gradient Policy method using Softmax Policy, with discount factor \u03b3 = 0.95 and learning rate \u03b1 = 0.1. For every episode a maximum number of 1000 iterations is fixed.", "After about 60 epochs (where 1 epoch is equal to 20 consecutive episodes) the agent learns a policy thanks to which we get a reward equal to 1000, that means that the pole doesn\u2019t fall for all the 1000 steps of the episode.", "In this Figures we can see how the choice of the action vary in function of the pole angle and the cart velocity (left figure) and in function of the pole angular velocity and the cart velocity (right figure). The red area is where the move left action is chosen, the green area is where the move right action is chosen, and the yellow area is where there are similar probabilities to choose one action or the other.", "A very interesting result is that if \u03b3 is greater than 0.9 the reward of a single episode grows with the number of epochs and arrives to the maximum value of 1000, while if \u03b3 is lower than 0.9, after some epochs the reward of a single episode stops growing. This means that in this problem the reward of the next steps is very important to find the best policy, and this is actually reasonable since the fundamental information to learn how to prevent the pole from falling is to know after how many steps it falls in each single episode.", "On GitHub it\u2019s possible to find many different implementations of this example.", "Another popular policy-based method is Actor-Critic. It is different from the Policy Gradient method because estimates both the policy and the value function, and updates both.", "In Policy Gradient, the vector of parameters \u03b8 is updated using the long term reward G_t , but this estimation often has high variance. To address this issue and reduce the wide changes in the results, the idea of the Actor-Critic method is to subtract from the total reward with discount G_t a baseline b(s).", "The obtained value \u03b4 = Gt - b(s), that is called Temporal Difference error, is used to update the vector of parameters \u03b8 in place of the long term reward G_t . The baselines can take several forms, but the most used is the estimation of the value function V(s).", "As in value-based methods, the value function V(s) can be learned with a Neural Network, whose output is the approximated value function V^(s , w), where w is the vector of weights. Then in every iteration the Temporal Difference error \u03b4 is used non only to adjust the vector of parameters \u03b8, but also to update the vector of weights w.", "This method is called Actor-Critic Methods, because:", "As already underlined, a Model-based method creates a virtual model starting from the original environment, and that the agent learns how to perform in the virtual model. A Model-based method starts considering a base parametric model, and then run the following 3 steps:", "One of the most used models to represent the system dynamics is the Gaussian Process, in which the prediction interpolates the observations using Gaussian distribution. Another possibility is to use the Gaussian Mixture Model, that is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It\u2019s a sort of generalization of k-means clustering that incorporates information about the covariance structure of the data as well as the centers of the latent Gaussians.", "The Model Predictive Control is an evolution of the method just described. The described Model-based algorithm is vulnerable to drifting: small errors accumulate fast along the trajectory, and the search space is too big for any base policy to be covered all over. For this reasons the trajectory may arrive in areas where the model has not been learned yet. Without a proper model around these areas, it\u2019s impossible to plan the optimal control.", "To address that, instead of learning the model at the beginning, sampling and fitting of the model are performed continuously during the trajectory. Nevertheless, the previous method executes all planned actions before fitting the model again.", "In Model Predictive Control, the whole trajectory is optimized, but only the first action is performed, then the new triplet (s, a, s\u2019) is added to the observations and the planning is done again. This allows to take a corrective action if the current state is observed again. For a stochastic model, this is particularly helpful.", "By constantly changing plan, MPC is less vulnerable to problems in the model. The new algorithm the run 5 steps, of which the first 3 are the same as the previous algorithm (acting, model learning, planning). Then we have:", "Model-based RL has a strong advantage of being very efficient with few samples, since many models behave linearly at least in the local proximity.", "Once the model and the reward function are known, the planning of the optimal controls doesn\u2019t require additional sampling. Generally the learning phase is fast, since there is no need to wait for the environment to respond nor to reset the environment to some state in order to resume learning.", "On the downside, if the model is inaccurate, we risk learning something completely different from the reality. Another point worth nothing is that Model-based algorithm still use Model-free methods either to construct the model or in the planning and simulation phases.", "This article is a high-level structural overview of many classical RL algorithms. However it\u2019s ovious that there are a lot of variants in each model family that we\u2019ve not covered. For example, in the Deep Q-Networks family, double Deep Q Networks give very interesting results.", "The main challenge in RL lays in preparing the simulation environment and choosing the most suitable approach. Those aspects are highly dependent on the task to be performed and are very important because many real world problems have enormous state or action spaces that must be represented efficiently and comprehensively.", "The other main tasks are to optimize the rewards in order to obtain the desired results, to set up the system in oder to let the learning process converge to the optimum in a reasonable time, and to avoid overfitting and forgetting.", "My articles on Towards Data Science: https://medium.com/@marcodelpra", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc99c8c0720ef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marcodelpra?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcodelpra?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Marco Del Pra"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F514558708826&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&user=Marco+Del+Pra&userId=514558708826&source=post_page-514558708826----c99c8c0720ef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/iar-afB0QQw", "anchor_text": "https://unsplash.com/photos/iar-afB0QQw"}, {"url": "https://github.com/teopir/ifqi", "anchor_text": "https://github.com/teopir/ifqi"}, {"url": "https://medium.com/@marcodelpra", "anchor_text": "https://medium.com/@marcodelpra"}, {"url": "https://www.linkedin.com/in/marco-del-pra-7179516/", "anchor_text": "https://www.linkedin.com/in/marco-del-pra-7179516/"}, {"url": "https://www.linkedin.com/groups/8974511/", "anchor_text": "https://www.linkedin.com/groups/8974511/"}, {"url": "https://github.com/openai/gym", "anchor_text": "https://github.com/openai/gym"}, {"url": "https://github.com/teopir/ifqi", "anchor_text": "https://github.com/teopir/ifqi"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----c99c8c0720ef---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c99c8c0720ef---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c99c8c0720ef---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c99c8c0720ef---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c99c8c0720ef---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&user=Marco+Del+Pra&userId=514558708826&source=-----c99c8c0720ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&user=Marco+Del+Pra&userId=514558708826&source=-----c99c8c0720ef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc99c8c0720ef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c99c8c0720ef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c99c8c0720ef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcodelpra?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marcodelpra?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marco Del Pra"}, {"url": "https://medium.com/@marcodelpra/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "205 Followers"}, {"url": "https://www.linkedin.com/groups/8974511/", "anchor_text": "https://www.linkedin.com/groups/8974511/"}, {"url": "https://www.linkedin.com/in/marco-del-pra-7179516/", "anchor_text": "https://www.linkedin.com/in/marco-del-pra-7179516/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F514558708826&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&user=Marco+Del+Pra&userId=514558708826&source=post_page-514558708826--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4d14374eeaae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroduction-to-reinforcement-learning-c99c8c0720ef&newsletterV3=514558708826&newsletterV3Id=4d14374eeaae&user=Marco+Del+Pra&userId=514558708826&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}