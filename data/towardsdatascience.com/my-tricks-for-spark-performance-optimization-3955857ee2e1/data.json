{"url": "https://towardsdatascience.com/my-tricks-for-spark-performance-optimization-3955857ee2e1", "time": 1683018215.643439, "path": "towardsdatascience.com/my-tricks-for-spark-performance-optimization-3955857ee2e1/", "webpage": {"metadata": {"title": "Tricks to Boost Your Spark Pipeline Performance | by Yu Zhou | Towards Data Science", "h1": "Tricks to Boost Your Spark Pipeline Performance", "description": "Spark is a popular choice for data engineering work because Spark promises extremely fast processing power on a huge volume of distributed data. Like the game between cats and mice, ever-growing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/docs/latest/tuning.html#memory-management-overview", "anchor_text": "spark.org", "paragraph_index": 5}, {"url": "https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/", "anchor_text": "AWS EMR", "paragraph_index": 5}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn", "anchor_text": "here", "paragraph_index": 7}], "all_paragraphs": ["Spark is a popular choice for data engineering work because Spark promises extremely fast processing power on a huge volume of distributed data. Like the game between cats and mice, ever-growing needs for deeper data processing can still overwhelm Spark, making Spark pipelines time consuming and painful to manage. My day job involves processing PTs of data, and our team maintains a nightly Spark pipeline in AWS EMR for that purpose. Every couple of months, we need Spark performance tuning. With hundreds of knobs to turn, it is always an uphill battle to squeeze more out of Spark pipelines. In this blog, I want to highlight three overlooked methods to optimize Spark pipelines: 1. tidy up pipeline output; 2. balance workload via randomization; 3. replace joins with window functions.", "0. What does my Spark pipeline look like?", "Before I go over my three methods, I want to describe my Spark pipeline first. My Spark pipeline includes thousands of independent Spark applications. Each Spark application intakes a customer\u2019s data in S3, makes a long chain of transformations, and finally writes outputs into S3. These Spark applications are all in Pyspark in AWS EMR. We concurrently run a hundred Spark applications from these thousands. On bad days, it takes up to 24 hours for the whole work. It\u2019s worthwhile to point out that one customer\u2019s data can much bigger than another customer\u2019s data, therefore these independent Spark applications actually have different resource footprint.", "The number one well-known strategy for performance optimization is reducing the size of data in your application: ingest only necessary raw data, and keep intermediate data frames compact. In one performance tuning sprint, I replaced one of our raw data source with another source that had lower granularity, I saw 50% performance improvement easily. If an alternative data source is not available, you can use input = spark.read.parquet(\"fs://path/file.parquet\").select(...) to limit reading to only useful columns. Reading fewer data into memory will speed up your application.", "It should be equally obvious that writing less output into your destination directory also improves performance easily. Because the last step often gets less attention, the last step where you write outputs is likely not the most compact. In our data pipeline, we were very liberal towards our output. There were columns and rows that were only informative the best; we created many unnecessary data. In one performance tuning sprint, I reduced our output size (by 40%), and I saw a 2-hour performance improvement. If you consider the cost of data storage, and the cost for downstream processes to handle unnecessary rows, tidy up your pipeline output spreads performance benefits to the whole system and company. Don\u2019t forget an easy win there.", "The number of cores for executors, the size of memory for executors are the very basics for Spark memory management. There is a long list of parameters: overhead memory size, default partition number, and more. You can refer to these two documents for details: spark.org, AWS EMR. To find a good resource configuration for a Spark application, you need solid engineering knowledge and experience. One insight I learned from playing with these knobs is that networking kills performance. By reducing spark.sql.shuffle.partitions, I once saw 4-hour performance improvement. Although high partitions improve resilience, low partitions allow clusters to avoid sending data around, hence saving networking resource for in-memory calculation. Most writings on Spark performance tuning concerns one application, however, we usually do not care much about individual application performance. In our Spark pipeline, we have thousands of Spark applications, we want the total run time to stay low, we do not want to tune and assign every single application its own perfect resource configuration. Without per-application resource configuration, we create three buckets: small, medium, and large. Customers with large data volume get a large memory configuration, customers with small data volume get a small memory configuration. With different \u201csizes\u201d of applications in the pipeline, we ran into issues such as a long-tail pipeline (one last application takes an extra long time), larger applications blocking smaller applications, and other issues. Although we could use delicate engineering to address these issues, I found randomization a quicker way to go. In one performance tuning sprint, I simply randomized the thousands of applications and observed a 4-hour performance improvement. Randomization of the application not only made the workload evenly distributed (easier cluster-level resource provisioning) but also well mixed different sizes of applications together (less idleness).", "3. Replace Joins with Window Functions", "Inside our Spark pipeline, we manipulate raw input data by creating and adding new columns. In this process, we inevitably make input data frames larger and create intermediate data frames. withColumn()is a common pyspark.sql function we use to create new columns, here links to its official Spark document. We create intermediate tables because we handle different business logic in each intermediate table, then in a later step, we would join the tables to get a final table. This pattern is all good and familiar. However, when the chain of data manipulation grows longer and longer, a simple join between intermediate tables later in the process can easily create memory error. Familiar techniques such as persist()to cache intermediate data does not even help.", "In one performance tuning sprint, I decided to avoid joins because of consistent memory problems. I instead used Window functions to create new columns that I would otherwise accomplish via joins. I not only avoid memory problem but also I saw a 33% performance improvement. Joins involved two tables, however, the Window function only involved a few columns. Needless to say, writing Window functions were less straightforward than joins, but replacing joins with Window functions certainly helped simplify the computation graph behind the scenes.", "Spark is a popular choice for data engineering work, but Spark performance tuning is the biggest pain point for serious data work. I hope Spark will handle more of its tuning automatically in the future, and it is always one step ahead of the growing data processing need. In the meantime, I hope my tricks are easy to understand and helpful for you to use.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @Cloudability, I am interested in everything data science, all my posts are my own. Twitter: @yuzhouyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3955857ee2e1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yuzhoux?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yuzhoux?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Yu Zhou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F350a2026c6db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&user=Yu+Zhou&userId=350a2026c6db&source=post_page-350a2026c6db----3955857ee2e1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/images/id-4394450/", "anchor_text": "https://pixabay.com/images/id-4394450/"}, {"url": "https://spark.apache.org/docs/latest/tuning.html#memory-management-overview", "anchor_text": "spark.org"}, {"url": "https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/", "anchor_text": "AWS EMR"}, {"url": "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn", "anchor_text": "here"}, {"url": "https://medium.com/tag/spark?source=post_page-----3955857ee2e1---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----3955857ee2e1---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/aws?source=post_page-----3955857ee2e1---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----3955857ee2e1---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/pipeline?source=post_page-----3955857ee2e1---------------pipeline-----------------", "anchor_text": "Pipeline"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&user=Yu+Zhou&userId=350a2026c6db&source=-----3955857ee2e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&user=Yu+Zhou&userId=350a2026c6db&source=-----3955857ee2e1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3955857ee2e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3955857ee2e1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3955857ee2e1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3955857ee2e1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3955857ee2e1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yuzhoux?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yuzhoux?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yu Zhou"}, {"url": "https://medium.com/@yuzhoux/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "287 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F350a2026c6db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&user=Yu+Zhou&userId=350a2026c6db&source=post_page-350a2026c6db--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F350a2026c6db%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmy-tricks-for-spark-performance-optimization-3955857ee2e1&user=Yu+Zhou&userId=350a2026c6db&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}