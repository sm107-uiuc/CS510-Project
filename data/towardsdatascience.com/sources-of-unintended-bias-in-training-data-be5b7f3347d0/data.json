{"url": "https://towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0", "time": 1683012730.7342348, "path": "towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0/", "webpage": {"metadata": {"title": "Sources of unintended bias in training data | by Cristina Goldfain | Towards Data Science", "h1": "Sources of unintended bias in training data", "description": "Almost every week, the press highlights examples of machine learning models with biased outputs. With discrimination at the forefront of public discussion, how is social inequality reflected in the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/watch?v=jIXIuYdnyyk", "anchor_text": "https://www.youtube.com/watch?v=jIXIuYdnyyk", "paragraph_index": 25}], "all_paragraphs": ["Almost every week, the press highlights examples of machine learning models with biased outputs. With discrimination at the forefront of public discussion, how is social inequality reflected in the biased outputs of ML models? Decisions made at every step of a typical data science pipeline, from formulating questions to collecting data and training and deploying models can ultimately harm downstream users\u00b9. Our goal is to achieve a practical understanding of how different sources of bias can be reflected in the data. To achieve this aim, we\u2019ll build examples using synthetic data to illustrate how different sources of bias impact ML outputs and their underlying characteristics. The guiding principle is that a good way to understand something is to build it yourself!", "We\u2019ll think about biased datasets in the context of classification, the task of predicting a binary target outcome (the label): \u201cWill the credit card offer be accepted or rejected?\u201d, \u201cWill the applicant pay back the loan or not?\u201d. A predictive model uses the features of a particular application (what the bank knows about the applicant) to predict the associated label. The workflow for building predictive models is to put together a dataset with features that may be relevant to the predicted label and train a model that predicts with the highest accuracy the label on the training dataset.", "For many data scientists, the goal when doing predictive modelling is to train a model of tolerable complexity that is sufficiently accurate at predicting the label. In recent years, many pointed out that this approach of optimizing model accuracy obfuscates the goal of building fair, equitable models for all users. There are many examples of models that are accurate but nevertheless lead to very harmful outcomes, especially against protected characteristics (for example, age or race) that have historically faced discrimination: from HR applications that invariably predict male applicants are more qualified for the job\u00b2, to bail models that predict black persons are more likely to re-offend\u00b3, to health insurance models that recommend less sick white people compared to more sick black people for preventative care\u2074. The law in many countries defines a person\u2019s protected attributes as race, gender, age, sexual orientation, physical or mental disability, and marital status\u2075 \u2076 . Discrimination appears between privileged (e.g., men) and unprivileged (e.g., women or LGBTQ) groups across a protected attribute (sexual orientation). It is important to also evaluate outcomes for groups at the intersection of several protected attributes, such as black LGBTQ individuals vs white men.", "Exploratory data analysis (EDA) and fairness metrics are important tools. EDA can warn us if there is a large disparity in the proportion of labels for a group (for example, only 10% of the positive labels are females), and motivate further investigations. Fairness metrics\u2077 \u2079 allow us to set a desirable output for the model and check whether this output is achieved across groups. One word of caution worth noting here, fairness metrics cannot all be satisfied simultaneously\u2077. We\u2019ll use two commonly employed fairness metrics: demographic parity and equal opportunity. Demographic parity asks that assigned labels are independent of group membership. The metric is computed as the ratio of the labels for the unprivileged and privileged groups. Demographic parity is 1 when the probability is independent of group membership; a ratio of 0.8 is reasonable based on the generalization of the 80 percent rule advocated by US EEOC\u2078, and smaller numbers are indicative of bias. The equal opportunity metric highlights the fact that a positive label is often a desirable outcome (\u201can opportunity\u201d such as \u201cthe mortgage loan is approved\u201d) and thus focuses on comparing the True Positive Rate (TPR) across groups: the rate at which positive labels are predicted correctly as positive. The TPR metric has the added benefit of accommodating different baseline rates for the compared groups, as it asks what percentage of the expected number of positive labels have been found.", "Consider the following example of male and female applicants evaluated for a loan application. Persons in green are approved, while those in red are rejected.", "Calculating the Demographic Parity (DP) metric for the predicted labels:", "The five we\u2019ll dig into (based on\u2079) are:", "Our starting point is a simple scenario. Imagine an HR company that is trying to predict an applicant\u2019s salary when considering what offer to extend them. The company has collected around 100k data points. For each applicant, the company knows their previous job type (backend or frontend developer), years of work experience, gender, extra certifications and salary, which they threshold for simplicity to under or over 52k. We built this dataset using a simple linear regression model:", "Salary = c_exp * Years_experience + c_job * Job + c_cert * Cert + c_noise * Noise", "Where c_exp, c_job, c_cert, c_noise are coefficients that determine the effect of each variable on the outcome Salary", "Years_experience: applicants\u2019 number of years of work experience", "Cert: binary feature encoding whether applicants have extra certifications", "Job: binary feature encoding whether applicants have a backend or frontend development job", "Noise: a random number, capturing the idea that there are other aspects that determine Salary which we didn\u2019t include", "We start with a synthetic dataset of ~ 100K applicants. Here are the first few rows of the data:", "Initially, there\u2019s no difference between salaries for men and women, while backend developers make on average slightly more than frontend engineers.", "There isn\u2019t an imbalance in the distribution of labels. The number of applicants with salaries over and under the threshold is comparable across genders.", "One source of bias that is very hard to avoid comes from features that are correlated with protected attributes like gender (proxies). In our dataset, there is a 0.7 correlation between gender and certifications. In a real dataset, most variables will have some correlation with gender (preferred sport, for example). Removing all the correlated variables would be impractical as we\u2019d be left with no data to train the model. If features correlated with variables like gender are not removed, then the model can use them for classification, thus potentially leading to biased outputs. Simply removing the gender variable is ineffectual because the model will still use the gender information available in proxy variables to classify based on gender. Furthermore, removing the gender information from the dataset makes it impossible to evaluate the model by computing fairness metrics. Thus the more realistic setup is that the model is trained on features that are correlated with protected characteristics such as gender to various degrees, though perhaps not on gender information directly.", "One type of measurement bias common in models built for the entire population is a lack of informative features for minority groups. We illustrate this type of bias in the example below, where everybody\u2019s salary is still a function of job type, years of work experience and certifications, but for a percentage of women who are backend developers, salary is determined by what software tools and frameworks they know (kubernetes!). Our example illustrates the scenario in which women are required to know more frameworks than their male counterparts as they are less likely to be hired on potential. We changed the salary of 70% of the women with 7 to 15 years of experience and backend jobs. Changing such a specific group of users allows us to check to what extent analysis tools can find them. For these women, we randomly pick their salary from a uniform distribution when in fact the salary would be fully specified had we collected information regarding software frameworks.", "We learn from exploratory data analysis that after the change in salaries, the salary distributions across the different variables does not change significantly, apart from a small drop in mean salary for women.", "Furthermore, the distribution of labels is similarly balanced across males and females (women have 42% of the salaries above the threshold, while men have 58%). This particular dataset is balanced in terms of the distributions of labels we are trying to predict, so we are not in the sample size disparity scenario which we\u2019ll describe in more detail below. This is why using actual fairness metrics is an important step in the model building process.", "With this dataset in hand, we\u2019re now training a model to predict if applicants make under or over 52K (the threshold). We\u2019re splitting our dataset into a training set and a test set, training the model on the training set and evaluating it on the test set. A minimally tuned XGBoost model trained on the modified dataset has a 86% accuracy in predicting whether the applicant\u2019s salary is over or under the cutoff. Let\u2019s compute the fairness metrics: demographic parity and equal opportunity.", "The equal opportunity metric highlights a gap between males and females with TPR=0.77. In the context of our modified dataset, as we\u2019d expect from the linear model we used to generate the data, there\u2019s a linear relationship between years of work experience and salary. After changing the salary of a portion of female applicants, the relationship between salary and years of experience changes. For applicants with 7 to 15 years of work experience, salary can take a larger range of values. The model will struggle to predict as accurately as before the outcomes for this group, making more errors, thus resulting in lower values for the equal opportunity metric which is the TPR.", "Synthetic datasets such as the one we created above can be very helpful in testing different debiasing approaches. In the case above, of datasets with limited features for applicants from unprivileged groups, the standard exploratory data analysis was not as helpful as fairness metrics in identifying poor model performance. Once we used the additional metrics and identified the limited feature issue, the task ahead is how to improve prediction performance for this group of applicants. An added source of difficulty with real life datasets is that many sources of bias will be present simultaneously, thus making the task of building an equitable model difficult. Lastly, it is worth keeping in mind that the sources of bias we\u2019ve explored so far are easier to diagnose because they cause models to predict outcomes less accurately for a particular group\u00b9\u2070. The most difficult task is to recognize when models make perfectly accurate predictions, but those predictions reflect the inequalities present in our society. Beyond the data itself there are many other sources of bias when it comes to machine learning. A diverse team evaluating model predictions and a critical attitude towards these predictions are absolutely needed to prevent automatic propagation of discrimination.", "In part 2, we\u2019ll look at other sources of bias: skewed samples, sample size disparity and tainted examples.", "[4] A. Narayanan , Tutorial: 21 fairness definitions and their politics, https://www.youtube.com/watch?v=jIXIuYdnyyk", "[7] The The US Equal Employment Opportunity Commission (EEOC), Uniform guidelines on employee selection procedures, March 2, 1979", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbe5b7f3347d0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@cristina.sdir?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cristina.sdir?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Cristina Goldfain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ff2e1d76e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&user=Cristina+Goldfain&userId=7ff2e1d76e43&source=post_page-7ff2e1d76e43----be5b7f3347d0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1901.10002", "anchor_text": "https://arxiv.org/abs/1901.10002"}, {"url": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G", "anchor_text": "https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"}, {"url": "https://science.sciencemag.org/content/366/6464/447", "anchor_text": "https://science.sciencemag.org/content/366/6464/447"}, {"url": "https://www.youtube.com/watch?v=jIXIuYdnyyk", "anchor_text": "https://www.youtube.com/watch?v=jIXIuYdnyyk"}, {"url": "https://www.citizensadvice.org.uk/law-and-courts/discrimination/about-discrimination/equality-act-2010-discrimination-and-your-rights/", "anchor_text": "https://www.citizensadvice.org.uk/law-and-courts/discrimination/about-discrimination/equality-act-2010-discrimination-and-your-rights/"}, {"url": "http://ssrn.com/abstract=2477899,", "anchor_text": "http://ssrn.com/abstract=2477899,"}, {"url": "https://fairmlbook.org/tutorial1.html", "anchor_text": "Fairness in machine learning"}, {"url": "https://mrtz.org/nips17/#/28", "anchor_text": "https://mrtz.org/nips17/#/28"}, {"url": "https://medium.com/tag/data-science?source=post_page-----be5b7f3347d0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----be5b7f3347d0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai-ethics?source=post_page-----be5b7f3347d0---------------ai_ethics-----------------", "anchor_text": "Ai Ethics"}, {"url": "https://medium.com/tag/bias-in-ai?source=post_page-----be5b7f3347d0---------------bias_in_ai-----------------", "anchor_text": "Bias In Ai"}, {"url": "https://medium.com/tag/fairness?source=post_page-----be5b7f3347d0---------------fairness-----------------", "anchor_text": "Fairness"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&user=Cristina+Goldfain&userId=7ff2e1d76e43&source=-----be5b7f3347d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&user=Cristina+Goldfain&userId=7ff2e1d76e43&source=-----be5b7f3347d0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbe5b7f3347d0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----be5b7f3347d0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----be5b7f3347d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cristina.sdir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@cristina.sdir?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Cristina Goldfain"}, {"url": "https://medium.com/@cristina.sdir/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "15 Followers"}, {"url": "http://etiq.ai", "anchor_text": "etiq.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7ff2e1d76e43&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&user=Cristina+Goldfain&userId=7ff2e1d76e43&source=post_page-7ff2e1d76e43--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7ff2e1d76e43%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsources-of-unintended-bias-in-training-data-be5b7f3347d0&user=Cristina+Goldfain&userId=7ff2e1d76e43&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}