{"url": "https://towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136", "time": 1682996558.526229, "path": "towardsdatascience.com/neural-architecture-search-nas-the-future-of-deep-learning-c99356351136/", "webpage": {"metadata": {"title": "Neural Architecture Search (NAS)- The Future of Deep Learning | by Md Ashiqur Rahman | Towards Data Science", "h1": "Neural Architecture Search (NAS)- The Future of Deep Learning", "description": "Most of us have probably heard about the success of ResNet, winner of ILSVRC 2015 in image classification, detection, and localization and Winner of MS COCO 2015 detection, and segmentation. It is an\u2026"}, "outgoing_paragraph_urls": [{"url": "https://theaiacademy.blogspot.com/2020/05/neural-architecture-search-nas-future.html", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1611.01578.pdf", "anchor_text": "reference", "paragraph_index": 16}], "all_paragraphs": ["(Revised Version of this blog can be found here)", "Most of us have probably heard about the success of ResNet, winner of ILSVRC 2015 in image classification, detection, and localization and Winner of MS COCO 2015 detection, and segmentation. It is an enormous architecture with skip connections all over. While using this ResNet as a pre-trained network for my machine learning project, I thought \u201c How can someone come out with such an architecture? \u2019\u2019", "Soon after I learned that many engineers and scientists with their years of experience build this architecture. And there is more of a hunch than complete math that will tell you \u201c we need to have a 5x5 filter now to achieve the best accuracy \u2019\u2019. We have wonderful architectures for the image classification task, but for other tasks, we have to spend much of our energy to find an architecture with reasonable performance for those tasks. It would certainly be better if we can automate this architecture modeling task just as we learn the parameters of machine learning models.", "Neural Architecture Search (NAS), the process of automating architecture engineering i.e. finding the design of our machine learning model. Where we need to provide a NAS system with a dataset and a task (classification, regression, etc), and it will give us the architecture. And this architecture will perform best among all other architecture for that given task when trained by the dataset provided. NAS can be seen as a subfield of AutoML and has a significant overlap with hyperparameter optimization. To understand NAS we need to look deeply into what it is doing. It finds an architecture from all possible architectures by following a search strategy that will maximize the performance. The following figure summarizes the NAS algorithm.", "It has 3 separate dimensions: Search Space, Search Strategy and Performace Estimation.", "Search space defines what neural architecture a NAS approach might discover in principle. It can be chain-like architecture where the output of layer (n-1) is fed as an input of layer (n). Or it can be a modern complex architecture with skip connection (multi-branch network).", "Sometimes people do want to use handcrafted outer architecture( macro-architectures) with repeated motifs or cells. In such cases, the outer structure is fixed and NAS only searches for the cell architectures. This type of search is known as micro-search or cell search.", "In many NAS methods, both micro and macro structures are searched in a hierarchical fashion; which consists of several levels of motifs. The first level consists of the set of primitive operations, the second level of different motifs that connect primitive operations via a directed acyclic graph, the third level of motifs that encode how to connect second-level motifs, and so on.", "To explain the search strategy and performance estimation, three different NAS methods will be discussed in the following part.", "Reinforcement learning is the problem faced by an agent( a program) that must learn behavior through trial and error interactions with a dynamic environment to maximize some reward. An agent( performs some action according to some policy parametrized by \u03b8. Then that agent updates the policy \u03b8 from the reward for that action taken. In the case of NAS, the agent produces the model architecture, child network( the action). Then the model is trained on the dataset and the performance of the model on the validation data is taken as a reward.", "In general, a Recurrent Neural Network (RNN) is taken as a controller or agent. It produces string and the model is built form that string stochastically.", "A sample output for such RNN is shown in the above figure. The hyperparameters for each layer of a convolutional neural net are produced repeatedly up to a certain number of times. And then a model is then built following the output of RNN, which is then trained and validation accuracy is calculated.", "Here, the parameters of RNN acts as the policy \u03b8 of the agent. Production string (height, filter width, stride height, etc) is the action of the agent. The performance of the model on the validation set is the reward of the agent in this reinforcement method. The RNN updates its parameters \u03b8 in a way such that, the expected validation accuracy of the proposed architectures is maximized.", "The RNN is trained by a policy gradient method to iteratively update the policy \u03b8. The main target is to maximize the expected accuracy i.e.", "Here R is the accuracy of the model, a_1: T is the string of length T produced by RNN( the action), \u03b8 is the parameter of RNN.", "As J(\u03b8) is not differentiable, a policy gradient method is used to iteratively update \u03b8. The update function looks like", "This is an approximation of the original equation. The expectation term of J(\u03b8) is replaced by sampling. The outer summation of this updated equation is for that sampling i.e. m numbers of models sampled from RNN and their average is taken. P(a_t|a_(t\u22121):1; \u03b8c), is the probability of taking an action a_t at time t given all actions from 1 to (t-1) according to policy \u03b8 or predicted by RNN with parameter \u03b8 (reference).", "Encoding skip connections is a bit tricky. For this RNN for each layer generates an output called Anchor point. The anchor point is used to indicate skip connections. At layer N, the anchor point will contain N \u2212 1 content-based sigmoids to indicate the previous layers that need to be connected.", "PNAS does a cell search as discussed in the search space part of this tutorial. They construct cells from blocks and construct the Complete network by adding the cells in a predefined manner.", "Cells are connected in a series in a predefined number to form the network. And each cell is formed by several blocks( 5 used in the original paper).", "The blocks consist of predefined operations.", "Operations that have shown in the figure are used in the original paper and can be extended.", "On the left, an example of a complete is shown. Even in this cell or micro search, there are in total 10 \u00b9\u2074 valid combinations to check to find the best cell structure.", "So, to reduce complexity first only cells with only 1 block are constructed. It is easy because with the mentioned operation there are only 256 of different cells that are possible. Then top K best-performing cells are chosen to expand for 2 block cells and it is repeated up to 5 blocks.", "But for a reasonable K, too many 2-block candidates to train. As a solution to this problem, a \u201ccheap\u201d surrogate model that predicts the final performance simply by reading the string(cell is encoded into a string) is trained. The data for this training is collected when the cells are constructed, trained and validated.", "For example, we can construct all 256 of single block cells and measure their performance. And train the surrogate model with this data. Then use this model to predict the performance of 2 block cells without training or testing them. And of course, the surrogate model should be capable of handling variable size input. And then top K best performing 2 block cells as predicted by the model is chosen. These 2 block cells are then trained, the \u201csurrogate\u2019\u2019 model is finetuned and these cells are expanded to 3 blocks and it is iterated until we get 5 block cells.", "The search space for neural architectures is discrete i.e one architecture is different from the other by at least a layer or some parameter in the layer, for example, 5x5 filter vs 7x7 filter. In this method, continuous relaxation is applied to this discrete search which enables direct gradient-based optimization.", "The cell we search can be though as a directed acyclic graph where each node x is a latent representation (e.g. a feature map in convolutional networks) and each directed edge (i, j) is associated with some operation o(i,j) (convolution, max-pooling, etc) that transforms x(i) and stored a latent representation at node x(j)", "the output of each node can be calculated by the equation on the left. The nodes are enumerated in such a way, that there is an edge(i,j) from node x(i) to x(j), then i<j.", "In continuous relaxation, instead of having a single operation between two nodes. a convex combination of each possible operation is used. To model this in the graph, multiple edges between two nodes are kept, each corresponding to a particular operation. And each edge also has a weight \u03b1.", "Now O(i,j) the operation between node x(i) and x(j) is a convex combination of a set of operations o(i,j) where o(.)\u03f5 S, where S is the set of all possible operations.", "The output of O(i,j) is calculated by the left equation.", "The training and the validation loss are denoted by L_train and L_val respectively. Both losses are determined not only by the architecture parameters \u03b1 but also by the weights \u2018w\u2019 in the network. The goal for architecture search is to find \u03b1\u2217 that minimizes the validation loss L_val(w \u2217 , \u03b1\u2217 ), where the weights \u2018w\u2217\u2019 associated with the architecture is obtained by minimizing the training loss.", "This implies a bilevel optimization problem with \u03b1 as the upper-level variable and w as the lower-level variable:", "After training some \u03b1\u2019s of some edges become much larger than the others. To derive the discrete architecture for this continuous model, in between two nodes the only edge with maximum weight is kept.", "When the cells are found, these are then used to construct larger networks.", "There are many other methods that are also used in neural architecture search. And they achieved an impressive performance. But still, we have very little knowledge for answering the question \u2018why does a particular architecture work better?\u2019. I think we are on our way to answering that question.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc99356351136&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c99356351136--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c99356351136--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashiqbuet14?source=post_page-----c99356351136--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashiqbuet14?source=post_page-----c99356351136--------------------------------", "anchor_text": "Md Ashiqur Rahman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc1a468ef277&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&user=Md+Ashiqur+Rahman&userId=fc1a468ef277&source=post_page-fc1a468ef277----c99356351136---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://theaiacademy.blogspot.com/2020/05/neural-architecture-search-nas-future.html", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "image source"}, {"url": "https://arxiv.org/pdf/1808.05377.pdf", "anchor_text": "reference"}, {"url": "https://arxiv.org/pdf/1808.05377.pdf", "anchor_text": "image source"}, {"url": "https://arxiv.org/pdf/1808.05377.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1611.01578.pdf", "anchor_text": "reference"}, {"url": "https://arxiv.org/pdf/1611.01578.pdf", "anchor_text": "image source"}, {"url": "https://arxiv.org/pdf/1611.01578.pdf", "anchor_text": "reference"}, {"url": "https://arxiv.org/pdf/1611.01578.pdf", "anchor_text": "reference"}, {"url": "https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf", "anchor_text": "reference"}, {"url": "https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf", "anchor_text": "image source"}, {"url": "https://cs.jhu.edu/~cxliu/slides/pnas-talk-eccv.pdf", "anchor_text": "reference"}, {"url": "https://arxiv.org/pdf/1806.09055", "anchor_text": "reference"}, {"url": "https://arxiv.org/pdf/1806.09055", "anchor_text": "reference"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c99356351136---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/automl?source=post_page-----c99356351136---------------automl-----------------", "anchor_text": "Automl"}, {"url": "https://medium.com/tag/ai?source=post_page-----c99356351136---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c99356351136---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/autodl?source=post_page-----c99356351136---------------autodl-----------------", "anchor_text": "Autodl"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&user=Md+Ashiqur+Rahman&userId=fc1a468ef277&source=-----c99356351136---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&user=Md+Ashiqur+Rahman&userId=fc1a468ef277&source=-----c99356351136---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c99356351136--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc99356351136&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c99356351136---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c99356351136--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c99356351136--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c99356351136--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c99356351136--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c99356351136--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c99356351136--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c99356351136--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c99356351136--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashiqbuet14?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashiqbuet14?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Md Ashiqur Rahman"}, {"url": "https://medium.com/@ashiqbuet14/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "108 Followers"}, {"url": "http://sites.google.com/view/ashiqurrahman/home", "anchor_text": "sites.google.com/view/ashiqurrahman/home"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc1a468ef277&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&user=Md+Ashiqur+Rahman&userId=fc1a468ef277&source=post_page-fc1a468ef277--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F87eb6f86f29e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-architecture-search-nas-the-future-of-deep-learning-c99356351136&newsletterV3=fc1a468ef277&newsletterV3Id=87eb6f86f29e&user=Md+Ashiqur+Rahman&userId=fc1a468ef277&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}