{"url": "https://towardsdatascience.com/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8", "time": 1682994651.639085, "path": "towardsdatascience.com/lessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8/", "webpage": {"metadata": {"title": "Lessons Learned from Tic-Tac-Toe: Practical Reinforcement Learning Tips | by Shaked Zychlinski | Towards Data Science", "h1": "Lessons Learned from Tic-Tac-Toe: Practical Reinforcement Learning Tips", "description": "Just recently I posted an introduction post to Reinforcement Learning and Deep Q Networks. But as we all know, there\u2019s a huge difference between understanding the theory and actually implementing it\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "introduction post to Reinforcement Learning and Deep Q Networks", "paragraph_index": 0}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "can be found on my GitHub page", "paragraph_index": 2}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "introduction to DQN", "paragraph_index": 3}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "see my implementation", "paragraph_index": 8}, {"url": "https://github.com/yanji84/tic-tac-toe-rl", "anchor_text": "this guy\u2019s game implementation", "paragraph_index": 11}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "my solution of the \u201cfill the board\u201d example", "paragraph_index": 18}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz", "paragraph_index": 23}], "all_paragraphs": ["Just recently I posted an introduction post to Reinforcement Learning and Deep Q Networks. But as we all know, there\u2019s a huge difference between understanding the theory and actually implementing it in the real world.", "It took me a while to find what would be a worthy first challenge for DQN. Most tutorials I saw implemented a DQN + Convolutional nets, and attempted to design an agent that beats Atari or Doom games. This seemed like a distraction: I didn\u2019t see any point of wasting time on designing image-processing networks to solve a Reinforcement problem. And this is why I picked Tic-Tac-Toe: it\u2019s rather simple, so I can train it on my MacBook, no image processing is required as I can code the game, and yet it still requires strategy design in order to win.", "Though I thought it will be straight-forward and simple, it didn\u2019t took me long to realize I was wrong \u2014 and so, I\u2019d like to describe some of the most important lessons I learned while solving this challenge. The code, by the way, can be found on my GitHub page.", "As obvious as it sounds, this is something we all tend to forget every once in a while. If you\u2019ve read my introduction to DQN, you might remember I gave as a \u201cHello World\u201d exercise a very-very simple game: an agent that needs to fill empty slots (and if you haven\u2019t \u2014 no worries, we\u2019ll come back to it very soon). I didn\u2019t come up with this exercise out of thin air.", "When I decided to implement Tic-Tac-Toe, I sat down and wrote pretty much all the code required at once: Game class, Player class, Experience Replay class, Q Network class, Double Deep Q Network implementation \u2014 and pushed play. Guess what? It didn\u2019t learn a thing. Not even what are valid or invalid moves, which is the absolute basics of doing anything.", "Now go figure what\u2019s causing this \u2014 is it the way I represent the states? Is it a hyperparameters issue? Is there a bug in the code? If so, where? So many possibilities to make mistakes, and no idea where to start.", "So I\u2019ve decided to strip it down to the simplest issue I had: the agent can tell apart valid moves from invalid? Then let\u2019s train an agent that only learns to tell apart vacant from non-vacant slots. Nothing else. This quickly proved itself, as I was able to isolate and fix several bugs in my network implementation. Yet still, the agent refused to learn. As now the network was implemented correctly, it took me a while to realize what I was missing:", "As mentioned, the simple task I gave the agent was to fill in all vacant spots of a 4 cells board. An empty cell is represented by \u201c0\u201d and a filled cell by \u201c1\u201d. How many games would you need to play till you understand the logic? Probably not too many. But you\u2019re human, and the computer isn\u2019t.", "I thought that a few hundreds games should be enough, but it turned out that my implementation required around 2500 games to master this. It turned out that 100\u2013200 games were so insufficient, that the cost barely changed. To make it even more confusing, the cost was really low \u2014 but as I figured out later, that was due to network cold-start and insufficient number of trainings (see my implementation). Basically, I didn\u2019t give the network enough trainings to learn anything \u2014 because I underestimated how hard it will be for the agent to learn this.", "Bottom line \u2014 what seems simple to us as humans might not be simple at all for computers (and vice versa \u2014 how much time will it take you to calculate 147x819?). Train the model longer.", "At this point I was able to go back to the Tic-Tac-Toe challenge. The network was working, and the agent was learning. I could easily see it picked up on which moves are valid and which aren\u2019t, and when I examined how it plays, it was obvious it learned how to win \u2014 its attempts to make 3-in-a-row were crystal clear, and I was happy \u2014 but not too much. Despite what it learned, it was also clear he paid no attention to what his opponent was doing, and made no efforts to try and block his opponent from having 3-in-a-row. No matter how long I trained it, how many layers I added to the net or how much I tuned the hyperparameters \u2014 nothing changed that.", "It took me longer than I thought to realize what I was doing wrong. And it had a lot to do with the fact that pretty much all code examples of DQN I saw were of single-player games \u2014 I had no idea how to tackle the multiplayer case. It was only when I found this guy\u2019s game implementation that I understood what I did wrong \u2014 and for this I\u2019ll have to explain how I designed the game. Take a look at the game described below:", "Transitions to odd states are performed by player X, and to even states by player O. This is exactly how I added records to the memory buffer of the model: if s=0 and player X chose the left-middle cell, then s\u2019=1. Player O then received state s=1 and made a transition to s=2. These were two separate records I added to the model\u2019s Experience Replay memory.", "But that means a player can never lose when it\u2019s his turn to play. Let\u2019s look at s=3: player O chose the top-middle cell and transitioned to s=4. He didn\u2019t lose yet. Now it\u2019s player X\u2019s turn, he makes the obvious choice and transitions to s=5, and wins. Only now does player O knows he lost \u2014 and so, in order to have better judgement next time, the negative reward player O receives at s=5 needs to propagate two states backwards, to the Q Values of s=3. Theoretically, this shouldn\u2019t be a problem due to the recursivity of Bellman Equation and repeated training of the network. Theoretically.", "In practice, that doesn\u2019t work \u2014 and for two reasons, I believe. The first, and the more intuitive one, is that the model saw no risk in transitioning from s=3 to s=4, because it didn\u2019t know it is not him who chooses how to transition from s=4. As far as the model knew, he can simply choose a different action when in s=4, preventing him from losing.", "Once I realized this, fixing was easy \u2014 all I needed to do is define the next state as the next board representation the player will see \u2014 so for player O, if s=3 and the action is top-middle cell, then s\u2019=5 and not 4 \u2014 and now the model can learn that this action will cause him to lose the game.", "And what\u2019s the second reason? well \u2014", "Several times during this debugging process, I examined the Q Values the network learned, trying to see if maybe this will assist in finding what I was doing wrong. It took me a while to realize this is pointless, and I\u2019ll explain why.", "This might be a good point to go back to my solution of the \u201cfill the board\u201d example I described above. If you go over the Q-Table and Q-Network algorithms I implemented there, you can see I plotted the predictions of all Q Values when testing them. The Q Values of the table make sense (despite being a little off sometimes due to \u201croads less taken\u201d during training) \u2014 but that\u2019s not the case with the Q-Network predicted values. These have absolutely nothing to do with the ones computed by the table algorithm.", "And when you think about it, that\u2019s actually not that surprising. Remember the network is only an approximation \u2014 and what you actually asking it to do is to predict the best action, which corresponds to the highest Q Value it predicted. Think about it for a second: you don\u2019t really ask the network to learn the correct Q Value, but only that the best action will have the highest predicted value. And these are two completely different things. The network\u2019s inability to predict correct Q Values makes even more sense when you remember that the cost function uses the network\u2019s own predictions when minimizing \u2014 it has no ground truth to compare to. Here it is as a reminder:", "I guess the key message here is that while they\u2019re called Deep Q-Network, the Q Values are not what they actually learn. Confusing, but that\u2019s life.", "Implementing theoretical knowledge is one of the greatest challenges when discovering new fields. Knowing how it works is not the same as making it work. I hope this post will assist you on you journey in Reinforcement Learning and Deep Q-Networks, and I\u2019d be more than happy to hear about your lessons-learned when developing your own algorithms. Make it happen.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Recommendations at Lightricks. Lives in Tel-Aviv, Israel. See me on shakedzy.xyz"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5cac654a45a8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shakedzy.medium.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688----5cac654a45a8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "introduction post to Reinforcement Learning and Deep Q Networks"}, {"url": "https://github.com/shakedzy/tic_tac_toe", "anchor_text": "can be found on my GitHub page"}, {"url": "https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677", "anchor_text": "introduction to DQN"}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "see my implementation"}, {"url": "https://github.com/yanji84/tic-tac-toe-rl", "anchor_text": "this guy\u2019s game implementation"}, {"url": "https://github.com/shakedzy/notebooks/tree/master/q_learning_and_dqn", "anchor_text": "my solution of the \u201cfill the board\u201d example"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5cac654a45a8---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5cac654a45a8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----5cac654a45a8---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5cac654a45a8---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----5cac654a45a8---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&user=Shaked+Zychlinski&userId=43218078e688&source=-----5cac654a45a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&user=Shaked+Zychlinski&userId=43218078e688&source=-----5cac654a45a8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5cac654a45a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5cac654a45a8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5cac654a45a8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5cac654a45a8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5cac654a45a8--------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shakedzy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shaked Zychlinski"}, {"url": "https://shakedzy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.3K Followers"}, {"url": "http://shakedzy.xyz", "anchor_text": "shakedzy.xyz"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F43218078e688&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&user=Shaked+Zychlinski&userId=43218078e688&source=post_page-43218078e688--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4123ceb9438d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flessons-learned-from-tic-tac-toe-practical-reinforcement-learning-tips-5cac654a45a8&newsletterV3=43218078e688&newsletterV3Id=4123ceb9438d&user=Shaked+Zychlinski&userId=43218078e688&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}