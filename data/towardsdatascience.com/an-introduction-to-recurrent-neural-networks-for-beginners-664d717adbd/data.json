{"url": "https://towardsdatascience.com/an-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd", "time": 1682997411.998621, "path": "towardsdatascience.com/an-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd/", "webpage": {"metadata": {"title": "An Introduction to Recurrent Neural Networks for Beginners | by Victor Zhou | Towards Data Science", "h1": "An Introduction to Recurrent Neural Networks for Beginners", "description": "Recurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They\u2019re often used in Natural Language Processing (NLP) tasks because of their effectiveness in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://victorzhou.com/tag/natural-language-processing", "anchor_text": "Natural Language Processing", "paragraph_index": 0}, {"url": "https://www.numpy.org/", "anchor_text": "numpy", "paragraph_index": 0}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/", "anchor_text": "introduction to Neural Networks", "paragraph_index": 1}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "CNNs", "paragraph_index": 2}, {"url": "https://victorzhou.com/blog/intro-to-rnns/", "anchor_text": "victorzhou.com", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Hyperbolic_function", "anchor_text": "tanh", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid", "paragraph_index": 11}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/", "anchor_text": "introduction to Neural Networks", "paragraph_index": 12}, {"url": "https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py", "anchor_text": "dataset", "paragraph_index": 14}, {"url": "https://victorzhou.com/blog/softmax/", "anchor_text": "Softmax", "paragraph_index": 16}, {"url": "https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py", "anchor_text": "dataset", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/One-hot", "anchor_text": "one-hot", "paragraph_index": 21}, {"url": "https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html", "anchor_text": "np.random.randn()", "paragraph_index": 25}, {"url": "https://victorzhou.com/subscribe/?src=intro-to-rnns-medium", "anchor_text": "Subscribe to my newsletter", "paragraph_index": 30}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss", "anchor_text": "Cross-Entropy Loss", "paragraph_index": 33}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/#3-training-a-neural-network-part-1", "anchor_text": "Training a Neural Network", "paragraph_index": 36}, {"url": "https://github.com/vzhou842/rnn-from-scratch", "anchor_text": "Github", "paragraph_index": 36}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-2/#2-training-overview", "anchor_text": "Training Overview", "paragraph_index": 38}, {"url": "https://repl.it/@vzhou842/A-RNN-from-scratch", "anchor_text": "Run this RNN in your browser", "paragraph_index": 60}, {"url": "https://github.com/vzhou842/rnn-from-scratch", "anchor_text": "Github", "paragraph_index": 60}, {"url": "https://victorzhou.com/tag/machine-learning/", "anchor_text": "Machine Learning", "paragraph_index": 62}, {"url": "https://victorzhou.com/subscribe/?src=intro-to-rnns-medium", "anchor_text": "subscribe to my newsletter", "paragraph_index": 62}, {"url": "https://victorzhou.com", "anchor_text": "https://victorzhou.com", "paragraph_index": 64}], "all_paragraphs": ["Recurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They\u2019re often used in Natural Language Processing (NLP) tasks because of their effectiveness in handling text. In this post, we\u2019ll explore what RNNs are, understand how they work, and build a real one from scratch (using only numpy) in Python.", "This post assumes a basic knowledge of neural networks. My introduction to Neural Networks covers everything you\u2019ll need to know, so I\u2019d recommend reading that first.", "One issue with vanilla neural nets (and also CNNs) is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs. Here are a few examples of what RNNs can look like:", "This ability to process sequences makes RNNs very useful. For example:", "Later in this post, we\u2019ll build a \u201cmany to one\u201d RNN from scratch to perform basic Sentiment Analysis.", "Note: I recommend reading the rest of this post on victorzhou.com \u2014 much of the math formatting looks better there.", "RNNs work by iteratively updating a hidden state h, which is a vector that can also have arbitrary dimension. At any given step t,", "Here\u2019s what makes a RNN recurrent: it uses the same weights for each step. More specifically, a typical vanilla RNN uses only 3 sets of weights to perform its calculations:", "We\u2019ll also use two biases for our RNN:", "We\u2019ll represent the weights as matrices and the biases as vectors. These 3 weights and 2 biases make up the entire RNN!", "Here are the equations that put everything together:", "All the weights are applied using matrix multiplication, and the biases are added to the resulting products. We then use tanh as an activation function for the first equation (but other activations like sigmoid can also be used).", "No idea what an activation function is? Read my introduction to Neural Networks like I mentioned. Seriously.", "Let\u2019s get our hands dirty! We\u2019ll implement an RNN from scratch to perform a simple Sentiment Analysis task: determining whether a given text string is positive or negative.", "Here are a few samples from the small dataset I put together for this post:", "Since this is a classification problem, we\u2019ll use a \u201cmany to one\u201d RNN. This is similar to the \u201cmany to many\u201d RNN we discussed earlier, but it only uses the final hidden state to produce the one output y:", "Each x_i\u200b will be a vector representing a word from the text. The output y will be a vector containing two numbers, one representing positive and the other negative. We\u2019ll apply Softmax to turn those values into probabilities and ultimately decide between positive / negative.", "The dataset I mentioned earlier consists of two Python dictionaries:", "We\u2019ll have to do some pre-processing to get the data into a usable format. To start, we\u2019ll construct a vocabulary of all words that exist in our data:", "vocab now holds a list of all words that appear in at least one training text. Next, we'll assign an integer index to represent each word in our vocab.", "We can now represent any given word with its corresponding integer index! This is necessary because RNNs can\u2019t understand words \u2014 we have to give them numbers.", "Finally, recall that each input x_i to our RNN is a vector. We\u2019ll use one-hot vectors, which contain all zeros except for a single one. The \u201cone\u201d in each one-hot vector will be at the word\u2019s corresponding integer index.", "Since we have 18 unique words in our vocabulary, each x_i will be a 18-dimensional one-hot vector.", "We\u2019ll use createInputs() later to create vector inputs to pass in to our RNN.", "It\u2019s time to start implementing our RNN! We\u2019ll start by initializing the 3 weights and 2 biases our RNN needs:", "We use np.random.randn() to initialize our weights from the standard normal distribution.", "Next, let\u2019s implement our RNN\u2019s forward pass. Remember these two equations we saw earlier?", "Here are those same equations put into code:", "Pretty simple, right? Note that we initialized h to the zero vector for the first step, since there\u2019s no previous h we can use at that point.", "Our RNN works, but it\u2019s not very useful yet. Let\u2019s change that\u2026", "Liking this introduction so far? Subscribe to my newsletter to get notified about new Machine Learning posts like this one.", "In order to train our RNN, we first need a loss function. We\u2019ll use cross-entropy loss, which is often paired with Softmax. Here\u2019s how we calculate it:", "where is our RNN\u2019s predicted probability for the correct class (positive or negative). For example, if a positive text is predicted to be 90% positive by our RNN, the loss is:", "Want a longer explanation? Read the Cross-Entropy Loss section of my introduction to Convolutional Neural Networks (CNNs).", "Now that we have a loss, we\u2019ll train our RNN using gradient descent to minimize loss. That means it\u2019s time to derive some gradients!", "\u26a0\ufe0f The following section assumes a basic knowledge of multivariable calculus. You can skip it if you want, but I recommend giving it a skim even if you don\u2019t understand much. We\u2019ll incrementally write code as we derive results, and even a surface-level understanding can be helpful.", "If you want some extra background for this section, I recommend first reading the Training a Neural Network section of my introduction to Neural Networks. Also, all of the code for this post is on Github, so you can follow along there if you\u2019d like.", "Next, we need to edit our forward phase to cache some data for use in the backward phase. While we\u2019re at it, we\u2019ll also setup the skeleton for our backwards phase. Here\u2019s what that looks like:", "Curious about why we\u2019re doing this caching? Read my explanation in the Training Overview of my introduction to CNNs, in which we do the same thing.", "It\u2019s math time! We\u2019ll start by calculating \u2202L\u200b/\u2202y. We know:", "I\u2019ll leave the actual derivation of \u2202L\u200b/\u2202y using the Chain Rule as an exercise for you \ud83d\ude09, but the result comes out really nice:", "Nice. Next up, let\u2019s take a crack at gradients for Why\u200b and by\u200b, which are only used to turn the final hidden state into the RNN\u2019s output. We have:", "where h_n is the final hidden state. Thus,", "We can now start implementing backprop()!", "Reminder: We created self.last_hs in forward() earlier.", "Finally, we need the gradients for Whh\u200b, Wxh\u200b, and b_h\u200b, which are used every step during the RNN. We have:", "because changing Wxh affects every h_t, which all affect y and ultimately L. In order to fully calculate the gradient of Wxh, we\u2019ll need to backpropagate through all timesteps, which is known as Backpropagation Through Time (BPTT):", "Wxh\u200b is used for all x_t\u200b \u2192 h_t\u200b forward links, so we have to backpropagate back to each of those links.", "Once we arrive at a given step t, we need to calculate \u2202h_t/\u2202Wxh\u200b\u200b\u200b:", "The derivative of tanh is well-known:", "We use Chain Rule like usual:", "The last thing we need is \u2202y/\u2202h_t\u200b\u200b. We can calculate this recursively:", "We\u2019ll implement BPTT starting from the last hidden state and working backwards, so we\u2019ll already have \u200b\u2202y/\u2202h_{t+1}\u200b by the time we want to calculate \u200b\u2202y/\u2202h_t\u200b! The exception is the last hidden state, h_n\u200b:", "We now have everything we need to finally implement BPTT and finish backprop():", "We\u2019ve done it! Our RNN is complete.", "It\u2019s finally the moment we been waiting for \u2014 let\u2019s test our RNN!", "First, we\u2019ll write a helper function to process data with our RNN:", "Now, we can write the training loop:", "Running main.py should output something like this:", "Not bad from a RNN we built ourselves. \ud83d\udcaf", "Want to try or tinker with this code yourself? Run this RNN in your browser. It\u2019s also available on Github.", "That\u2019s it! In this post, we completed a walkthrough of Recurrent Neural Networks, including what they are, how they work, why they\u2019re useful, how to train them, and how to implement one. There\u2019s still much more you can do, though:", "I write a lot about Machine Learning, so subscribe to my newsletter if you\u2019re interested in getting future ML content from me.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "CS @ Princeton University. I write about web development, machine learning, and more at https://victorzhou.com."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F664d717adbd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://victorczhou.medium.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Victor Zhou"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd190d205cab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&user=Victor+Zhou&userId=dd190d205cab&source=post_page-dd190d205cab----664d717adbd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://victorzhou.com/tag/natural-language-processing", "anchor_text": "Natural Language Processing"}, {"url": "https://www.numpy.org/", "anchor_text": "numpy"}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/", "anchor_text": "introduction to Neural Networks"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "anchor_text": "CNNs"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Andrej Karpathy"}, {"url": "https://victorzhou.com/blog/intro-to-rnns/", "anchor_text": "victorzhou.com"}, {"url": "https://en.wikipedia.org/wiki/Hyperbolic_function", "anchor_text": "tanh"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "sigmoid"}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/", "anchor_text": "introduction to Neural Networks"}, {"url": "https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py", "anchor_text": "dataset"}, {"url": "https://victorzhou.com/blog/softmax/", "anchor_text": "Softmax"}, {"url": "https://github.com/vzhou842/rnn-from-scratch/blob/master/data.py", "anchor_text": "dataset"}, {"url": "https://en.wikipedia.org/wiki/One-hot", "anchor_text": "one-hot"}, {"url": "https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randn.html", "anchor_text": "np.random.randn()"}, {"url": "https://victorzhou.com/blog/softmax/", "anchor_text": "quick explanation of Softmax"}, {"url": "https://victorzhou.com/subscribe/?src=intro-to-rnns-medium", "anchor_text": "Subscribe to my newsletter"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss", "anchor_text": "Cross-Entropy Loss"}, {"url": "https://victorzhou.com/blog/intro-to-neural-networks/#3-training-a-neural-network-part-1", "anchor_text": "Training a Neural Network"}, {"url": "https://github.com/vzhou842/rnn-from-scratch", "anchor_text": "Github"}, {"url": "https://victorzhou.com/blog/intro-to-cnns-part-2/#2-training-overview", "anchor_text": "Training Overview"}, {"url": "https://docs.scipy.org/doc/numpy/reference/generated/numpy.clip.html", "anchor_text": "np.clip()"}, {"url": "https://en.wikipedia.org/wiki/Vanishing_gradient_problem", "anchor_text": "Exploding or vanishing gradients"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTMs"}, {"url": "https://repl.it/@vzhou842/A-RNN-from-scratch", "anchor_text": "Run this RNN in your browser"}, {"url": "https://github.com/vzhou842/rnn-from-scratch", "anchor_text": "Github"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "Long short-term memory"}, {"url": "https://en.wikipedia.org/wiki/Gated_recurrent_unit", "anchor_text": "Gated Recurrent Units"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks", "anchor_text": "Bidirectional RNNs"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "Word Embeddings"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "Word2Vec"}, {"url": "https://www.nltk.org/", "anchor_text": "Natural Language Toolkit"}, {"url": "https://victorzhou.com/tag/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://victorzhou.com/subscribe/?src=intro-to-rnns-medium", "anchor_text": "subscribe to my newsletter"}, {"url": "https://victorzhou.com/blog/intro-to-rnns/", "anchor_text": "https://victorzhou.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----664d717adbd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----664d717adbd---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----664d717adbd---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----664d717adbd---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----664d717adbd---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&user=Victor+Zhou&userId=dd190d205cab&source=-----664d717adbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&user=Victor+Zhou&userId=dd190d205cab&source=-----664d717adbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F664d717adbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----664d717adbd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----664d717adbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----664d717adbd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----664d717adbd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----664d717adbd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----664d717adbd--------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://victorczhou.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Victor Zhou"}, {"url": "https://victorczhou.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "925 Followers"}, {"url": "https://victorzhou.com", "anchor_text": "https://victorzhou.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdd190d205cab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&user=Victor+Zhou&userId=dd190d205cab&source=post_page-dd190d205cab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8d9c8575861&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-recurrent-neural-networks-for-beginners-664d717adbd&newsletterV3=dd190d205cab&newsletterV3Id=b8d9c8575861&user=Victor+Zhou&userId=dd190d205cab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}