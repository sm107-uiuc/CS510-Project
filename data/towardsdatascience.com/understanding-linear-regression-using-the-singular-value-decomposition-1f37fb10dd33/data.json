{"url": "https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33", "time": 1683015038.31031, "path": "towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33/", "webpage": {"metadata": {"title": "Understanding Linear Regression using the Singular Value Decomposition | by Thalles Silva | Towards Data Science", "h1": "Understanding Linear Regression using the Singular Value Decomposition", "description": "It is very common to see blog posts and educational material explaining linear regression. In most cases, probably because of the big data and deep learning biases, most of these educational\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA", "paragraph_index": 0}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html", "anchor_text": "Boston house-prices dataset", "paragraph_index": 29}, {"url": "https://scikit-learn.org/stable/datasets/index.html#boston-dataset", "anchor_text": "full description here", "paragraph_index": 30}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/", "paragraph_index": 37}], "all_paragraphs": ["It is very common to see blog posts and educational material explaining linear regression. In most cases, probably because of the big data and deep learning biases, most of these educational resources take the gradient descent approach to fit lines, planes, or hyperplanes to high dimensional data. In this post, we will also talk about solving linear regression problems but through a different perspective. Most specifically, we will talk about one of the most fundamental applications of linear algebra and how we can use it to solve regression problems. Yes, I am talking about the SVD or the Singular Value Decomposition. This computational tool is used as a basis to solve a myriad of problems, including dimensionality reduction, with PCA, and statistical learning using linear regression.", "Through the lens of linear algebra, a regression problem reduces to solving systems of linear equations of the form Ax = b. Here, A and b are known, and x is the unknown. We can think of x as our model. In other words, we want to solve the system for x, and hence, x is the variable that relates the observations in A to the measures in b.", "Here, A is a data matrix. We can think of the rows of A as representing different instances of the same phenomenon. They can represent records for individual patients submitted to a hospital, records for different houses being sold, or pictures of different people\u2019s faces. Complementary, we can view the columns of the matrix A as recording different characteristics of each instance in the rows of A. In a patient hospital example, such features might include the blood pressure when he/she arrived at the hospital or if the patient has had a surgical procedure or not.", "Also, note that the matrix A might have different shapes. First, A could be a square matrix. Yes, it is very unlikely (for the situations we usually encounter in data science) but otherwise possible.", "Second, A could have more columns than rows. In this scenario, A would have a short and wide shape. Lastly, (and that is the most usual case in data science), the matrix A assumes the form of a tall and skinny matrix, with many more rows than columns.", "But why should I care for the shape of the matrix A?", "Interestingly, the shape of A will dictates whether the linear system of equations has a solution, has infinitely many solutions, or does not have a solution at all.", "Let\u2019s start with the boring case. If the matrix is squared (number of rows equals the number of columns) and it is invertible, meaning that the matrix A has full rank (all columns are linearly independent), that pretty solves the problem.", "However, if the matrix has more columns than it has rows, we are likely dealing with the case where there are infinitely many solutions. To visualize this curious scenario, picture a 3 \u00d7 6 matrix, i.e., 3 rows and 6 columns. We can think of it as having a 3D space and 6 different vectors that we can use to span the 3D space. However, to span a 3D space, we only need 3 linearly independent vectors, but we have 6! This leaves 3 dependent vectors that can be used to formulate infinitely many solutions.", "Finally, by analogy, if we have a matrix A with more rows than columns, we can view it as trying to span a very high-dimensional space with fewer vectors than we would need. For instance, picture a matrix with 6 rows and 2 columns. Here, we have a 6D space, but we only got 2 vectors to span it. It does not matter how much we try it, in the best case, we can only span a plane on 6D. And that is crucial because we only have a solution to Ax = b if the vector b is in the column space of A. But here, the column space of A spans 2D subspace (a plane) on a much larger 6D space. This makes the probability of the vector b to be in the subspace spanned by the columns of A improbable.", "To visualize how unlikely it is, picture a 3D space and a subspace spanned by two vectors (a plane in 3D). Now, imagine you choose 3 values at random. This will give you a point on the 3D space. Now, ask yourself: what is the probability that my randomly chosen point will be on the plane?", "Nonetheless, in situations where we do not have a solution for a linear system of equations Ax = b (or we have infinitely many solutions), we still want to do our best. And to do this, we need to find the best approximate solution. Here is where the SVD kicks in.", "The main idea of the singular value decomposition, or SVD, is that we can decompose a matrix A, of any shape, into the product of 3 other matrices.", "Here, U is an m \u00d7 m square matrix, \u03a3 is a rectangular matrix of shape m \u00d7 n, and V\u1d40 is a square matrix and has shape n \u00d7 n.", "The matrices U and V\u1d40 have a very special property. They are unitary matrices. One of the main benefits of having unitary matrices like U and V\u1d40 is that if we multiply one of these matrices by its transpose (or the other way around), the result equals the identity matrix.", "On the other hand, the matrix \u03a3 is diagonal, and it stores non-negative singular values ordered by relevance.", "Note that, since the \u03a3 matrix is diagonal, only the first n row diagonal values are worth keeping. Indeed the last n rows of \u03a3 are filled with 0s. For this reason, it is very common to keep only the first r \u00d7 r non-negative diagonal values of \u03a3, along with the corresponding r columns and rows of U and V\u1d40 respectively. Note that r = min(m, n). This is commonly referred to as the economy (or compact) SVD, and from this point on, we will assume the matrices U, \u03a3, and V\u1d40 are derived from the economy procedure.", "It is important to note that the economy SVD produces a change in the shape of the matrices U and \u03a3 (if one of the diagonal values of \u03a3 is zero, V\u1d40 also suffers a shape change). If the diagonal values of \u03a3 are all positives, thus r = n, we discard the right half of the U matrix (the orthogonal complement of U), which gives U a rectangular m \u00d7 r shape. More critical, U, and possibly V\u1d40, are now semi-unitary matrices, which means that only U\u1d40U = V\u1d40V = I.", "The SVD provides a basis that allows us to reconstruct the input signal in terms of low-rank matrix approximations. Let me be more clear. If we combine each column of U with the corresponding row of V\u1d40, and scale the resulting matrix by the corresponding \u03c3 value, we will get the best rank-1 approximation of A in terms of least squares.", "And as we continue combining the columns of U with rows of V\u1d40, scaled by the corresponding \u03c3, we get the next best rank-i approximation of the data matrix A. Indeed, that is another excellent application of the SVD \u2014 data compression. But that is a subject for another writing.", "As we said before, the problem of working with a non-square matrix A is that we cannot invert it. And that is the main reason why we cannot solve the system of equations as we would for a square matrix A. However, if we cannot invert the matrix A, I invite you to ask yourself the following question.", "What would be the best matrix A\u207a that, when multiplied by A, would come as close as possible to identity matrix I?", "The answer to this question solves the problem of finding the best possible solution when the system of equations has infinitely many solutions or no solution at all. Luckily, the answer also lies in the SVD.", "If we know that the SVD always exists (for matrices of any shape), and by combining the columns of U, the rows of V\u1d40, and the singular values \u03c3, we can reconstruct the original input matrix nearly perfectly, what happens if we try to invert the SVD?", "Let me spoil it with no further ado. It turns out that the best matrix A\u207a that approximately solves the question A\u207aA \u2248 I is the inverse of the SVD. In other words, the best approximation for A\u207b\u00b9 is SVD\u207b\u00b9 (the inverse of the SVD). Let\u2019s follow the math.", "First, we compute the SVD of A and get the matrices USV\u1d40. To solve the system of equations for x, I need to multiply both sides of the equation by the inverse of the SVD matrices. Luckily now, it is very easy to invert each one of the 3 SVD matrices. To invert the product of the 3 matrices USV\u1d40, I take the product of the inverse matrices in reverse order!", "After inverting the USV\u1d40 matrices, if we look closely at the left-hand side, we can see that most matrices will cancel like crazy, leaving us with the best approximate solution x\u0302. Note that since the matrix U is semi-unitary, only U\u1d40U = I holds. Moreover, if (and we assume that) all the singular values are non-negative, then V\u1d40 continuous to be a unitary matrix. Hence, to invert U and V\u1d40, we just multiply each one by their transpose, i.e., U\u1d40U = I and VV\u1d40 = I.", "If we go further and substitute our best solution x\u0302 into Ax\u0302, we will see that most of the matrices cancel each other as well, until we reach UU\u1d40. As we said before, U is semi-unitary, and UU\u1d40 is not the identity matrix. Instead, UU\u1d40 is the projection of b onto the subspace spanned by the columns of U (hence columns of A), which is the best approximate solution in terms of least squares, i.e., we found the least squares solution x\u0302 = minimum(\u2016Ax-b\u2016\u2082).", "Note that if A has more columns than rows and infinitely many solutions, the SVD picks the solution with the minimum 2-norm, i.e., x\u0302 = minimum(\u2016x\u0302\u2016\u2082).", "Once we have established the required SVD jargon, we can use it to find approximate solutions for real-world problems. In this example, I am going to use the Boston house-prices dataset. The house-prices data matrix A contains 506 rows (representing individual houses), and 13 columns (each describing a different characteristic of the houses). Some of these 13 features include:", "You can see the full description here.", "We want to predict the median value home price in $1000\u2019s. These measurements are real values ranging from 5 to 50, and they represent the b vector in our system of equations Ax = b.", "As usual, the matrix has many more rows than columns. This means that we cannot invert A to find the solution to Ax = b. Also, it drastically reduces the possibilities of finding a solution. Indeed, such a solution would only be possible if b is a linear combination of the columns of A. However, using the SVD, we will be able to derive the pseudo-inverse A\u207a, to find the best approximate solution in terms of least squares \u2014 which is the projection of the vector b onto the subspace spanned by the columns of A.", "The code is very simple to follow, and the results are excellent. Indeed, they are the best possible for a linear model.", "One quick note, look at line 9 of the python code above. At this line, I appended a column full of 1s to the data matrix A. This column will allow the linear model to learn a bias vector that will add an offset to the hyperplane so that it does not cross the origin.", "Take a look at the train and test results below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Vision & Deep Learning. Personal blog: https://sthalles.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1f37fb10dd33&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@thalles.silva?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca----1f37fb10dd33---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "PCA"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html", "anchor_text": "Boston house-prices dataset"}, {"url": "https://scikit-learn.org/stable/datasets/index.html#boston-dataset", "anchor_text": "full description here"}, {"url": "https://sthalles.github.io/svd-for-regression/", "anchor_text": "https://sthalles.github.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1f37fb10dd33---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----1f37fb10dd33---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/programming?source=post_page-----1f37fb10dd33---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1f37fb10dd33---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----1f37fb10dd33---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&user=Thalles+Silva&userId=f8db098eb9ca&source=-----1f37fb10dd33---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&user=Thalles+Silva&userId=f8db098eb9ca&source=-----1f37fb10dd33---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1f37fb10dd33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1f37fb10dd33---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1f37fb10dd33--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@thalles.silva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thalles Silva"}, {"url": "https://medium.com/@thalles.silva/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "https://sthalles.github.io/", "anchor_text": "https://sthalles.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff8db098eb9ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&user=Thalles+Silva&userId=f8db098eb9ca&source=post_page-f8db098eb9ca--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fea9a35433442&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funderstanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33&newsletterV3=f8db098eb9ca&newsletterV3Id=ea9a35433442&user=Thalles+Silva&userId=f8db098eb9ca&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}