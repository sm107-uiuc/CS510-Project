{"url": "https://towardsdatascience.com/dirichlet-distribution-a82ab942a879", "time": 1682994497.2400708, "path": "towardsdatascience.com/dirichlet-distribution-a82ab942a879/", "webpage": {"metadata": {"title": "Dirichlet distribution. A few months ago, I built a recommender\u2026 | by Sue Liu | Towards Data Science", "h1": "Dirichlet distribution", "description": "A few months ago, I built a recommender system that employed topic modelling to display relevant tasks to employees. The algorithm used was Latent Dirichlet Allocation (LDA), a generative model that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "scikit-learn", "paragraph_index": 0}, {"url": "http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/", "anchor_text": "visualising the Dirichlet distribution", "paragraph_index": 9}, {"url": "https://github.com/yusueliu/medium/blob/master/scripts/plot_dirichlet.py", "anchor_text": "code in my Github repository", "paragraph_index": 22}], "all_paragraphs": ["A few months ago, I built a recommender system that employed topic modelling to display relevant tasks to employees. The algorithm used was Latent Dirichlet Allocation (LDA), a generative model that has been around since the early 2000s\u00b9. Of course, I didn\u2019t rewrite LDA from scratch but used the implementation in Python\u2019s scikit-learn. But it started me thinking about the sequence of research that lead to the creation of the LDA model. The problem with such libraries is that it\u2019s all too easy to include a few lines in your code and just move on, so I dug out my old machine learning books with the goal of knowing enough to be able to explain LDA in all its gory probabilistic detail. At one point there was a worry that it would turn into an infinite regression, but in the end reason prevailed, and this sequence of articles was constructed. In reverse order, we have:", "V: Latent Dirichlet Allocation (LDA)IV: Latent Semantic Indexing (LSA)III: Mixture models and the EM algorithmII: Bayesian generative modelsI: Dirichlet distribution", "Hopefully by the time we get to the end the aim would have been achieved. We shall start with the Dirichlet distribution.", "Looking up the Dirichlet distribution in any textbook and we encounter the following definition:", "The Dirichlet distribution Dir(\u03b1) is a family of continuous multivariate probability distributions parameterized by a vector \u03b1 of positive reals. It is a multivariate generalisation of the Beta distribution. Dirichlet distributions are commonly used as prior distributions in Bayesian statistics.", "An immediate question is why is the Dirichlet distribution used as a prior distribution in Bayesian statistics? One reason is that it is the conjugate prior to a number of important probability distributions: the categorical distribution and the multinomial distribution. Using it as a prior makes the maths a lot easier.", "In Bayesian probability theory, if the posterior distribution p(\u03b8|x) and the prior distribution p(\u03b8) are from the same probability distribution family, then the prior and posterior are called conjugate distributions, and the prior is the conjugate prior for the likelihood function.", "If we think about the problem of inferring the parameter \u03b8 for a distribution from a given set of data x, then Bayes\u2019 theorem says that the posterior distribution is equal to the product of the likelihood function \u03b8 \u2192 p(x|\u03b8) and the prior p(\u03b8), normalised by the probability of the data p(x):", "Since the likelihood function is usually defined from the data generating process, we can see that the difference choices of prior can make the integral more or less difficult to calculate. If the prior has the same algebraic form as the likelihood, then often we can obtain a closed-form expression for the posterior, avoiding the need of numerical integration.", "We show how the Dirichlet distribution can be used to characterise the random variability of a multinomial distribution. I\u2019ve borrowed this example from a great blog post on visualising the Dirichlet distribution.", "Suppose we are going to manufacture 6-sided dice but allow the outcomes of a toss to be only 1, 2 or 3 (this is so the later visualisation is easier). If the die is fair then the probabilities of the three outcomes will be the same and equal to 1/3. We can represent the probabilities for the outcomes as a vector \u03b8 =(\u03b8\u2081, \u03b8\u2082, \u03b8\u2083).", "\u03b8 has two important properties: first, the sum of the probabilities for each entry must equal one, and none of the probabilities can be negative. When these conditions hold, then the results associated with rolling of the die can be described by a multinomial distribution.", "In other words, if we observe n dice rolls, D={x\u2081,\u2026,x_k}, then the likelihood function has the form", "Where N_k is the number of times the value k\u2208{1, 2, 3} has occurred.", "We expect there will be some variability in the characteristics of the dice we produce, so even if we try to produce fair dice, we won\u2019t expect the probabilities of each outcome for a particular die will be exactly 1/3, due to variability in the production process. To characterise this variability mathematically, we would like to know the probability density of every possible value of \u03b8 for a given manufacturing process. To do this, let\u2019s consider each element of \u03b8 as being an independent variable. That is, for \u03b8 =(\u03b8\u2081, \u03b8\u2082, \u03b8\u2083), we can treat \u03b8\u2081, \u03b8\u2082 and \u03b8\u2083 each as an independent variable. Since the multinomial distribution requires that these three variables sum to 1, we know that the allowable values of \u03b8 are confined to a plane. Furthermore, since each value \u03b8\u1d62 must be greater than or equal to zero, the set of all allowable values of \u03b8 is confined to a triangle.", "What we want to know is the probability density at each point on this triangle. This is where the Dirichlet distribution can help us: we can use it as the prior for the multinomial distribution.", "The Dirichlet distribution defines a probability density for a vector valued input having the same characteristics as our multinomial parameter \u03b8. It has support (the set of points where it has non-zero values) over", "where K is the number of variables. Its probability density function has the following form:", "The Dirichlet distribution is parameterised by the vector \u03b1, which has the same number of elements K as our multinomial parameter \u03b8. So you can interpret p(\u03b8|\u03b1) as answering the question \u201cwhat is the probability density associated with multinomial distribution \u03b8, given that our Dirichlet distribution has parameter \u03b1.\u201d", "We see the Dirichlet distribution indeed has the same form as the multinomial likelihood distribution. But what does it actually look like?", "To see this we need to note that it is the multivariate generalisation of the beta distribution. The beta distribution is defined on the interval [0, 1] parameterised by two positive shape parameters \u03b1 and \u03b2. As might be expected, they are the conjugate priors for the binomial (including Bernoulli) distributions. The figure shows the probability density function for the Beta distribution with a number of \u03b1 and \u03b2 values.", "As we can see, the beta density function can take a wide variety of different shapes depending on \u03b1 and \u03b2. When both \u03b1 and \u03b2 are less than 1, the distribution is U-shaped. In the limit of \u03b1 = \u03b2 \u2192 0, it is a 2-point Bernoulli distribution with equal probability 1/2 at each Dirac delta function ends x=0 and x=1, and zero probability everywhere else. When \u03b1=\u03b2=1 we have the uniform [0, 1] distribution, which is the distribution with the largest entropy. When both \u03b1 and \u03b2 are greater than 1 the distribution is unimodal. This diversity of shapes by varying only two parameters makes it particularly useful for modelling actual measurements.", "For the Dirichlet distribution Dir(\u03b1) we generalise these shapes to a K simplex. For K=3, visualising the distribution requires us to do the following: 1. Generate a set of x-y coordinates over our triangle, 2. Map the x-y coordinates to the 2-simplex coordinate space, 3. Compute Dir(\u03b1) for each point. Below are some examples, you can find the code in my Github repository.", "We see it is now the parameter \u03b1 that governs the shapes of the distribution. In particular the sum \u03b1\u2080=\u2211\u03b1\u1d62 controls the strength of the distribution (how peaked it is). If \u03b1\u1d62 < 1 for all i, we get \u2018spikes\u2019 at the corners of the simplex. For values of \u03b1\u1d62 > 1, the distribution tends toward the centre of the simplex. As \u03b1\u2080 increases, the distribution becomes more tightly concentrated around the centre of the simplex.", "In the context of our original dice experiment, we would produce consistently fair dice as \u03b1\u1d62 \u2192 \u221e. For a symmetric Dirichlet distribution with \u03b1\u1d62 > 1, we will produce a fair dice, on average. If the goal is to produce loaded dice (e.g. with a higher probability of rolling a 3), we would want an asymmetric Dirichlet distribution with a higher value for \u03b1\u2083.", "We now have seen what the Dirichlet distribution is, what it looks like and the implications of using it as a prior for a multinomial likelihood function in the context of a dice-manufacturing example. In the next post we\u2019ll dive into Bayesian generative models and how to perform inference.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior Machine Learning Engineer at UserTesting"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa82ab942a879&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a82ab942a879--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@yu.sue.liu?source=post_page-----a82ab942a879--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yu.sue.liu?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Sue Liu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc249737ff5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&user=Sue+Liu&userId=fc249737ff5a&source=post_page-fc249737ff5a----a82ab942a879---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html", "anchor_text": "scikit-learn"}, {"url": "http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/", "anchor_text": "visualising the Dirichlet distribution"}, {"url": "https://github.com/yusueliu/medium/blob/master/scripts/plot_dirichlet.py", "anchor_text": "code in my Github repository"}, {"url": "http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf", "anchor_text": "Latent Dirichlet Allocation"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a82ab942a879---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----a82ab942a879---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&user=Sue+Liu&userId=fc249737ff5a&source=-----a82ab942a879---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&user=Sue+Liu&userId=fc249737ff5a&source=-----a82ab942a879---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa82ab942a879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a82ab942a879---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a82ab942a879--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a82ab942a879--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a82ab942a879--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a82ab942a879--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a82ab942a879--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yu.sue.liu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@yu.sue.liu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sue Liu"}, {"url": "https://medium.com/@yu.sue.liu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "112 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffc249737ff5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&user=Sue+Liu&userId=fc249737ff5a&source=post_page-fc249737ff5a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ffc249737ff5a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdirichlet-distribution-a82ab942a879&user=Sue+Liu&userId=fc249737ff5a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}