{"url": "https://towardsdatascience.com/what-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278", "time": 1682995074.664315, "path": "towardsdatascience.com/what-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278/", "webpage": {"metadata": {"title": "What my first Silver Medal taught me about Text Classification and Kaggle in general? | by Rahul Agarwal | Towards Data Science", "h1": "What my first Silver Medal taught me about Text Classification and Kaggle in general?", "description": "Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037 In this\u2026"}, "outgoing_paragraph_urls": [{"url": "https://bit.ly/2UZNv8u", "anchor_text": "Natural Language Processing", "paragraph_index": 1}, {"url": "https://bit.ly/2GCXYnt", "anchor_text": "Advanced machine learning specialization", "paragraph_index": 1}, {"url": "https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch", "anchor_text": "This", "paragraph_index": 9}, {"url": "https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/", "anchor_text": "here", "paragraph_index": 9}, {"url": "https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout", "anchor_text": "kernel", "paragraph_index": 9}, {"url": "https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings", "anchor_text": "SRK", "paragraph_index": 12}, {"url": "https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding", "anchor_text": "Shujian", "paragraph_index": 12}, {"url": "https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2", "anchor_text": "Theo Viel", "paragraph_index": 12}, {"url": "https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/", "anchor_text": "first post", "paragraph_index": 14}, {"url": "https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/", "anchor_text": "second post", "paragraph_index": 14}, {"url": "https://mlwhiz.com/blog/2018/12/17/text_classification/", "anchor_text": "What Kagglers are using for Text Classification", "paragraph_index": 15}, {"url": "https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/", "anchor_text": "how to switch from Keras to Pytorch", "paragraph_index": 15}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80665", "anchor_text": "scraping", "paragraph_index": 17}, {"url": "https://www.kaggle.com/mlwhiz/multimodel-ensemble-clean-kernel?scriptVersionId=10279838", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://www.kaggle.com/wowfattie/3rd-place", "anchor_text": "kernel", "paragraph_index": 21}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568", "anchor_text": "architecture", "paragraph_index": 24}, {"url": "https://www.kaggle.com/kentaronakanishi/18th-place-solution", "anchor_text": "18th place kernel", "paragraph_index": 31}, {"url": "https://amzn.to/2XSZBWV", "anchor_text": "NLP", "paragraph_index": 32}, {"url": "https://bit.ly/2IlOqP7", "anchor_text": "\u201cHow to win a data science competition\u201d", "paragraph_index": 33}, {"url": "https://www.linkedin.com/in/rahulagwl/", "anchor_text": "LinkedIn", "paragraph_index": 35}, {"url": "https://twitter.com/MLWhiz", "anchor_text": "Twitter", "paragraph_index": 35}, {"url": "https://medium.com/@rahul_agarwal", "anchor_text": "Medium", "paragraph_index": 35}, {"url": "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/", "anchor_text": "mlwhiz.com", "paragraph_index": 36}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal", "paragraph_index": 38}], "all_paragraphs": ["Kaggle is an excellent place for learning. And I learned a lot of things from the recently concluded competition on Quora Insincere questions classification in which I got a rank of 182/4037 In this post, I will try to provide a summary of the things I tried. I will also try to summarise the ideas which I missed but were a part of other winning solutions.", "As a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.", "So first a little bit of summary about the competition for the uninitiated. In this competition, we had to develop models that identify and flag insincere questions. The challenge was not only a test for performance but also a test of efficient code writing skills. As it was a kernel competition with limited outside data options, competitors were limited to use only the word embeddings provided by the competition organizers. That means we were not allowed to use State of the art models like BERT. We were also limited in the sense that all our models should run in a time of 2 hours. So say bye bye to stacking and monster ensembles though some solutions were able to do this by making their code ultra-efficient. More on this later.", "There were a couple of learnings about kaggle as a whole that I would like to share before jumping into my final solution:", "One of the things that genuinely baffled a lot of people in this competition was that a good CV score did not necessarily translate well to a good LB score. The main reason for this was a small test dataset(only 65k rows) in the first stage(around 15% of total test data).", "A common theme on discussion forums was focussing on which submissions we should select as the final submission:", "And while it seems simple to say to trust your CV, common sense goes for a toss when you see that your LB score is going down or remaining constant whenever your Local CV score increases.", "Luckily I didn\u2019t end up making the mistake of not trusting my CV score. Owing to a lot of excellent posts on Kaggle discussion board, I selected a kernel with Public LB score of 0.697 and a Local CV of 0.701, which was around >1200 rank on Public LB as of the final submission. It achieved a score of 0.702 and ranked 182 on the private LB.", "While this seems like a straightforward choice post-facto, it was a hard decision to make at a time when you have at your disposal some public kernels having Public LB score >= 0.70", "This Pytorch kernel by Benjamin Minixhofer is awesome. It made the base of many of my submissions for this competition. But this kernel had a mistake. It didn\u2019t implement spatial dropout in the right way. You can find the correct implementation of spatial dropout in my post here or on my kernel. Implementing spatial dropout in the right way gave a boost of around 0.004 to the local CV.", "Nonetheless, I learned pytorch using this kernel, and I am grateful to him for the same.", "I will talk about two things here:", "You can learn a lot just by being part of discussion forums and following public kernels. This competition had a lot of excellent public kernels on embeddings by SRK, Models by Shujian, and Preprocessing by Theo Viel which gave everyone a headstart. As the competition progressed, the discussions also evolved. There were discussions on speeding up the code, working approaches, F1 threshold finders, and other exciting topics which kept me occupied with new ideas and improvements.", "Even after the end, while reading up discussions on solutions overview, I learned a lot. And I would say it is very vital to check out the winning solutions.", "Sharing is everything on Kaggle. People have shared their codes as well as their ideas while competing as well as after the competition ended. It is only together that we can go forward. I like blogging, so I am sharing the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I will delve deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. To make this post platform generic I will try to write code in both Keras and Pytorch. We will try to use various other models which we were not able to use in this competition like ULMFit transfer learning approaches in the fourth post in the series.", "It might take me a little time to write the whole series. Till then you can take a look at my other posts too: What Kagglers are using for Text Classification, which talks about various deep learning models in use in NLP and how to switch from Keras to Pytorch.", "We were going along happily towards the end of the competition with two weeks left. Scores were increasing slowly. The top players were somewhat stagnant. And then came Pavel and team with a Public LB score of 0.782. The next group had an LB score of 0.713. Such a huge difference. I was so sure that there was some leakage in the data which nobody has caught yet except for Pavel. I spent nearly half a day to do EDA again.", "In the end, it turned out that what they did was scraping \u2014 nicely played!", "They also have some pretty awesome ideas around including additional data, which could have worked but did not in this competition.", "My main focus was on meta-feature engineering and on increasing embedding coverage and quality. That means I did not play much with various Neural Net architectures. Here are the things that I included in my final submission:", "You can find the kernel for my final submission here.", "In the third place solution kernel, wowfattie uses stemming, lemmatization, capitalize, lower, uppercase, as well as embedding of the nearest word using a spell checker to get embeddings for all words in his vocab. Such a great idea. I liked this solution the best as it can do what I was trying to do and finished at a pretty good place. Also, the code is very clean.", "Get a lot of models at no cost. Most of the winning solutions have some version of checkpoint ensembling. For the third place solution, the predictions are a weighted average of predictions after the 4th epoch and predictions after the 5th epoch. I got this idea but forgot to implement it in my ensemble based kernel submission.", "A lot of winning solutions ended up using weighted meta embeddings where they provided a higher weight to the Glove embedding. Some solutions also used concatenated embeddings.", "One surprising thing I saw people doing was to use a 1Dconv layer just after the Bidirectional layer. For example, This is the architecture used by the team that placed first in the competition.", "Another thing I noticed is the increased number of hidden units as compared to many public kernels. Most of the public kernels used a hidden unit size of 60 due to time constraints. I used 80 units at the cost of training one less network. A lot of high scoring kernels were able to use a higher number of hidden units owing to variable sequence length idea or bucketing. From the 1st place kernel discussion:", "We do not pad sequences to the same length based on the whole data, but just on a batch level. That means we conduct padding and truncation on the data generator level for each batch separately, so that length of the sentences in a batch can vary in size. Additionally, we further improved this by not truncating based on the length of the longest sequence in the batch but based on the 95% percentile of lengths within the sequence. This improved runtime heavily and kept accuracy quite robust on single model level, and improved it by being able to average more models.", "Bucketing is to make a minibatch from instances that have similar lengths to alleviate the cost of padding. This makes the training speed more than 3x faster, and thus I can run 9 epochs for each split of 5-fold.", "Thus the use of this technique also allowed some competitors to fit many more epochs in less time and run more models at the same time. Pretty Neat!", "Most of us saw a distribution of question length and took the length that covered maximum questions fully as the maxlen parameter. I never tried to tune it, but it seems like it could have been tuned. One of the tricks was to use maxlen ranging from 35 to 60. This made the kernels run a lot faster.", "Most of the winning solutions didn\u2019t use capsule networks as they took a lot of time to train.", "Another thing I saw was in the 18th place kernel which uses a single model", "It was a good and long 2-month competition, and I learned a lot about Text and NLP during this time. I want to emphasize here is that I ended up trying a lot of things that didn\u2019t work before reaching my final solution. It was a little frustrating at times, but in the end, I was happy that I ended up with the best data science practices.", "Would also like to thank Kaggle master Kazanova who along with some of his friends released a \u201cHow to win a data science competition\u201d Coursera course. I learned a lot from this course.", "Let me know in the comments if you think something is missing/wrong or if I could add more tips/tricks for this competition. Or maybe what are your thoughts and what you would have done.", "Feel free to connect with me on LinkedIn, follow me on Twitter / Medium or message me for comments. Keep tuned and keep learning.", "Originally published at mlwhiz.com on February 19, 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "4M Views. Bridging the gap between Data Science and Intuition. MLE@FB, Ex-WalmartLabs, Citi. Connect on Twitter @mlwhiz \u2615\ufe0f ko-fi.com/rahulagarwal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Febae0df16278&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mlwhiz.medium.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9----ebae0df16278---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://bit.ly/2UZNv8u", "anchor_text": "Natural Language Processing"}, {"url": "https://bit.ly/2GCXYnt", "anchor_text": "Advanced machine learning specialization"}, {"url": "https://www.kaggle.com/bminixhofer/deterministic-neural-networks-using-pytorch", "anchor_text": "This"}, {"url": "https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/", "anchor_text": "here"}, {"url": "https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-spatial-dropout", "anchor_text": "kernel"}, {"url": "https://www.kaggle.com/sudalairajkumar/a-look-at-different-embeddings", "anchor_text": "SRK"}, {"url": "https://www.kaggle.com/shujian/mix-of-nn-models-based-on-meta-embedding", "anchor_text": "Shujian"}, {"url": "https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2", "anchor_text": "Theo Viel"}, {"url": "https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/", "anchor_text": "first post"}, {"url": "https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/", "anchor_text": "second post"}, {"url": "https://mlwhiz.com/blog/2018/12/17/text_classification/", "anchor_text": "What Kagglers are using for Text Classification"}, {"url": "https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/", "anchor_text": "how to switch from Keras to Pytorch"}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80665", "anchor_text": "scraping"}, {"url": "https://www.kaggle.com/mlwhiz/multimodel-ensemble-clean-kernel?scriptVersionId=10279838", "anchor_text": "here"}, {"url": "https://www.kaggle.com/wowfattie/3rd-place", "anchor_text": "kernel"}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80568", "anchor_text": "architecture"}, {"url": "https://www.kaggle.com/c/quora-insincere-questions-classification/discussion/80561", "anchor_text": "discussion"}, {"url": "https://www.kaggle.com/kentaronakanishi/18th-place-solution", "anchor_text": "18th place kernel"}, {"url": "https://amzn.to/2XSZBWV", "anchor_text": "NLP"}, {"url": "https://bit.ly/2IlOqP7", "anchor_text": "\u201cHow to win a data science competition\u201d"}, {"url": "https://www.linkedin.com/in/rahulagwl/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/MLWhiz", "anchor_text": "Twitter"}, {"url": "https://medium.com/@rahul_agarwal", "anchor_text": "Medium"}, {"url": "https://mlwhiz.com/blog/2019/02/19/siver_medal_kaggle_learnings/", "anchor_text": "mlwhiz.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ebae0df16278---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ebae0df16278---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ebae0df16278---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----ebae0df16278---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ebae0df16278---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----ebae0df16278---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&user=Rahul+Agarwal&userId=e8cce06956c9&source=-----ebae0df16278---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Febae0df16278&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ebae0df16278---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ebae0df16278--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ebae0df16278--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ebae0df16278--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ebae0df16278--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ebae0df16278--------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mlwhiz.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rahul Agarwal"}, {"url": "https://mlwhiz.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13.9K Followers"}, {"url": "http://ko-fi.com/rahulagarwal", "anchor_text": "ko-fi.com/rahulagarwal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe8cce06956c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&user=Rahul+Agarwal&userId=e8cce06956c9&source=post_page-e8cce06956c9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff41165c9f72f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-my-first-silver-medal-taught-me-about-text-classification-and-kaggle-in-general-ebae0df16278&newsletterV3=e8cce06956c9&newsletterV3Id=f41165c9f72f&user=Rahul+Agarwal&userId=e8cce06956c9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}