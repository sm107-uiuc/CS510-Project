{"url": "https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef", "time": 1683016900.895829, "path": "towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef/", "webpage": {"metadata": {"title": "Deep Learning Frameworks for Parallel and Distributed Infrastructures | by Jordi TORRES.AI | Towards Data Science", "h1": "Deep Learning Frameworks for Parallel and Distributed Infrastructures", "description": "[This post will be used in the master course Supercomputers Architecture at UPC Barcelona Tech with the support of the BSC] This third post of this series will explore some fundamental concepts in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture", "paragraph_index": 0}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 0}, {"url": "https://bsc.es/", "anchor_text": "BSC", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=EeMCEQa85tw", "anchor_text": "video 4:49", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/tagged/supercomputing-for-a-i", "anchor_text": "this series", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/convolutional-neural-networks-for-beginners-using-keras-and-tensorflow-2-c578f7b3bf25", "anchor_text": "predictive accuracy of the model", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/supercomputing-the-heart-of-deep-learning-and-artificial-intelligence-49218c6bdee5", "anchor_text": "computational speed of the process", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1312.6186", "anchor_text": "interesting property", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1404.5997", "anchor_text": "it requires less communication between nodes, as it benefits from a high amount of computations per weight", "paragraph_index": 14}, {"url": "https://dl.acm.org/citation.cfm?doid=2901318.2901323", "anchor_text": "model has to fit on each node entirely", "paragraph_index": 14}, {"url": "http://jmlr.org/proceedings/papers/v28/coates13.pdf", "anchor_text": "and works well for GPUs in a single server that shares a high-speed bus", "paragraph_index": 16}, {"url": "https://arxiv.org/pdf/1802.09941.pdf", "anchor_text": "this survey", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d", "anchor_text": "As we presented in the first post of this series", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow of Pytorch", "paragraph_index": 30}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html", "anchor_text": "torch.nn.parallel.DistributedDataParallel", "paragraph_index": 30}, {"url": "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy", "anchor_text": "tf.distribute.MirroredStrategy", "paragraph_index": 30}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "where 54 servers are linked together with an InfiniBand network on optical fiber", "paragraph_index": 31}, {"url": "https://horovod.ai/", "anchor_text": "Horovod", "paragraph_index": 32}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 32}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 32}, {"url": "http://groups.csail.mit.edu/vision/TinyImages/", "anchor_text": "80 million tiny images dataset", "paragraph_index": 34}, {"url": "https://github.com/jorditorresBCN/SA-MIRI-2020", "anchor_text": "course repository GitHub", "paragraph_index": 36}, {"url": "https://keras.io/api/applications/", "anchor_text": "Keras Applications", "paragraph_index": 38}, {"url": "https://keras.io/api/applications/", "anchor_text": "here", "paragraph_index": 39}, {"url": "https://keras.io/api/applications/resnet/#resnet50v2-function", "anchor_text": "ResNet50v2", "paragraph_index": 39}, {"url": "https://keras.io/api/applications/resnet/#resnet152v2-function", "anchor_text": "ResNet152v2", "paragraph_index": 39}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "eep Residual Learning for Image Recognition", "paragraph_index": 40}, {"url": "https://arxiv.org/pdf/1603.05027.pdf", "anchor_text": "Identity Mapping in Deep Residual Network", "paragraph_index": 41}, {"url": "https://github.com/jorditorresBCN/SA-MIRI-2020", "anchor_text": "course repository GitHub", "paragraph_index": 46}, {"url": "https://keras.io/api/applications/", "anchor_text": "networks available in Keras", "paragraph_index": 50}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "salloc", "paragraph_index": 51}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "sbatch", "paragraph_index": 51}, {"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture", "paragraph_index": 53}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 53}, {"url": "https://bsc.es/", "anchor_text": "BSC", "paragraph_index": 53}, {"url": "https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af", "anchor_text": "the next post", "paragraph_index": 54}, {"url": "https://www.bsc.es/dominguez-bermudez-juan-luis", "anchor_text": "Juan Luis Dom\u00ednguez", "paragraph_index": 55}, {"url": "https://www.bsc.es/aranda-llorens-oriol", "anchor_text": "Oriol Aranda", "paragraph_index": 55}, {"url": "https://www.bsc.es/tripiana-carlos", "anchor_text": "Carlos Tripiana", "paragraph_index": 55}, {"url": "https://www.bsc.es/jover-alvarez-alvaro", "anchor_text": "Alvaro Jover Alvarez", "paragraph_index": 55}, {"url": "https://www.bsc.es/escobar-castells-miquel", "anchor_text": "Miquel Escobar Castells", "paragraph_index": 55}, {"url": "https://www.bsc.es/garcia-fuentes-raul", "anchor_text": "Raul Garcia Fuentes", "paragraph_index": 55}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 57}], "all_paragraphs": ["[This post will be used in the master course Supercomputers Architecture at UPC Barcelona Tech with the support of the BSC]", "\u201cMethods that scale with computation are the future of Artificial Intelligence\u201d \u2014 Rich Sutton, father of reinforcement learning (video 4:49)", "This third post of this series will explore some fundamental concepts in distributed and parallel Deep Learning training and introduce current deep learning frameworks used by the community. We also introduce a sequential code of an image classification problem that we will use as a baseline to calculate the scalability performance that can be obtained with the proposed parallel and distributed frameworks in the next two posts.", "DNN base their success on building high learning capacity models with millions of parameters that are tuned in a data-driven fashion. These models are trained by processing millions of examples so that the development of more accurate algorithms are usually limited by the throughput of the computing devices on which they are trained.", "In order to make the training process faster we are going to need some performance metrics to measure it. The term performance in these systems has a double interpretation. On the one hand, it refers to the predictive accuracy of the model. On the other, to the computational speed of the process.", "The accuracy is independent of the platform, and it is the performance metric to compare multiple models. In contrast, the computation speed depends on the platform on which the model is deployed. This post will measure it by metrics such as Speedup, the ratio of solution time for the sequential (or 1 GPU) algorithms versus its parallel counterpart (with many GPUs). This is a prevalent concept in our daily argot in the Supercomputing community ;-) .", "Another important metric is Throughput; for example, the number of images per unit time that can be processed. This can give us a good benchmark of performance (although it depends on the network type).", "Finally, a concept that we usually use is Scalability. It is a more generic concept that refers to the ability of a system to handle a growing amount of work efficiently. These metrics will be highly dependent on the cluster configuration, the type of network used, or the framework\u2019s efficiency using the libraries and managing resources.", "For this reason, an approach for parallel and distributed training is used. The main idea behind this computing paradigm is to run tasks in parallel instead of serially, as it would happen in a single machine.", "DNN are often compute-intensive, making them similar to traditional supercomputing (high-performance computing, HPC) applications. Thus, large learning workloads perform very well on accelerated systems such as general-purpose graphics processing units (GPU) that have been used in the Supercomputing field.", "Multiple GPUs increase both memory and compute available for training a DNN. In a nutshell, we have several choices, given a minibatch of training data that we want to classify. In the next subsection, we will go into more detail about this.", "To achieve the distribution of the training step, there are two principal implementations, and it will depend on the needs of the application to know which one will perform better, or even if a mix of both approaches can increase the performance.", "For example, different layers in a Deep Learning model may be trained in parallel on different GPUs. This training procedure is commonly known as Model parallelism. Another approach is Data parallelism, where we use the same model for every execution unit, but train the model in each computing device using different training samples.", "In this mode, the training data is divided into multiple subsets, and each one of them is run on the same replicated model in a different GPU (worker nodes). These will need to synchronize the model parameters (or its \u201cgradients\u201d) at the end of the batch computation to ensure they are training a consistent model (just as if the algorithm run on a single processor) because each device will independently compute the errors between its predictions for its training samples and the labeled outputs (correct values for those training samples). Therefore, each device must send all of its changes to all of the models on all the other devices.", "One interesting property of this setting is that it will scale with the amount of data available, and it speeds up the rate at which the entire dataset contributes to the optimization. Also, it requires less communication between nodes, as it benefits from a high amount of computations per weight. On the other hand, the model has to fit on each node entirely, and it is mainly used for speeding computation of convolutional neural networks with large datasets.", "We could partition the network layers across multiple GPUs (even we could split the work required by individual layers). That is, each GPU takes as input the data flowing into a particular layer, processes data across several subsequent layers in the neural network, and then sends the data to the next GPU.", "In this case (also known as Network Parallelism), the model will be segmented into different parts that can run concurrently, and each one will run on the same data in different nodes. This method\u2019s scalability depends on the degree of task parallelization of the algorithm, and it is more complex to implement than the previous one. It may decrease the communication needs, as workers need only to synchronize the shared parameters (usually once for each forward or backward-propagation step) and works well for GPUs in a single server that shares a high-speed bus. It can be used with larger models as hardware constraints per node are no more a limitation.", "In general, the parallelization of the algorithm is more complex to implement than run the same model in a different node with a subset of data.", "In this series of posts, we will focus on the Data Parallelism approach.", "In distributed environments, there may be multiple instances of Stochastic gradient descent (SGD) running independently. Thus, the overall algorithm must be adapted and should consider different issues related to the model consistency or parameters distribution.", "Stochastic gradient descent (SGD) is an iterative algorithm that involves multiple rounds of training, where the results of each round are incorporated into the model in preparation for the next round. The rounds can be run on multiple devices, either synchronously or asynchronously.", "Each SGD iteration runs on a mini-batch of training samples. In synchronous training, all the devices train their local model using different parts of data from a single (large) mini-batch. They then communicate their locally calculated gradients (directly or indirectly) to all devices. Only after all devices have successfully computed and sent their gradients th model is updated. The updated model is then sent to all nodes along with splits from the next mini-batch.", "In asynchronous training, no device waits for updates to the model from any other device. The devices can run independently and share results as peers or communicate through one or more central servers known as \u201cparameter\u201d servers. In the peer architecture, each device runs a loop that reads data, computes the gradients, sends them (directly or indirectly) to all devices, and updates the model to the latest version.", "In practical implementations, the approaches are synchronous for up to 32\u201350 nodes and asynchronous for larger clusters and heterogeneous environments, according to this survey from ETH (Zurich). In this post, we will focus on a synchronous training approach.", "For synchronous training, we can choose between two main schemes: centralized or decentralized. The choice between designing a centralized and a decentralized scheme for DNN training depends on multiple factors, including the network topology, bandwidth, communication latency, parameter update frequency, or desired fault tolerance.", "The centralized scheme would typically include a so-called Parameter Server strategy.", "When parallel SGD uses parameter servers, the algorithm starts by broadcasting the model to the workers (servers). Each worker reads its own split from the mini-batch in each training iteration, computing its own gradients, and sending those gradients to one or more parameter servers. The parameter servers aggregate all the gradients from the workers and wait until all workers have completed before they calculate the new model for the next iteration, which is then broadcasted to all workers.", "The decentralized scheme would rely on ring-allreduce to communicate parameter updates among the nodes. In the ring-allreduce architecture, there is no central server that aggregates gradients from workers. Instead, in a training iteration, each worker reads its own split for a mini-batch, calculates its gradients, sends its gradients to its successor neighbor on the ring, and receives gradients from its predecessor neighbor on the ring.", "But luckily there are software libraries, known as DL Frameworks, that facilitate this parallelization or distribution that we saw in the previous section.", "In this section, we will make a very brief introduction to the frameworks that we will use in the next two posts to speed up the training process using the techniques presented in the previous section.", "As we presented in the first post of this series, we can use frameworks as TensorFlow of Pytorch to program multi-GPU training in one server. To parallelize the training of the model, you only need to wrap the model with torch.nn.parallel.DistributedDataParallelin PyTorch and with tf.distribute.MirroredStrategyin TensorFlow.", "However, the number of GPUs that we can place in a server is very limited, and the solution goes through putting many of these servers together, as we did at the BSC, with the CTE-POWER supercomputer, where 54 servers are linked together with an InfiniBand network on optical fiber.", "In this new scenario, we need an extension of the software stack to deal with multiple distributed GPUs in the neural network training process. There are other options, but in our research group at BSC, we decided to use Horovod, from Uber. Horovod Plugs into TensorFlow, PyTorch.", "But before continuing, we take advantage of this post to introduce the sequential code of an image classification problem that we will use as a baseline to calculate the scalability performance that can be obtained with the proposed parallel and distributed frameworks.Specifically, we will use a neural network known as ResNet50V2 with 25,613,800 parameters. And as a dataset, we will use the popular CIFAR10.", "CIFAR-10 is an established computer-vision dataset used for object recognition. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing 10 object classes, with 6000 images per class. It was collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. There are 50,000 training images and 10,000 test images.", "For academic purposes, to make the training even harder and being able to see larger training times for better comparison, we have applied a resize operation to make the images of 128x128 size. We created a custom load_data function (/gpfs/projects/nct00/nct00002/cifar-utils/load_cifar.py) that applies this resize operation and splits the data into training and test sets. We can use it as:", "load_cifar.py can be obtained from the course repository GitHub for readers that want to review it (for the students of the master course it is not necessary to download it).", "AlexNet, by Alex Krizhevsky, is the neural network architecture that won the ImageNet 2012 competition. GoogleLeNet, which with its inception module drastically reduces the parameters of the network (15 times less than AlexNet). Others, such as the VGGnet, helped to demonstrate that the depth of the network is a critical component for good results. The interesting thing about many of these networks is that we can find them already preloaded in most of the Deep Learning frameworks.", "Keras Applications are prebuilt deep learning models that are made available. These models differ in architecture and the number of parameters; you can try some of them to see how the larger models train slower than, the smaller ones and achieve different accuracy.", "A list of all available models can be found here (the top-1 and top-5 accuracy refers to the model\u2019s performance on the ImageNet validation dataset.). For this post, we will consider two architectures from the family of ResNet as a case study: ResNet50v2 and ResNet152v2.", "ResNet is a family of extremely deep neural network architectures showing compelling accuracy and nice convergence behaviors, introduced by He et al. in their 2015 paper, Deep Residual Learning for Image Recognition.", "A few months later, the same authors published a new paper, Identity Mapping in Deep Residual Network, with a new proposal for the basic component, the residual unit, which makes training easier and improves generalization. And this lets the V2 versions:", "The \u201c50\u201d and \u201c152\u201d stand for the number of weight layers in the network. The arguments for both networks are:", "Note that if weights=\"imagenet\", Tensorflow middleware requires a connection to the internet to download the imagenet weights (pre-training on ImageNet). Due we are not centering our interest in Accuracy, we didn\u2019t download the file with the imagenet weights, therefore, it must be used weights=None.", "As an example of how to train neural networks, let\u2019s start with a sequential version of the ResNet50 and use TensorFlow as a framework.", "The ResNet50_seq.py code could be the following one:", "ResNet50_seq.py can be downloaded from the course repository GitHub.", "To run it after being granted with the required resources with salloc and loading the corresponding modules with module load, we can do it like this:", "Note that we redirect the standard error (otherwise, it would go through mixed screen system information) in order to see the result of the output that gives us the Keras:", "For the purpose of this post, to calculate the time, which will be the metric we will use to compare, we will use the time that Keras himself tells us that it takes an epoch (sometimes we discard the first epoch as it is different from the rest since it has to create structures in memory and initialize them). Remember that we are in an academic example, and with this approximate measure of time, we have enough for the course goals.", "For any other of the networks available in Keras, simply change in the code the piece that is in bold, resnet_v2.ResNet50V2, for the corresponding network.", "If you want to learn more and consolidate the knowledge acquired, now it\u2019s your turn to get your hands dirty. The proposal is to reproduce the above results for the ResNet152V2, both with salloc and the Batch sbatch command. Your results based on ResNet152V2 will be used in the next two posts.", "This post has described an overview of the techniques that allow accelerating a Deep Neural Network training by distributing it over many GPUs. We have introduced two frameworks that implement these techniques, TensorFlow and Horovod, that we will use in the next two posts.", "This post is part of the documentation used in my master course Supercomputers Architecture at UPC Barcelona Tech with BSC\u2019s support. I\u2019m sharing it in case it can be useful for other readers.", "See you in the next post where we present how to parallelize with the tf.distributed.MirroredStrategy() TensorFlow API.", "Acknowledgment: Many thanks to Juan Luis Dom\u00ednguez and Oriol Aranda from BSC-CNS, who wrote the first version of the codes that appear in this series of posts, and to Carlos Tripiana for the essential support with the deployment of the software stack of POWER-CTE Supercomputer. Also many thanks to Alvaro Jover Alvarez, Miquel Escobar Castells and Raul Garcia Fuentes for their contributions to the proofreading of this document.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe5fb4a956bef&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----e5fb4a956bef---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/supercomputing-for-a-i", "anchor_text": "SUPERCOMPUTING FOR ARTIFICIAL INTELLIGENCE \u2014 03"}, {"url": "https://bsc.es/", "anchor_text": "BSC"}, {"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture"}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://bsc.es/", "anchor_text": "BSC"}, {"url": "https://www.youtube.com/watch?v=EeMCEQa85tw", "anchor_text": "video 4:49"}, {"url": "https://towardsdatascience.com/tagged/supercomputing-for-a-i", "anchor_text": "this series"}, {"url": "https://towardsdatascience.com/convolutional-neural-networks-for-beginners-using-keras-and-tensorflow-2-c578f7b3bf25", "anchor_text": "predictive accuracy of the model"}, {"url": "https://towardsdatascience.com/supercomputing-the-heart-of-deep-learning-and-artificial-intelligence-49218c6bdee5", "anchor_text": "computational speed of the process"}, {"url": "https://arxiv.org/abs/1312.6186", "anchor_text": "interesting property"}, {"url": "https://arxiv.org/abs/1404.5997", "anchor_text": "it requires less communication between nodes, as it benefits from a high amount of computations per weight"}, {"url": "https://dl.acm.org/citation.cfm?doid=2901318.2901323", "anchor_text": "model has to fit on each node entirely"}, {"url": "http://jmlr.org/proceedings/papers/v28/coates13.pdf", "anchor_text": "and works well for GPUs in a single server that shares a high-speed bus"}, {"url": "https://arxiv.org/pdf/1802.09941.pdf", "anchor_text": "this survey"}, {"url": "https://towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d", "anchor_text": "As we presented in the first post of this series"}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow of Pytorch"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html", "anchor_text": "torch.nn.parallel.DistributedDataParallel"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy", "anchor_text": "tf.distribute.MirroredStrategy"}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "where 54 servers are linked together with an InfiniBand network on optical fiber"}, {"url": "https://horovod.ai/", "anchor_text": "Horovod"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "http://groups.csail.mit.edu/vision/TinyImages/", "anchor_text": "80 million tiny images dataset"}, {"url": "http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf", "anchor_text": "Learning Multiple Layers of Features from Tiny Images"}, {"url": "http://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "CIFAR-10"}, {"url": "http://www.cs.toronto.edu/~kriz/cifar.html", "anchor_text": "http://www.cs.toronto.edu/~kriz/cifar.html"}, {"url": "https://github.com/jorditorresBCN/SA-MIRI-2020", "anchor_text": "course repository GitHub"}, {"url": "https://keras.io/api/applications/", "anchor_text": "Keras Applications"}, {"url": "https://keras.io/api/applications/", "anchor_text": "here"}, {"url": "https://keras.io/api/applications/resnet/#resnet50v2-function", "anchor_text": "ResNet50v2"}, {"url": "https://keras.io/api/applications/resnet/#resnet152v2-function", "anchor_text": "ResNet152v2"}, {"url": "https://arxiv.org/pdf/1512.03385.pdf", "anchor_text": "eep Residual Learning for Image Recognition"}, {"url": "https://arxiv.org/pdf/1603.05027.pdf", "anchor_text": "Identity Mapping in Deep Residual Network"}, {"url": "https://github.com/jorditorresBCN/SA-MIRI-2020", "anchor_text": "course repository GitHub"}, {"url": "https://keras.io/api/applications/", "anchor_text": "networks available in Keras"}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "salloc"}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "sbatch"}, {"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture"}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://bsc.es/", "anchor_text": "BSC"}, {"url": "https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af", "anchor_text": "the next post"}, {"url": "https://towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d", "anchor_text": "Artificial Intelligence is a Supercomputing problem"}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "Using Supercomputers for Deep Learning Training"}, {"url": "https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef", "anchor_text": "Deep Learning Frameworks for Parallel and Distributed Infrastructures"}, {"url": "https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af", "anchor_text": "Train a Neural Network on multi-GPU with TensorFlow"}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004cb2", "anchor_text": "Distributed Deep Learning with Horovod"}, {"url": "https://www.bsc.es/dominguez-bermudez-juan-luis", "anchor_text": "Juan Luis Dom\u00ednguez"}, {"url": "https://www.bsc.es/aranda-llorens-oriol", "anchor_text": "Oriol Aranda"}, {"url": "https://www.bsc.es/tripiana-carlos", "anchor_text": "Carlos Tripiana"}, {"url": "https://www.bsc.es/jover-alvarez-alvaro", "anchor_text": "Alvaro Jover Alvarez"}, {"url": "https://www.bsc.es/escobar-castells-miquel", "anchor_text": "Miquel Escobar Castells"}, {"url": "https://www.bsc.es/garcia-fuentes-raul", "anchor_text": "Raul Garcia Fuentes"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e5fb4a956bef---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/supercomputing?source=post_page-----e5fb4a956bef---------------supercomputing-----------------", "anchor_text": "Supercomputing"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e5fb4a956bef---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e5fb4a956bef---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/supercomputing-for-a-i?source=post_page-----e5fb4a956bef---------------supercomputing_for_a_i-----------------", "anchor_text": "Supercomputing For A I"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e5fb4a956bef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e5fb4a956bef---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe5fb4a956bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e5fb4a956bef---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e5fb4a956bef--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fscalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}