{"url": "https://towardsdatascience.com/jax-differentiable-computing-by-google-78310859b4ad", "time": 1683015804.209583, "path": "towardsdatascience.com/jax-differentiable-computing-by-google-78310859b4ad/", "webpage": {"metadata": {"title": "JAX: Differentiable Computing by Google | by Branislav Holl\u00e4nder | Towards Data Science", "h1": "JAX: Differentiable Computing by Google", "description": "Since deep learning took off in the early 2010s, many frameworks were written to facilitate deep learning in both research and production. For the record, let us mention Caffe, Theano, Torch\u2026"}, "outgoing_paragraph_urls": [{"url": "https://caffe.berkeleyvision.org/", "anchor_text": "Caffe", "paragraph_index": 0}, {"url": "http://deeplearning.net/software/theano/", "anchor_text": "Theano", "paragraph_index": 0}, {"url": "http://torch.ch/", "anchor_text": "Torch", "paragraph_index": 0}, {"url": "https://lasagne.readthedocs.io/en/latest/", "anchor_text": "Lasagne", "paragraph_index": 0}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow", "paragraph_index": 0}, {"url": "https://keras.io/", "anchor_text": "Keras", "paragraph_index": 0}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 0}, {"url": "https://github.com/google/jax", "anchor_text": "JAX", "paragraph_index": 1}, {"url": "https://www.tensorflow.org/xla", "anchor_text": "XLA", "paragraph_index": 6}, {"url": "https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.jacfwd", "anchor_text": "JAX docs", "paragraph_index": 15}], "all_paragraphs": ["Since deep learning took off in the early 2010s, many frameworks were written to facilitate deep learning in both research and production. For the record, let us mention Caffe, Theano, Torch, Lasagne, Tensorflow, Keras or PyTorch. Some of these frameworks disappeared after a while. Others survived and thrive to this day, mainly PyTorch and Tensorflow. Over time, these frameworks evolved into large ecosystems with many different functionalities. On one hand, they support the training of new models including massively parallel training on many-GPU systems. On the other hand, they allow users to deploy the trained models easily in the cloud or on mobile devices.", "Unfortunately, this streamlining sometimes comes at the cost of flexibility required by machine learning researchers to test out new model ideas. It is precisely this user group that Google is targeting with its new framework called JAX. In this blog post, I will show you how JAX encourages experimentation by providing a low-level, high-performance interface to vectorized computing and gradient computation.", "The goal of JAX is to allow the user to speed up raw Python and NumPy functions by just-in-time compilation and auto-parallelization as well as to compute gradients of these functions. In order to do this, JAX employs a functional design. Functions such as gradient computation are implemented as functional transforms (or functors) acting on user-defined Python functions. For instance, to calculate the gradient of the absolute value function, you can write:", "As you can see, abs_val is a normal Python function, which is transformed by the functor grad.", "In order to be able to use JAX functors, the user-defined functions have to follow some restrictions:", "In terms of philosophy, JAX is similar to what you would experience when using purely functional languages such as Haskell. In fact, the developers even use Haskell-type signatures in the documentation to explain some of the transformations.", "One of the main features of JAX is the ability to speed up execution of Python code by JIT. Internally, JAX uses the XLA compiler to accomplish this. XLA is able to compile code not only for CPUs, but also for GPUs or even TPUs. This makes JAX very powerful and versatile. In order to compile a function using XLA, you can use the jit functor like this:", "Alternatively, you may also use jit as a decorator on top of the function definition:", "The jit functor transforms the input function by returning a compiled version of it to the caller. Calling the original selu function will use the Python interpreter, while calling selu_jit will call the compiled version, which should be much faster, especially for vectorized inputs such as numpy arrays. Furthermore, the JIT compilation only occurs once and is cached thereafter, making subsequent calls of the function very efficient.", "When training machine learning models, you are usually computing some loss for a subset of input data, then updating the model parameters. Since computing the forward function of the model for every input sequentially would be too slow, it is customary to batch together the subset of data into one batch and computing the forward function in a parallel fashion on an accelerator such as the GPU. This is accomplished in JAX by using the function vmap:", "This code snippet takes a forward function of a fully-connected layer with ReLU activation and parallelizes its execution over a batch of inputs. The parameters in_axes and out_axes specify over which parameters and axes the parallelization occurs. In this case, (0, None, None) means that the input is parallelized over the 0-th axis while w and b are left untouched. The output is parallelized over the 0-th axis.", "What makes JAX particularly interesting to machine learning researchers is its ability to compute gradients of arbitrary pure functions. JAX inherited this capability from autograd, a package to compute derivatives of NumPy arrays.", "To compute the gradient of a function, simply use the grad transformation:", "If you want to compute higher-order derivatives, you can simply chain together multiple grad transformations like this:", "While applying grad to a function in R (the set of real numbers) will give you a single number as the output, you may also apply it to a vector-valued function to obtain a Jacobian:", "Here, we do not use the grad transformation, since it only works on scalar-output functions. Instead, we use a similar function called jacfwd for automatic forward-mode differentiation (for a detailed discussion of forward- and backward-mode differentiation please refer to the JAX docs).", "Note that we may chain together the functional transformations in JAX. For instance, we may vectorize our function using vmap, calculate the gradients using grad and then compile the resulting function using jit, such as in this example of a standard mini-batch training loop in deep learning:", "Here, loss_grad computes the gradient of the cross-entropy loss, while paralleling over the input (x, y) and the single output (the loss). The entire function is jitted to allow for fast computation on the GPU. The resulting function is cached, allowing it to be called with different outputs without any further overhead.", "The composition of multiple transformations makes the framework extremely powerful and gives the user great flexibility in designing the data flow.", "JAX provides a useful alternative to more high-level frameworks such as PyTorch or Tensorflow for researchers who need extra flexibility. The ability to differentiate through native Python and NumPy functions is amazing and the JIT compilation and auto-vectorization features greatly simplify writing efficient code for massively-parallel architectures like GPUs or TPUs. What I like most about JAX, however, is its clean functional interface. Surely it\u2019s only a matter of time before an ecosystem of mature high-level APIs will evolve around it. Stay tuned!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI | Software Development | Other Crazy Interests"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F78310859b4ad&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----78310859b4ad--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@branislav.hollander?source=post_page-----78310859b4ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Branislav Holl\u00e4nder"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9a2fa1a025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=post_page-cb9a2fa1a025----78310859b4ad---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/thedigitalartist-202249/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=969757\"", "anchor_text": "Pete Linforth"}, {"url": "https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=969757", "anchor_text": "Pixabay"}, {"url": "https://caffe.berkeleyvision.org/", "anchor_text": "Caffe"}, {"url": "http://deeplearning.net/software/theano/", "anchor_text": "Theano"}, {"url": "http://torch.ch/", "anchor_text": "Torch"}, {"url": "https://lasagne.readthedocs.io/en/latest/", "anchor_text": "Lasagne"}, {"url": "https://www.tensorflow.org/", "anchor_text": "Tensorflow"}, {"url": "https://keras.io/", "anchor_text": "Keras"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://github.com/google/jax", "anchor_text": "JAX"}, {"url": "https://www.tensorflow.org/xla", "anchor_text": "XLA"}, {"url": "https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.jacfwd", "anchor_text": "JAX docs"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----78310859b4ad---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----78310859b4ad---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/python?source=post_page-----78310859b4ad---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----78310859b4ad---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/numpy?source=post_page-----78310859b4ad---------------numpy-----------------", "anchor_text": "Numpy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=-----78310859b4ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=-----78310859b4ad---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F78310859b4ad&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----78310859b4ad---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----78310859b4ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----78310859b4ad--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----78310859b4ad--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----78310859b4ad--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----78310859b4ad--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@branislav.hollander?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Branislav Holl\u00e4nder"}, {"url": "https://medium.com/@branislav.hollander/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcb9a2fa1a025&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=post_page-cb9a2fa1a025--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F61e7fe52ba73&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fjax-differentiable-computing-by-google-78310859b4ad&newsletterV3=cb9a2fa1a025&newsletterV3Id=61e7fe52ba73&user=Branislav+Holl%C3%A4nder&userId=cb9a2fa1a025&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}