{"url": "https://towardsdatascience.com/ternarybert-quantization-meets-distillation-1b902ac31bd6", "time": 1683014779.574311, "path": "towardsdatascience.com/ternarybert-quantization-meets-distillation-1b902ac31bd6/", "webpage": {"metadata": {"title": "TernaryBERT: Quantization Meets Distillation | by Julia Turc | Towards Data Science", "h1": "TernaryBERT: Quantization Meets Distillation", "description": "The ongoing trend of building ever larger models like BERT and GPT-3 has been accompanied by a complementary effort to reduce their size at little or no cost in accuracy. Effective models are built\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "Pre-trained Distillation", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "DistilBERT", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/2004.02984", "anchor_text": "MobileBERT", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1909.10351", "anchor_text": "TinyBERT", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1909.05840.pdf", "anchor_text": "Q-BERT", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1910.06188.pdf", "anchor_text": "Q8BERT", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1605.04711.pdf", "anchor_text": "Ternary Weight Networks", "paragraph_index": 12}, {"url": "https://arxiv.org/pdf/1802.08635.pdf", "anchor_text": "Loss-Aware Ternarization", "paragraph_index": 12}, {"url": "https://gluebenchmark.com/", "anchor_text": "GLUE benchmark", "paragraph_index": 13}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1605.04711.pdf", "anchor_text": "Ternary Weight Networks", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1802.08635.pdf", "anchor_text": "Loss-Aware Ternarization", "paragraph_index": 16}, {"url": "http://storia.ai", "anchor_text": "storia.ai", "paragraph_index": 28}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc", "paragraph_index": 28}], "all_paragraphs": ["The ongoing trend of building ever larger models like BERT and GPT-3 has been accompanied by a complementary effort to reduce their size at little or no cost in accuracy. Effective models are built either via distillation (Pre-trained Distillation, DistilBERT, MobileBERT, TinyBERT), quantization (Q-BERT, Q8BERT) or parameter pruning.", "On September 27, Huawei introduced TernaryBERT, a model that leverages both distillation and quantization to achieve accuracy comparable to the original BERT model with ~15x decrease in size. What is truly remarkable about TernaryBERT is that its weights are ternarized, i.e. have one of three values: -1, 0, or 1 (and can hence be stored in only two bits).", "TernaryBERT cleverly pieces together existing quantization and distillation techniques. The paper heavily references prior work and is therefore quite dense. The goal of this article is to provide a self-contained walk-through, with additional context where needed.", "Quantization is the process of reducing the number of bits used to represent a single scalar parameter.", "When successful, quantization is relatively headache-free because it allows the model designer to keep the architecture and configuration of the original model intact: by switching from 32-bit to 8-bit parameter representations, one could achieve a 4x reduction in size without having to revisit settings like the number of layers or the hidden sizes. Quantization generally maps real values to integer values, which can be added and multiplied much more effectively.", "The quantization scheme is the algorithm that determines how a real value r is mapped to one of the integers (or quanta) q that can be represented by the target number of bits (for 8 bits: -128 to 127 inclusive, or 0 to 255 exclusive). Most commonly, quantization schemes are linear; the relationship between r and q can be expressed in terms of a scaling factor S and a zero-point Z:", "In the simplest case where the real-valued model parameters are uniformly distributed in the interval [-1, 1] and the quanta are indexed from -127 to 127 (ignoring the -128 bucket for now), r = 1/127 * q. In other words, the scaling factor S is 1/127, and the zero point Z is 0. When r is not uniformly distributed (say its values are predominantly negative), the zero point can shift accordingly. For instance, when Z=10, we are re-allocating 10 quanta to negative values, thus increasing precision in the area where most r values lie.", "TernaryBERT applies linear quantization both to its weights and activations (more on this below). But just for context, you should know that there are other schemes too. For instance, the paper that introduced ternarization (i.e. quantization to -1, 0, and 1) in the context of neural networks [Q2] proposed a stochastic algorithm to convert a real value r \u2208 [-1, 1] into a quantized value q \u2208 {-1, 0, 1}:", "The most convenient time to apply quantization would be post-training: after training a model with 32-bit real-valued parameters, apply one of the standard quantization schemes, then use the quantized model for effective inference. In practice, however, this na\u00efve approach often incurs a significant drop in accuracy, especially when targeting ultra low precision (2 or 4 bits). Even if we were to perform an additional fine-tuning step after this post-training quantization, results can still be unsatisfactory; quantization reduces the resolution of the model parameters and their gradients (i.e. how many distinct values can be represented), thus hindering the learning process.", "To address this issue, Jacob et al. [Q1] proposed training with simulated quantization: during the forward pass, the model acts as if it were already quantized. The loss and gradients are thus computed with respect to this bit-constrained model, but the backward pass happens as normal, on the full-precision weights. This encourages the model to perform well during a quantized forward pass (which is what happens at inference time), while continuing to leverage the better representational power of longer-bit parameters and gradients. Q8BERT [Q3] used this technique (also known as quantization-aware training) to reduce the original BERT model from a 32-bit to 8-bit integer representation. TernaryBERT employs a similar strategy.", "TernaryBERT converts its 32-bit real-valued weights into 2-bit ternary representations with values from the set {-1, 0, 1} via a linear quantization scheme as described above. While the zero-point is fixed to Z=0, the scaling factor S > 0 (denoted by \u03b1 from here on) is learnt together with the model parameters.", "Needless to say, downgrading parameters from 32 to 2 bits comes with a huge loss in precision. In order to restore some of the lost expressiveness of the model, it is common practice to use multiple scaling factors \u03b1\u1d62 (instead of a single \u03b1 for the entire network), one for each natural grouping i of parameters (matrices, vectors, kernels, layers, etc.). For instance, Rastegari et al. [Q4] used a separate scaling factor for each filter on each layer of their Convolutional Neural Network. Similarly, TernaryBERT adds one \u03b1\u1d62 for each of BERT\u2019s Transformer layers, and separate per-row scaling factors for the token embedding matrix.", "So how are these scaling factors learnt? As mentioned above, the scaling factors \u03b1\u1d62, the full-precision weights w and the quantized weights b are all learnt during the training process. TernaryBERT compares two existing methods for approximating these parameters: Ternary Weight Networks (TWN) [Q5] and Loss-Aware Ternarization (LAT) [Q6]. While the two have seemingly different formulations, they both boil down to minimizing the prediction loss of the quantized forward pass with an additional constraint where the quantized weights \u03b1b are encouraged to stay close to the full-precision weights w during each training step.", "If you are short on time, you can skip directly to the \u201cActivation Quantization\u201d section, resting assured that TWN and LAT score comparably on the GLUE benchmark (which mostly contains classification tasks) and on SQuAD, a popular question answering dataset.", "Ternary Weight Networks (TWN) formulate the problem in terms of minimizing the distance between the full-precision and the quantized parameters:", "Since this minimization is performed on every training step, an efficient implementation is crucial. Luckily, there is an approximate analytical solution, so computing b and \u03b1 is as easy as applying a formula that depends on w (excluded from this article for simplicity).", "The other approach, Loss-Aware Ternarization (LAT), is to directly minimize the loss computed with respect to the quantized weights; this expression completely circumvents the full-precision parameters (note there\u2019s no w below):", "At first, the expression above should look somewhat suspicious: we have already established that training a model in low-bit representation is hindered by the loss of precision. Surely, the full-precision weights w need to get involved somehow. It turns out that this expression can be reformulated in terms of a per-iteration minimization sub-problem, where the quantized weights are again encouraged to be close to the full-precision ones, but in a way that also takes into account the loss at the current time step:", "This expression is very similar to equation (5), with the difference that it includes v, which is a statistic of the loss in equation (6). Similarly to equation (5), it has an approximate analytical solution that is a function of w and can be computed effectively (again, excluded from this article for simplicity).", "After weight quantization, the model can be described as a set of ternary weights grouped into logical units (e.g. matrices), each of them having its own real-valued scaling factor \u03b1\u1d62. Because of this, the values flowing through the network (the inputs and outputs of the layers), also known as activations, are real-valued. In order to speed up matrix multiplication, activations can too be quantized. However, as noted in previous work [Q7], activations are more sensitive to quantization; this is most likely the reason why TernaryBERT decided to quantize activations down to 8 bits rather than 2.", "Based on the observation that Transformer activations tend to be more negative than positive, the authors of TernaryBERT opted for a non-symmetric quantization algorithm or, in terms of the expression for linear quantization above r = S(q-Z), the zero-point Z is not fixed to 0, but rather the mid-point between the minimum and maximum possible values for r.", "We\u2019ve come a long way from the initial na\u00efve proposal to perform post-training quantization. We established that quantization needs to be part of training, that multiple scaling factors are needed to attenuate the loss in precision caused by downgrading from 32 to 2 bits, and explored two different ways of bringing the quantized weights close to the full-precision ones. But TernaryBERT teaches us we can do even better by taking advantage of yet another technique from the machine learning toolbox.", "Distillation (or knowledge distillation) is the process through which a large and accurate model (the teacher) transfers its knowledge to a smaller model with less representational power (the student).", "In other words, distillation is a two-step process: 1) train a large teacher on the gold labels, and 2) train a smaller student on the labels produced by the teacher, also known as soft labels. Hinton et al. [D1] explained that distillation outperforms standard training because the soft labels carry additional information, or dark knowledge. For instance, consider tagging the word \u201cwalk\u201d with its correct part of speech in the sentence \u201cI enjoyed the walk\u201d. A soft label of the form p(noun)=0.9 is more informative than a hard label p(noun)=1.0, which fails to capture the fact that \u201cwalk\u201d could be a verb in other contexts. TernaryBERT uses the same technique during fine-tuning, learning from the soft distribution produced by a much larger teacher:", "More elaborate formulations of distillation such as FitNets [D2] encourage direct alignment between the internal representations of teachers and students. TernaryBERT does this too, pulling the hidden layers of the full-precision student close to the ones of the teacher, and encouraging the attention scores throughout the two Transformer networks to be similar:", "TernaryBERT puts together established techniques from quantization and distillation under a single end-to-end recipe for training a ternarized model. On each training step:", "At the end of training, the quantized model is ready for inference with no further adjustments.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Building storia.ai | Ex-Google Research | twitter.com/juliarturc"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1b902ac31bd6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@turc.raluca?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc----1b902ac31bd6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "Pre-trained Distillation"}, {"url": "https://arxiv.org/pdf/1910.01108.pdf", "anchor_text": "DistilBERT"}, {"url": "https://arxiv.org/abs/2004.02984", "anchor_text": "MobileBERT"}, {"url": "https://arxiv.org/abs/1909.10351", "anchor_text": "TinyBERT"}, {"url": "https://arxiv.org/pdf/1909.05840.pdf", "anchor_text": "Q-BERT"}, {"url": "https://arxiv.org/pdf/1910.06188.pdf", "anchor_text": "Q8BERT"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://unsplash.com/@markusspiske?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@florenciaviadana?utm_source=medium&utm_medium=referral", "anchor_text": "Florencia Viadana"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1605.04711.pdf", "anchor_text": "Ternary Weight Networks"}, {"url": "https://arxiv.org/pdf/1802.08635.pdf", "anchor_text": "Loss-Aware Ternarization"}, {"url": "https://gluebenchmark.com/", "anchor_text": "GLUE benchmark"}, {"url": "https://rajpurkar.github.io/SQuAD-explorer/", "anchor_text": "SQuAD"}, {"url": "https://arxiv.org/pdf/1605.04711.pdf", "anchor_text": "Ternary Weight Networks"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://arxiv.org/pdf/1802.08635.pdf", "anchor_text": "Loss-Aware Ternarization"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://unsplash.com/@only1simonharmer?utm_source=medium&utm_medium=referral", "anchor_text": "Simon Harmer"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://arxiv.org/pdf/2009.12812.pdf", "anchor_text": "TernaryBERT"}, {"url": "https://arxiv.org/pdf/1712.05877.pdf", "anchor_text": "Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference"}, {"url": "https://arxiv.org/pdf/1510.03009.pdf", "anchor_text": "Neural Networks with Few Multiplications"}, {"url": "https://arxiv.org/pdf/1910.06188.pdf", "anchor_text": "Q8BERT: Quantized 8Bit BERT"}, {"url": "https://arxiv.org/pdf/1603.05279.pdf", "anchor_text": "Rastegari et al., XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks"}, {"url": "https://arxiv.org/pdf/1605.04711.pdf", "anchor_text": "Ternary Weight Networks"}, {"url": "https://arxiv.org/pdf/1802.08635.pdf", "anchor_text": "Loss-aware Weight Quantization of Deep Networks"}, {"url": "https://arxiv.org/abs/1503.02531", "anchor_text": "Distilling the Knowledge in a Neural Network"}, {"url": "https://arxiv.org/abs/1412.6550", "anchor_text": "FitNets: Hints for Thin Deep Nets"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----1b902ac31bd6---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1b902ac31bd6---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1b902ac31bd6---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/ai?source=post_page-----1b902ac31bd6---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&user=Julia+Turc&userId=f758859396fc&source=-----1b902ac31bd6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&user=Julia+Turc&userId=f758859396fc&source=-----1b902ac31bd6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1b902ac31bd6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1b902ac31bd6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1b902ac31bd6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@turc.raluca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Julia Turc"}, {"url": "https://medium.com/@turc.raluca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "392 Followers"}, {"url": "http://storia.ai", "anchor_text": "storia.ai"}, {"url": "http://twitter.com/juliarturc", "anchor_text": "twitter.com/juliarturc"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff758859396fc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&user=Julia+Turc&userId=f758859396fc&source=post_page-f758859396fc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F779df2fab045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fternarybert-quantization-meets-distillation-1b902ac31bd6&newsletterV3=f758859396fc&newsletterV3Id=779df2fab045&user=Julia+Turc&userId=f758859396fc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}