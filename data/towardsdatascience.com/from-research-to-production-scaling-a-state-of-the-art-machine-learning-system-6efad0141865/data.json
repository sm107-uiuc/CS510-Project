{"url": "https://towardsdatascience.com/from-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865", "time": 1683016366.2973208, "path": "towardsdatascience.com/from-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865/", "webpage": {"metadata": {"title": "From research to production: scaling a state-of-the-art machine learning system | by Lester Solbakken | Towards Data Science", "h1": "From research to production: scaling a state-of-the-art machine learning system", "description": "Imagine you\u2019ve created a machine learning system that surpasses state-of-the-art performance on some task. You\u2019ve optimized for a set of objectives like classification accuracy, F1 scores, or AUC\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Pareto_efficiency#Pareto_frontier", "anchor_text": "Pareto frontier", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/efficient-open-domain-question-answering-on-vespa-ai-72562121dcd8", "anchor_text": "previous blog post", "paragraph_index": 3}, {"url": "https://github.com/facebookresearch/DPR", "anchor_text": "Dense Passage Retrieval (DPR)", "paragraph_index": 3}, {"url": "https://vespa.ai", "anchor_text": "Vespa.ai", "paragraph_index": 3}, {"url": "https://docs.vespa.ai/documentation/performance/sizing-search.html", "anchor_text": "quite a few options for serving time optimizations", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/efficient-open-domain-question-answering-on-vespa-ai-72562121dcd8", "anchor_text": "previous blog post", "paragraph_index": 6}, {"url": "https://ark.intel.com/content/www/us/en/ark/products/192443/intel-xeon-gold-6240-processor-24-75m-cache-2-60-ghz.html", "anchor_text": "Intel Xeon Gold 6240 processor", "paragraph_index": 13}, {"url": "https://docs.vespa.ai/documentation/reference/services-content.html#requestthreads", "anchor_text": "multiple threads per search", "paragraph_index": 16}, {"url": "https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333", "anchor_text": "evaluation time is linear with batch size", "paragraph_index": 17}, {"url": "https://docs.vespa.ai/documentation/performance/sizing-search.html", "anchor_text": "Vespa serving scaling guide", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "\u201cWell-Read Students Learn Better: On the Importance of Pre-training Compact Models,\u201d", "paragraph_index": 26}, {"url": "https://huggingface.co/google/bert_uncased_L-12_H-768_A-12", "anchor_text": "Transformers model repository", "paragraph_index": 26}, {"url": "https://github.com/facebookresearch/DPR#5-reader-training", "anchor_text": "DPR repository", "paragraph_index": 27}, {"url": "https://ark.intel.com/content/www/us/en/ark/products/192443/intel-xeon-gold-6240-processor-24-75m-cache-2-60-ghz.html", "anchor_text": "Intel Xeon Gold 6240", "paragraph_index": 31}, {"url": "https://github.com/vespa-engine/sample-apps/tree/master/dense-passage-retrieval-with-ann/", "anchor_text": "companion sample application", "paragraph_index": 32}, {"url": "https://github.com/microsoft/fastformers", "anchor_text": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding", "paragraph_index": 34}, {"url": "https://arxiv.org/abs/2007.14062", "anchor_text": "the Big Bird architecture", "paragraph_index": 34}], "all_paragraphs": ["Imagine you\u2019ve created a machine learning system that surpasses state-of-the-art performance on some task. You\u2019ve optimized for a set of objectives like classification accuracy, F1 scores, or AUC. Now you want to create a web service from it. Other objectives, such as the time or cost of delivering a result to the user, become more important.", "These two sets of objectives are typically in conflict. More accurate models are often large and computationally expensive to evaluate. Various optimizations like reducing the models\u2019 precision and complexity are often introduced to use such models in production. While beneficial for decreasing cost and energy consumption, this, unfortunately, hurts accuracy.", "Obviously, inference time can be drastically lowered if accuracy is not important. Likewise, very accurate responses can be produced at high cost. Which solution to ultimately choose lies somewhere between these extremes. A useful technique for selecting the best solution is to enumerate them in terms of accuracy and cost. The set of solutions not dominated by others is called the Pareto frontier and identifies the best trade-offs between accuracy and cost.", "In a previous blog post, we introduced a serving system that reproduces state-of-the-art accuracy in open-domain question-answering. We based this on Facebook\u2019s Dense Passage Retrieval (DPR), which is a Python-based research system. We built the serving system using Vespa.ai, the open-source big data serving engine, which is uniquely suited to tasks like this due to its native support for fast similarity search and machine learned models in search and ranking. The result is a web service taking a single question and returning an exact answer.", "While this system reproduced DPR\u2019s result and thus had excellent accuracy metrics, the response time was initially poor, as measured in end-to-end latency. This post will describe the various optimizations we made to bring performance to acceptable levels for a production system.", "Vespa.ai is built for production and thus has quite a few options for serving time optimizations. We will particularly use Vespa.ai\u2019s ability to retrieve and rank documents using multiple worker threads per query to significant effect. However, this application\u2019s main cost is in evaluating two BERT models. One of the questions we would like to answer is whether smaller models with full precision are preferable to larger models with quantized parameters. We\u2019ll develop the Pareto frontier to evaluate the merits of the various optimizations.", "We\u2019ll start with an overview of the serving system and identify which parts of the system initially drive the cost. For more details on the implementation, we refer to the previous blog post in this series.", "The system\u2019s task is to produce a textual answer in response to a question given in natural language. There are primarily three stages involved:", "The following figure illustrates the process:", "The encoder first creates a tokenized representation from the question. This vector of token IDs is sent as input to the encoder BERT model. This is initially a standard BERT-base model with 12 layers and a hidden layer size of 768. The final hidden layer state is used as a representation vector for the question. As seen in the figure above, this primarily happens in the stateless container layer in Vespa. The token and vector representation is passed down (\u201cscattered\u201d) to all content nodes to perform the query.", "On the content nodes, the passages have been indexed with their own representation vectors. These vectors have been constructed so that the euclidean distance between question and passage vectors indicate similarity. This is used in the HNSW algorithm to perform an approximate nearest neighbor search. The 10 passages with the smallest euclidean distance are sent to the next stage.", "The second-phase ranking stage, also performed on each content node, evaluates the reader BERT model. Like the encoder model, this is initially a BERT-base model with 12 layers and hidden length 768. The token representations from the query and each passage are combined to form the model input. The reader model produces three probability scores: the relevance score and the start and end indices of the answer in the passage\u2019s token sequence. The passage with the best relevance score is selected as the winner, and its token representation is returned to the stateless layer. There, custom code extracts the best span using the start and end indices, de-tokenizes it, and returns the resulting textual answer.", "Now, that\u2019s a lot of work. Here is an example of a response to the question \u201cWho won Tour De France in 2015\u201d, where the most relevant passage is retrieved and the correct answer \u201cChris Froome\u201d is extracted:", "To measure performance, we deployed the system on a single machine with an Intel Xeon Gold 6240 processor with 200 GB RAM and SSD disk. We evaluate the system over 3610 questions and record the average latency and exact-match score. Initially, the system achieves an exact-match score of 40.64. Before any consideration has been made to optimize performance, the time spent in the three stages mentioned above is:", "Obviously, a total end-to-end latency of 9.4 seconds is not anything close to acceptable as a service. In the following, we\u2019ll lower this to well below 100 ms.", "Initially, the most expensive step by far is the reader stage. By default, Vespa does all ranking for a query on a single thread. This is a reasonable default to maximize throughput when the computational cost is low. In this case, this means that the reader model is evaluated \u2014 in sequence \u2014 for each of the top 10 passages. This results in high query latency.", "However, Vespa has an option of using multiple threads per search. Setting this value brings the average end-to-end latency down to 2.04 seconds, a more than 4x improvement without affecting the exact match score.", "It is worth clarifying that the reader model is not evaluated batch-wise. This is due to Vespa\u2019s ranking framework, where ranking expressions score a single passage and query pair. For BERT models, however, this is not significant as evaluation time is linear with batch size. One reason is tensor multiplications with tensors of 3 or more dimensions, as these iterate over several hardware-optimized matrix-matrix multiplications anyway.", "In general, Vespa has many options to tune performance, such as easily distributing the workload on additional content nodes. While we don\u2019t explore that here, see the Vespa serving scaling guide for more information.", "One of the defining and most prominent features of BERT models is the full-attention layer. While this was a significant breakthrough in language understanding, it has an unfortunate O(n\u00b2) effect on evaluation time.", "So, the length of the token sequence input to the BERT model significantly impacts inference time. Initially, the encoder BERT model had an input length of 128. By reducing it to 30, we decrease inference time from 300 ms to 125 ms without loss of accuracy.", "Likewise, the reader model initially had an input length of 380. By reducing this to 128, we reduce average latency from 1.9 seconds to 741 ms, a significant reduction. However, we do get a decrease in accuracy, as some question and passage combinations can result in token sequences longer than 128. This reduced the exact match score to 39.61.", "Both the encoder and reader model support dynamic length inputs, but Vespa currently only supports fixed length inputs. This will be fixed in the near future, however. In summary, shortening token input lengths of the encoder and reader models result in a 3x speedup.", "Neural network models are commonly trained using single-precision floating-point numbers. However, for inference in production, it has been shown that this level of precision is not always necessary. The parameters can be converted to a much smaller integer representation without significant loss in accuracy. Converting the parameters from a 32-bit floating-point to 8-bit integers reduces the model size by 75%. More importantly, integer operations execute much faster. Modern CPUs that support AVX512 Vector Neural Network Instructions (VNNI) are designed to accelerate INT8 inference performance. Additionally, evaluating such quantized models requires less power.", "Quantizing the reader model brings its size down from 435Mb to 109Mb. The latency for the system drops on average to 374 ms. This has a slightly unfortunate effect on accuracy, dropping the exact match to 37.98. Likewise, quantizing the encoder model results in a similar size reduction, and system evaluation time drops to 284 ms. The exact-match score drops to 37.87.", "In summary, model quantization of both reader and encoder models result in another 3x speedup.", "Until this point, both the encoder and reader models are based on pre-trained BERT-base models, containing 12 layers with hidden dimension size of 768 and thus around 110 million parameters. These are reasonably large models, particularly when used in time-constrained environments. However, in the paper \u201cWell-Read Students Learn Better: On the Importance of Pre-training Compact Models,\u201d the authors show that smaller models can indeed work very well. The \u201cminiature\u201d models referenced in this paper can be found in the Transformers model repository.", "We trained new reader models as described in the DPR repository, basing them on the following pre-trained BERT miniature models:", "We also quantize each model. The full overview of all 20 models (5 reader models, with and without quantization, with and without quantized encoder model) with exact match scores and average latency is given in the table below:", "In the figure above, the red line represents the Pareto front. The points along this front are also marked in bold in the table above. Recall that these points represent the best trade-offs between exact match and latency, meaning that there are no other points that are superior in both exact match and latency for each point along this front.", "One interesting result that can be seen here is that, in general, quantized models dominate other models with higher precision. For instance, the medium quantized model has better exact match and latency numbers than the small models with higher precision. So, in this case, even though quantization reduces accuracy, it is more beneficial to choose a large model that has been quantized over a smaller model that has not.", "The Pareto front visualizes the objectively best solutions, and our subjective preferences would guide us in finding the optimal solution. The tests above have been run on a single Intel Xeon Gold 6240 machine. More powerful processors would lower the overall latency numbers but not change the overall shape. The exact solution to choose is then based on our latency and hardware budgets. For instance, organizations with considerable resources that can scale up sufficiently can justify moving to the right on this front. The economy of scale can mitigate the cost of hardware investments and energy consumption to make the service viable. Such a solution might be out of reach for others.", "Please refer to the companion sample application for more details and instructions on how to run this application yourself.", "In summary, we\u2019ve taken a research application with poor performance to levels suitable for production. From 9.4 seconds for the full model down to 70ms for the tiny model, this represents a 130x speedup. Unfortunately, to get down to these levels, we noted a significant drop in exact-match as well. The best choice lies somewhere between these extremes. If we were to bring this application into production, we could use more powerful hardware to bring the latency below 100ms with acceptable exact match metrics.", "There are quite a few optimizations we didn\u2019t try that are outside the scope of this article. For instance, FastFormers: Highly Efficient Transformer Models for Natural Language Understanding includes some additional optimizations for more efficient inference such as model pruning. Also, new generations of BERT models attempt to alleviate the performance problems associated with the full-attention mechanism. For instance, the Big Bird architecture seems promising.", "We omitted training miniature encoder models. From a latency point of view, using the miniature BERT models in question encoding have an additional benefit as the vector representation for the question and passages are shorter. Thus the approximate nearest neighbor search would become more efficient. However, this would likely result in a significant drop in accuracy and the time spent in the ANN is not a significant driver of latency anyway.", "Adding additional content nodes would allow for distributing the workload. This would likely not reduce latency but would increase the number of passages we can evaluate with the reader model. We will return to this in an upcoming blog post."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6efad0141865&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/tagged/making-sense-of-big-data", "anchor_text": "Making Sense of Big Data"}, {"url": "https://medium.com/@lester.solbakken?source=post_page-----6efad0141865--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Lester Solbakken"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F344c89cb19cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=post_page-344c89cb19cc----6efad0141865---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6efad0141865&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=-----6efad0141865---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6efad0141865&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=-----6efad0141865---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://en.wikipedia.org/wiki/Pareto_efficiency#Pareto_frontier", "anchor_text": "Pareto frontier"}, {"url": "https://towardsdatascience.com/efficient-open-domain-question-answering-on-vespa-ai-72562121dcd8", "anchor_text": "previous blog post"}, {"url": "https://github.com/facebookresearch/DPR", "anchor_text": "Dense Passage Retrieval (DPR)"}, {"url": "https://vespa.ai", "anchor_text": "Vespa.ai"}, {"url": "https://docs.vespa.ai/documentation/performance/sizing-search.html", "anchor_text": "quite a few options for serving time optimizations"}, {"url": "https://towardsdatascience.com/efficient-open-domain-question-answering-on-vespa-ai-72562121dcd8", "anchor_text": "previous blog post"}, {"url": "https://ark.intel.com/content/www/us/en/ark/products/192443/intel-xeon-gold-6240-processor-24-75m-cache-2-60-ghz.html", "anchor_text": "Intel Xeon Gold 6240 processor"}, {"url": "https://docs.vespa.ai/documentation/reference/services-content.html#requestthreads", "anchor_text": "multiple threads per search"}, {"url": "https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333", "anchor_text": "evaluation time is linear with batch size"}, {"url": "https://docs.vespa.ai/documentation/performance/sizing-search.html", "anchor_text": "Vespa serving scaling guide"}, {"url": "https://arxiv.org/abs/1908.08962", "anchor_text": "\u201cWell-Read Students Learn Better: On the Importance of Pre-training Compact Models,\u201d"}, {"url": "https://huggingface.co/google/bert_uncased_L-12_H-768_A-12", "anchor_text": "Transformers model repository"}, {"url": "https://github.com/facebookresearch/DPR#5-reader-training", "anchor_text": "DPR repository"}, {"url": "https://ark.intel.com/content/www/us/en/ark/products/192443/intel-xeon-gold-6240-processor-24-75m-cache-2-60-ghz.html", "anchor_text": "Intel Xeon Gold 6240"}, {"url": "https://github.com/vespa-engine/sample-apps/tree/master/dense-passage-retrieval-with-ann/", "anchor_text": "companion sample application"}, {"url": "https://github.com/microsoft/fastformers", "anchor_text": "FastFormers: Highly Efficient Transformer Models for Natural Language Understanding"}, {"url": "https://arxiv.org/abs/2007.14062", "anchor_text": "the Big Bird architecture"}, {"url": "https://medium.com/tag/big-data?source=post_page-----6efad0141865---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/nlp?source=post_page-----6efad0141865---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6efad0141865---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/search?source=post_page-----6efad0141865---------------search-----------------", "anchor_text": "Search"}, {"url": "https://medium.com/tag/making-sense-of-big-data?source=post_page-----6efad0141865---------------making_sense_of_big_data-----------------", "anchor_text": "Making Sense Of Big Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6efad0141865&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=-----6efad0141865---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6efad0141865&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=-----6efad0141865---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6efad0141865&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=post_page-----6efad0141865--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F344c89cb19cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=post_page-344c89cb19cc----6efad0141865---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcbf2841f937f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&newsletterV3=344c89cb19cc&newsletterV3Id=cbf2841f937f&user=Lester+Solbakken&userId=344c89cb19cc&source=-----6efad0141865---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Written by Lester Solbakken"}, {"url": "https://medium.com/@lester.solbakken/followers?source=post_page-----6efad0141865--------------------------------", "anchor_text": "39 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "http://Vespa.ai", "anchor_text": "Vespa.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F344c89cb19cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&user=Lester+Solbakken&userId=344c89cb19cc&source=post_page-344c89cb19cc----6efad0141865---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcbf2841f937f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-research-to-production-scaling-a-state-of-the-art-machine-learning-system-6efad0141865&newsletterV3=344c89cb19cc&newsletterV3Id=cbf2841f937f&user=Lester+Solbakken&userId=344c89cb19cc&source=-----6efad0141865---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Lester Solbakken"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Using approximate nearest neighbor search in real world applicationsFrom text search and recommendation to ads and online dating, ANN search rarely works in isolation"}, {"url": "https://towardsdatascience.com/using-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "9 min read\u00b7Dec 17, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa75c351445d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d&user=Lester+Solbakken&userId=344c89cb19cc&source=-----a75c351445d----0-----------------clap_footer----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d?source=author_recirc-----6efad0141865----0---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa75c351445d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-approximate-nearest-neighbor-search-in-real-world-applications-a75c351445d&source=-----6efad0141865----0-----------------bookmark_preview----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----6efad0141865----1---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----6efad0141865----1-----------------bookmark_preview----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----6efad0141865----2---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----6efad0141865----2-----------------bookmark_preview----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Lester Solbakken"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "Stateful model serving: how we accelerate inference using ONNX RuntimeThere\u2019s a difference between stateless and stateful machine-learned model serving."}, {"url": "https://towardsdatascience.com/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": "13 min read\u00b7Dec 14, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa2875a77478d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d&user=Lester+Solbakken&userId=344c89cb19cc&source=-----a2875a77478d----3-----------------clap_footer----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/stateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d?source=author_recirc-----6efad0141865----3---------------------416581ff_aa32_41ae_b8e6_1ead88f88222-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa2875a77478d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstateful-model-serving-how-we-accelerate-inference-using-onnx-runtime-a2875a77478d&source=-----6efad0141865----3-----------------bookmark_preview----416581ff_aa32_41ae_b8e6_1ead88f88222-------", "anchor_text": ""}, {"url": "https://medium.com/@lester.solbakken?source=post_page-----6efad0141865--------------------------------", "anchor_text": "See all from Lester Solbakken"}, {"url": "https://towardsdatascience.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://skanda-vivek.medium.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Skanda Vivek"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Fine-Tune Transformer Models For Question Answering On Custom DataA tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and obtaining significant performance boosts"}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b75 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&user=Skanda+Vivek&userId=220d9bbb8014&source=-----513eaac37a80----0-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513eaac37a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80&source=-----6efad0141865----0-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@weiyunna91?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@weiyunna91?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "YUNNA WEI"}, {"url": "https://medium.com/trigger-ai?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Efficient Data+AI Stack"}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "MLOps in Practice \u2014 Machine Learning (ML) model deployment patterns (Part 1)Machine Learning (ML) model serving and deployment is one of the most critical components of any solid ML solution architecture. This\u2026"}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b711 min read\u00b7Jan 26"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftrigger-ai%2Fce7cb575feda&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftrigger-ai%2Fmlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda&user=YUNNA+WEI&userId=4b47aa84fc4&source=-----ce7cb575feda----1-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/trigger-ai/mlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fce7cb575feda&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftrigger-ai%2Fmlops-in-practice-machine-learning-ml-model-deployment-patterns-part-1-ce7cb575feda&source=-----6efad0141865----1-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----6efad0141865----0---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "91"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----6efad0141865----0-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@AmyGrabNGoInfo?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Amy @GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "GrabNGoInfo"}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "The Ultimate Guide to Evaluating Your Recommendation SystemUnderstand the key metrics to measure the performance of your recommender engine"}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b720 min read\u00b7Apr 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgrabngoinfo%2Fd4fc8d4423cc&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fthe-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc&user=Amy+%40GrabNGoInfo&userId=ef6171ffb4ed&source=-----d4fc8d4423cc----1-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/grabngoinfo/the-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc?source=read_next_recirc-----6efad0141865----1---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4fc8d4423cc&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgrabngoinfo%2Fthe-ultimate-guide-to-evaluating-your-recommendation-system-d4fc8d4423cc&source=-----6efad0141865----1-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----2-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----6efad0141865----2---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----6efad0141865----2-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----3-----------------clap_footer----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----6efad0141865----3---------------------53264112_4b9e_4d0a_b20a_057fad939d4b-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----6efad0141865----3-----------------bookmark_preview----53264112_4b9e_4d0a_b20a_057fad939d4b-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6efad0141865--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----6efad0141865--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}