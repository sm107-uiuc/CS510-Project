{"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "time": 1682994869.9603012, "path": "towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504/", "webpage": {"metadata": {"title": "The keys of Deep Learning in 100 lines of code | by Javier Ideami | Towards Data Science", "h1": "The keys of Deep Learning in 100 lines of code", "description": "Predict malignancy in cancer tumors with your own neural network built from scratch in Python. Understand the keys of deep learning in 100 lines of code."}, "outgoing_paragraph_urls": [{"url": "https://fast.ai", "anchor_text": "Jeremy Howard", "paragraph_index": 3}, {"url": "https://twitter.com/karpathy?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor", "anchor_text": "Andrej Karpathy", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Geoffrey_Hinton", "anchor_text": "Geoffrey", "paragraph_index": 3}, {"url": "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "anchor_text": "Essence of Linear Algebra series", "paragraph_index": 39}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "let\u2019s jump right into the code in the part 2 of this article", "paragraph_index": 67}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1", "paragraph_index": 68}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2", "paragraph_index": 68}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3", "paragraph_index": 68}], "all_paragraphs": ["In this 3 part article you are going to:", "Let\u2019s go together down the loss landscape of Deep Learning. Ready?", "These are exciting times for those passionate about the mysteries and possibilities of deep learning.", "Many of the heroes in the field share their expertise through videos and articles. They include people like Jeremy Howard at fast.ai, Andrew Ng at Coursera, Andrej Karpathy, Yann Lecun, Ian Goodfellow, Yoshua Bengio, Lex Fridman, Geoffrey Hinton, J\u00fcrgen Schmidhuber and many others.", "One of the key things that many of them recommend is to get hands on as soon as possible, coding the key principles of deep learning on your own.", "Nowadays, we have amazing libraries at our disposal. They include Tensorflow, PyTorch, Fast.ai, Keras, Mxnett, Nctk, DL4J and others. But if you only make use of those powerful libraries, you may be missing out on something important. The chance to reflect deeply about some of the most important parts of these processes.", "Coding a net on your own forces you to confront, face to face, the key issues and obstacles in this fascinating adventure. And also, the hidden marvels behind Deep Learning.", "Consider all the fancy architectures and latest developments within deep Learning. Convolutions, recurrent networks, GANs and so much more. What is really fascinating is that behind almost every success in the field, we find the same familiar friends. Back-Propagation and Gradient Descent.", "If you understand deeply those 2 concepts, the sky is the limit. You can then work with the powerful libraries holding a deeper perspective. And you will also be prepared to reflect on your own about new ways in which these tools can be improved.", "Ready to begin? It\u2019s going to be quite a ride!", "A lot of what happens in the universe can be expressed with functions. A function is a mathematical construction that takes an input and produces an output. Cause and effect. Input and Output.", "When we look at the world and its challenges, we see information, we see data. And we can learn a lot from that data.", "The learning we do with that data can be of different kinds. Let\u2019s highlight 3 very common types within deep learning:", "In this article we will focus on supervised learning, the area where, so far, deep learning has had the most success.", "Therefore, we have some data, some input, and we also have some output that corresponds to that input. And we want to understand how inputs and outputs are connected by means of a mystery function.", "Thing is, when the data involved reaches a certain complexity, finding that function becomes really hard. Enter neural networks and deep learning.", "At its heart, a neural network connects your input data and your desired outputs through a series of intermediate \u201cweights\u201d. These weights are really just numbers.", "Through their architectures and the optimization algorithms we will soon explore, neural networks become universal approximators. They are able to eventually compute any function that connects their inputs and outputs (when having the right architecture and parameters. To expand on this, see the universal approximation theorem of the mathematical theory of artificial neural networks).", "And the best way to understand a neural network is.. to build one! Yes, and from scratch, using the Python programming language in this case. So let\u2019s go for it, and in the process we are going to explore a lot of interesting topics and concepts.", "The best way to understand a neural network is.. to build one", "Below this paragraph you see the network we will build. It has 2 layers (the input layer is never counted).", "We could add more layers and have a network with 10 or 20 layers. For simplicity we will work with 2 in this article. A 2 layer neural network can do a lot, as we will find out shortly.", "So where will the learning take place within this network?", "Let\u2019s recap. In the input layer of our network we put some data. We will also show the network what output corresponds to that input, what result should appear at the output of the network (the second layer).", "Each unit within the layers of the network has an associated weight (and a bias, more about that later). Those weights are just numbers that at the beginning of the learning process are typically initialized randomly.", "The neural network performs some computations combining the input data with those weights. And those computations spread through the network until they produce a final result at its output.", "The result of those computations expresses a function that maps the inputs to the outputs.", "What we want is for the network to learn the best possible value of those weights. Because it\u2019s through the computations that the network performs, using those weights in combination with the different layers, that it\u2019s able to approximate different kinds of functions.", "Let\u2019s now understand deeper this mystery function that we are looking for. In order to do this, it\u2019s crucial that we clarify with precision the names of all the variables involved in our mission.", "Let\u2019s begin by saying that the first layer, our hidden layer, performs this computation: W X (the product between W and X)", "In a way, the weights express how strong or weak the connections are, the strength of the links between the different units of the network.", "And now we are going to add something extra to that product, a bias term: WX+b", "Adding a bias term gives more flexibility to the network. It allows it to \u201cmove around\u201d the linear computations of the units, increasing the potential of the network to learn faster those mystery functions.", "b: It represents the bias term of the units.", "There we have it: WX+ b. This is what we call a linear equation. Linear because it, by means of a product and a sum, represents a linear relationship between the input and the output (a relationship that can be expressed with a line).", "Now, remember that a neural network can have multiple layers. In our example we will have 2, but we could have 20 or 200.", "Therefore, we will use numbers to indicate to what layer these terms belong. The linear equation that defines the computations of our hidden layer, which is also our layer 1 is: W1 X+ b1", "We are going to also give a name to the output of that computation", "Notice that this computation should be done for each unit of each layer. When we program the network we will use a vectorized implementation. This means that we will make use of matrices to combine all the computations of a layer within a single mathematical operation.", "It\u2019s not essential for this tutorial that you understand matrices in depth, but If you want to refresh your understanding of them, you may check the great videos of 3Blue1Brown and his Essence of Linear Algebra series in YouTube.", "So far, so good. Now, imagine a network with many layers. Each of the layers performs a linear computation like the one above. When you chain all those linear computations together, the network is able to compute complex functions.", "However, there is a little problem..", "The world is complex, the world is a mess. The relationship between inputs and outputs in real life cannot typically be expressed with a line. It tends to be messy, it tends to be non-linear.", "Functions that are complex are often non-linear. And it\u2019s difficult for a neural network to compute non-linear behaviors if it\u2019s architecture is composed of only linear computations. That\u2019s why neural networks add at the end of each of their layers something extra: an activation function.", "An activation function is a non-linear function that introduces non-linear changes in the output of the layer. This will ensure that the network is capable of computing all sorts of complex functions, including those that are heavily non-linear.", "Now, there are a lot of different kinds of activation functions. Let\u2019s do a quick intro of 4 of the most typical ones.", "To explain these activation functions, I need to quickly introduce the concept of the gradient, which we will explore later in depth. The gradient of a function at a point is also called its derivative, and expresses the rate of change of the output of the function at that point.", "How much, in what direction and how strongly is the output of the function changing in response to changes in a specific input variable?", "When gradients (derivatives) become really small (the output of the function becomes really flat), we talk about vanishing gradients. Later on we will learn that the back-propagation algorithm, heavily used in deep learning, decides how to tweak the values of the weights of the network by using gradients to understand how each parameter of the network is influencing the network\u2019s output (is a change in this parameter making the output of the network increase or decrease?)", "Vanishing gradients are a problem because if the gradient at a point becomes too small or zero, it\u2019s very hard to understand the direction in which the output of the system is changing at that point.", "We can also talk about the opposite issue, exploding gradients. When the gradient values become very large, the network can become really unstable.", "Different activation functions can have different advantages. But they can also suffer of vanishing and exploding gradient issues.", "Let\u2019s quickly introduce the most popular activation functions.", "In this article, we will use the Sigmoid function in our output layer and the ReLU in our hidden layer.", "All right, now that we understand activation functions, we need to give them a name!", "Therefore, at our hidden layer, the computation we perform will be:", "And at our second layer, our output layer, the computation will be:", "Notice the use of A1 in the equation of Z2, because the input of the second layer is the output of the first one, which is A1.", "Finally, notice that Yh=A2. The output of layer 2 is also the final output of the network.", "So that\u2019s it. Now, if we put those computations together, if we chain those functions, we find that the total computation of the neural network is this one:", "That\u2019s it. That\u2019s the whole computation that our 2 layer neural network performs.", "So, in effect, a neural network is a chain of functions, some linear and some non-linear, which together produce a complex function, that mystery function that is going to connect your input data to your desired outputs.", "At this stage, notice that out of all the variables in that equation, the values of W and b are the big unknowns. Here is where learning must happen.", "Somehow, the network must learn the correct values of W and b that will allow it to compute the correct function.", "We will therefore train our network to find the correct values of W1, b1, W2 and b2. But before we can begin that training, we must first initialize those values.", "How to initialize the weights and biases of a network is a whole topic in itself and we will go deeper into it later. For now, we are going to initialize them with random values.", "At this stage, we can begin to code our neural network. Let\u2019s build a class in Python that initializes its main parameters. Then we will see how we can train it to learn our mystery function.", "So let\u2019s jump right into the code in the part 2 of this article and we will be learning and exploring on the go.", "Links to the 3 parts of this article:Part 1 | Part 2 | Part 3", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A multidisciplinary engineer, researcher, creative director, artist and entrepreneur, from augmented reality to deep learning, filmmaking, 3D and beyond."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F907398c76504&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----907398c76504--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----907398c76504--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ideami?source=post_page-----907398c76504--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=post_page-----907398c76504--------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84----907398c76504---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://fast.ai", "anchor_text": "Jeremy Howard"}, {"url": "https://twitter.com/karpathy?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor", "anchor_text": "Andrej Karpathy"}, {"url": "https://en.wikipedia.org/wiki/Geoffrey_Hinton", "anchor_text": "Geoffrey"}, {"url": "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab", "anchor_text": "Essence of Linear Algebra series"}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "let\u2019s jump right into the code in the part 2 of this article"}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----907398c76504---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----907398c76504---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----907398c76504---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----907398c76504---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----907398c76504---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&user=Javier+Ideami&userId=7f7b5d730c84&source=-----907398c76504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&user=Javier+Ideami&userId=7f7b5d730c84&source=-----907398c76504---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----907398c76504--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F907398c76504&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----907398c76504---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----907398c76504--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----907398c76504--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----907398c76504--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----907398c76504--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----907398c76504--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----907398c76504--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----907398c76504--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----907398c76504--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/@ideami/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c3e5da2cfc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-keys-of-deep-learning-in-100-lines-of-code-907398c76504&newsletterV3=7f7b5d730c84&newsletterV3Id=4c3e5da2cfc1&user=Javier+Ideami&userId=7f7b5d730c84&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}