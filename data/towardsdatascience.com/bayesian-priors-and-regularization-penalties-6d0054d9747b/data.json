{"url": "https://towardsdatascience.com/bayesian-priors-and-regularization-penalties-6d0054d9747b", "time": 1683000301.373605, "path": "towardsdatascience.com/bayesian-priors-and-regularization-penalties-6d0054d9747b/", "webpage": {"metadata": {"title": "Bayesian Priors and Regularization Penalties | by Ray Heberer | Towards Data Science", "h1": "Bayesian Priors and Regularization Penalties", "description": "Bayesian methods of performing machine learning offer several advantages over their counterparts, notably the ability to estimate uncertainty and the option to encode contextual knowledge as prior\u2026"}, "outgoing_paragraph_urls": [{"url": "https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/", "anchor_text": "Bayesian Methods for Hackers", "paragraph_index": 5}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov Chain Monte Carlo", "paragraph_index": 9}, {"url": "https://docs.pymc.io/", "anchor_text": "PyMC3", "paragraph_index": 9}, {"url": "https://www.kaggle.com/rayheberer/bayesian-priors-and-regularization-penalties", "anchor_text": "Notebook on Kaggle", "paragraph_index": 10}, {"url": "https://www.kaggle.com/rayheberer/priors-penalties-functions", "anchor_text": "this utility script", "paragraph_index": 11}], "all_paragraphs": ["Bayesian methods of performing machine learning offer several advantages over their counterparts, notably the ability to estimate uncertainty and the option to encode contextual knowledge as prior distributions. Why then, aren\u2019t they more widely used?", "To be unequivocally considered an upgrade, Bayesian models must both have an equivalent theoretical formulation for all popular machine learning models, and recover the sort of predictive performance observed in them. They must also be about as easy to use.", "These are all tough challenges, and there are many technical hurdles to overcome. However, I am not here today to wax philosophical about the state of probabilistic programming. Rather, I would like to take some time to explore an equivalence that is presented in many beginner texts on Bayesian methods: that between the prior distribution of coefficients in a Bayesian linear model, and the penalty term used in regularized least-squares regression.", "I find this duality compelling because on its own, regularization seems a little \u201chacky.\u201d Knowing that it can be understood and formalized within a larger framework is comforting, and is something I thought was worth poking at empirically.", "There are many good theoretical treatments of this equivalence, so I chose to test it out by varying the tuning parameters of regularized linear models and seeing how the magnitude of the largest coefficient, as well as the error on a regression problem, responded. I did the same thing using a Bayesian GLM, tuning the variance of the prior distributions of parameters. It was a fun exercise, and I think the results are worth sharing.", "I would like to emphasize right now that my purpose here is not to introduce Bayesian linear models to those who are unfamiliar. If you\u2019re new to probabilistic programming and have experience with Python, I recommend Bayesian Methods for Hackers. In any case, my goal is to provide another angle of looking at how two formulations of linear regression are equivalent.", "Nevertheless, I will briefly outline the Bayesian view of linear regression. In this formulation, the response variable Y is treated as a random variable, with mean equal to the weighted sum of the features \u03b2X.", "That is, Y\u223cN(\u03b2X,\u03c3). This is equivalent to what you would find in the standard formulation of linear regression if the noise term is normally distributed(Y=\u03b2X+\u03f5, with \u03f5\u223cN(0, \u03c3)).", "Furthermore, we can specify prior distributions over the parameters \u03b2. A common choice is the Gaussian distribution. If we center this distribution around 0, this would indicate that we expect the parameters to be small. Choosing small values for the standard deviation of this prior would correspond to a tighter distribution, indicating a stronger initial belief in small parameters \u2014 similar to a large penalty in regularized least-squares regression.", "In the probabilistic programming approach, an unnormalized function proportional to the posterior distribution for all the random variables we are interested in (for a linear model, the coefficients and intercept) is generated from the data and the priors following Bayes\u2019 theorem. A sampling algorithm, typically some variant of Markov Chain Monte Carlo, then generates an estimate of this posterior. I used PyMC3 to build my Bayesian model and sample the distributions of its parameters.", "I chose to perform my little mini-experiment using a Notebook on Kaggle. My reasoning for this was twofold:", "Once I knew the sort of comparison and plots I wanted to make, it was fairly straightforward proceedings. For those interested in the technical details, most of the code for the notebook got put into this utility script. It contains the main ingredients needed for my experiment:", "Below are the plots produced by the notebook for Ridge (L2) Regression and a Bayesian Linear Model with Gaussian priors. Using logarithmic x-axes with appropriate ranges, the curves are remarkably similar, as we would expect.", "What exactly are we seeing here? First, let\u2019s review the hyperparameters.", "Alpha is a tuning parameter that controls the relative importance of the L2 penalty term in ridge regression. The loss function is given by L = MSE + \u03b1|| \u03b8||, where MSE is the mean-squared-error and \u03b8 is the vector of coefficients. So when alpha is large, the loss is dominated by this term and the best way to minimize it is to set all coefficients equal to zero.", "Sigma gives the standard deviation of the prior distribution of coefficients in the Bayesian model. This prior is chosen to be a Normal (Gaussian) distribution. If it is small, then our prior is very tightly centered around zero, and it would take an overwhelming amount of evidence (data) to shift the mass of the posterior distribution of parameters away from zero.", "Knowing this, do the results above make sense? Let\u2019s see:", "Even though the implementations behind these two models are very different, with one relying on optimization and the other on sampling, we observe nearly identical behavior in the two measures chosen.", "Bayesian Linear Models are often presented as introductory material for those seeking to learn probabilistic programming, coming in with an existing understanding in frequentist statistical learning models. I believe this is effective because it allows one to scaffold new knowledge on top of existing knowledge, and even fit something that was already understood \u2014 perhaps as just one tool among many \u2014 into a wider and more theoretically satisfying framework.", "The relationship between the prior distribution of parameters chosen in a Bayesian linear model and the penalty term in regularized least-squares regression is already well known. Despite this, I feel that I was able to come to a more visceral and intuitive understanding of this equivalence by empirically examining the effect of tweaking hyperparameters of each model. I hope my small experiment can do the same for you, and be supplemental to the existing proofs that are available.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Proximate Research. A little interested in a lot of things, a lot interested in a little."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6d0054d9747b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rayheberer.medium.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Ray Heberer"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6efdc3f1390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&user=Ray+Heberer&userId=d6efdc3f1390&source=post_page-d6efdc3f1390----6d0054d9747b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@introspectivedsgn?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Erik Mclean"}, {"url": "https://unsplash.com/s/photos/dice?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/", "anchor_text": "Bayesian Methods for Hackers"}, {"url": "https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo", "anchor_text": "Markov Chain Monte Carlo"}, {"url": "https://docs.pymc.io/", "anchor_text": "PyMC3"}, {"url": "https://www.kaggle.com/rayheberer/bayesian-priors-and-regularization-penalties", "anchor_text": "Notebook on Kaggle"}, {"url": "https://www.kaggle.com/rayheberer/priors-penalties-functions", "anchor_text": "this utility script"}, {"url": "https://medium.com/@rayheberer/generating-matplotlib-subplots-programmatically-cc234629b648", "anchor_text": "programmatically generating matplotlib subplots"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6d0054d9747b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----6d0054d9747b---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----6d0054d9747b---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/linear-regression?source=post_page-----6d0054d9747b---------------linear_regression-----------------", "anchor_text": "Linear Regression"}, {"url": "https://medium.com/tag/probability?source=post_page-----6d0054d9747b---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&user=Ray+Heberer&userId=d6efdc3f1390&source=-----6d0054d9747b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&user=Ray+Heberer&userId=d6efdc3f1390&source=-----6d0054d9747b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6d0054d9747b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6d0054d9747b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6d0054d9747b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6d0054d9747b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6d0054d9747b--------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rayheberer.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ray Heberer"}, {"url": "https://rayheberer.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "362 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6efdc3f1390&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&user=Ray+Heberer&userId=d6efdc3f1390&source=post_page-d6efdc3f1390--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Facb0180c171c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbayesian-priors-and-regularization-penalties-6d0054d9747b&newsletterV3=d6efdc3f1390&newsletterV3Id=acb0180c171c&user=Ray+Heberer&userId=d6efdc3f1390&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}