{"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "time": 1683010035.587382, "path": "towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7/", "webpage": {"metadata": {"title": "Recommendation System Series Part 6: The 6 Variants of Autoencoders for Collaborative Filtering | by James Le | Towards Data Science", "h1": "Recommendation System Series Part 6: The 6 Variants of Autoencoders for Collaborative Filtering", "description": "Update: This article is part of a series where I explore recommendation systems in academia and industry. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, and Part 6. Many\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "anchor_text": "Part 5", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Part 6", "paragraph_index": 0}, {"url": "https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf", "anchor_text": "Autoencoders Meet Collaborative Filtering", "paragraph_index": 19}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/AutoRec-TensorFlow", "anchor_text": "my TensorFlow implementation", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1708.01715", "anchor_text": "Training Deep Autoencoders for Collaborative Filtering", "paragraph_index": 29}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/NVIDIA-DeepRec-TensorFlow", "anchor_text": "my TensorFlow implementation", "paragraph_index": 35}, {"url": "https://alicezheng.org/papers/wsdm16-cdae.pdf", "anchor_text": "\u201cCollaborative Denoising Autoencoders for Top-N Recommender Systems", "paragraph_index": 36}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/CDAE-PyTorch", "anchor_text": "my PyTorch implementation", "paragraph_index": 44}, {"url": "https://arxiv.org/abs/1802.05814", "anchor_text": "Variational Autoencoders for Collaborative Filtering", "paragraph_index": 45}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/VAE-PyTorch", "anchor_text": "my PyTorch implementation", "paragraph_index": 62}, {"url": "https://arxiv.org/abs/1811.09975", "anchor_text": "Sequential Variational Auto-encoders for Collaborative Filtering", "paragraph_index": 63}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/SVAE-PyTorch", "anchor_text": "my PyTorch implementation", "paragraph_index": 76}, {"url": "https://arxiv.org/abs/1905.03375", "anchor_text": "Embarrassingly Shallow Autoencoders for Sparse Data", "paragraph_index": 77}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/ESAE-PyTorch", "anchor_text": "my PyTorch implementation", "paragraph_index": 86}, {"url": "https://github.com/khanhnamle1994/transfer-rec/blob/master/Autoencoders-Experiments/README.md", "anchor_text": "README", "paragraph_index": 87}, {"url": "https://medium.com/@james_aka_yale", "anchor_text": "Medium", "paragraph_index": 91}, {"url": "https://github.com/khanhnamle1994", "anchor_text": "GitHub", "paragraph_index": 91}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/", "paragraph_index": 91}, {"url": "https://twitter.com/le_james94", "anchor_text": "Twitter", "paragraph_index": 91}, {"url": "http://www.linkedin.com/in/khanhnamle94", "anchor_text": "find me on LinkedIn", "paragraph_index": 91}, {"url": "http://eepurl.com/deWjzb", "anchor_text": "Sign up for my newsletter", "paragraph_index": 91}], "all_paragraphs": ["Update: This article is part of a series where I explore recommendation systems in academia and industry. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, and Part 6.", "Many recommendation models have been proposed during the last few years. However, they all have their limitations in dealing with data sparsity and cold-start issues.", "To solve these problems, recent approaches have exploited side information about users or items. However, the improvement of the recommendation performance is not significant due to the limitations of such models in capturing the user preferences and item features.", "Auto-encoder is a type of neural network suited for unsupervised learning tasks, including generative modeling, dimensionality reduction, and efficient coding. It has shown its superiority in learning underlying feature representation in many domains, including computer vision, speech recognition, and language modeling. Given that knowledge, new recommendation architectures have incorporated autoencoder and thus brought more opportunities in re-inventing user experiences to satisfy customers.", "In this post and those to follow, I will be walking through the creation and training of recommendation systems, as I am currently working on this topic for my Master Thesis.", "In Part 6, I explore the use of Auto-Encoders for collaborative filtering. More specifically, I will dissect six principled papers that incorporate Auto-Encoders into their recommendation architecture. But first, let\u2019s walk through a primer on auto-encoder and its variants.", "As illustrated in the diagram below, a vanilla auto-encoder consists of an input layer, a hidden layer, and an output layer. The input data is passed into the input layer. The input layer and the hidden layer constructs an encoder. The hidden layer and the output layer constructs a decoder. The output data comes out of the output layer.", "The encoder encodes the high-dimensional input data x into a lower-dimensional hidden representation h with a function f:", "where s_f is an activation function, W is the weight matrix, and b is the bias vector.", "The decoder decodes the hidden representation h back to a reconstruction x\u2019 by another function g:", "where s_g is an activation function, W\u2019 is the weight matrix, and b\u2019 is the bias vector.", "The choices of s_f and s_g are non-linear, for example, Sigmoid, TanH, or ReLU. This allows auto-encoder to learn more useful features than other unsupervised linear approaches, say Principal Component Analysis.", "I can train the auto-encoder to minimize the reconstruction error between x and x\u2019 via either the squared error (for regression tasks) or the cross-entropy error (for classification tasks).", "This is the formula for the squared error:", "This is the formula for the cross-entropy error:", "Finally, it is always a good practice to add a regularization term to the final reconstruction error of the auto-encoder:", "The reconstruction error function above can be optimized via either stochastic gradient descent or alternative least square.", "There are many variants of auto-encoders currently used in recommendation systems. The four most common are:", "Okay, it\u2019s time to review the different auto-encoder based recommendation framework!", "One of the earliest models that consider the collaborative filtering problem from an auto-encoder perspective is AutoRec from \u201cAutoencoders Meet Collaborative Filtering\u201d by Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie.", "In the paper\u2019s setting, there are m users, n items, and a partially filled user-item interaction/rating matrix R with dimension m x n. Each user u can be represented by a partially filled vector r\u1d64 and each item i can be represented by a partially filled vector r\u1d62. AutoRec directly takes user rating vectors r\u1d64 or item rating r\u1d62 as input data and obtains the reconstructed rating at the output layer. There are two variants of AutoRec depending on two types of inputs: item-based AutoRec (I-AutoRec) and user-based AutoRec (U-AutoRec). Both of them have the same structure.", "Figure 1 from the paper illustrates the structure of I-AutoRec. The shaded nodes correspond to observed ratings, and the solid connections correspond to weights that are updated for the input r\u1d62.", "Given the input r\u1d62, the reconstruction is:", "where f and g are the activation functions, and the parameter Theta includes W, V, mu, and b.", "AutoRec uses only the vanilla auto-encoder structure. The objective function of the model is similar to the loss function of auto-encoder:", "This function can be optimized by resilient propagation (converges faster and produces comparable results) or L-BFGS (Limited-memory Broyden Fletcher Goldfarb Shanno algorithm).", "Here are some important things about AutoRec:", "The TensorFlow code of the AutoRec model class is given below for illustration purpose:", "For my TensorFlow implementation, I trained AutoRec architecture with a hidden layer of 500 units activated by a sigmoid non-linear function. Other hyper-parameters include a learning rate of 0.001, a batch size of 512, the Adam optimizer, and a lambda regularizer of 1.", "DeepRec is a model created by Oleisii Kuchaiev and Boris Ginsburg from NVIDIA, as seen in \u201cTraining Deep Autoencoders for Collaborative Filtering.\u201d The model is inspired by the AutoRec model described above, with several important distinctions:", "The figure above depicts a typical 4-layer autoencoder network. The encoder has 2 layers e_1 and e_2, while the decoder has 2 layers d_1 and d_2. They are fused together on the representation z. The layers are represented as f(W * x + b), where f is some non-linear activation function. If the range of the activation function is smaller than that of the data, the last layer of the decoder should be kept linear. The authors found it to be very important for activation function f in hidden layers to contain a non-zero negative part, and use SELU units in most of their experiments.", "Since it doesn\u2019t make sense to predict zeros in user\u2019s representation vector x, the authors optimize the Masked Mean Squared Error loss:", "where r_i is the actual rating, y_i is the reconstructed rating, and m_i is a mask function such that m_i = 1 if r_i is not 0 else m_i = 0.", "During forward pass and inference pass, the model takes a user represented by his vector of ratings from the training set x. Note that x is very sparse, while the output of the decoder f(x) is dense and contains rating predictions for all items in the corpus. Thus, to explicitly enforce fixed-point constraint and perform dense training updates, the authors augment every optimization iteration with an iterative dense re-feeding step as follows:", "The TensorFlow code of the DeepRec model definition is given below for illustration purpose:", "For my TensorFlow implementation, I trained DeepRec with the following architecture: [n, 512, 512, 1024, 512, 512, n]. So n is the number of ratings that the user has given, the encoder has 3 layers of size (512, 512, 1034), the bottleneck layer has size 1024, and the decoder has 3 layers of size (512, 512, n). I trained the model using stochastic gradient descent with a momentum of 0.9, a learning rate of 0.001, a batch size of 512, and a dropout rate of 0.8. Parameters are initialized via the Xavier initialization scheme.", "\u201cCollaborative Denoising Autoencoders for Top-N Recommender Systems\u201d by Yao Wu, Christopher DuBois, Alice Zheng, and Martin Ester is a neural network with one hidden layer. Compared to AutoRec and DeepRec, CDAE has the following differences:", "The figure above shows a sample structure of CDAE, which consists of 3 layers: the input, the hidden, and the output.", "The corrupted input r_corr of CDAE is drawn from a conditional Gaussian distribution p(r_corr | r). The reconstruction of r_corr is formulated as follows:", "where W\u2081 is the weight matrix corresponding to the encoder (going from the input layer to the hidden layer), W\u2082 is the weight matrix corresponding to the decoder (going from the hidden layer to the output layer). V\u1d64 is the weight matrix for the red user node, while both b\u2081 and b\u2082 are the bias vectors.", "The parameters of CDAE are learned by minimizing the average reconstruction error as follows:", "The loss function L(r_corr, h(r_corr)) in the equation above can be square loss or logistic loss. CDAE uses the square L2 norm to control the model complexity. It also (1) applies stochastic gradient descent to learn the model\u2019s parameters and (2) adopts AdaGrad to automatically adapt the training step size during the learning procedure.", "The authors also propose a negative sampling technique to extract a small subset from items that user did not interact with for reducing the time complexity substantially without degrading the ranking quality. At inference time, CDAE takes a user\u2019s existing preference set (without corruption) as input and recommends the items with the largest prediction values on the output layer to that user.", "The PyTorch code of the CDAE architecture class is given below for illustration purpose:", "For my PyTorch implementation, I used a CDAE architecture with a hidden layer of 50 units. I trained the model using stochastic gradient descent with a learning rate of 0.01, a batch size of 512, and a corruption ratio of 0.5.", "One of the most influential papers in this discussion is \u201cVariational Autoencoders for Collaborative Filtering\u201d by Dawen Liang, Rahul Krishnan, Matthew Hoffman, and Tony Jebara from Netflix. It proposes a variant of VAE for recommendation with implicit data. In particular, the authors introduced a principled Bayesian inference approach to estimate model parameters and show favorable results than commonly used likelihood functions.", "The paper uses U to index all users and I to index all items. The user-by-item interaction matrix is called X (with dimension U x I). The lower case x\u1d64 is a bag-of-words vector with the number of clicks for each item from user u. For implicit feedback, this matrix is binarized to have only 0s and 1s.", "The generative process of the model is seen in equation 11 and broken down as follows:", "The log-likelihood for user u (conditioned on the latent representation) is:", "The authors believe that the multinomial distribution is suitable for this collaborative filtering problem. Specifically, the likelihood of the interaction matrix in equation 11 rewards the model for putting probability mass on the non-zero entries in x\u1d64. However, considering that \u03c0(z\u1d64) must sum to 1, the items must compete for a limited budget of probability mass. Therefore, the model should instead assign more probability mass to items that are more likely to be clicked, making it suitable to achieve a solid performance in the top-N ranking evaluation metric of recommendation systems.", "In order to train the generative model in equation 11, the authors estimate \u03b8 by approximating the intractable posterior distribution p(z\u1d64 | x\u1d64) via variational inference. This method approximates the true intractable posterior with a simpler variational distribution q(z\u1d64) \u2014 which is a fully diagonal Gaussian distribution. The objective of variational inference is to optimize the free variational parameters {\u03bc\u1d64, \u03c3\u1d64\u00b2} so that the Kullback-Leiber divergence KL(q(z\u1d64) || p(z\u1d64 | x\u1d64)) is minimized.", "The issue with variational inference is that the number of parameters to optimize {\u03bc\u1d64, \u03c3\u1d64\u00b2} grows with the number of users and items in the dataset. VAE helps solve this issue by replacing the individual variational parameters with a data-dependent function:", "This function is parameterized by \u03d5 \u2014 in which both \u03bc_{\u03d5} (x\u1d64) and \u03c3_{\u03d5} (x\u1d64) are vectors with K dimensions. The variational distribution is then set as follows:", "Using the input x\u1d64, the inference model returns the corresponding variational parameters of variational distribution q_{\u03d5} (z\u1d64|x\u1d64). When being optimized, this variational distribution approximates the intractable posterior p_{\u03d5} (z\u1d64 | x\u1d64).", "To learn latent-variable models with variational inference, the standard approach is to lower-bound the log marginal likelihood of the data. The objective function to maximize for user u now becomes:", "Another term to call this objective is evidence lower bound (ELBO). Intuitively, we should be able to obtain an estimate of ELBO by sampling z\u1d64 \u223c q_\u03d5 and optimizing it with a stochastic gradient ascent. However, we cannot differentiate ELBO to get the gradients with respect to \u03d5. The reparametrization trick comes in handy here:", "Essentially, we isolate the stochasticity in the sampling process, and thus the gradient with respect to \u03d5 can be back-propagated through the sampled z\u1d64.", "From a different perspective, the first term of equation 15 can be interpreted as reconstruction error and the second term of equation 15 can be interpreted as regularization. The authors, therefore, extend equation 15 with an additional parameter \u03b2 to control the strength of regularization:", "This parameter \u03b2 engages a tradeoff between how well the model fits the data and how close the approximate posterior stays to the prior during learning. The authors tune \u03b2 via KL annealing, a common heuristic used for training VAEs when there is concern that the model is being under-utilized.", "Given a user\u2019s click history x, the model ranks all items based on the un-normalized predicted multinomial probability f_\u03d5 (z). The latent representation z for x is simply the mean of the variational distribution z = \u03bc_\u03d5 (x).", "Figure 2 from the paper provides a unified view of different variants of autoencoders.", "The PyTorch code of the MultVAE architecture class is given below for illustration purpose:", "For my PyTorch implementation, I keep the architecture for the generative model f and the inference model g symmetrical and use an MLP with 1 hidden layer. The dimension of the latent representation K is set to 200 and of other hidden layers is set to 600. The overall architecture for a MultVAE now becomes [I -> 600 -> 200 -> 600 -> I], where I is the total number of items. Other model details include tanH activation function, dropout rate with probability 0.5, Adam optimizer, batch size of 512, and learning rate of 0.01.", "In \u201cSequential Variational Auto-encoders for Collaborative Filtering,\u201d Noveen Sachdeva, Giuseppe Manco, Ettore Ritacco, and Vikram Pudi propose an extension of MultVAE by exploring the rich information present in the past preference history. They introduce a recurrent version of MultVAE, where instead of passing a subset of the whole history regardless of temporal dependencies, they pass the consumption sequence subset through a recurrent neural network. They show that handling temporal information is crucial for improving the accuracy of VAE.", "The problem setting is exactly similar to that of the MultVAE paper: U is a set of users, I is a set of items, and X is the user-item preference matrix with dimension U x I. The principal difference is that SVAE considers precedence and temporal relationships within the matrix X.", "The figure above from the paper shows the architectural difference between MultVAE, SVAE, and another model called RVAE (which I won\u2019t discuss here). Looking at the SVAE architecture, I can observe the recurrent relationship occurring in the layer upon which z_{u(t)} depends. The basic idea behind SVAE is that latent variable modeling should be able to express temporal dynamics and hence causalities and dependencies among preferences in a user\u2019s history.", "Let\u2019s review the math. Within this SVAE framework, the authors model temporal dependencies by conditioning each event to the previous events. Given a sequence x_{(1: T}, then its probability is:", "This probability represents a recurrent relationship between x_{(t+1} and x_{(1:t)}. Thus, the model can handle each timestep separately.", "Recall the generative process in equation 11, we can add a timestamp t as seen below:", "Equation 19 results in the joint likelihood:", "The posterior likelihood in equation 20 can be approximated with a factorized proposal distribution:", "where the right-term side is a Gaussian distribution whose parameters \u03bc and \u03c3 depend upon the current history x_{u(1:t-1)}, by means of a recurrent layer h_t:", "Finally, the loss function that SVAE optimized is:", "In this SVAE model, the proposal distribution introduces a dependency of the latent variable from a recurrent layer, which allows us to recover the information from the previous history. Given a user history x_{u(1:t-1)}, we can use equation 22 and set z = \u03bc_{\u03bb} (t), upon which we can devise the probability for the x_{u(t)} by means of \u03c0(z).", "The PyTorch code of the SVAE architecture class is given below for illustration purpose:", "Another unique property of this paper is the way the evaluation protocol works. The authors partitioned users into training, validation, and test set; and then trained the model using the full histories of the users in the training set. During the evaluation, for each user in the validation/test set, they split the time-sorted user history into two parts, fold-in and fold-out split.", "For my PyTorch implementation, I follow the same code provided by the authors.", "Harald Steck\u2019s \u201cEmbarrassingly Shallow Autoencoders for Sparse Data\u201d is a fascinating one that I want to bring into this discussion. The motivation here is that, according to his literature review, deep models with a large number of hidden layers typically do not obtain a notable improvement in ranking accuracy in collaborative filtering, compared to \u2018deep\u2019 models with only one, two, or three hidden layers. This is a stark contrast to other areas like NLP or computer vision.", "Embarrassingly Shallow Auto-encoders (ESAE) is a linear model without a hidden layer. The (binary) input vector X vector indicates which items a user has interacted with, and ESAE\u2019s objective is to predict the best items to recommend to that user in the output layer (as seen in the figure above). For implicit feedback, a value of 1 in X indicates that the user interacted with an item, while a value of 0 in X indicates that there is no observed interaction.", "The item-item weight matrix B represents the parameters of ESAE. Here, the self-similarity of an item in the input layer with itself in the output layer is omitted, so that ESAE can generalize effectively during the reconstruction step. Thus, the diagonal of this weight-matrix B is constrained to 0 (diag(B) = 0).", "For an item j and a user u, we want to predict S_{u, j}, where X_{u,.} refers to row u and B_{.,j} refers to column j:", "With respect to diag(B) = 0, ESAE has the following convex objective for learning the weights B:", "Here are important notes about this convex objective:", "In the paper, Harald derived a closed-form solution from the training objective in equation 25. He argues that the traditional neighborhood-based collaborative filtering approaches are based on conceptually incorrect item-item similarity matrices, while the ESAE framework utilizes principled neighborhood models. I won\u2019t go over the math derivation here, but you should take a look at section 3.1 from the paper for the detail.", "Notably, ESAE\u2019s similarity matrix is based on the inverse of the given data matrix. As a result, the learned weights can also be negative and thus the model can learn the dissimilarities between items (besides the similarities). This proves to be essential to obtain good ranking accuracy. Furthermore, the data sparsity problem (there possibly is only a small amount of data available for each user) does not affect the uncertainty in estimating weight matrix B if the number of users in the data matrix X is sufficiently large.", "The Python code of the learning algorithm is given above. The training requires only the item-item matrix G = X^T * X as input, instead of the user-item matrix X. This is very efficient if the size of G is smaller than the size of X.", "For my PyTorch implementation, I set the L2-Norm regularization hyper-parameter \u03bb to be 1000, the learning rate to be 0.01, and the batch size to be 512.", "The result table is at the bottom of my repo\u2019s README:", "In this post, I have discussed the nuts and bolts of Auto-encoders and their use in collaborative filtering. I also walked through 6 different papers that use Auto-encoders for the recommendation framework: (1) AutoRec, (2) DeepRec, (3) Collaborative Denoising Auto-encoder, (4) Multinomial Variational Auto-encoder, (5) Sequential Variational Auto-encoder, and (6) Embarrassingly Shallow Auto-encoder.", "There are several emerging research directions that are happening in this area:", "Stay tuned for future blog posts of this series that explore different modeling architectures that have been made for collaborative filtering.", "If you would like to follow my work on Recommendation Systems, Deep Learning, and Data Science Journalism, you can check out my Medium and GitHub, as well as other projects at https://jameskle.com/. You can also tweet at me on Twitter, email me directly, or find me on LinkedIn. Sign up for my newsletter to receive my latest thoughts on machine learning in research and in the industry right at your inbox!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbd7b9eae2ec7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://le-james94.medium.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "James Le"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52aa38cb8e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&user=James+Le&userId=52aa38cb8e25&source=post_page-52aa38cb8e25----bd7b9eae2ec7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4"}, {"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "anchor_text": "Part 5"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7", "anchor_text": "Part 6"}, {"url": "https://www.vox.com/recode/2020/2/25/21152585/tiktok-recommendations-profile-look-alike", "anchor_text": "https://www.vox.com/recode/2020/2/25/21152585/tiktok-recommendations-profile-look-alike"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-1-an-executive-guide-to-building-recommendation-system-608f83e2630a", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-2-the-10-categories-of-deep-recommendation-systems-that-189d60287b58", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/recommendation-system-series-part-3-the-6-research-directions-of-deep-recommendation-systems-that-3a328d264fb7", "anchor_text": "Part 3"}, {"url": "https://towardsdatascience.com/recsys-series-part-4-the-7-variants-of-matrix-factorization-for-collaborative-filtering-368754e4fab5", "anchor_text": "Part 4"}, {"url": "https://towardsdatascience.com/recsys-series-part-5-neural-matrix-factorization-for-collaborative-filtering-a0aebfe15883", "anchor_text": "Part 5"}, {"url": "https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf", "anchor_text": "Autoencoders Meet Collaborative Filtering"}, {"url": "https://dl.acm.org/doi/10.1145/2740908.2742726", "anchor_text": "https://dl.acm.org/doi/10.1145/2740908.2742726"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/AutoRec-TensorFlow", "anchor_text": "my TensorFlow implementation"}, {"url": "https://arxiv.org/abs/1708.01715", "anchor_text": "Training Deep Autoencoders for Collaborative Filtering"}, {"url": "https://arxiv.org/abs/1708.01715", "anchor_text": "https://arxiv.org/abs/1708.01715"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/NVIDIA-DeepRec-TensorFlow", "anchor_text": "my TensorFlow implementation"}, {"url": "https://alicezheng.org/papers/wsdm16-cdae.pdf", "anchor_text": "\u201cCollaborative Denoising Autoencoders for Top-N Recommender Systems"}, {"url": "https://dl.acm.org/doi/10.1145/2835776.2835837", "anchor_text": "https://dl.acm.org/doi/10.1145/2835776.2835837"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/CDAE-PyTorch", "anchor_text": "my PyTorch implementation"}, {"url": "https://arxiv.org/abs/1802.05814", "anchor_text": "Variational Autoencoders for Collaborative Filtering"}, {"url": "https://matsen.fredhutch.org/general/2019/08/24/vbpi.html", "anchor_text": "https://matsen.fredhutch.org/general/2019/08/24/vbpi.html"}, {"url": "https://arxiv.org/abs/1802.05814", "anchor_text": "https://arxiv.org/abs/1802.05814"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/VAE-PyTorch", "anchor_text": "my PyTorch implementation"}, {"url": "https://arxiv.org/abs/1811.09975", "anchor_text": "Sequential Variational Auto-encoders for Collaborative Filtering"}, {"url": "https://arxiv.org/abs/1811.09975", "anchor_text": "https://arxiv.org/abs/1811.09975"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/SVAE-PyTorch", "anchor_text": "my PyTorch implementation"}, {"url": "https://arxiv.org/abs/1905.03375", "anchor_text": "Embarrassingly Shallow Autoencoders for Sparse Data"}, {"url": "https://arxiv.org/abs/1905.03375", "anchor_text": "https://arxiv.org/abs/1905.03375"}, {"url": "https://mathworld.wolfram.com/FrobeniusNorm.html", "anchor_text": "Frobenius norm"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments/ESAE-PyTorch", "anchor_text": "my PyTorch implementation"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments", "anchor_text": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Autoencoders-Experiments"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/ml-1m", "anchor_text": "MovieLens 1M"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Matrix-Factorization-Experiments", "anchor_text": "Matrix Factorization"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/tree/master/Multilayer-Perceptron-Experiments", "anchor_text": "Multilayer Perceptron"}, {"url": "https://www.comet.ml/", "anchor_text": "Comet ML"}, {"url": "https://github.com/khanhnamle1994/transfer-rec/blob/master/Autoencoders-Experiments/README.md", "anchor_text": "README"}, {"url": "https://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf", "anchor_text": "Autoencoders Meet Collaborative Filtering"}, {"url": "https://arxiv.org/abs/1708.01715", "anchor_text": "Training Deep Autoencoders for Collaborative Filtering"}, {"url": "https://alicezheng.org/papers/wsdm16-cdae.pdf", "anchor_text": "Collaborative Denoising Autoencoders for Top-N Recommender Systems"}, {"url": "https://arxiv.org/abs/1802.05814", "anchor_text": "Variational Autoencoders for Collaborative Filtering"}, {"url": "https://arxiv.org/abs/1811.09975", "anchor_text": "Sequential Variational Autoencoders for Collaborative Filtering"}, {"url": "https://arxiv.org/abs/1905.03375", "anchor_text": "Embarrassingly Shallow Autoencoders for Sparse Data"}, {"url": "https://medium.com/@james_aka_yale", "anchor_text": "Medium"}, {"url": "https://github.com/khanhnamle1994", "anchor_text": "GitHub"}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/"}, {"url": "https://twitter.com/le_james94", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/in/khanhnamle94", "anchor_text": "find me on LinkedIn"}, {"url": "http://eepurl.com/deWjzb", "anchor_text": "Sign up for my newsletter"}, {"url": "https://medium.com/tag/recommendations?source=post_page-----bd7b9eae2ec7---------------recommendations-----------------", "anchor_text": "Recommendations"}, {"url": "https://medium.com/tag/autoencoder?source=post_page-----bd7b9eae2ec7---------------autoencoder-----------------", "anchor_text": "Autoencoder"}, {"url": "https://medium.com/tag/collaborative-filtering?source=post_page-----bd7b9eae2ec7---------------collaborative_filtering-----------------", "anchor_text": "Collaborative Filtering"}, {"url": "https://medium.com/tag/research?source=post_page-----bd7b9eae2ec7---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----bd7b9eae2ec7---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&user=James+Le&userId=52aa38cb8e25&source=-----bd7b9eae2ec7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&user=James+Le&userId=52aa38cb8e25&source=-----bd7b9eae2ec7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbd7b9eae2ec7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bd7b9eae2ec7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bd7b9eae2ec7--------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://le-james94.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Le"}, {"url": "https://le-james94.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17.6K Followers"}, {"url": "https://jameskle.com/", "anchor_text": "https://jameskle.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F52aa38cb8e25&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&user=James+Le&userId=52aa38cb8e25&source=post_page-52aa38cb8e25--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F171511b90ce0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frecommendation-system-series-part-6-the-6-variants-of-autoencoders-for-collaborative-filtering-bd7b9eae2ec7&newsletterV3=52aa38cb8e25&newsletterV3Id=171511b90ce0&user=James+Le&userId=52aa38cb8e25&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}