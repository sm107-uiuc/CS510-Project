{"url": "https://towardsdatascience.com/model-selection-with-large-neural-networks-and-small-data-955b4d929d55", "time": 1683012460.617236, "path": "towardsdatascience.com/model-selection-with-large-neural-networks-and-small-data-955b4d929d55/", "webpage": {"metadata": {"title": "Model Selection with Large Neural Networks and Small Data | by Andreas Holm Nielsen | Towards Data Science", "h1": "Model Selection with Large Neural Networks and Small Data", "description": "The title statement is certainly a bold claim, and I suspect many of you are shaking your heads right now. You would think this effect is more pronounced for small datasets where the number of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Paper.pdf", "anchor_text": "Deepmind", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1912.02292.pdf", "anchor_text": "Nakkiran et al. (2019)", "paragraph_index": 15}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "Guo et al. (2017)", "paragraph_index": 25}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "Guo et al. (2019)", "paragraph_index": 39}, {"url": "https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/examples/mnist.ipynb", "anchor_text": "GitHub repo", "paragraph_index": 39}, {"url": "https://arxiv.org/pdf/1710.05381.pdf", "anchor_text": "review paper", "paragraph_index": 43}], "all_paragraphs": ["The title statement is certainly a bold claim, and I suspect many of you are shaking your heads right now.", "Under the classical teachings of statistical learning, this contradicts the well-known bias-variance tradeoff. This theory defines a sweet-spot where, if you increase model complexity further, generalization error tends to increase (the typical U-shaped test error curve).", "You would think this effect is more pronounced for small datasets where the number of parameters, p, is larger than the number of observations, n, but this is not necessarily the case.", "In a recent ICML 2020 paper by Deepmind (Bornschein, 2020), it was shown that one can train on a smaller subset of the training data while maintaining generalizable results, even for large overparameterized models. If this is true, we can reduce the computational overhead in model selection and hyperparameter tuning significantly.", "Think for a moment regarding the implications of this. This could dramatically alter how we select optimal models or tune hyperparameters (for example in Kaggle competitions) since we can include significantly more models in our grid search (or the like).", "Is this too good to be true? And how can we prove it?", "Here are the main takeaways before we get started:", "Before we get started, I will offer you two options. If you are tired of hearing about the bias-variance trade-off for the 100th time, please read the TLDR at the end of Section 1 and then move on to Section 2. Otherwise, I will briefly introduce the bare minimum needed to understand the basics before moving on with the actual paper.", "The predictive error for all supervised learning algorithms can be broken into three (theoretical) parts, which are essential to understand the bias-variance tradeoff. These are; 1) Bias 2) Variance 3) Irreducible error (or noise term)", "The irreducible error (sometimes called noise) is a term disconnected from the chosen model which can never be reduced. It is an aspect of the data arising due to an imperfect framing of the problem, meaning we will never be able to capture the true relationship of the data \u2014 no matter how good our model is.", "The bias term is generally what people think of when they refer to model (predictive) errors. In short, it measures the difference between the \u201caverage\u201d model prediction and the ground truth. Average might seem strange in this case as we typically only train one model. Think of it this way. Due to small perturbations (randomness) in our data, we can get slightly different predictions even with the same model. By averaging the range of predictions we get due to these perturbations, we obtain the bias term. High bias is a sign of poor model fit (underfitting), as it will have a large prediction error on both the training and test set.", "Finally, the variance term refers to the variability of the model prediction for a given data point. It might sound similar, but the key difference lies in the \u201caverage\u201d versus \u201cdata point\u201d. High variance implies high generalization error. For example, while a model might be relatively accurate on the training set, it can achieve a considerably poor fit on the test set. This latter scenario (high variance, low bias) is typically the most likely when training overparameterized neural networks, i.e., what we refer to as overfitting.", "The practical implication of these terms implies balancing the bias and variance (hence the name trade-off), typically controlled via model complexity. The ultimate goal is to obtain low bias and low variance. This is the typical U-shape test error curve you might have seen before.", "Alright, I will assume you know enough about the bias-variance trade-off to understand why the original claim that overparameterized neural networks do not necessarily imply high variance is puzzling, indeed.", "TLDR; high variance, low bias is a sign of overfitting. Overfitting happens when a model achieves high accuracy on the training set but low accuracy on the test set. This typically happens for overparameterized neural networks.", "In practice, we typically optimize the bias-variance trade-off using a validation set with (for example) early stopping. Interestingly, this approach might be completely wrong. Over the past few years, researchers have found that if you keep fitting increasingly flexible models, you obtain what is termed double descent, i.e., generalization error will start to decrease again after reaching an intermediary peak. This finding is empirically validated in Nakkiran et al. (2019) for modern neural network architectures on established and challenging datasets. See the following figure from OpenAI, which shows this scenario;", "These findings imply that larger models are generally better due to the double descent phenomena, which challenges the long-held viewpoint regarding overfitting for overparameterized neural networks.", "Having established that large overparameterized neural networks can generalize well, we want to take it one step further. Enter the relative ranking hypothesis. Before we explain the hypothesis, we note that if proven true, then you can potentially perform model selection and hyperparameter tuning on a small subset of your training dataset for your next experiment, and by doing so save computational resources and valuable training time.", "We will briefly introduce the hypothesis followed by a few experiments to validate the claim. As an additional experiment not included in the literature (as far as we know), we will investigate one setting that could potentially invalidate the relative ranking hypothesis, which is imbalanced datasets.", "One of the key hypotheses of Bornschein (2020) is;", "\u201coverparameterized model architectures seem to maintain their relative ranking in terms of generalization performance, when trained on arbitrarily small subsets of the training set\u201d.", "They call this observation the relative ranking-hypothesis.", "In layman terms; let\u2019s say we have 10 models to choose from, numbered from 1 to 10. We train our models on a 10% subset of the training data and find that model 6 is the best, followed by 4, then 3, and so on..", "The ranking hypothesis postulates, that as we gradually increase the subset percentage from 10% all the way up to 100%, we should obtain the exact same ordering of optimal models.", "If this hypothesis is true, we can essentially perform model selection on a small subset of the original data to the added benefit of much faster convergence. If this was not controversial enough, the authors even take it one step further as they found some experiments where training on small datasets led to more robust model selection (less variance), which certainly seems counterintuitive given that we would expect relatively more noise for smaller datasets.", "One strange phenomenon when training neural network classifiers is, that cross-entropy error tends to increase while classification error decreases. This seems counterintuitive but is simply due to models becoming overconfident in their predictions ( Guo et al. (2017)). We can use something called temperature scaling, which calibrates the cross-entropy estimates on a small held-out dataset. This yields more generalizable and well-behaved results compared to classical cross-entropy, especially relevant for overparameterized neural networks. As a rough analogy, you can think of this as providing less \u201cfalse negatives\u201d regarding the number of overfitting cases.", "While Bornschein (2020) do not provide explicit details on the exact softmax temperature calibration procedure used in their paper, we use the following procedure for our experiments;", "Let us now turn to the experimental setting.", "We will conduct two experiments in this post. One for validating the relative ranking-hypothesis on the MNIST dataset, and one for evaluating how our conclusions change if we synthetically make MNIST imbalanced. This latter experiment is not included in the Bornschein (2020) paper, and could potentially invalidate the relative ranking-hypothesis for imbalanced datasets.", "We start by replicating the Bornschein (2020) study on MNIST, before moving on with the imbalanced dataset experiment. This is not meant to disprove any of the claims in the paper, but simply to ensure we have replicated their experimental setup as closely as possible (with some modifications).", "The authors also mention experimenting with replacing ReLU with tanh, batch-norm, layer-norm, etc., but it is unclear if these tests were included in their final results. Thus, we only consider the experiment using the above settings.", "As an initial experiment, we want to validate why temperature scaling is needed. For this, we train an MLP using ReLU and 3 hidden layers of 2048 units each, respectively. We do not include dropout and we train for 50 epochs.", "Our hypothesis is: The test cross-entropy should gradually increase while test accuracy decreases over time (motivation for temperature scaling in the first place, i.e., model overconfidence).", "Here are the results of this initial experiment:", "Clearly, the test entropy does decline initially and then gradually increases over time while test accuracy keeps improving. This is evidence in favor of hypothesis 1. Figure 3 in Guo et al. (2017) demonstrates the exact same effect on CIFAR-100.Note: We have smoothed the results a bit (5-window rolling mean) to make the effect more visible.", "To remedy this effect, we can incorporate temperature scaling which", "Having shown that temperature scaling is needed, we now turn to the primary experiment \u2014 i.e., how does test cross-entropy vary as a function of the size of our training dataset. Our results look as follows:", "Note, we do not obtain the exact same \u201csmooth\u201d results as Bornschein (2020). This is most likely due to the fact, that we have not replicated their experiment completely, as they for example include many more different seeds. Nevertheless, we can draw the following conclusions:", "We will now conduct an experiment for the case of imbalanced datasets, which is not included in the actual paper, as it could be a setting where the tested hypothesis is invalid.", "We sample an artificially imbalanced version of MNIST similar to Guo et al. (2019). The procedure is as follows. For each class in our dataset, we subsample between 0 and 100 percent of the original training and test dataset. We use the following GitHub repo for this sampling procedure.", "Then, we select our calibration dataset similar to the previous experiment, i.e., random 90/10% split between training and calibration.", "We include a visualization of the classes distribution for the original MNIST training dataset", "Given this large difference in the frequency distribution, you can clearly see how this version is much more imbalanced compared to the original MNIST.", "While a plethora of different methods for overcoming the problem of imbalanced datasets exist (see the following review paper), we want to investigate and isolate the effects of having an imbalanced dataset for the relative ranking hypothesis, i.e., does the relative ranking-hypothesis still hold in the imbalanced data setting?", "We run all our models again using this synthetically imbalanced MNIST dataset, and obtain the following results:", "This is quite an optimistic result, as we are now more confident, that the relative ranking-hypothesis is mostly true in the case of imbalanced datasets. We believe this could also be the reason behind the quote from the Bornschein (2020) paper regarding the sampling strategy;", "\u201cWe experimented with balanced subset sampling, i.e. ensuring that all subsets always contain an equal number of examples per class. But we did not observe any reliable improvements from doing so and therefore reverted to a simple i.i.d sampling strategy.\u201d", "The primary difference between the balanced and imbalanced version is the more \u201cjumpy\u201d results, which makes sense given that there might be classes available in the test set not seen during training for the chosen models.", "I hope that you might be able to apply these findings in your next machine learning experiments and remember, larger is (almost) always better.", "[1] J. Bornschein, F. Visin, and S. Osindero, Small Data, Big Decisions: Model Selection in the Small-Data Regime (2020), in International Conference on Machine Learning (ICML).", "[4] T. Guo, X. Zhu, Y. Wang, and F. Chen, Discriminative Sample Generation for Deep Imbalanced Learning (2019), in International Joint Conferences on Artificial Intelligence Organization (IJCAI) (pp. 2406\u20132412).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F955b4d929d55&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----955b4d929d55--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andreashn?source=post_page-----955b4d929d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreashn?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Andreas Holm Nielsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20c7980a1b4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&user=Andreas+Holm+Nielsen&userId=20c7980a1b4f&source=post_page-20c7980a1b4f----955b4d929d55---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@danielkcheung?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Daniel Cheung"}, {"url": "https://unsplash.com/s/photos/small?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://proceedings.icml.cc/static/paper_files/icml/2020/6899-Paper.pdf", "anchor_text": "Deepmind"}, {"url": "https://www.digitalvidya.com/blog/bias-variance-tradeoff/", "anchor_text": "https://www.digitalvidya.com/blog/bias-variance-tradeoff/"}, {"url": "https://arxiv.org/pdf/1912.02292.pdf", "anchor_text": "Nakkiran et al. (2019)"}, {"url": "https://openai.com/blog/deep-double-descent/", "anchor_text": "https://openai.com/blog/deep-double-descent/"}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "Guo et al. (2017)"}, {"url": "https://github.com/gpleiss/temperature_scaling", "anchor_text": "see this Github repo by Guo et al. (2017)"}, {"url": "https://arxiv.org/pdf/1706.04599.pdf", "anchor_text": "Guo et al. (2019)"}, {"url": "https://github.com/ufoym/imbalanced-dataset-sampler/blob/master/examples/mnist.ipynb", "anchor_text": "GitHub repo"}, {"url": "https://arxiv.org/pdf/1710.05381.pdf", "anchor_text": "review paper"}, {"url": "https://holmdk.github.io/2020/08/14/deep_learning_small_data.html", "anchor_text": "https://holmdk.github.io"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----955b4d929d55---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/small-data?source=post_page-----955b4d929d55---------------small_data-----------------", "anchor_text": "Small Data"}, {"url": "https://medium.com/tag/modelling?source=post_page-----955b4d929d55---------------modelling-----------------", "anchor_text": "Modelling"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----955b4d929d55---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----955b4d929d55---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&user=Andreas+Holm+Nielsen&userId=20c7980a1b4f&source=-----955b4d929d55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&user=Andreas+Holm+Nielsen&userId=20c7980a1b4f&source=-----955b4d929d55---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F955b4d929d55&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----955b4d929d55---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----955b4d929d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----955b4d929d55--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----955b4d929d55--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----955b4d929d55--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----955b4d929d55--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreashn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andreashn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andreas Holm Nielsen"}, {"url": "https://medium.com/@andreashn/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "58 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20c7980a1b4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&user=Andreas+Holm+Nielsen&userId=20c7980a1b4f&source=post_page-20c7980a1b4f--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff125c24ec397&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmodel-selection-with-large-neural-networks-and-small-data-955b4d929d55&newsletterV3=20c7980a1b4f&newsletterV3Id=f125c24ec397&user=Andreas+Holm+Nielsen&userId=20c7980a1b4f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}