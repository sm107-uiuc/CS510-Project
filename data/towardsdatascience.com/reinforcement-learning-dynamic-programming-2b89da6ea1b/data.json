{"url": "https://towardsdatascience.com/reinforcement-learning-dynamic-programming-2b89da6ea1b", "time": 1683003799.900341, "path": "towardsdatascience.com/reinforcement-learning-dynamic-programming-2b89da6ea1b/", "webpage": {"metadata": {"title": "Reinforcement Learning: Dynamic Programming | by Justin Tennenbaum | Towards Data Science", "h1": "Reinforcement Learning: Dynamic Programming", "description": "In two previous articles, I broke down the first things most people come across when they delve into reinforcement learning: the Multi Armed Bandit Problem and Markov Decision Processes. Most games\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@jmtennenbaum/starting-with-reinforcement-learning-the-multi-armed-bandit-problem-f124076775a4", "anchor_text": "Multi Armed Bandit Problem", "paragraph_index": 0}, {"url": "https://medium.com/@jmtennenbaum/markov-decision-processes-and-grid-world-45dee8d85fdb", "anchor_text": "Markov Decision Processes", "paragraph_index": 0}, {"url": "https://github.com/jmt0221/reinforcement_learning", "anchor_text": "Github", "paragraph_index": 1}, {"url": "https://medium.com/@jmtennenbaum/markov-decision-processes-and-grid-world-45dee8d85fdb", "anchor_text": "Markov Decision Processes", "paragraph_index": 5}], "all_paragraphs": ["In two previous articles, I broke down the first things most people come across when they delve into reinforcement learning: the Multi Armed Bandit Problem and Markov Decision Processes. Most games satisfy the requirements of a Markov Decision Process, and the game in particular that we will be using is called Grid World. For those who are not familiar, the environment is a two-dimensional grid with each tile/node either having a value (usually 0 but some variations have negative values), a dead space, or a reward which ends the game (called a terminal state). The agent is able to move in any direction and its goal is to maximize its reward. The algorithm we are going to use to estimate these rewards is called Dynamic Programming.", "Before we can dive into how the algorithm works we first need to build our game (Here is the link to my Github with all the other algorithms I have built). To do this we will only need Python. We can create Grid World using a Class structure so the first thing we will do is establish the attributes we will need. To instantiate a variable as the Grid Class, we need to pass it three arguments, both Height and Weight are integers which control the size of our grid; start is a tuple with 2 integers that control the starting position of our agent.", "From here we need to build out a few methods that will allow us to set/update the rewards, actions, and states for our grid so we can play around with values both at the start and middle of the game. We only need five more methods after this and our class will be complete. We use three to simply return information such as: the current state, whether a state is terminal, and if the game is over. After this, all that\u2019s left is to give our agent the ability to move and create a set of all possible states in our game.", "Below is the layout of my grid. There are two terminal tiles in the right corner and one dead space in the middle. A few things you can do to personalize your grid is change the size, grid layout, or add a negative reward for all states that are not terminal to incentivize the agent to move the fewest amount of steps. You shouldn\u2019t make the penalty that large but you can play with it to see how the agent changes its behavior.", "At the very end, we create one final function grid which we will import later when we get into Dynamic Programming. This function simply creates our Grid Class object and then establishes what locations have rewards and what actions are possible if we aren\u2019t using a policy but just randomly moving for exploration purposes.", "As I stated in my previous blog about Markov Decision Processes, in order to estimate the rewards of each state and action, we need to solve for the Bellman Equation.", "We can break the above equation into three parts:", "Since this is a recursive equation that depends on future values we need to use Dynamic Programming to converge to a solution for the value function. The other problem we face is to optimize our policy. One policy is \u201cbetter\u201d than another if V\u03c02 >V\u03c01 and our optimal policy is labeled as \u03c0*.", "Dynamic Programming actually consists of two different versions of how it can be implemented:", "I will briefly cover Policy Iteration and then show how to implement Value Iteration in code. Policy Iterations consists of an inner and outer loop, but before this, the first step is to randomly initialize the policy and set the Value function for each state to 0. We then perform the inner loop called Iterative Policy Evaluation where we go through and update the value function iteratively until the change is below some threshold where we can then break and move on. Once this happens we move into the outer loop where we move through each state and see if updating the policy will increase our reward. If our policy changes we go back to Iterative Policy Evaluation and repeat this until we no longer change policies.", "Value Iteration is an alternative technique to Policy Iteration which is much simpler and faster to run since it only requires one loop as opposed to two. Also, in the first half of Policy Iteration we have to wait for policy evaluation to converge, but in reality, we don\u2019t need to wait for this to happen and we can estimate the value function before the policy evaluation step is finished. Value Iteration combines the policy evaluation and iteration into one step by taking the max over all possible actions as where Policy Iteration uses argmax.", "All we need for Value Iteration is Python, our previous Grid-World file, and Numpy. In the following code our game is deterministic, so we don\u2019t how to worry about P(s\u2019,r|s,a). We first need to set a threshold to break out of our loop, a Discount Factor Gamma to penalize future rewards, and a tuple of all possible actions we can take.", "Next, we move onto our main block of code to run and to start we create our grid world object. Then randomly select actions for the policy and initialize the value function between 0 and 1 for all possible states. The only exception to this are terminal states which always have no policy and a value of 0.", "The next block of code is repeated until our value function converges.", "As you can see we go through all states we have a policy for and set out value for the action which maximized our reward. As with policy iteration, we repeat this until the value function converges, however, there is no second loop we have to satisfy, so we can move directly into optimizing our policy. We simply go through all states and set the policy to the action which just maximized the value function.", "While Dynamic Programming is much more efficient than brute force, it still requires a full model of the environment in order to be used. In a real world scenario or even a more complicated game this can\u2019t always be obtained. After this, I am going to move onto a model free method known as Monte Carlo and see how it takes a different approach to solving grid world and how the agent is much closer to truly \u201cplaying\u201d the game.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Flatiron passionate about Math and Technology"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2b89da6ea1b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jmtennenbaum?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmtennenbaum?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Justin Tennenbaum"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d54f7f47f17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&user=Justin+Tennenbaum&userId=6d54f7f47f17&source=post_page-6d54f7f47f17----2b89da6ea1b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@jmtennenbaum/starting-with-reinforcement-learning-the-multi-armed-bandit-problem-f124076775a4", "anchor_text": "Multi Armed Bandit Problem"}, {"url": "https://medium.com/@jmtennenbaum/markov-decision-processes-and-grid-world-45dee8d85fdb", "anchor_text": "Markov Decision Processes"}, {"url": "https://github.com/jmt0221/reinforcement_learning", "anchor_text": "Github"}, {"url": "https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff", "anchor_text": "TowardsDataScience"}, {"url": "https://medium.com/@jmtennenbaum/markov-decision-processes-and-grid-world-45dee8d85fdb", "anchor_text": "Markov Decision Processes"}, {"url": "https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/", "anchor_text": "https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----2b89da6ea1b---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----2b89da6ea1b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2b89da6ea1b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2b89da6ea1b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&user=Justin+Tennenbaum&userId=6d54f7f47f17&source=-----2b89da6ea1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&user=Justin+Tennenbaum&userId=6d54f7f47f17&source=-----2b89da6ea1b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2b89da6ea1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2b89da6ea1b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2b89da6ea1b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmtennenbaum?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jmtennenbaum?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Justin Tennenbaum"}, {"url": "https://medium.com/@jmtennenbaum/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "142 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6d54f7f47f17&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&user=Justin+Tennenbaum&userId=6d54f7f47f17&source=post_page-6d54f7f47f17--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F14d81184f4ed&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-dynamic-programming-2b89da6ea1b&newsletterV3=6d54f7f47f17&newsletterV3Id=14d81184f4ed&user=Justin+Tennenbaum&userId=6d54f7f47f17&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}