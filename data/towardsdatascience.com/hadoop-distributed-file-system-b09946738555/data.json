{"url": "https://towardsdatascience.com/hadoop-distributed-file-system-b09946738555", "time": 1683006790.913255, "path": "towardsdatascience.com/hadoop-distributed-file-system-b09946738555/", "webpage": {"metadata": {"title": "Hadoop Distributed File System | Towards Data Science", "h1": "Hadoop Distributed File System", "description": "A comprehensive guide to understanding Apache Hadoop Ecosystem's Hadoop Distributed File System and it's inner workings"}, "outgoing_paragraph_urls": [{"url": "https://hadoop.apache.org/docs/r1.2.1/hdfs_imageviewer.html", "anchor_text": "Offline Image Viewer", "paragraph_index": 21}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/", "paragraph_index": 31}], "all_paragraphs": ["From a computing perspective, there are essentially 2 types of scaling \u2014 vertical and horizontal. In vertical scaling, we simply add more RAM and storage to a single computer/machine aka \u201cnode\u201d. In horizontal scaling, we add more nodes connected through a common network, thereby increasing the overall capacity of the system. With that in mind, let\u2019s dive in.", "When a file is saved in HDFS, the file is broken into smaller chunks or \u201cblocks\u201d, as can be seen in the GIF above. The number of blocks is dependent on the \u201cBlock Size\u201d. The default is 128 MB but can be changed/configured easily.", "In our example, a 500 MB file needs to be broken into blocks of 128 MB. 500/128 = 3 blocks of 128 MB and 1 block of 116 MB. The residual block space of 12 MB is returned back to the name node for usage elsewhere, thus preventing any wastage. This is true of any file system really, for example, Windows NTFS has a block size between 4 KB and 64 KB depending on file size (up to 256 TB). Considering petabytes and above for Big Data processing, KBs would be highly inefficient, as you can imagine. This is why HDFS has a block size of 128 MB.", "HDFS is a fault-tolerant and resilient system, meaning it prevents a failure in a node from affecting the overall system\u2019s health and allows for recovery from failure too. In order to achieve this, data stored in HDFS is automatically replicated across different nodes.", "How many copies are made? This depends on the \u201creplication factor\u201d. By default, it is set to 3 i.e. 1 original and 2 copies. This is also easily configurable.", "In the GIF to the left, we see a file broken into blocks and each block replicated across other data nodes for redundancy.", "Hadoop Distributed File System (HDFS) follows a Master \u2014 Slave architecture, wherein, the \u2018Name Node\u2019 is the master and the \u2018Data Nodes\u2019 are the slaves/workers. This simply means that the name node monitors the health and activities of the data node. The data node is where the file is actually stored in blocks.", "Let's continue with the same example of a file of size = 500 MB in the image above. With HDFS\u2019 default block size of 128 MB, this file is broken into 4 blocks B1 \u2014 B4. Please note that A \u2014 E are our Data Nodes. With HDFS\u2019 default replication factor of 3, the blocks are replicated across our 5 node cluster. Block B1 (in yellow) is replicated across Nodes A, B and D and so on and so forth (follow the coloured lines).Here, the Name Node maintains the metadata, i.e. data about data. Which replica of which block of which file is stored in which node is maintained in NN \u2014 replica 2 of block B1 of file xyz.csv is stored in node B.", "So a file of size 500 MB requires a total storage capacity in HDFS of 1500 MB due to its replication. This is abstracted from the end users\u2019 perspective and the user can only see 1 file of size 500 MB stored within HDFS.", "Now\u2019s a good time as any to get some hands on:", "The algorithm starts by searching for the topology.map file under HDFS\u2019 default configuration folder. This .map file contains metadata information on all the available racks and nodes it contains. In our example case in the image above, we have 2 racks and 10 data nodes.", "Once the file is divided into blocks, the first copy of the first block is inserted into the rack and data node which is nearest to the client i.e. end-user. The copy of this first block is created and moved onto the next available rack i.e. Rack 2 through TCP/IP and stored in any available data node. Another copy is created here and moved onto the next available rack through TCP/IP and so on. But, since we have only 2 racks, the algorithm looks for the next available data node on the same rack (i.e. Rack 2) and stores the third copy there. This redundancy is put in place such that even if one rack fails, we still have the second rack to retrieve the data, thus enabling fault-tolerance and resilience.", "In Hadoop 1.x, the ecosystem was shipped with 1 Name Node only, resulting in a single point of failure. There was a Secondary or Backup Name Node which took over an hour of manual intervention to bring up. Subsequently, any data lost was irrecoverable.", "In Hadoop 2.x, High Availability as an alternative to standard mode, was provided. In the standard mode, you still have a primary and secondary name node. In the high availability mode, you have an active and passive name node.", "The data nodes send an activity update to the \u201cActive\u201d Name Node (every 5 seconds at minimum \u2014 configurable). This metadata is synced to the \u201cStand by\u201d Name Node in real-time. Thus, when the \u201cActive\u201d fails, the \u201cStand by\u201d has all the necessary metadata to switch over.", "The Zookeeper, through its Fail-over Controller, monitors the health of the Active and Stand by Name Nodes through a heart beat or instant notification it receives from each NN (every 5 seconds, again configurable). It also has the information of all the stand by name nodes available (Hadoop 3.x allows for multiple stand by name nodes).", "Thus, a connectivity between the data nodes, name nodes and zookeeper is established. The moment an active name node fails, the Zookeeper elects an appropriate stand by name node and facilitates the automatic switch over. The Stand by becomes the new Active Name Node and broadcasts this election to all the data nodes. The data nodes now send their activity updates to the newly elected Active Name Node within a few minutes.", "The name node (NN) metadata consists of two persistent files, namely, FsImage \u2014 namespace and Edit logs \u2014 transaction logs (insert, append)", "In every file system, there is a path to the required files \u2014 On Windows: C:\\Users\\username\\learning\\BigData\\namenode.txt and on Unix: /usr/username/learning/BigData/namenode.txt.HDFS follows the Unix way of namespace. This namespace is stored as part of the FsImage. Every detail of the file i.e. who, what, when, etc. is also stored in the FsImage snapshot. The FsImage is stored on the disk for consistency, durability and security.", "Any real-time changes to all files are logged in what is known as \u201cEdit logs\u201d. These are recorded in-memory (RAM) and contain every little detail of the change and the respective file/block.", "On HDFS startup, the metadata is read from the FsImage and the changes are written to Edit Logs. Once the data is recorded for the day in Edit Logs, it is flushed down onto the FsImage. This is how the two work in tandem.", "As an aside, the FsImage and Edit Logs are not human readable. They are binary-compressed (serialized) and stored in the file system. However, for debugging purposes it can be converted into an xml format to be read using Offline Image Viewer.", "As you can imagine or see in the image \u2018HDFS High Availability Architecture\u2019, the name node metadata is a single point of failure, hence this metadata is replicated to introduce redundancy and enable high-availability (HA).", "We now know that there exists an Active Name Node and a Standby Name Node. Any change in the active is synced in real-time to the shared folder/storage i.e. network file system (NFS). This NFS is accessible to the standby, which downloads all of the relevant incremental information in real-time to maintain the sync between the Namenodes. Thus, in the event of a failure of the active, the standby name node already has all the relevant information to continue \u201cbusiness as usual\u201d post fail-over. This is not used in the Production environment.", "\u201cQuorum\u201d means minimum required to facilitate an event. The term is generally used in politics; it is the minimum number of representatives required to conduct proceedings in the house.", "Here, we use this concept to determine the minimum number of journal nodes aka quorum that is needed to establish a majority and maintain metadata sync.", "The image shows three (always odd) journal nodes (process threads not physical nodes) that help to establish metadata sync. When an Active NN receives a change, it pushes it to majority of the QJ Nodes (follow a single colour). The Standby NN, in real-time, requests the majority number of QJ Nodes for the required metadata to establish the sync.", "The minimum number for QJN to function is 3 and the quorum/majority is determined by the following formula:", "The QJN is the preferred Production method of metadata sync as it is also \u201chighly available\u201d. In the event of a failure of any of the QJ nodes, any of the remaining nodes are available to provide the required data to maintain metadata sync. Thus, the standby already has all the relevant information to continue \u201cbusiness as usual\u201d post fail-over.", "This brings us to the end of my comprehensive guide on HDFS and it\u2019s inner workings.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Tech Enthusiast \u2014 Data Engineering | Data Analytics | LinkedIN: https://www.linkedin.com/in/prathameshnimkar/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb09946738555&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b09946738555--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b09946738555--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----b09946738555--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=post_page-----b09946738555--------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e----b09946738555---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@prathamesh.nimkar/hdfs-commands-79dccfd721d7", "anchor_text": "HDFS CommandsCommon HDFS Commandsmedium.com"}, {"url": "https://hadoop.apache.org/docs/r1.2.1/hdfs_imageviewer.html", "anchor_text": "Offline Image Viewer"}, {"url": "https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html", "anchor_text": "HDFS Architecture"}, {"url": "https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/admin_hdfs_config.html", "anchor_text": "Managing HDFS"}, {"url": "https://medium.com/@prathamesh.nimkar/cloudera-manager-on-google-cloud-3da9b4d64d74", "anchor_text": "Cloudera Manager on Google CloudStep-by-step installation of the Hadoop Ecosystem through CM 6.3.1 on GCPmedium.com"}, {"url": "https://towardsdatascience.com/simplifying-hdfs-erasure-coding-9d9588975113", "anchor_text": "HDFS Erasure CodingReduce storage overhead significantly in your HDFS cluster by leveraging Erasure Codingtowardsdatascience.com"}, {"url": "https://medium.com/@prathamesh.nimkar/big-data-analytics-using-the-hadoop-ecosystem-411d629084d3", "anchor_text": "Big Data Analytics Pipeline using the Hadoop EcosystemLanding Pagemedium.com"}, {"url": "https://medium.com/tag/big-data?source=post_page-----b09946738555---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----b09946738555---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b09946738555---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/hadoop?source=post_page-----b09946738555---------------hadoop-----------------", "anchor_text": "Hadoop"}, {"url": "https://medium.com/tag/hdfs?source=post_page-----b09946738555---------------hdfs-----------------", "anchor_text": "Hdfs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----b09946738555---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=-----b09946738555---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b09946738555--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb09946738555&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b09946738555---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b09946738555--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b09946738555--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b09946738555--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b09946738555--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b09946738555--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b09946738555--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b09946738555--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b09946738555--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prathamesh.nimkar?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prathamesh Nimkar"}, {"url": "https://medium.com/@prathamesh.nimkar/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "186 Followers"}, {"url": "https://www.linkedin.com/in/prathameshnimkar/", "anchor_text": "https://www.linkedin.com/in/prathameshnimkar/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6f0e324a756e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=post_page-6f0e324a756e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4018850c89c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhadoop-distributed-file-system-b09946738555&newsletterV3=6f0e324a756e&newsletterV3Id=4018850c89c8&user=Prathamesh+Nimkar&userId=6f0e324a756e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}