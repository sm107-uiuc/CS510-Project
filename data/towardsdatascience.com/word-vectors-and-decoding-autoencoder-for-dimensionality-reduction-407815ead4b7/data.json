{"url": "https://towardsdatascience.com/word-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7", "time": 1683005592.393764, "path": "towardsdatascience.com/word-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7/", "webpage": {"metadata": {"title": "Word Vectors and Dimensionality Reduction | by Kaustubh | Towards Data Science", "h1": "Word Vectors and Dimensionality Reduction", "description": "So this is my first medium article, and I\u2019m writing this to take a break from my adventures in researching in the field of Natural Language Processing mostly Aspect Based Sentiment Analysis (ABSA)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://science.howstuffworks.com/science-vs-myth/everyday-myths/see-the-fourth-dimension.htm", "anchor_text": "click here", "paragraph_index": 14}, {"url": "https://ieeexplore.ieee.org/document/9036908/authors#authors", "anchor_text": "paper", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e", "anchor_text": "here", "paragraph_index": 23}, {"url": "https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/", "anchor_text": "info", "paragraph_index": 23}], "all_paragraphs": ["So this is my first medium article, and I\u2019m writing this to take a break from my adventures in researching in the field of Natural Language Processing mostly Aspect Based Sentiment Analysis (ABSA) and Machine Translation so today you\u2019ll read about one of the most fundamental concepts in Natural Language Understanding which is Word Vector and Dimensionality Reduction (DR), we\u2019ll focus mostly on DR and then discuss Autoencoders as these are quite popular and the current systems are somewhat based on the same ideology. First, we\u2019ll dive into some history then progressively move to the matter itself.", "So if you have read a bit about words and grammar you\u2019d probably have come across this famous quote \u201cYou shall know a word by the company it keeps\u201d by Firth, 1957 and this is what most of our knowledge of Natural language is based on, quite logical as well. A word when used should have a relationship with the ones surrounding it. At a higher level, we can see that while doing semantic and syntax analysis, where certain parts of speech follow certain parts of speech like in the sentence \u201cShe promised to pay the bill\u201d where the following represents the POS tags, and it is often you see this structure where the preposition follows the verb and the verb follows determiner. But this is totally based on the sentence and our language is more ambiguous than the future.", "So a better strategy to represent a word in a sentence was a form of a vector, so one of the vectors which can easily be intuited is \u201cone-hot encoding\u201d where we put a 1 at the occurrence of the word, so it is a long n X 1 vector where n is the size of the sentence. So this vector would be extremely sparse and huge and it will be impractical to move forward with this. So we usually use two types of vector, term-document matrix, and a term context matrix which we shall discuss in the next section.", "So let\u2019s start with the term-document matrix, so this is quite similar to our one-hot encoding but it's short in the sense we only look at the word important to us. So the following matrix shows the count vector of the word in the columns and the document in the row.", "So now if we infer this table there is a good idea we\u2019ll have of the similarity between the words. The idea is that if the words are similar if their counts are close enough.", "Now I don\u2019t have anything against the clowns but these are just the facts. But this model is not really practical as scooping words and creating such a matrix from a huge dataset would take a lot of time and wouldn't be as practical. The other matrix that I mentioned was the term-context matrix. Now instead of taking a full document, we take some paragraphs that define some context for the words. Here is our \u201ccorpus\u201d.", "1. An equal amount of sugar, a sliced lemon, a tablespoonful of apricot preserve or jam, a pinch each of clove and nutmeg.", "2.Onboard for their enjoyment. Cautiously she sampled her first pineapple and another fruit whose taste she likened to that of.", "3.Of a recursive type well suited to programming on the digital computer. In finding the optimal R-stage policy from that of.", "4. Substantially affect commerce, for the purpose of gathering data and information necessary for the study authorized in the first section of this.", "The words in bold are our so targets and we have to find similar words. So let's make a matrix with the contexts.", "Here we can immediately see that apricot and pineapple similar makes sense right, but how will this make sense to a machine and how are we gonna scale it for a bigger set. So now we have to use PMI (Pointwise Mutation Information) to get the similarity between the words :", "Now what this function is doing is that it\u2019s getting the probability of the word existing with the context divided with the probability of the words occurring independently. But why? Well because this will give out the true essence of the similarity that is if the number of individual occurrences is lower and the together are comparatively higher than the then the term inside of the log bigger and the PMI will be higher and the similarity will be higher. The mysterious log function wraps that inverse probability. There is another variation of the PMI that is the PPMI which replaces the negative values of the PMI with zero. Another variation we can make is the add- K Laplace smoothing which won\u2019t lead to negative or insignificant values. We can go one step further and calculate the distance between these words which is calculated by the cosine distance, which is given by", "Now this will tell us the final similarity.", "So we humans are complex but we don't imagine more than 3 dimensions if you want to read more on this click here I just think it's so cool but Avatar gives a pretty good idea as well. So let\u2019s talk about these dimensions when we talk about structure it means an x,y, z-axis but here we don\u2019t have any physical structure, here we have data and a high dimension in data means that there is some information that is not directly visible. So that's why we reduce it to get direct dependencies and smaller matrices. Although this is mostly and widely studied in the field of Applied Linear Algebra it is quite important in the field of NLP and Machine Learning in general.", "A little disclaimer, with this article I hope to give you some intuition behind Autoencoders but if you want to learn more about the mathematics of dimensionality reduction refer to this paper by one of my professors.", "In this section, I\u2019ll try to provide a little intuition behind the LSA which is a kind of a stepping stone into autoencoders. So LSA is based on a truncated SVD (Single Value Decomposition) which if you know are widely used in Recommender Systems. If you don't here is the idea behind them so you take a matrix and you covert it into 3 different matrices each giving some information on the initial matrix so, here you see I'm literally breaking my matrix to understand it more intuitively.", "A fact about these matrices is that they are hierarchically arranged meaning that they are in order with the most important at the beginning. So why do we need this? To answer this we\u2019ll use our previous example given for the term-context method. So if we calculate the SVD for that we will get these matrices:", "Now if we truncate and just look at the first 2 columns and all the rows for the first matrix the first 2 x 2 submatrix of b and we multiply it we get:", "Now what exactly those values tell us is the similarity so if you take a close look the first two rows are the same that means that my first two words are similar and the last two are as well as they yield a positive value. Now there are two things that raise a question here how did I choose 2 as for truncating and how do we get distant values as -1.4142 and 1.7486 are close but the two words don\u2019t seem to be similar. The answer to the first question is that you can have any value instead of two it only depends on the size of your matrix, you can also try plotting the performance to get the best value.", "Now to answer the second question we should actually be using the PMI matrix we talked about earlier to get a better result.", "Let\u2019s look at our first deep learning dimensionality reduction method. A really cool thing about this autoencoder is that it works on the principle of unsupervised learning, we\u2019ll get to that in some time. So autoencoder has 2 layers and encoder(duh) and a decoder.", "So with this model, we can see we input our count vector and do a series of operations and get a value that has the same dimension as our vector. So what actually happens is that these layers you see are basically operations that are minimizing my original count vector that is they are encoding and the decoding layers try to recreate that original input using the bottleneck. The only way to recreate is by using back prop as we move from the bottleneck to the decoding layer. A boost to this method would be in performing LDA to essentially compress that input vector.", "Although here we discussed more on the Natural Language side of things Dimensionality reduction is also an issue in Computer Vision(here) and Autoencoders still come into play there (more info). I hope this article has provided a good sense of how dimensionality works and why it\u2019s required to reduce dimensions but this is just a mere step in the direction. There is more to the mathematics of Dimensionality Reduction which is not really needed for working with Autoencoders. To know more about Autoencoders check the acknowledgments section.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F407815ead4b7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://kaustubh-q.medium.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": ""}, {"url": "https://kaustubh-q.medium.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Kaustubh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc0f461fe5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&user=Kaustubh&userId=c0f461fe5681&source=post_page-c0f461fe5681----407815ead4b7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Markus Spiske"}, {"url": "https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://science.howstuffworks.com/science-vs-myth/everyday-myths/see-the-fourth-dimension.htm", "anchor_text": "click here"}, {"url": "https://ieeexplore.ieee.org/document/9036908/authors#authors", "anchor_text": "paper"}, {"url": "https://images.app.goo.gl/n6f4HXUtKDiHmkqn6", "anchor_text": "Source"}, {"url": "https://miro.medium.com/max/3148/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png", "anchor_text": "source"}, {"url": "https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e", "anchor_text": "here"}, {"url": "https://www.analyticsvidhya.com/blog/2018/06/unsupervised-deep-learning-computer-vision/", "anchor_text": "info"}, {"url": "https://ieeexplore.ieee.org/document/9036908", "anchor_text": "Analysis of Dimensionality Reduction Techniques on Big Data - IEEE Journals & MagazineDue to digitization, a huge volume of data is being generated across several sectors such as healthcare, production\u2026ieeexplore.ieee.org"}, {"url": "http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/", "anchor_text": "Unsupervised Feature Learning and Deep Learning TutorialSo far, we have described the application of neural networks to supervised learning, in which we have labeled training\u2026ufldl.stanford.edu"}, {"url": "http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf", "anchor_text": "http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----407815ead4b7---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/speech-recognition?source=post_page-----407815ead4b7---------------speech_recognition-----------------", "anchor_text": "Speech Recognition"}, {"url": "https://medium.com/tag/signal-processing?source=post_page-----407815ead4b7---------------signal_processing-----------------", "anchor_text": "Signal Processing"}, {"url": "https://medium.com/tag/audio?source=post_page-----407815ead4b7---------------audio-----------------", "anchor_text": "Audio"}, {"url": "https://medium.com/tag/conversational-ui?source=post_page-----407815ead4b7---------------conversational_ui-----------------", "anchor_text": "Conversational UI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&user=Kaustubh&userId=c0f461fe5681&source=-----407815ead4b7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&user=Kaustubh&userId=c0f461fe5681&source=-----407815ead4b7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F407815ead4b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----407815ead4b7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----407815ead4b7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----407815ead4b7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----407815ead4b7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----407815ead4b7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----407815ead4b7--------------------------------", "anchor_text": ""}, {"url": "https://kaustubh-q.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://kaustubh-q.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kaustubh"}, {"url": "https://kaustubh-q.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "50 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc0f461fe5681&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&user=Kaustubh&userId=c0f461fe5681&source=post_page-c0f461fe5681--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb4fc03fbef0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-vectors-and-decoding-autoencoder-for-dimensionality-reduction-407815ead4b7&newsletterV3=c0f461fe5681&newsletterV3Id=b4fc03fbef0&user=Kaustubh&userId=c0f461fe5681&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}