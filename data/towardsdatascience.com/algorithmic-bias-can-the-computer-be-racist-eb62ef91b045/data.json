{"url": "https://towardsdatascience.com/algorithmic-bias-can-the-computer-be-racist-eb62ef91b045", "time": 1682999816.3687801, "path": "towardsdatascience.com/algorithmic-bias-can-the-computer-be-racist-eb62ef91b045/", "webpage": {"metadata": {"title": "Algorithmic bias: can the computer be racist? | by Max Miller | Towards Data Science", "h1": "Algorithmic bias: can the computer be racist?", "description": "In recent years algorithmic bias has been under increasing scrutiny, particularly following the 2016 publication of Cathy O\u2019Neil\u2019s superb Weapons of Math Destruction (required reading for anyone\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.jobscan.co/blog/fortune-500-use-applicant-tracking-systems/", "anchor_text": "vast majority of large companies", "paragraph_index": 1}, {"url": "https://www.globenewswire.com/news-release/2019/07/22/1885923/0/en/Applicant-Tracking-System-ATS-Market-To-Reach-USD-2-34-Billion-By-2026-Reports-And-Data.html", "anchor_text": "projected to grow", "paragraph_index": 1}, {"url": "https://www.npr.org/sections/health-shots/2019/04/16/713902890/how-well-do-workplace-wellness-programs-work", "anchor_text": "probably don\u2019t work", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/CompStat", "anchor_text": "CompStat", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/PredPol", "anchor_text": "PredPol", "paragraph_index": 1}, {"url": "https://www.latimes.com/local/lanow/la-me-lapd-precision-policing-data-20190703-story.html", "anchor_text": "don\u2019t actually work very well", "paragraph_index": 1}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "demonstrably biased against African Americans", "paragraph_index": 2}, {"url": "https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html", "anchor_text": "the algorithm might be better", "paragraph_index": 2}, {"url": "https://twitter.com/RealSaavedra/status/1087627739861897216", "anchor_text": "driven by math", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Travelling_salesman_problem", "anchor_text": "traveling salesman problem", "paragraph_index": 5}, {"url": "https://www.wired.com/insights/2014/03/potholes-big-data-crowdsourcing-way-better-government/", "anchor_text": "a smartphone app", "paragraph_index": 11}, {"url": "https://hbr.org/2013/04/the-hidden-biases-in-big-data", "anchor_text": "under-resource roadwork", "paragraph_index": 11}, {"url": "https://www.npr.org/templates/story/story.php?storyId=106268439", "anchor_text": "famously crude proxy for fitness", "paragraph_index": 15}], "all_paragraphs": ["In recent years algorithmic bias has been under increasing scrutiny, particularly following the 2016 publication of Cathy O\u2019Neil\u2019s superb Weapons of Math Destruction (required reading for anyone considering a career in data science). As more and more organizations seek to utilize the ever increasing pool of data being generated (a commonly cited number is 2.5 quintillion bytes of data every day, though I can\u2019t find good sourcing for that figure) with complicated and frequently opaque machine learning models, the disruptive potential of algorithmic bias has done nothing but grow.", "Frustratingly, despite increasing awareness in some circles, it doesn\u2019t seem like we\u2019ve found a way to handle the sort of issues O\u2019Neil identified in her book. Indeed, there has been pretty much nothing but growth in the industries and use cases she highlighted in the years since Weapons of Math Destruction came out. Back in 2016, O\u2019Neil discussed the problems with applicant tracking systems, programs which read in resumes and other applicant information and automatically sort or disqualify job candidates, but ATS systems remain widespread (used by the vast majority of large companies) and the ATS market is projected to grow. She highlighted some of the problems with the rise of so-called Wellness Programs which employers use to try and incentivize healthy behavior to lower healthcare costs, but wellness programs are still popular despite mounting evidence that they probably don\u2019t work. O\u2019Neil demonstrated the ways that \u2018hot spot\u2019 policing programs like CompStat or PredPol, which ostensibly seek to improve policing by focusing resources on areas where crime is most likely to occur, are dangerously prone to bias against poor and minority communities and liable to create vicious, self-fulfilling cycles of prediction and enforcement, but these programs are still in use. There is a somewhat ironic silver lining that many police departments appear to be rethinking these programs, not because the programs are highly discriminatory, but because they\u2019re deciding that they don\u2019t actually work very well.", "Why are we only very slowly coming to grips with these issues? For one thing, in some cases the non-algorithmic, human alternatives are themselves discriminatory and it isn\u2019t clear that the algorithm hasn\u2019t presented an improvement on the status quo. Yes, the algorithmic models many courts are now using to set bail and determine sentencing are flawed and demonstrably biased against African Americans, but the human alternative is also biased, and frequently so capricious and uneven that the algorithm might be better.", "Another reason is a seemingly widespread conception that algorithms and machine learning models are somehow inherently neutral or \u2018fair\u2019. Statistical models have an air of science around them \u2014 aren\u2019t they derived from the data, and nothing else? When issues of algorithmic bias are brought up in the news, it is still easy to find responses along the lines that the algorithms can\u2019t really biased because they\u2019re driven by math. To think that an algorithm is simply a bit of applied mathematics and therefore can\u2019t be biased is to fundamentally misunderstand what an algorithm is, and it is important for anyone working with data to understand why.", "While algorithms are definitely mathematical tools, they aren\u2019t necessarily infallibly proved and unambiguous in the way that a theorem might be. An algorithm is more like a strategy, a set of guidelines about how to find some answer that you think will generally work regardless of what the input is. For instance, say I shuffle a deck of cards and then ask you to put it back into new deck order, with all the suits together and arranged ace to king. How would you go about doing that? In a way there isn\u2019t \u201can answer\u201d of what specific moves in the deck you need to do that you can state ahead of time. You can\u2019t say something like \u201cI\u2019ll take the card in the 15th position and move it to the first\u201d because the deck has been shuffled and you don\u2019t know where each card is.", "Instead, what you can do is approach the problem algorithmically, and think of a series of operations that will work for any configuration of cards. Step 1, maybe, go through each card and place them into four separate piles according to suit. Step 2, pick one pile and find the ace, and then the 2 and so on. Step 3, repeat for each pile, etc. That series of steps is the algorithm, a set of instructions that get you to a solution in a general way. Crucially, algorithms, by their very nature, aren\u2019t necessarily infallible. My card sorting algorithm should always work, but I don\u2019t know that it is the most efficient possible algorithm; maybe there\u2019s a better way to go about the task that will generally be quicker or require fewer moves. Even worse, some problems, such as the famous traveling salesman problem, are notoriously difficult and resistant to algorithmic simplification.", "Shuffling cards may seem a little removed from the sorts of real world consequences I brought up at the beginning of this post, but the principle of the algorithms at play is the same. Consider, for instance, a bank deciding which applicants will receive mortgages. The bank wants to lend money only to people who will pay it back and to only lend as much as any given person is able to pay back. Just like how you didn\u2019t know the exact order of cards in a shuffled deck, each loan application is going to be different and unknown ahead of time, so there isn\u2019t one simple answer to the question of \u2018should this applicant be given a loan and if so how much?\u2019", "The bank will end up making its lending decision with an algorithm of its own. Maybe, step one would be to weed out all applicants with sufficiently low credit scores, step two might be to look at the applicant\u2019s income, and so on. Unlike with the deck of cards example, the bank\u2019s algorithm is not guaranteed to get a \u2018correct\u2019 answer. The bank will likely end up lending to some candidates who will default and denying others who would not have. At this point it should be clear that being part of the algorithm is no guarantee of either effectiveness or fairness and that algorithms can absolutely be biased; \u2018don\u2019t lend to African Americans\u2019 would be an algorithm.", "A bank refusing to lend to African Americans would be transparently racist, but I want to stress that algorithms can biased in much more subtle, but equally insidious, ways. Even models created without conscious human bias can produce biased results for a variety of reasons that are important to understand. Training on \u2018data\u2019 is no guarantee of neutrality, because the data itself may have been gathered in a way that introduces bias into the results.", "Risk 1: The data gathering is inherently biased", "A model is only as good as the data it\u2019s trained on and in the real world, data sources are often unreliable. Historically, gathering data used to be expensive and time consuming, so complete data was hard to come by. Someone looking to research a particularly topic would try and get the most bang for their buck when it came to gathering data, and often that meant going where the data was easiest to collect. This is the reason so much psychological research is done with college students even though they are not necessarily demographically representative of the larger population. For a researcher on a college campus, it\u2019s easier to find young, white, upper-middle-class students to take a test than it is to send people around the country to find a range of subjects.", "Nowadays, researchers can cheaply tap into an incredible amount of data generated online or by connected devices, but the coverage is still not universal or unbiased. Data generated by connected devices, for instance, by definition will favor people who are currently connected. In one fascinating case study, the city of Boston released a smartphone app that drivers could use to passively identify potholes while they drove. The app uses the phone\u2019s accelerometer to identify bumps in the journey which are mapped using the phone\u2019s GPS, helping the city decide where to do roadwork. This is great, but the results are not quite perfect. For one thing, at the time of release smartphone penetration could still be low in lower income communities, and even lower among older people. The app therefore would therefore tend to under-resource roadwork in poorer areas or places with more older drivers.", "When models are operationalized and used to guide policy, this issue can be self-reinforcing: think of crime hotspot predictors like CompStat which train on crime data but also influence where police are sent. Many minor, \u2018nuisance crimes\u2019 would likely go unreported if there wasn\u2019t a police officer to report them. Police are sent to certain neighborhoods by the model and report instance of loitering or jaywalking in those areas, but they don\u2019t report the same minor crimes in the neighborhoods they\u2019re not sent to. As a result, more crimes are reported in the places the police are already being sent, making them look more crime ridden to the model which chooses to send more police there.", "Risk 2: Data being used is an imperfect or biased proxy", "A skeptical reader may have rightly pointed out that my earlier example of a bank simply denying loans to black people is a little facetious; it would be illegal for a bank to use race in its lending decisions like that, after all. That\u2019s true, but then the bank could come to the same racist results without explicitly using \u2018race\u2019 as a consideration if they are using some other factor that correlates strongly with race. Consider residential segregation \u2014 different neighborhoods may have different demographics such that if you know where a person lives, you may not need to explicitly ask after their race. In many cities, simply asking a resident their zip code may be enough to tell their race. A crucial aspect is that, particularly as more of the algorithmic decisions are automatically computer generated, you can get biased results in this manner without someone making an explicitly racist decision: someone who hasn\u2019t thought about residential segregation thinks that neighborhood seems like a reasonable thing to include on a loan application, and feeds that information into the computer without a second thought. Later the computer seems to be denying loans to black people, but the results aren\u2019t questioned because the computer can\u2019t be racist, right?", "This is also an issue when a particular factor isn\u2019t or can\u2019t be directly measured and data scientists or researchers are forced to try and find a proxy for it. Human health, for instance, is hard to quantify, being related to so many different factors and manifesting in so many different ways. A complete picture of person\u2019s health or physical fitness would be hard to get. A person\u2019s body mass index, on the other hand, is relatively simple to measure since it requires knowing only the person\u2019s height and weight. BMI is, however, a famously crude proxy for fitness. Data scientists are frequently tempted to use poor and possibly biased proxies because they are all that is available, but these decisions should be scrutinized.", "Models or practical implementations have rules or limitations imposed on them by the creators and therefore, sort of by definition, carry the goals/ideology of the model maker. If the creator decides that zipcode should be an important factor in the lending algorithm, then that proxy for race has gotten through the door. No model is created in a vacuum; someone is always deciding what factors to include, what data to feed into the machine and how to operationalize the algorithm once it starts making predictions. It is a mistake to think of machine learning algorithms as being free from human influence since they are computer derived.", "Risk 4: The success condition is biased", "Any given algorithm or model has a goal, some value it\u2019s trying to predict or decision to make. In every case the computer needs to be trained on what a successful prediction is, or on what the \u2018correct\u2019 decision is. Supervised learning models require well labeled data sets, including a target variable that the model will seek predict on incoming data. How the target variable was previously determined can have major implications for a model trained on that data. Consider a bank using its past lending history \u2014 marred by red-lining \u2014 to train a model that decides who is approved for a loan. Or a resume reader which passes applicants based on prior hiring decisions, which may discount women applicants. If prior lending or hiring decisions were racist or sexist, training a model on those decisions will give you a biased model.", "There is a related issue of ongoing feedback. A computer model won\u2019t change unless it is explicitly changed by incorporating new rules or incoming data. Intervention is needed if the algorithm is returning biased or inaccurate results.", "Challenges for everyone working with data", "Something I want to stress is that these sources of bias are not limited to big social issues like racism or sexism, these are issues that may crop up in any application of data science. Every data scientist needs to grapple with algorithmic bias. The results may be benign, like a little bit of lost accuracy on model, or the may be severe, like someone being denied bail by a recidivism algorithm, but the issue is everywhere, and as machine learning models scale, their biases scale with them.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Feb62ef91b045&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----eb62ef91b045---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.smithsonianmag.com/innovation/artificial-intelligence-is-now-used-predict-crime-is-it-biased-180968337/", "anchor_text": "Source"}, {"url": "https://www.jobscan.co/blog/fortune-500-use-applicant-tracking-systems/", "anchor_text": "vast majority of large companies"}, {"url": "https://www.globenewswire.com/news-release/2019/07/22/1885923/0/en/Applicant-Tracking-System-ATS-Market-To-Reach-USD-2-34-Billion-By-2026-Reports-And-Data.html", "anchor_text": "projected to grow"}, {"url": "https://www.npr.org/sections/health-shots/2019/04/16/713902890/how-well-do-workplace-wellness-programs-work", "anchor_text": "probably don\u2019t work"}, {"url": "https://en.wikipedia.org/wiki/CompStat", "anchor_text": "CompStat"}, {"url": "https://en.wikipedia.org/wiki/PredPol", "anchor_text": "PredPol"}, {"url": "https://www.latimes.com/local/lanow/la-me-lapd-precision-policing-data-20190703-story.html", "anchor_text": "don\u2019t actually work very well"}, {"url": "https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing", "anchor_text": "demonstrably biased against African Americans"}, {"url": "https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html", "anchor_text": "the algorithm might be better"}, {"url": "https://twitter.com/RealSaavedra/status/1087627739861897216", "anchor_text": "driven by math"}, {"url": "https://en.wikipedia.org/wiki/Travelling_salesman_problem", "anchor_text": "traveling salesman problem"}, {"url": "https://www.wired.com/insights/2014/03/potholes-big-data-crowdsourcing-way-better-government/", "anchor_text": "a smartphone app"}, {"url": "https://hbr.org/2013/04/the-hidden-biases-in-big-data", "anchor_text": "under-resource roadwork"}, {"url": "https://www.npr.org/templates/story/story.php?storyId=106268439", "anchor_text": "famously crude proxy for fitness"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----eb62ef91b045---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/algorithmic-bias?source=post_page-----eb62ef91b045---------------algorithmic_bias-----------------", "anchor_text": "Algorithmic Bias"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&user=Max+Miller&userId=dfd5ba1a8332&source=-----eb62ef91b045---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&user=Max+Miller&userId=dfd5ba1a8332&source=-----eb62ef91b045---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Feb62ef91b045&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----eb62ef91b045---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----eb62ef91b045--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----eb62ef91b045--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----eb62ef91b045--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Falgorithmic-bias-can-the-computer-be-racist-eb62ef91b045&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}