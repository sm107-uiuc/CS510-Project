{"url": "https://towardsdatascience.com/laymans-introduction-to-knn-c793ed392bc2", "time": 1682996318.89793, "path": "towardsdatascience.com/laymans-introduction-to-knn-c793ed392bc2/", "webpage": {"metadata": {"title": "Layman\u2019s Introduction to KNN. k-nearest neighbour algorithm is where\u2026 | by Rishi Sidhu | Towards Data Science", "h1": "Layman\u2019s Introduction to KNN", "description": "kNN stands for k-Nearest Neighbours. It is a supervised learning algorithm. This means that we train it under supervision. We train it using the labelled data already available to us. Given a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://link.medium.com/tJY9BkPMmW", "anchor_text": "random forests", "paragraph_index": 1}, {"url": "https://link.medium.com/3h3C1XczsU", "anchor_text": "linear regression", "paragraph_index": 1}, {"url": "https://link.medium.com/bDfhu5lzsU", "anchor_text": "logistic regression", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Feature_vector", "anchor_text": "feature vectors", "paragraph_index": 19}], "all_paragraphs": ["kNN stands for k-Nearest Neighbours. It is a supervised learning algorithm. This means that we train it under supervision. We train it using the labelled data already available to us. Given a labelled dataset consisting of observations (x,y), we would like to capture the relationship between x \u2014 the data and y \u2014 the label. More formally, we want to learn a function g : X\u2192Y so that given an unseen observation X, g(x) can confidently predict the corresponding output Y.", "Other examples of supervised learning algorithms include random forests, linear regression and logistic regression.", "kNN is very simple to implement and is most widely used as a first step in any machine learning setup. It is often used as a benchmark for more complex classifiers such as Artificial Neural Networks (ANN) and Support Vector Machines (SVM). Despite its simplicity, k-NN can outperform more powerful classifiers and is used in a variety of applications such as economic forecasting, data compression and genetics.", "As with most technological progress in the early 1900s, KNN algorithm was also born out of research done for the armed forces. Two offices of USAF School of Aviation Medicine \u2014 Fix and Hodges (1951) wrote a technical report introducing a non-parametric method for pattern classification that has since become popular as the k-nearest neighbor (kNN) algorithm.", "Let\u2019s say we have a dataset with two kinds of points \u2014 Label 1 and Label 2. Now given a new point in this dataset we want to figure out its label. The way it is done in kNN is by taking a majority vote of its k nearest neighbours. k can take any value between 1 and infinity but in most practical cases k is less than 30.", "Let\u2019s say we have two groups of points \u2014 blue-circles and orange-triangles. We want to classify the Test Point = black circle with a question mark, as either a blue circle or an orange triangle.", "Goal: To label the black circle.", "For K = 1 we will look at the first nearest neighbour. Since we take majority vote and there is only 1 voter we assign its label to our black test point. We can see that the test point will be classified as a blue circle for k=1.", "Expanding our search radius to K=3 also keeps the result same, except that this time it is not an absolute majority, it\u2019s 2 out of 3. Still with k=3 test point is predicted to have the class blue-circle \u2192because the majority of points are blue.", "Let\u2019s see how k=5 and K =9 do. To look at the nearest neighbors we draw circle with test point at the centre and stop when 5 points fall inside the circle.", "When we look at the 5 and subsequently at K = 9, the majority of the closest neighbors of our test point are orange-triangles. That indicates that the test point must be an orange triangle.", "Now that we have labelled this one test point we repeat the same process over all the unknown points (i.e. the test set). Once all test points are labelled using k-NN we try separating them using a decision boundary. Decision boundary shows how well the training set is separated.", "First they would choose k. We already saw above that a bigger k takes a vote of larger number of points. This means higher chances of being correct. But at what cost, you say?", "Getting the k nearest neighbours means sorting through the distances. That is a costly operation. A very high processing power is needed which translates to either longer processing time or costlier processor. Higher the K costlier the whole procedure. But too low a k would result in overfitting.", "A very low k will fail to generalize. A very high k is costly.", "As we go to higher K\u2019s the boundaries become smooth.Blue and red regions are broadly separated. Some blue and red soldiers are left behind the enemy lines. They are collateral damage. They account for loss in training accuracy but lead to better generalisation and high test accuracy i.e. high accuracy of correct labelling for new points.", "A graph of validation error when plotted against K would typically look like this. We can see that around K = 8 the error is minimum. It goes up on either side.", "Then they would try to find distances between points. How do we decide which neighbours are near and which are not?", "In chess, the distance between squares on the chessboard for rooks is measured in Manhattan distance; kings and queens use Chebyshev distance \u2014 Wikipedia", "Once we know how to compare points based on distance we would like to train our model. The best part about k-NN is that there is no explicit training step for it. We already know all that is to know about our dataset \u2014 its labels. In essence the training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.", "K-NN is a lazy learner because it doesn\u2019t learn a discriminative function from the training data but memorizes the training dataset instead.", "An eager learner has a model fitting or training step. A lazy learner does not have a training phase.", "k-NN is a life saver when one has to quickly deliver a solution with fairly accurate results. In most tools like MATLAB, python, R it is given as a single line command. Despite that it is very easy to implement and fun to try.", "Source code for the R Plots", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Blockchain | Machine Learning | Product Management"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc793ed392bc2&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://rishisid.medium.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": ""}, {"url": "https://rishisid.medium.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Rishi Sidhu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F941be470fb83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&user=Rishi+Sidhu&userId=941be470fb83&source=post_page-941be470fb83----c793ed392bc2---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@the_roaming_platypus?utm_source=medium&utm_medium=referral", "anchor_text": "timJ"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://link.medium.com/tJY9BkPMmW", "anchor_text": "random forests"}, {"url": "https://link.medium.com/3h3C1XczsU", "anchor_text": "linear regression"}, {"url": "https://link.medium.com/bDfhu5lzsU", "anchor_text": "logistic regression"}, {"url": "https://www.edureka.co/blog/what-is-machine-learning/", "anchor_text": "Edureka"}, {"url": "https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-S1-S11", "anchor_text": "A Regression-based K nearest neighbor algorithm for gene function prediction from heterogeneous\u2026As a variety of functional genomic and proteomic techniques become available, there is an increasing need for\u2026bmcbioinformatics.biomedcentral.com"}, {"url": "http://www.scielo.br/scielo.php?pid=S0001-37652018000100295&script=sci_arttext", "anchor_text": "Estimating Stand Height and Tree Density in Pinus taeda plantations using in-situ data, airborne\u2026Accurate forest inventory is of great economic importance to optimize the entire supply chain management in pulp and\u2026www.scielo.br"}, {"url": "https://ieeexplore.ieee.org/abstract/document/8630470", "anchor_text": "Air traffic flow prediction"}, {"url": "https://ieeexplore.ieee.org/abstract/document/8630470", "anchor_text": "Air traffic flow prediction based on k nearest neighbor regressionIn order to improve the prediction accuracy of air traffic flow, a method of applying k nearest neighbor regression to\u2026ieeexplore.ieee.org"}, {"url": "https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf", "anchor_text": "DTIC"}, {"url": "https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/", "anchor_text": "AnalyticsVidhya"}, {"url": "https://en.wikipedia.org/wiki/Lp_space", "anchor_text": "L\u221e"}, {"url": "https://lyfat.wordpress.com/2012/05/22/euclidean-vs-chebyshev-vs-manhattan-distance/", "anchor_text": "Lyfat"}, {"url": "https://en.wikipedia.org/wiki/Feature_vector", "anchor_text": "feature vectors"}, {"url": "https://xkcd.com/1445/", "anchor_text": "xkcd"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c793ed392bc2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c793ed392bc2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c793ed392bc2---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----c793ed392bc2---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----c793ed392bc2---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&user=Rishi+Sidhu&userId=941be470fb83&source=-----c793ed392bc2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&user=Rishi+Sidhu&userId=941be470fb83&source=-----c793ed392bc2---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc793ed392bc2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c793ed392bc2---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c793ed392bc2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c793ed392bc2--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c793ed392bc2--------------------------------", "anchor_text": ""}, {"url": "https://rishisid.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://rishisid.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rishi Sidhu"}, {"url": "https://rishisid.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F941be470fb83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&user=Rishi+Sidhu&userId=941be470fb83&source=post_page-941be470fb83--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2727bb635294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flaymans-introduction-to-knn-c793ed392bc2&newsletterV3=941be470fb83&newsletterV3Id=2727bb635294&user=Rishi+Sidhu&userId=941be470fb83&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}