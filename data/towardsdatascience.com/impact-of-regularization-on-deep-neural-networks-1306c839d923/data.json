{"url": "https://towardsdatascience.com/impact-of-regularization-on-deep-neural-networks-1306c839d923", "time": 1683010624.9593349, "path": "towardsdatascience.com/impact-of-regularization-on-deep-neural-networks-1306c839d923/", "webpage": {"metadata": {"title": "Impact of Regularization on Deep Neural Networks | by Ujjwal Kumar | Towards Data Science", "h1": "Impact of Regularization on Deep Neural Networks", "description": "In this era of the information superhighway, the world around us is changing for good and it would not be an overstatement to say that Deep Learning is the next transformation. Deep Learning is a set\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this era of the information superhighway, the world around us is changing for good and it would not be an overstatement to say that Deep Learning is the next transformation. Deep Learning is a set of powerful mathematical tools that enable us, to represent, interpret, and control the complex world around us.", "The programming paradigm is changing: Instead of programming a computer, we teach it to learn something and it does what we want.", "This particular notion is extremely captivating and has driven machine learning practitioners to develop models to take these concepts and ideas further along and apply them in real-world scenarios.", "However, the fundamental problem in building sophisticated machine learning models is how to make the architecture to do good not just on the training data but also on the testing data, i.e., on previously unseen features. In order to overcome this central problem, we must modify the model by applying certain strategies to reduce the test error, possibly at the expense of increased training error. These strategies are collectively known as regularization techniques. Nevertheless, regularization is useful only when the model is suffering from a high variance problem where the network overfits to the training data but fails to generalize new features (validation or test data).", "One of the most extensively used regularization strategies is the utilization of the L2 norm. This can be implemented by adding a regularization term to the cost function of the network.", "The first part of the equation corresponds to the computation of net network loss. The terms w & b represents the weights and biases that the model has learned. The second part corresponds to the regularization term where the norm of the weight vector (w) is calculated. This regularization term is explicitly referred to as the famous L2 norm or Weight Decay. The net result of this is that the weight matrices of the network are penalized according to the value of the regularization parameter lambda (\u03bb). So, the regularization parameter \u03bb can be thought of as another hyper-parameter that is required to fine-tune.", "The intuition behind the regularizing impact of the L2 norm can be understood by taking an extreme case. Let\u2019s set the value of the regularization parameter \u03bb to be on the higher end. This would heavily penalize the weight matrices (w) of the network and they will be assigned with the values that are near to zero. The immediate impact of this is that the net activations of the neural network are reduced and the forward pass effect is diminished. Now, with a much simplified neural network architecture, the model would not be able to overfit to the training data and will be able to generalize much better on novel data and features.", "This intuition was based on the fact that the value of the regularization parameter \u03bb was set to be very high. If however, an intermediate value of this parameter is chosen, it would increase the model performance on the testing set.", "Figuring out the intuition behind the concept of regularization is all well and good, however, one more important aspect is to know whether your network really needs regularization or not. Here, plotting learning curves play an essential role.", "Graphing out the model's training and validation loss over the number of epochs is the most widely used method to determine whether a model has overfitted or not. The concept behind this methodology is based upon the fact that, if the model has overfitted to the training data, then the training loss and validation loss will differ by a lot, moreover the validation loss will always be higher than the training loss. The reason for this is that, if the model cannot generalize well on previously unseen data, then the corresponding loss or cost value will be inevitably high.", "As evident from the graph above, there exists a huge gap between training loss and validation loss which shows that the model has clearly overfitted to the training samples.", "The practical implementation of the L2 norm can be demonstrated easily. For this purpose, let\u2019s take the Diabetes Dataset from sklearn and plot the learning curves of first an unregularized model and then of a regularized model.", "The dataset has 442 instances and takes in ten baseline variables: age, sex, BMI, average BP, and six blood serum measurements (S1 to S6) as its training features (called x) and the measure of disease progression after one year as its labels (called y).", "Let\u2019s import the dataset using TensorFlow and sklearn.", "We now split the data into training and testing sets. The testing set will have 10% of the training data reserved. This can be done using the train_test_split() function available in sklearn.", "We now create an unregularized model using only Dense layers from the Sequential API from Keras. This unregularized model will have 6 hidden layers each with a ReLU activation function.", "After designing the model architecture we now compile the model with the Adam optimizer and mean squared error loss function. Training occurs for 100 epochs and the metrics are stored in a variable that can be used for plotting the learning curves.", "We now plot the loss of the model as observed for both the training data and the validation data:", "The outcome here turns to be that, the validation loss continuously spikes up after approximately 10 epochs whereas the training loss keeps decreasing. This converse trend generates a huge gap between the two losses which indicates that the model has overfitted to the training data.", "We have concluded from the previous learning curve that an unregularized model suffers from overfitting, and to fix this issue we introduce the L2 norm.", "Dense layers as well as convolutional layers have an optional kernel_regularizer keyword argument. To add in Weight Decay or L2 regularization, we set the kernel regularizer argument equal to tf.keras.regularizers.l2(parameter). This parameter object is created with one required argument, which is the coefficient that multiplies the sum of squared weights or the regularization parameter \u03bb.", "We now create a similar network architecture, but this time, we include the kernel_regularizer argument.", "We compile and train the model with the same optimizer, loss function, metrics, and the number of epochs as it makes the comparison between the two models easier.", "After training, the following learning curve was observed when the training and validation loss was plotted against the number of epochs:", "The regularized model\u2019s learning curve is much smoother and the training, as well as the validation losses, are much closer to each other, which represents less variance in the network. Both, the training loss and the validation loss are now comparable to each other which shows the increased confidence of the model to generalize new features.", "In this edifying journey, we did an extensive analysis by comparing the performance of two models, one of which suffered from overfitting and the other to solve the former model\u2019s drawbacks. The problem of Bias and Variance in Machine Learning cannot be disregarded, and strategies must be applied to overcome these non-trivial complications. We covered one such strategy today to avoid variance in our model.", "However, the journey doesn't stop here, and therefore, we have to look beyond the horizon and must keep our noses to the wind.", "Here is the link to the author\u2019s GitHub repository which can be referred for the unabridged code.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Science undergraduate at SRM-IST, Artificial Intelligence Aficionado."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1306c839d923&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ujjwalkumar2607.medium.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": ""}, {"url": "https://ujjwalkumar2607.medium.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Ujjwal Kumar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab3d7812aa01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&user=Ujjwal+Kumar&userId=ab3d7812aa01&source=post_page-ab3d7812aa01----1306c839d923---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/ujjwalkumar2607/Regularisation-techniques-tensorflow", "anchor_text": "ujjwalkumar2607/Regularisation-techniques-tensorflowContribute to ujjwalkumar2607/Regularisation-techniques-tensorflow development by creating an account on GitHub.github.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1306c839d923---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1306c839d923---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1306c839d923---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----1306c839d923---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/regularization?source=post_page-----1306c839d923---------------regularization-----------------", "anchor_text": "Regularization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&user=Ujjwal+Kumar&userId=ab3d7812aa01&source=-----1306c839d923---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&user=Ujjwal+Kumar&userId=ab3d7812aa01&source=-----1306c839d923---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1306c839d923&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1306c839d923---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1306c839d923--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1306c839d923--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1306c839d923--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1306c839d923--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1306c839d923--------------------------------", "anchor_text": ""}, {"url": "https://ujjwalkumar2607.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ujjwalkumar2607.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ujjwal Kumar"}, {"url": "https://ujjwalkumar2607.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "26 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fab3d7812aa01&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&user=Ujjwal+Kumar&userId=ab3d7812aa01&source=post_page-ab3d7812aa01--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9539331dcc1f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimpact-of-regularization-on-deep-neural-networks-1306c839d923&newsletterV3=ab3d7812aa01&newsletterV3Id=9539331dcc1f&user=Ujjwal+Kumar&userId=ab3d7812aa01&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}