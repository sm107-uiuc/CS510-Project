{"url": "https://towardsdatascience.com/attention-in-neural-networks-e66920838742", "time": 1682997018.506453, "path": "towardsdatascience.com/attention-in-neural-networks-e66920838742/", "webpage": {"metadata": {"title": "Attention in Neural Networks. Some variations of attention\u2026 | by Mahendran Venkatachalam | Towards Data Science", "h1": "Attention in Neural Networks", "description": "In an earlier post on \u201cIntroduction to Attention\u201d we saw some of the key challenges that were addressed by the attention architecture introduced there (and referred in Fig 1 below). While in the same\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Luong et al. in their paper", "paragraph_index": 6}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "Xu et al. in their paper", "paragraph_index": 6}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "paper", "paragraph_index": 6}, {"url": "https://papers.nips.cc/paper/6284-latent-attention-for-if-then-program-synthesis.pdf", "anchor_text": "this paper", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "In this paper", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1606.01933.pdf", "anchor_text": "this paper by Parikh et al", "paragraph_index": 17}, {"url": "http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf", "anchor_text": "this paper by Jaitly", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "the earlier post", "paragraph_index": 20}], "all_paragraphs": ["In an earlier post on \u201cIntroduction to Attention\u201d we saw some of the key challenges that were addressed by the attention architecture introduced there (and referred in Fig 1 below). While in the same spirit, there are other variants that you might come across as well. Among other aspects, these variants differ on are \u201cwhere\u201d attention is used ( standalone, in RNN, in CNN etc) and \u201chow\u201d attention is derived (global vs local, soft vs hard etc). This post is a brief listing of some of the variants.", "Disclaimer 1: The idea here is just to get a sense of how attention mechanisms are leveraged in solutions proposed in different papers. So the focus will be less on the type of task the paper was trying to solve and more on the usage of attention mechanisms in the solution.", "Disclaimer 2: There is no rigor or reason behind why these papers/variants were chosen. This list is just a product of random scouting and top search hits.", "Disclaimer 3: Self attention and Transformers deserve a separate post (truly, I lost steam for the day) and are not touched upon here.", "Global attention is the same as what was explored in the \u201cIntroduction to Attention\u201d post. It is when we use ALL encoder hidden states to define the attention based context vector for each decoder step. But as you might have guessed, this could become expensive.", "Local attention on the other hand attends to only a few hidden states that fall within a smaller window. This window is centered around the \u201cp\u201dth encoder hidden state and includes \u201cD\u201d hidden states that appear on either side of \u201cp\u201d. So that makes the length of this window, i.e. the number of hidden states considered, 2D+1. Monotonic alignment is when p is simply set to be the same as the decoder position (5th output will have p = 5, and if D = 2, the attention will be only on 3,4,5,6,7 hidden states). Predictive alignment is when \u201cp\u201d is defined as a function of the decoder hidden state ht (paper uses S \u00b7 sigmoid(vp\u22a4 tanh(Wpht))) and the parameters of this function are jointly learnt by the model.", "Referred by Luong et al. in their paper and described by Xu et al. in their paper, soft attention is when we calculate the context vector as a weighted sum of the encoder hidden states as we had seen in the figures above. Hard attention is when, instead of weighted average of all hidden states, we use attention scores to select a single hidden state. The selection is an issue, because we could use a function like argmax to make the selection, but it is not differentiable (we are selecting an index corresponding to max score when we use argmax and nudging the weights to move the scores a little as part of backprop will not change this index selection) and therefore more complex techniques are employed. Note, the paper uses hard attention in an image captioning context, so \u201cthe encoder hidden states\u201d are really the \u201cfeature vectors\u201d generated by a CNN.", "I stumbled upon this paper presented by Chen et al. which deals with translating natural language sentences in to \u201cIf-Then\u201d programs. i.e., given a statement like \u201cPost your Instagram photos to Tumblr\u201d, the network should predict the most relevant words describing the trigger (\u201cInstagram photos\u201d) and action (Tumblr) which would then help arrive at the corresponding labels (trigger=Instagram.Any_new_photo_by_you, action=Tumblr.Create_a_photo_post).", "How could we apply attention to arrive at this? Let\u2019s look at another example, \u201cPost photos in your Dropbox folder to Instagram\u201d. Compared to the previous one, here \u201cInstagram\u201d is the most relevant for action and \u201cDropbox\u201d is the trigger. The same word can be either the trigger or the action. So determining what role the word plays require us to investigate how the prepositions like \u201cto\u201d are used in such sentences. The paper introduces a \u201cLatent Attention\u201d model to do this.", "A \u201cJ\u201d dimensional \u201cLatent attention\u201d vector is prepared \u2014 each dimension here represents a word, and the softmax gives a sense of relative importance across the words in the vector.", "Next, \u201cActive Attention\u201d is prepared similar to above, but instead of using a \u201cd\u201d dimensional vector like \u201cu\u201d, a \u201cd x J\u201d dimensional trainable matrix V is used, resulting in a \u201cJ x J\u201d Active attention matrix. Column-wise softmax is done between the dimensions of each word.", "The \u201cActive Weights\u201d are then computed as the product of these two. Another set of word embeddings are then weighted by these \u201cActive Weights\u201d to derive the output which is the softmaxed to arrive at the predictions.", "To me, the derivation of the active weights as a product of vectors representing each word in the input and the latent attention vector that represents the importance across the words is a form of \u201cself attention\u201d, but more on self attention later.", "In this paper Yin et al presented ABCNN \u2014 Attention Based CNN to model a pair of sentences, used in answer selection, paraphrase identification and textual entailment tasks. The key highlight of the proposed attention based model was that it considers the impact/relationship/influence that exists between the different parts or words or whole of one input sentence with the other, and provides an interdependent sentence pair representation that can be used in subsequent tasks. Let\u2019s take a quick look at the base network first before looking at how attention was introduced into it.", "Note that the input to the first layer is words, next layer is short phrases (in the example above, a filter width of 3 makes it a phrase of 3 words), next layer is larger phrases and so on until the final layer where the output is a sentence representation. In other words, with each layer, an abstract representation of lower to higher granularity is produced.", "The paper presents three ways in which attention is introduced into this base model.", "ABCNN-3, simply combines both essentially applying attention to both the input of convolution and to the convolution output while pooling.", "For natural language inference, this paper by Parikh et al first creates the attention weights matrix comparing each word in one sentence with all of another and normalized as shown in the image. But after this, in the next step, the problem is \u201cdecomposed into sub-problems\u201d that are solved separately. i.e. a feed forward network is used to take concatenated word embedding and corresponding normalized alignment vector to generate the \u201ccomparison vector\u201d. This comparison vectors for each sentence are then summed to create two aggregate comparison vectors representing each sentence which is then fed through another feed forward network for final classification. The word order doesn\u2019t matter in this solution and only attention is used.", "For online tasks, such as real time speech recognition, where we do not have the luxury of processing through an entire sequence this paper by Jaitly et al introduced the Neural Transducer that makes incremental prediction while processing blocks of input at a time, as opposed to encoding or generating attention over the entire input sequence.", "The input sequence is divided into multiple blocks of equal length (except possibly the last block) and the Neural Transducer model computes attention only for the inputs in the current block, which is then used to generate the output corresponding to that block. The connection with prior blocks exists only via the hidden state connections that are part of the RNN on the encoder and decoder side. While this is similar to an extent to the local attention described earlier, there is no explicit \u201cposition alignment\u201d as described there.", "Refer back to Fig 1, an illustration of the base introductory attention model we saw in the earlier post. A generalized abstraction of alignment is that it is like querying the memory as we generate the output. The memory is some sort of representation of the input and the query is some sort of representation of output. In Fig 1, the memory or collection of keys was the encoder hidden states \u201ch\u201d, the blue nodes, and query was the current decoder hidden state \u201cs\u201d, the green nodes. The derived alignment score is then multiplied with \u201cvalues\u201d \u2014 another representation of the input, the gold nodes in Fig 1.", "Area attention is when attention is applied on to an \u201carea\u201d, not necessarily just one item like a vanilla attention model. \u201cArea\u201d is defined as a group of structurally adjacent items in the memory (i.e. the input sequence in a one dimensional input like sentence of words). An area is formed by combining adjacent items in the memory. In 2-D case like an image, the area will be any rectangular subset within the image.", "The \u201ckey\u201d vector for an area can be defined simply as the mean vector of the key of each item in the area. In a sequence to sequence translation task, this would be the mean of each of the hidden state vectors involved in the area. In the definition under \u201cSimple Key Vector\u201d in Fig 11, \u201dk\u201d is the hidden state vector. If we are defining an area containing 3 adjacent words, then the mean vector is the mean of the hidden state vectors generated after each of the three words in the encoder.", "The \u201cvalue\u201d on the other hand is defined as the sum of all value vectors in the area. In our basic example, this will again be the encoder hidden state vectors corresponding to the three words for which the area is being defined.", "We can also define a richer representation of the key vector that takes into consideration not just the mean, but also the standard deviation and shape vector as explained in the Fig 11. Shape vector here is defined as the concatenation of height and width vectors, which in turn are created from actual width and height numbers projected as vectors using embedding matrices, which I presume are learnt with the model. The key is derived as an output of a single layer perceptron that takes mean, std dev and shape vectors as input.", "Once the key and value vectors are defined, the rest of the network could be any attention utilizing model. If we are using a encoder-decoder RNN as seen in Fig 1, then plugging the derived area based key and value vectors in place of those in Fig 1 will make it an area based attention model.", "Reading through these papers gives an interesting perspective on how researchers have used attention mechanisms for various tasks and how the thinking has evolved. Hopefully this quick study gives a sense of how we could tweak and use one of these or a new variant in our own tasks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe66920838742&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e66920838742--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e66920838742--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----e66920838742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=post_page-----e66920838742--------------------------------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae----e66920838742---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://arxiv.org/pdf/1409.0473.pdf", "anchor_text": "paper by Bahdanau et al"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "Introduction to Attention"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Luong et al. here"}, {"url": "https://arxiv.org/pdf/1508.04025.pdf", "anchor_text": "Luong et al. in their paper"}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "Xu et al. in their paper"}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "paper"}, {"url": "http://proceedings.mlr.press/v37/xuc15.pdf", "anchor_text": "Xu et al."}, {"url": "https://papers.nips.cc/paper/6284-latent-attention-for-if-then-program-synthesis.pdf", "anchor_text": "this paper"}, {"url": "https://papers.nips.cc/paper/6284-latent-attention-for-if-then-program-synthesis.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "In this paper"}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1512.05193.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1606.01933.pdf", "anchor_text": "this paper by Parikh et al"}, {"url": "https://arxiv.org/pdf/1606.01933.pdf", "anchor_text": "this paper by Parikh et al"}, {"url": "http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf", "anchor_text": "this paper by Jaitly"}, {"url": "http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf", "anchor_text": "this paper by Jaitly et al"}, {"url": "https://towardsdatascience.com/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda", "anchor_text": "the earlier post"}, {"url": "https://arxiv.org/pdf/1810.10126.pdf", "anchor_text": "this paper"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e66920838742---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e66920838742---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/attention?source=post_page-----e66920838742---------------attention-----------------", "anchor_text": "Attention"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e66920838742---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e66920838742---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----e66920838742---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=-----e66920838742---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e66920838742--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe66920838742&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e66920838742---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e66920838742--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e66920838742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e66920838742--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e66920838742--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e66920838742--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e66920838742--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e66920838742--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e66920838742--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mahendran.venkatachalam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mahendran Venkatachalam"}, {"url": "https://medium.com/@mahendran.venkatachalam/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "296 Followers"}, {"url": "https://gotensor.com/", "anchor_text": "https://gotensor.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d2735a047ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=post_page-4d2735a047ae--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb8b9125d53ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fattention-in-neural-networks-e66920838742&newsletterV3=4d2735a047ae&newsletterV3Id=b8b9125d53ec&user=Mahendran+Venkatachalam&userId=4d2735a047ae&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}