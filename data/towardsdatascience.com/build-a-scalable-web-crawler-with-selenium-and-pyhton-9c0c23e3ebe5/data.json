{"url": "https://towardsdatascience.com/build-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5", "time": 1683003380.1843, "path": "towardsdatascience.com/build-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5/", "webpage": {"metadata": {"title": "Build a scalable web crawler with Selenium and Python | by Philipp Postels | Towards Data Science", "h1": "Build a scalable web crawler with Selenium and Python", "description": "An implementation within the Google Cloud Platform by using Docker, Kubernetes Engine and Cloud Datastore."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/most-in-demand-tech-skills-for-data-engineers-58f4c1ca25ab", "anchor_text": "common technologies", "paragraph_index": 2}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub Repository", "paragraph_index": 3}, {"url": "https://www.towardsdatascience.com/archive", "anchor_text": "archive page", "paragraph_index": 7}, {"url": "https://pypi.org/project/Scrapy/", "anchor_text": "Scrapy", "paragraph_index": 11}, {"url": "https://pypi.org/project/selenium/", "anchor_text": "Selenium", "paragraph_index": 11}, {"url": "https://pypi.org/project/beautifulsoup4/", "anchor_text": "BeautifulSoup4", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/archive", "anchor_text": "https://towardsdatascience.com/archive", "paragraph_index": 14}, {"url": "https://cloud.google.com/appengine/docs/standard/nodejs/building-app/creating-project", "anchor_text": "How to set up a Google Cloud Project", "paragraph_index": 23}, {"url": "https://cloud.google.com/iam/docs/creating-managing-service-account-keys?hl=en#iam-service-account-keys-create-console", "anchor_text": "link", "paragraph_index": 24}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository", "paragraph_index": 29}, {"url": "https://itnext.io/scaling-selenium-test-execution-with-kubernetes-c79bc53979f5", "anchor_text": "link", "paragraph_index": 39}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository", "paragraph_index": 41}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository", "paragraph_index": 45}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository", "paragraph_index": 53}, {"url": "https://towardsdatascience.com/spark-in-docker-in-kubernetes-a-practical-approach-for-scalable-nlp-9dd6ef47c31e", "anchor_text": "project of J\u00fcrgen Schmidl", "paragraph_index": 54}, {"url": "https://towardsdatascience.com/build-a-highly-scalable-dashboard-that-runs-on-kubernetes-fa2bc6271f1d", "anchor_text": "project of Arnold Lutsch", "paragraph_index": 55}], "all_paragraphs": ["Disclaimer: Since scraping of Medium.com Services is prohibited by the terms of use, I would like to point out that we immediately processed the underlying data within the project with NLP and no storing of the pure texts took place. The approach illustrated in this article is therefore for demonstration purposes only and can be used for other websites that allow web scraping.", "This article is part of a larger project. If you are also interested in performing Natural Language Processing on the results to extract technology names by using PySpark and Kubernetes or building highly scalable Dashboards in Python, you will find corresponding links at the end of the article.", "Life as a Data Scientist can be tough. It is not only the acquisition and quality of data and its interpretability that poses challenges. The rapid development of technologies, as well as constantly rising expectations from business (keyword rocket science), also make the work more difficult. However, in my experience, the acquisition and application of new technologies, in particular, is a source of enthusiasm for most data scientists. For this reason, I built a scalable web crawler with common technologies to improve my skillset.", "All files and code snippets that are referenced in this article can be found in my GitHub Repository.", "Towards Data Science (TWDS) is one of the best known and most instructive places to go for data science. It is a medium.com publication on which a large number of authors have published various articles. Recurrently used technologies are referenced and their use is often presented in case studies.Therefore I decided to build a web crawler that extracts the content of TWDS and stores it inside the NoSQL database \u201cGoogle Datastore\u201d. To make the web crawler scalable, I used Docker for containerizing my application and Kubernetes for the orchestration.", "The approach was to develop the web crawler in a Jupyter Notebook on my local machine and to constantly professionalize and increase the project (see Fig 2). For instance, I built a Python application with a dedicated crawler class and all necessary methods based on the Jupyter Notebook scripts. But let us have a more detailed look at the implementation steps.", "To develop a properly operating web crawler, it is important to familiarize yourself in advance with the site structure, available content, and irregularities.", "TWDS is a classic publication with many authors and a lot of articles. Thanks to an archive page it was easy to understand the page structure in detail (see Fig. 3). Fortunately, the authors were not only listed there but also provided with links that led to overview pages for these authors.", "The used HTML class was constantly used so that the links could easily be identified (see Fig. 4).", "For the overview pages of the authors, I figured out that at first only the author\u2019s articles published on TWDS were listed. Other articles published on Medium.com by the author were not displayed. It was therefore not necessary to check whether the specific article belonged to the TWDS publication. Unfortunately, the HTML class for these links was empty and the links could not be identified. However, the links contained the complete URL and thus the word \u201ctowards\u201d. Therefore, the identification of these links was just as unambiguous. However, another challenge occurred when examining the page. Not all of the author\u2019s articles were displayed directly, but when the website was scrolled down further content was dynamically reloaded using Javascript. To ensure completeness, this had to be taken into account for the development of the web crawler.", "Finally, I had to examine the structure of the individual articles for similarities and patterns to extract the relevant data fields. The required properties were author, URL, title, text, reading time, publishing date, tags, claps and the number of responses. As can be seen in Figure 5, the HTML source code has some challenges. For example, the class names are seemingly dynamically generated and have only minor matches across articles. But there are also rays of hope, e.g. reading time, title, URL, and publishing date are standardized in the page header. The remaining content was reasonably easy to access.", "At first, during development in Jupyter Notebooks, I was looking for Python packages I could use to fulfill all requirements. I quickly realized for Scrapy, one of the most commonly used packages for web scraping, that dynamic content reloading would be difficult. After focusing on this requirement, I became aware of Selenium. Selenium is a framework for automated software testing of web applications and can interact with browsers, e. g. to scroll down pages to load the dynamic javascript content and receive the full HTML source code.", "To work with the extracted HTML source code, I found the Python package BeautifulSoup4, which provides various methods to systematically search the HTML tree structure for relevant content. With these packages selected, I could fulfill all the requirements to develop a web crawler.", "During the development, I now worked along with the page structure shown in figure 3. So I started with the extraction of the author list.", "I defined the URL \u201chttps://towardsdatascience.com/archive\u201d to be crawled and used to start the Selenium Webdriver. In the following, I extracted all required parts of the code to run the Selenium Webdriver.", "Since the command \u201cdriver.get()\u201d only opens the browser and loads the referenced page, I further used a code snippet that automatically scrolled the page down to the end and thus allowed saving the complete HTML source code (\u201cdriver.page_source\u201d).", "This snippet is completely independent of any website specific structure and can be easily reused in another web crawling context as well.", "Since the output is still only the HTML source code and I was looking for a list of all authors, I wrote a \u201cfor loop\u201d to extract the links to the authors\u2019 profiles by using my knowledge from source inspection (see chapter 3.1).", "The result was now a list with links to the respective authors that could be easily further exploited (see fig. 6). I used the list as the input for my next iteration to receive the articles for each author. As a result, I stored the links of the articles and the link to the authors\u2019 profile page as a key-value pair inside a dictionary (see fig 7).", "With the links to the articles in access, I iterated over the different articles, extracted the relevant field contents and stored them inside a dictionary (tempdic). In some cases, this was done simply by specifying the location in the HTML structure.", "In other cases, the use of loops or regular expressions was necessary, e. g. by extracting the tags.", "Since I could now store the data of an article systematically in a dictionary, I had to find a suitable way to store the data.", "As I already had a perfectly filled dictionary per article and my focus was not supposed to prepare a fitting SQL-Database, I was choosing the Google Datastore to store my data. The Google Datastore is a managed, NoSQL, schemaless database for storing non-relational data \u2014 just perfect for this use case.", "To use Google Datastore, it is necessary to set up a project at Google Cloud Platform (How to set up a Google Cloud Project; Of course other cloud providers can be used instead). To access Google Datastore by using Python, it is likely to set up a service account with access rights to the Datastore (Role: Cloud Datastore-Owner) inside the project. This can be done in the menu path \u201cAPI & Services > Credentials\u201d by generating an access-key.", "The usage of the generated connection data is easiest when calling the data from a JSON-file. How this can be generated can be seen from the following link.", "In the web crawler source code, the connection has to be initialized first. The JSON-file is hereby referenced (\u201csa.json\u201d).", "After adding all relevant information, the entity can finally be stored in Datastore.", "The functionality of the web crawlers is now completed. As the implementation is still running inside Jupyter Notebook, it is now time for refactoring the code and using a crawler class with specified methods (see TWDS_Crawler.py)", "As Docker is the most relevant container platform in software development and part of many implementations, I will not explain any further background within this article. Nevertheless, this was my first use of Docker and I had a look for a convenient step-by-step tutorial to containerize my Python application that is likely to use.", "To build the first container image, I only used these four files (GitHub-Repository):", "To build the container image, it necessary to enter the directory folder with the referenced files inside the shell and write the following command:", "This just specified the name of the container image to \u201ctwds-crawler\u201d and placed the image in the current directory folder (\u201c.\u201d). To run the container the following command should be used:", "Due to the pre-configured Dockerfile, the Python application inside the container starts automatically after the container is running. The output should look somehow like:", "The web crawler application started (\u201cStart Crawler\u201d) and opened the getAuthors method (\u201cGet Authors\u201d) but crashed afterward due to the missing browser instance. For now, this can be ignored as the goal is to run this container inside a Kubernetes cluster.", "Kubernetes is an open-source system for automating the deployment, scaling, and management of (docker-)container applications. As it was developed by Google, the Google Cloud Platform delivers a nice implementation so that you can build a cluster only by using the Google Cloud Shell inside the browser and the following script. Just replace <Your Project Name> with the name of your Google Cloud Platform project.", "Note: I would recommend using the editor modus to show all stored files.", "To access the cluster from the shell after the deployment finished you simply use the following command:", "The created Kubernetes Cluster has auto-scaling and uses a minimum of 2 nodes and a maximum of 8 nodes (Note: To save some money, make sure to delete the cluster after using it, see main menu point \u201cKubernetes Engine\u201d).", "We are now ready to deploy the selenium grid and our containerized web crawler.", "The Selenium Grid is a hub/nodes construction of Selenium with potentially heterogeneous browser versions (nodes) and a control unit (hub) that distributes or parallelizes the work items e. g. unit tests or crawling jobs. To connect both objects there is also a Hub-Service. For a more detailed description check this link.", "To make the deployment process as easy as possible and reduce the necessary code to a minimum, I used YAML-Files and bash scripts. YAML-Files describe Kubernetes objects, e. g. in case of the nodes the number of different Selenium nodes to be deployed or the specific browser version. The bash scripts call the different YAML-Files in the right order.", "To work inside the Google Cloud shell it necessary to upload the different files. This can easily be done by drag and drop. The following files have to be in there (sa.json needs to be added individually, the rest can be found in my GitHub-Repository).", "By using the following command, a complete Selenium Grid with one Firefox-node will be deployed on the Kubernetes Cluster:", "To check if everything is working following command could be used:", "Since the Selenium Grid with a Firefox node is already running on the Kubernetes Cluster, it is time to go on with the web crawler. Due to the local development of the web crawler as well as the use of the local web browser, it is necessary to adjust the Webdriver to the Selenium Grid:", "Note: The adjusted version can be found in my GitHub-Repository. Just replace the code of the TWDS_Crawler.py or change the referenced file inside the Dockerfile to \u201cTWDS_Crawler_Cluster.py\u201d.", "After this change, a new Docker image can be built inside the Google Cloud Shell and published into the Google Cloud Container Registry (comparable to a repository). This can be done with the following commands:", "If everything worked fine, the web crawler can finally be deployed inside the Kubernetes Cluster with", "To check if the crawler runs and see the logs (e. g. the printed lines) you can use the following commands inside the Google Cloud Shell:", "The web crawler is now running. To increase the number of nodes, the YAML File for the Firefox-node has to be edited upfront, or during run time with the following command:", "The Selenium Grid will automatically use the deployed Firefox-node instances during the web crawling process.", "If everything worked fine, the results should be visible inside the Google Cloud Datastore just moments later as I chose an incremental approach to write the article details inside the database.", "Hope you enjoyed reading my article and good luck with your implementation.", "If you have any problems by setting up the project, please also have a look at the troubleshooting area in my GitHub-Repository.", "To see how to perform Natural Language Processing on the results and extract technology names by using PySpark and Kubernetes, please have a look at the project of J\u00fcrgen Schmidl.", "To see how to build a highly scalable Python Dashboard that runs on Kubernetes as well, please have a look at the project of Arnold Lutsch.", "Publishing and consuming articles about Data Science as well as Big Data Architecture concepts and implementations. Python <3 || Github: Postiii"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9c0c23e3ebe5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@Postiii?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@Postiii?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Philipp Postels"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd10953ac672a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=post_page-d10953ac672a----9c0c23e3ebe5---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c0c23e3ebe5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=-----9c0c23e3ebe5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c0c23e3ebe5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=-----9c0c23e3ebe5---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/most-in-demand-tech-skills-for-data-engineers-58f4c1ca25ab", "anchor_text": "common technologies"}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub Repository"}, {"url": "https://www.towardsdatascience.com/archive", "anchor_text": "archive page"}, {"url": "https://pypi.org/project/Scrapy/", "anchor_text": "Scrapy"}, {"url": "https://pypi.org/project/selenium/", "anchor_text": "Selenium"}, {"url": "https://pypi.org/project/beautifulsoup4/", "anchor_text": "BeautifulSoup4"}, {"url": "https://towardsdatascience.com/archive", "anchor_text": "https://towardsdatascience.com/archive"}, {"url": "https://cloud.google.com/appengine/docs/standard/nodejs/building-app/creating-project", "anchor_text": "How to set up a Google Cloud Project"}, {"url": "https://cloud.google.com/iam/docs/creating-managing-service-account-keys?hl=en#iam-service-account-keys-create-console", "anchor_text": "link"}, {"url": "https://www.wintellect.com/containerize-python-app-5-minutes/", "anchor_text": "Containerize a Python App in 5 Minutes \u2014 WintellectPython for better or worse has found cemented itself as the lingua franca of data science. With its rise in popularity\u2026www.wintellect.com"}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository"}, {"url": "https://itnext.io/scaling-selenium-test-execution-with-kubernetes-c79bc53979f5", "anchor_text": "link"}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository"}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository"}, {"url": "https://github.com/Postiii/twds-crawler", "anchor_text": "GitHub-Repository"}, {"url": "https://towardsdatascience.com/spark-in-docker-in-kubernetes-a-practical-approach-for-scalable-nlp-9dd6ef47c31e", "anchor_text": "project of J\u00fcrgen Schmidl"}, {"url": "https://towardsdatascience.com/build-a-highly-scalable-dashboard-that-runs-on-kubernetes-fa2bc6271f1d", "anchor_text": "project of Arnold Lutsch"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9c0c23e3ebe5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/selenium?source=post_page-----9c0c23e3ebe5---------------selenium-----------------", "anchor_text": "Selenium"}, {"url": "https://medium.com/tag/docker?source=post_page-----9c0c23e3ebe5---------------docker-----------------", "anchor_text": "Docker"}, {"url": "https://medium.com/tag/kubernetes?source=post_page-----9c0c23e3ebe5---------------kubernetes-----------------", "anchor_text": "Kubernetes"}, {"url": "https://medium.com/tag/python?source=post_page-----9c0c23e3ebe5---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c0c23e3ebe5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=-----9c0c23e3ebe5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c0c23e3ebe5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=-----9c0c23e3ebe5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c0c23e3ebe5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@Postiii?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd10953ac672a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=post_page-d10953ac672a----9c0c23e3ebe5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd10953ac672a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=-----9c0c23e3ebe5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@Postiii?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Written by Philipp Postels"}, {"url": "https://medium.com/@Postiii/followers?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "32 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd10953ac672a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=post_page-d10953ac672a----9c0c23e3ebe5---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd10953ac672a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbuild-a-scalable-web-crawler-with-selenium-and-pyhton-9c0c23e3ebe5&user=Philipp+Postels&userId=d10953ac672a&source=-----9c0c23e3ebe5---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----0-----------------clap_footer----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----9c0c23e3ebe5----0---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----9c0c23e3ebe5----0-----------------bookmark_preview----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----1-----------------clap_footer----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----9c0c23e3ebe5----1---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----9c0c23e3ebe5----1-----------------bookmark_preview----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "15 min read\u00b76 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----9c0c23e3ebe5----2---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----9c0c23e3ebe5----2-----------------bookmark_preview----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@nikoskafritsas?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Nikos Kafritsas"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "Time-Series Forecasting: Deep Learning vs Statistics \u2014 Who Wins?A comprehensive guide on the ultimate dilemma"}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": "\u00b714 min read\u00b7Apr 5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&user=Nikos+Kafritsas&userId=bec849d9e1d2&source=-----c568389d02df----3-----------------clap_footer----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/time-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df?source=author_recirc-----9c0c23e3ebe5----3---------------------52c7b014_2a51_4c00_b750_365b8a7af928-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc568389d02df&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftime-series-forecasting-deep-learning-vs-statistics-who-wins-c568389d02df&source=-----9c0c23e3ebe5----3-----------------bookmark_preview----52c7b014_2a51_4c00_b750_365b8a7af928-------", "anchor_text": ""}, {"url": "https://medium.com/@Postiii?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "See all from Philipp Postels"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----9c0c23e3ebe5----0-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----9c0c23e3ebe5----1-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Josue Luzardo Gebrim"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Data Quality in Python Pipelines!Discover What It Is And How To Achieve Data Quality In Your Data Streams!"}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b714 min read\u00b7Mar 14"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&user=Josue+Luzardo+Gebrim&userId=9f59dfc0edf7&source=-----4ad1e8eb6603----0-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://jlgjosue.medium.com/data-quality-in-python-pipelines-4ad1e8eb6603?source=read_next_recirc-----9c0c23e3ebe5----0---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4ad1e8eb6603&operation=register&redirect=https%3A%2F%2Fjlgjosue.medium.com%2Fdata-quality-in-python-pipelines-4ad1e8eb6603&source=-----9c0c23e3ebe5----0-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----1-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----9c0c23e3ebe5----1---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----9c0c23e3ebe5----1-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://thegrayarea.tech/5-python-automation-scripts-i-use-every-day-74c4313f2b47?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://blog.grahamzemel.com/?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://blog.grahamzemel.com/?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Graham Zemel"}, {"url": "https://thegrayarea.tech/?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "The Gray Area"}, {"url": "https://thegrayarea.tech/5-python-automation-scripts-i-use-every-day-74c4313f2b47?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "5 Python Automation Scripts I Use Every DayTL;DR- A quick list of the best Python scripts I use on a daily basis, plus some possible modifications."}, {"url": "https://thegrayarea.tech/5-python-automation-scripts-i-use-every-day-74c4313f2b47?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b74 min read\u00b7Jan 19"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fthe-gray-area%2F74c4313f2b47&operation=register&redirect=https%3A%2F%2Fthegrayarea.tech%2F5-python-automation-scripts-i-use-every-day-74c4313f2b47&user=Graham+Zemel&userId=79a7fbf51e21&source=-----74c4313f2b47----2-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://thegrayarea.tech/5-python-automation-scripts-i-use-every-day-74c4313f2b47?source=read_next_recirc-----9c0c23e3ebe5----2---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "15"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F74c4313f2b47&operation=register&redirect=https%3A%2F%2Fthegrayarea.tech%2F5-python-automation-scripts-i-use-every-day-74c4313f2b47&source=-----9c0c23e3ebe5----2-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://syal-anuj.medium.com/?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "Anuj Syal"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "12 Must-Have Skills to become a Data EngineerThe Essential Skills for a Successful Data Engineering Career"}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": "\u00b78 min read\u00b7Jan 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fdatadriveninvestor%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&user=Anuj+Syal&userId=df3997c527b4&source=-----35b100dbee0a----3-----------------clap_footer----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://medium.datadriveninvestor.com/12-must-have-skills-to-become-a-data-engineer-35b100dbee0a?source=read_next_recirc-----9c0c23e3ebe5----3---------------------a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F35b100dbee0a&operation=register&redirect=https%3A%2F%2Fmedium.datadriveninvestor.com%2F12-must-have-skills-to-become-a-data-engineer-35b100dbee0a&source=-----9c0c23e3ebe5----3-----------------bookmark_preview----a171e8d3_bb9b_40eb_95e5_dcc7be6468e6-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----9c0c23e3ebe5--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}