{"url": "https://towardsdatascience.com/logistic-regression-explained-afc267815943", "time": 1683017479.240371, "path": "towardsdatascience.com/logistic-regression-explained-afc267815943/", "webpage": {"metadata": {"title": "Logistic Regression Explained. A High-Level Overview of Logistic\u2026 | by Jason Wong | Towards Data Science", "h1": "Logistic Regression Explained", "description": "Last week I wrote an overview of Linear Regression and what\u2019s happening under the hood of OLS regression from statsmodels. This post will serve as a high-level overview of Logistic Regression to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/linear-regression-explained-1b36f97b7572", "anchor_text": "overview of Linear Regression", "paragraph_index": 0}, {"url": "https://www.statsmodels.org/stable/regression.html", "anchor_text": "OLS regression", "paragraph_index": 0}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#:~:text=Unlike%20linear%20regression%20which%20outputs,two%20or%20more%20discrete%20classes.", "anchor_text": "sigmoid", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/E_(mathematical_constant)", "anchor_text": "Euler\u2019s number", "paragraph_index": 5}, {"url": "https://www.statisticshowto.com/log-odds/", "anchor_text": "log of the odds", "paragraph_index": 10}, {"url": "https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf", "anchor_text": "rules for derivatives", "paragraph_index": 12}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent", "paragraph_index": 13}, {"url": "https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf", "anchor_text": "derivative chain rule", "paragraph_index": 16}, {"url": "https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/", "anchor_text": "MLE", "paragraph_index": 18}, {"url": "https://www.statlect.com/glossary/log-likelihood", "anchor_text": "log-likelihood", "paragraph_index": 19}, {"url": "https://online.stat.psu.edu/stat504/node/150/", "anchor_text": "negative log-likelihood", "paragraph_index": 19}, {"url": "https://www.steveklosterman.com/over-under/", "anchor_text": "overfitting and underfitting", "paragraph_index": 24}, {"url": "https://www.geeksforgeeks.org/regularization-in-machine-learning/", "anchor_text": "Regularization", "paragraph_index": 25}, {"url": "https://www.statisticssolutions.com/assumptions-of-logistic-regression/", "anchor_text": "The 5 assumptions for logistic regression", "paragraph_index": 29}, {"url": "https://www.statisticssolutions.com/binary-logistic-regression/", "anchor_text": "Binary logistic regression", "paragraph_index": 30}, {"url": "https://stats.idre.ucla.edu/spss/dae/ordinal-logistic-regression/", "anchor_text": "Ordinal logistic regression", "paragraph_index": 31}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle", "paragraph_index": 36}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "probability estimates", "paragraph_index": 39}], "all_paragraphs": ["Last week I wrote an overview of Linear Regression and what\u2019s happening under the hood of OLS regression from statsmodels. This post will serve as a high-level overview of Logistic Regression to perform classification tasks. Logistic Regression is a great first model to learn when introduced to classification.", "Supervised Learning is a term referring to machine learning algorithms that have the ability to \u201clearn\u201d from a labeled (ground truth) dataset. The data needs to be labeled so the supervised learning algorithms can evaluate their performance. The performance is evaluated by comparing the predictions with the actual labels for the training data. By doing this, the algorithms can improve their performance in classification.", "Supervised learning is made up of two categories, classification and regression. Regression helps us in forecasting values such as, how much something will sell for. Classification helps us determine which class something belongs to, e.g., male or female.", "In supervised learning, the loss function is used to evaluate the predictions from the algorithm with the actual labels. There are different types of loss functions used for classification and regression, they serve the same purpose, provide the model with the ability to evaluate how right or wrong a prediction is.", "With logistic regression, we are not trying to predict a continuous value, we\u2019re modeling the probability that an input variable belongs to the first/default class. This is where the sigmoid function comes in.", "Where e is Euler\u2019s number and t is the continuous output from the linear function. For a refresher, the linear equation:", "Where b0(beta0) is the intercept and b1(beta1) is the coefficient for that input value. By passing the linear equation into the sigmoid function, it will output a probability between 0 and 1. Our equation will become:", "Then, by multiplying the numerator and denominator by e to the linear equation, our equation turns into:", "Once we\u2019ve obtained the probability of class one given our betas, we can calculate the probability for class 0.", "When we have the probabilities, we can compute the odds ratio by dividing the probability of class 1 by the probability of class 0 (or all other classes).", "By computing the log of both sides, we will get the log of the odds that we predict the first class.", "A derivative is a function\u2019s rate of change. When working with linear functions, we can easily calculate the derivative by taking two points, and dividing the change in y by the change in x.", "When it comes to nonlinear functions, the function\u2019s rate of change is always changing. If we have larger values for the change in x (\u0394\ud835\udc65), the derivative will represent the rate of change less. However, if we were to decrease \u0394\ud835\udc65, our function should converge. The number that the function converges on is our derivative. Here\u2019s a helpful link on the rules for derivatives. Our derivative formula becomes:", "The term gradient descent refers to minimizing the cost function with the use of partial derivatives in order to find the regression line that best represents the data. Using gradient descent, we can determine how to change the regression line when both m (slope) and b (intercept) can be changed. This will help us change the regression line by the slope and the intercept to get the greatest decrease in cost. Using the RSS (residual sum of squares) we can calculate how to change both variables, our cost function becomes:", "Where J is just another way of saying residual sum of squares. J varies as the slope and intercept variables of the regression line change.", "To get the gradient of a function, we can calculate the partial derivatives of each variable.", "Using the derivative chain rule, our partial derivatives become:", "By subtracting the partial derivatives (formulas on right above) from the previous slope and intercept values, we can descend towards the minimum.", "MLE in logistic regression helps in finding which coefficients minimize the error within the predicted probabilities, i.e., find which underlying parameters maximize the likelihood of a probability that a value belongs to the default class. MLE looks at the conditional probabilities from each variable as an independent probability, and computes its total probability.", "When calculating the MLE, in order to simplify things we use the log-likelihood. We can also use the negative log-likelihood since we are looking for minimums.", "Taking the classes in our dataset (yi), and reward or penalize the model depending on how close the probability (pi) is to the actual class (yi). It will return a high value if the probability value is high and close to the actual class. For example, if we had a probability of .98 and the actual class was 1, we\u2019d see a higher score than a probability of .18 and the actual class 1.", "Since it is more difficult to take a derivative of multiplication than addition, we take the log of the likelihood.", "Since we\u2019re trying to minimize the error of a probability occurring, our cost function minimizes the negative log-likelihood.", "Now, we can take the derivative (rate of change) of the negative log-likelihood and set it zero in order to find the gradient and update the parameters.", "When modeling, we tend to see overfitting and underfitting. Overfitting refers to when the model is fit to the noise in the data, and isn\u2019t able to successfully generalize the data. Overfit models tend to perform significantly better on the train set than the test set. Underfitting refers to when the model is too simple, and not able to capture the information in the data.", "With regression, as well as many other models, we evaluate the performance by measuring the error. Regularization works to minimize the function by reducing either a model\u2019s residuals or the size of the coefficients. The three main types of regularization are:", "Also known as L2 Norm Regularization, minimizes the cost function by adding a penalty term \u03bb (lambda) to the magnitude of the coefficients squared. Ridge tends to shrink the coefficients while helping reduce the complexity of the model. In the formula above, the first term is the sum of squares, the second is the cost function, and the final term adds a penalty using the squares of the coefficients.", "Lasso regularization also adds a penalty term to the magnitude of the coefficients but they are not squared. This method of regularization works with the sum of the absolute values. In the formula above, the first two terms are the sum of squares and the cost function. The final term adds a penalty using the absolute values of the coefficients.", "This method is a combination of ridge and lasso regularization. Elastic Net allows us to specify how much of each regularization term to use.", "Unlike linear regression, there doesn\u2019t need to be a linear relationship between the dependent and independent variables. The 5 assumptions for logistic regression are:", "Binary logistic regression \u2014 dependent variable is binary", "Ordinal logistic regression \u2014 dependent variable is ordinal", "The observations are independent of each other.", "There needs to be little or no multicollinearity between the independent variables.", "While there does not need to be a linear relationship between the dependent and independent variables, there does need to be a linear relationship between the independent variables and the log odds.", "There generally needs to be large sample size when performing logistic regression. A good \u201crule of thumb\u201d is to have a minimum of 10 cases with the least frequent outcome for each independent variable.", "Let\u2019s go ahead and see how the concepts above can be easily implemented with Sklearn. Once again, I will be using the infamous titanic dataset. The dataset was obtained from Kaggle. The goal being to predict whether a given person survived or not.", "There are 177 out of 891 missing values in the Age column. For the purposes of this pipeline tutorial, I am going to go ahead and fill in the missing Age values with the mean age. There are 687 out of 891 missing values in the Cabin column. I am removing this feature since approximately 77% of values are missing. The Embarked feature is only missing 2 values so we can fill these with the most common value. The Name and Ticket features both hold unique values to each passenger and will not be needed for predictive classification so they will also be dropped.", "Now that we\u2019ve handled the missing values in the dataset, we can move on to defining the continuous and categorical variables.", "This returned a higher accuracy score than I expected using the default parameters. Remember how logistic regression models probabilities? We can see the probability estimates with predict_proba.", "Each row in the array above tells us the probability of that output being a 0 (left) and 1 (right). For the first row of the probability estimates, there is approximately a 94% chance of the output being 0 and approximately a 6% chance it is a 1.", "In order to find the most optimal inverse regularization strength, we can create a list with Numpy containing 1000 values from 1 to 1000. In case the model performs the better with a really high regularization strength, we will insert 0 and 0.0001 to this list. By running a for loop and fitting a logistic regression model with each C value, we can store the each score and c value in a dictionary for reference.", "This only increased our model\u2019s performance by a small amount. However, the training accuracy decreasing and the testing accuracy increasing tells us the model may be overfit. Let\u2019s take a look at the confusion matrix.", "If we wanted to protect the model from false negatives, we could increase the threshold to be higher than the default 0.5.", "There you have it, hopefully this explanation of logistic regression cleared some things up for you. We went over quite a few formulas but luckily there are libraries and modules that perform a majority of these steps for us.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a passion for statistical analysis and machine learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fafc267815943&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jwong853.medium.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": "Jason Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1240e6b56e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&user=Jason+Wong&userId=e1240e6b56e3&source=post_page-e1240e6b56e3----afc267815943---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.shutterstock.com/g/majcot", "anchor_text": "majcot"}, {"url": "https://towardsdatascience.com/linear-regression-explained-1b36f97b7572", "anchor_text": "overview of Linear Regression"}, {"url": "https://www.statsmodels.org/stable/regression.html", "anchor_text": "OLS regression"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#:~:text=Unlike%20linear%20regression%20which%20outputs,two%20or%20more%20discrete%20classes.", "anchor_text": "sigmoid"}, {"url": "https://en.wikipedia.org/wiki/E_(mathematical_constant)", "anchor_text": "Euler\u2019s number"}, {"url": "https://www.statisticshowto.com/log-odds/", "anchor_text": "log of the odds"}, {"url": "https://unsplash.com/@jcw", "anchor_text": "Jim Witkowski"}, {"url": "https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf", "anchor_text": "rules for derivatives"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html", "anchor_text": "gradient descent"}, {"url": "https://www.math.ucdavis.edu/~kouba/Math17BHWDIRECTORY/Derivatives.pdf", "anchor_text": "derivative chain rule"}, {"url": "https://www.shutterstock.com/g/icefields", "anchor_text": "desdemona72"}, {"url": "https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/", "anchor_text": "MLE"}, {"url": "https://www.statlect.com/glossary/log-likelihood", "anchor_text": "log-likelihood"}, {"url": "https://online.stat.psu.edu/stat504/node/150/", "anchor_text": "negative log-likelihood"}, {"url": "https://www.shutterstock.com/g/Duplass", "anchor_text": "Duplass"}, {"url": "https://www.steveklosterman.com/over-under/", "anchor_text": "overfitting and underfitting"}, {"url": "https://www.geeksforgeeks.org/regularization-in-machine-learning/", "anchor_text": "Regularization"}, {"url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "anchor_text": "Ridge (\u201cL2\u201d)"}, {"url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "anchor_text": "Lasso (\u201cL1\u201d)"}, {"url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "anchor_text": "Elastic Net"}, {"url": "https://www.statisticssolutions.com/assumptions-of-logistic-regression/", "anchor_text": "The 5 assumptions for logistic regression"}, {"url": "https://www.statisticssolutions.com/binary-logistic-regression/", "anchor_text": "Binary logistic regression"}, {"url": "https://stats.idre.ucla.edu/spss/dae/ordinal-logistic-regression/", "anchor_text": "Ordinal logistic regression"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Kaggle"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html", "anchor_text": "probability estimates"}, {"url": "https://www.statsmodels.org/stable/regression.html", "anchor_text": "https://www.statsmodels.org/stable/regression.html"}, {"url": "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html", "anchor_text": "https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html"}, {"url": "https://online.stat.psu.edu/stat504/node/150/", "anchor_text": "https://online.stat.psu.edu/stat504/node/150/"}, {"url": "https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/", "anchor_text": "https://machinelearningmastery.com/logistic-regression-with-maximum-likelihood-estimation/"}, {"url": "https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training", "anchor_text": "https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training"}, {"url": "https://www.statlect.com/glossary/log-likelihood", "anchor_text": "https://www.statlect.com/glossary/log-likelihood"}, {"url": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net", "anchor_text": "https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net"}, {"url": "https://www.statisticssolutions.com/assumptions-of-logistic-regression/", "anchor_text": "https://www.statisticssolutions.com/assumptions-of-logistic-regression/"}, {"url": "https://www.sciencedirect.com/topics/computer-science/negative-log-likelihood", "anchor_text": "https://www.sciencedirect.com/topics/computer-science/negative-log-likelihood"}, {"url": "https://www.statisticshowto.com/log-odds/", "anchor_text": "https://www.statisticshowto.com/log-odds/"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----afc267815943---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/regression-analysis?source=post_page-----afc267815943---------------regression_analysis-----------------", "anchor_text": "Regression Analysis"}, {"url": "https://medium.com/tag/classification-algorithms?source=post_page-----afc267815943---------------classification_algorithms-----------------", "anchor_text": "Classification Algorithms"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----afc267815943---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----afc267815943---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&user=Jason+Wong&userId=e1240e6b56e3&source=-----afc267815943---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&user=Jason+Wong&userId=e1240e6b56e3&source=-----afc267815943---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fafc267815943&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----afc267815943---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----afc267815943--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----afc267815943--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----afc267815943--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----afc267815943--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----afc267815943--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----afc267815943--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----afc267815943--------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jason Wong"}, {"url": "https://jwong853.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "199 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1240e6b56e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&user=Jason+Wong&userId=e1240e6b56e3&source=post_page-e1240e6b56e3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdec1d5eb8281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-explained-afc267815943&newsletterV3=e1240e6b56e3&newsletterV3Id=dec1d5eb8281&user=Jason+Wong&userId=e1240e6b56e3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}