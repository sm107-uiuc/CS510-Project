{"url": "https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53", "time": 1682993375.8838139, "path": "towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53/", "webpage": {"metadata": {"title": "Hyperparameter Optimization with Keras | by Mikko | Towards Data Science", "h1": "Hyperparameter Optimization with Keras", "description": "With the right process in place, it will not be difficult to find state-of-the-art hyperparameter configuration for a given prediction task. Out of the three approaches \u2014 manual, machine-assisted\u2026"}, "outgoing_paragraph_urls": [{"url": "http://github.com/autonomio/talos", "anchor_text": "hyperparameter optimization solution", "paragraph_index": 4}, {"url": "https://github.com/autonomio/talos", "anchor_text": "here", "paragraph_index": 6}, {"url": "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data", "anchor_text": "Wisconsin Breast Cancer", "paragraph_index": 9}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "anchor_text": "here", "paragraph_index": 24}, {"url": "https://nbviewer.jupyter.org/github/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20on%20Keras%20with%20Breast%20Cancer%20Data.ipynb", "anchor_text": "notebook", "paragraph_index": 43}], "all_paragraphs": ["With the right process in place, it will not be difficult to find state-of-the-art hyperparameter configuration for a given prediction task. Out of the three approaches \u2014 manual, machine-assisted, and algorithmic \u2014 this article will focus on machine-assisted. The article will cover how I do it, get to the proof that the method works, and provide the understanding of why it works. The main principle is simplicity.", "The first point about performance relates to the issue of accuracy (and other more robust metrics) as a way to measure model performance. Consider f1 score as an example. If you have a binary prediction task with 1% positives, then a model that makes everything a 0 will get close to perfect f1 score and accuracy. This can be handled with some changes to the way f1 score deals with corner cases such as \u201call zeros,\u201d \u201call ones,\u201d and \u201cno true positives.\u201d But that\u2019s a big topic, and outside of the scope of this article, so for now I just want to make it clear that this problem is a very important part of getting systemic hyperparameter optimization to work. We have a lot of research in this field, but the research is focused more on algorithms, and less on the fundamentals. Indeed, you can have the fanciest algorithm in the world \u2014 often also really complex \u2014 making decisions based on a metric that does not make sense. That\u2019s not going to be hugely useful for dealing with \u201creal-life\u201d problems.", "Make no mistake; EVEN WHEN WE DO GET THE PERFORMANCE METRIC RIGHT (yes I\u2019m yelling), we need to consider what happens in the process of optimizing a model. We have a training set, and then we have a validation set. As soon as we start to look at the validation results, and start making changes based on that, we start to create a bias towards the validation set. Now we end up with the training results that are a product of the bias the machine has, and we have the validation results, that is the product of the bias we have. In other words, the model we get as a result does not have the properties of a well-generalized model. Instead, it\u2019s biased away from being generalized. So it would be very important to keep this point in mind.", "The key point about a more advanced fully-automated (unsupervised) approach to hyperparameter optimization, involves first solving these two problems. Once these two are solved \u2014 and yes there are ways to do that \u2014 the resulting metrics would need to be implemented as a single score. Then that score becomes the metric against which the hyperparameter optimization process is optimized. Otherwise, no algorithm in the world will help, as it will optimize towards something else than what we are after. What are we after again? A model that will do the task that the prediction task articulates. Not just one model for one case (which is often the case in the papers covering the topic), but all kinds of models, for all kinds of prediction tasks. That is what a solution such as Keras allows us to do, and any attempt to automate parts of the process of using a tool such as Keras should embrace that idea.", "For everything in this article, I used Keras for the models, and Talos, which is a hyperparameter optimization solution I built. The benefit is that it exposes Keras as-is, without introducing any new syntax. It allows me to do in minutes what used to take days while having fun instead of painful repetition.", "You can try it for yourself:", "Or look at the codes / docs here.", "But the information I want to share, and the point I want to make, is not related to a tool, but the process. You could follow the same procedure any which way you like.", "One of the more prominent issues with automated hyperparameter optimization and related tools is that you generally tend to end up far away from the way you\u2019re used to working. The key to successful prediction-task-agnostic hyperparameter optimization \u2014 as is with all complex problems \u2014 is in embracing cooperation between man and the machine. Every experiment is an opportunity to learn more about the practice (of deep learning) and the technology (in this case Keras). That opportunity should not be missed at the expense of process automation. At the same time, we should be able to take away the blatantly redundant parts of the process. Think of doing shift-enter in Jupyter for a few hundred times and waiting for a minute or two between each iteration. In summary, at this point, the goal should not be in a fully-automated approach to finding the right model, but in minimizing procedural redundancy on burdening the human. Instead of mechanically operating the machine, the machine operates itself. Instead of analyzing the results of various model configurations one by one, I want to analyze them by the thousands or by hundreds of thousands. There are over 80,000 seconds in a day, and a lot of parameter space can be covered in that time without me having to do anything about it.", "For the sake of example, I will first provide the code that I used throughout the experiment covered in this article. The dataset I used is the Wisconsin Breast Cancer dataset.", "Once the Keras model is defined, it\u2019s time to decide the initial parameter boundaries. The dictionary gets then fed into the process in a way where a single permutation is picked once, and then disregarded.", "Depending on the losses, optimizers, and activations we want to include in the scan, we\u2019ll need to import those functions/classes from Keras first. Next, with the model and parameters ready, it\u2019s time to start the experiment.", "Note that I\u2019m not going to share more codes, as all I did was change the parameters in the parameter dictionary related with the insights provided in the following sections. For completeness, at the end of the post, I will share a link to a notebook with the codes.", "Because there are many permutations (over 180,000 in total) in this first round of the experiment, I randomly pick just 1% of the total, and we\u2019re left with 1,800 permutations.", "In this case, I\u2019m running from a 2015 make MacBook Air, and it looks like I have just the time to meet a friend and have a cup of coffee (or two).", "For this article, using the Wisconsin Breast Cancer dataset, I\u2019ve set up the experiment assuming no previous knowledge about optimal parameters, or the dataset. I\u2019ve prepared the dataset by dropping one column, and by transforming all the rest so that the mean for each feature is 0, with standard deviation of 1.", "After the initial run of 1,800 permutations, it\u2019s time to look at the results and decide on how we\u2019re going to limit (or otherwise alter) the parameter space.", "A simple rank order correlation shows that lr (learning rate) has the strongest effect on our performance metric, which in this case is val_acc (validation accuracy). For this dataset, val_acc is ok, as there is a good number of Positives. For datasets where there is a significant disparity between Falses and Positives, accuracy is not a good metric. It seems that hidden_layers, lr (learning rate), and dropout all have a notable negative correlation with val_acc. A simpler network will do better in this task. For positive correlations, the number of epochs is the only one to stands out. Let\u2019s look a little closer. In the below graph we have epochs (50, 100 and 150) on the x-axis, val_acc on the y-axis, learning rate in the columns and dropout as hue. The trend seems to be generally as the correlation suggests; smaller dropouts do better than larger ones.", "Another way to look at the dropout is through Kernel Density Estimation. Here we can see that there is a slight tendency towards higher val_acc with dropout 0 or 0.1, as well as less tendency to have low val_acc (around the 0.6 mark).", "The first action item for the next round of scanning is to get rid of the higher dropout rates altogether and focus on values between 0 and 0.2. Let\u2019s take a look at learning rate more closely next. Note that learning rates are normalized across optimizers to a scale where 1 represents the Keras default value of that optimizer.", "The situation is pretty clear; the smaller learning rates work well for both loss function, and the difference is particularly pronounced with logcosh. But because binary cross-entropy is clearly outperforming on all learning rate levels, it will be the loss of our choice for the remainder of the experiment. A sanity check is still needed though. How about if what we\u2019re seeing is not factoring in over-fitting towards the training data? How if val_loss is all over the place and we\u2019re just getting carried away looking at one side of the picture? A simple regression analysis show that\u2019s not the case. Other than a few outliers, everything is packed nicely in the lower left corner where we want it. The tendency is that both train and validation loss is close to zero.", "I think for now we know enough; it\u2019s time to set up the next round of the experiment! As a point of reference, the parameter space for the next experiment looks like this:", "In addition to refining the learning rate, dropout, and batch size boundaries, I\u2019ve added kernel_initializer \u2018uniform.\u2019 Remember that at this stage the objective is to learn about the prediction task, as opposed to being too focused on finding the solution. The key point here is experimentation and learning about the overall process, in addition to learning about the specific prediction challenge.", "Initially, the less we focus on the result (and more on the process), the more likely we\u2019re going to get a good result. It\u2019s like playing chess; if at first you\u2019re too focused on winning the game, you will not focus on the opening and mid-game. Competitive chess is won in the endgame, based on playing a strong beginning and middle. If things go well, the second iteration in the hyperparameter optimization process is the middle. We\u2019re not entirely focused on winning the game yet, but it helps to have the eye on the prize already. In our case, the results from the first round (94.1% validation accuracy) indicate that with the given dataset, and the set parameter boundaries, there are predictions to be made.", "In this case, the prediction task here is to say if breast cancer is benign or malignant. This type of predictions is a kind of a big deal in the sense that both false positives and false negatives do matter. Getting the prediction wrong will have some negative effect on the person\u2019s life. In case you are interested, there is a bunch of papers written on this dataset, and some other relevant info, which you can all find here.", "The result for the second round is 96% validation accuracy. The below correlation shows that the only thing sticking out at this point, is the number of epochs, so for the third round that\u2019s one thing I\u2019m going to change.", "If you look at the correlation alone, there is a danger of missing something in the bigger picture. In hyperparameter optimization, the big picture is about individual values within a given parameter, and their interconnectedness with all other values. Now that we\u2019ve eliminated the logcosh loss function, and have just one loss (binary_crossentropy) in the parameter space, I want to learn a little bit about how the different optimizers are performing in the context of the epochs.", "It is exactly like the correlation suggests regarding epochs (now on the x-axis). Because RMSprop underperforms in both 100 and 150, let\u2019s also drop that from the next round.", "Before moving on, let\u2019s consider very briefly a fundamental question related to hyperparameter optimization as an optimization challenge. What is it that we\u2019re trying to achieve? The answer can be summarized using two simple concepts;", "Prediction optimum is where we have a model that is both precise and generalized. Result entropy is where the entropy is as close to zero (minimal) as possible. Result entropy can be understood as a measure of similarity between between all the results within a result set (one round of going through n permutations). The ideal scenario is where the prediction optimum is 1, which is 100% prediction performance and 100% generality, and the resulting entropy is 0. This means that no matter what we do within the hyperparameter space, we only get the perfect result every time. This is not feasible for several reasons but is helpful to keep in mind regarding the objectives of the process of optimizing the process of hyperparameter optimization. The other way to look at answering the question is through three levels of consideration;", "You might then ask if this leads us to an infinite progression where we then need optimizers on top of optimizers, and the answer is yes. In my view, what makes the hyperparameter optimization problem interesting, is the way it leads us to the solution for the problem of \u201cmodels that build models.\u201d But that would take us far from the scope of this article.", "With the second, and particularly the third aspects in mind, we need to consider the computational efficiency of the process. The less we waste the computational resource, the more we have of it for finding the best possible result regarding the aspects one and two. Consider the below graphs in this light.", "The second round KDE looks much better in the sense of having the resources allocated where we need them to be. They are closer to 1 on the x-axis, and there is very little in terms of \u201cspillage\u201d towards the 0. Whatever compute resources are going into the scan, they\u2019re are doing important work. The ideal picture here is one of a single straight line with the x value of 1.", "Let\u2019s get right to it. The peak validation accuracy is now 97.1%, and it looks like we\u2019re going in the right direction. I made the mistake of just adding 175 epochs as max, and based on the below; it looks like we have to go further than that. At least with this configuration. Which makes me think\u2026maybe for the last and final round, we should try something surprising.", "As it was discussed in the foreword, it\u2019s important to consider generalization as well. Every time we look at the result, there is the effect where our insights start to affect the experiment. The net result is that we start to get less generalized models that work well with the validation dataset, but might not work well with a \u201creal-life\u201d dataset. In this case, we don\u2019t have a good way to test for this kind of bias, but at least we can take measures to assess the degree of pseudo-generalization with what we have. Let\u2019s see training and validation accuracy first.", "Even though this does not give us an affirmative confirmation of having a well-generalized model, in fact, it falls short from it a great deal; the regression analysis result could not be much better. Then let\u2019s look at loss.", "It\u2019s even better. Things are looking good. For the last round, I\u2019m going to increase the number of epochs, but I\u2019m also going to try another approach. So far I\u2019ve only had very small batch sizes, which take a lot of time to process. In the third round, I only included batch sizes 1 through 4. For the next, I\u2019m going to throw in 30 or something, and see what that does.", "A few words about early stopping. Keras provides a very convenient way to use callbacks through EarlyStopping functionality. As you might have noticed, I\u2019m not using that. Very generally speaking, I would recommend using it, but it is not as trivial as everything we\u2019ve done here so far. Getting the settings right in the way where it\u2019s not limiting your ability to find the best possible results is not straightforward. The most important aspect has to do with metrics; I would want to have a custom metric created first, and then use that as my EarlyStopping mode (instead of using val_acc or val_loss). That said, EarlyStopping, and callbacks in general, provide a very powerful way to add to your hyperparameter optimization process.", "Before diving into the results, let\u2019s look at one more visualization from the results of the last round. This time 5-dimensional. I wanted to see the remaining parameters \u2014 kernel initializer, batch size, hidden layers, and epochs \u2014 all in the same picture compared against validation accuracy and loss. First accuracy.", "Mostly it\u2019s neck-to-neck, but some things do stand out. The first thing is that if a hidden layer value (hue) is down, in most cases its one hidden layer. For batch sizes (columns) it\u2019s hard to say, as is for kernel initializer (rows). Let\u2019s next take a look at the validation loss on the y-axis, and see if we can learn more from there. And remember, here we\u2019re looking for smaller values; we\u2019re trying to minimize the loss function with each parameter permutation.", "Uniform kernel initializer is doing a great job in keeping the loss down throughout all epoch, batch size, and hidden layer variations. But because the results are a little inconsistent, I\u2019ll keep both initializers until the end.", "The winning combination is it came from the last minute idea to try bigger batch size to save time and in fewer epochs too):", "The highest result for the small batch sizes was validation accuracy 97.7%. With the larger batch size approach, there is also the upside of having the model converge very fast. At the end of this article, I will provide a video where you can see it for yourself. To be honest, once I saw how well the bigger batch size worked, I did set up a separate test just focusing on that. It took less than a minute to set it up as all I needed to change was the batch size (and for this smaller epochs), and the scan finished in 60 minutes. Regarding the plots, there is nothing much to see, as more or less all the results were near 100%. There is one more thing though I want to share, as it relates to the idea of entropy from a different standpoint than what we had already discussed. Entropy can be an effective way to assess overfitting (and therefore a proxy for generalization). In this case, I measure the val_loss and val_acc entropy, using KL divergence, against training loss and accuracy respectively.", "Here is the code complete notebook for the last round. And the video I had promised\u2026", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Worked with machine intelligence for 15 years, and built the interwebs for 25. Nothing here is my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb82e6364ca53&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mikkokotila.medium.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": ""}, {"url": "https://mikkokotila.medium.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Mikko"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F415c7038aba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&user=Mikko&userId=415c7038aba8&source=post_page-415c7038aba8----b82e6364ca53---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://github.com/autonomio/talos", "anchor_text": "hyperparameter optimization solution"}, {"url": "https://github.com/autonomio/talos", "anchor_text": "here"}, {"url": "https://www.kaggle.com/uciml/breast-cancer-wisconsin-data", "anchor_text": "Wisconsin Breast Cancer"}, {"url": "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)", "anchor_text": "here"}, {"url": "https://nbviewer.jupyter.org/github/autonomio/talos/blob/master/examples/Hyperparameter%20Optimization%20on%20Keras%20with%20Breast%20Cancer%20Data.ipynb", "anchor_text": "notebook"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b82e6364ca53---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----b82e6364ca53---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----b82e6364ca53---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b82e6364ca53---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b82e6364ca53---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&user=Mikko&userId=415c7038aba8&source=-----b82e6364ca53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&user=Mikko&userId=415c7038aba8&source=-----b82e6364ca53---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb82e6364ca53&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b82e6364ca53---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b82e6364ca53--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b82e6364ca53--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b82e6364ca53--------------------------------", "anchor_text": ""}, {"url": "https://mikkokotila.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mikkokotila.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mikko"}, {"url": "https://mikkokotila.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F415c7038aba8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&user=Mikko&userId=415c7038aba8&source=post_page-415c7038aba8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F966fe0b54989&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyperparameter-optimization-with-keras-b82e6364ca53&newsletterV3=415c7038aba8&newsletterV3Id=966fe0b54989&user=Mikko&userId=415c7038aba8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}