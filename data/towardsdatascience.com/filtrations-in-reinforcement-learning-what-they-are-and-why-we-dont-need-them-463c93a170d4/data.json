{"url": "https://towardsdatascience.com/filtrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4", "time": 1683012127.9133818, "path": "towardsdatascience.com/filtrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4/", "webpage": {"metadata": {"title": "Filtrations in Reinforcement Learning \u2014 What They Are and Why We Don\u2019t Need Them | by Wouter van Heeswijk, PhD | Towards Data Science", "h1": "Filtrations in Reinforcement Learning \u2014 What They Are and Why We Don\u2019t Need Them", "description": "Once in a while, when reading papers in the Reinforcement Learning domain, you may stumble across mysterious-sounding phrases such as \u2018we deal with a filtered probability space\u2019, \u2018the expected value\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Once in a while, when reading papers in the Reinforcement Learning domain, you may stumble across mysterious-sounding phrases such as \u2018we deal with a filtered probability space\u2019, \u2018the expected value is conditional on a filtration\u2019 or \u2018the decision-making policy is \u2131\u209c-measurable\u2019. Without formal training in measure theory [2,3], it may be difficult to grasp what exactly such a filtration entails. Formal definitions look something like this:", "Boilerplate language for those familiar with measure theory, no doubt, but hardly helpful otherwise. Googling for answers likely leads through a maze of \u03c3-algebras, Borel sets, Lebesgue measures and Hausdorff spaces, again presuming that one already knows the basics. Fortunately, only a very basic understanding of a filtration is needed to grasp its implications within the RL domain. This article will provide far from a full discussion on the topic, but aims to give a brief and (hopefully) intuitive outline of the core concept.", "In RL, we typically define an outcome space \u03a9 that contains all possible outcomes or samples that may occur, with \u03c9 being a specific sample path. For the sake of illustration, we will assume that our RL problem relates to a stock with price S\u209c at day t . Of course we\u2019d like to buy low and sell high (the precise decision-making problem is irrelevant here). We might denote the buying/selling decision as x\u209c(\u03c9), i.e., the decision is conditional on the price path. We start with price S\u2080 (a real number) and every day the price goes up or down according to some random process. We may simulate (or mathematically define) such a price path \u03c9=[\u03c9\u2081,\u2026,\u03c9\u209c] up front, before running the episode. However, that does not mean we should know stock price movements before they actually happen \u2014 even Warren Buffett could only dream of having such information! To claim we base our decisions on \u03c9 without being a clairvoyant, we may state the outcome space is \u2018filtered\u2019 (using the symbol \u2131 ) meaning we can only observe the sample up to time t.", "For most RL practitioners, this restriction must sound familiar. Don\u2019t we usually base our decisions on the current state S\u209c? Indeed, we do. In fact, as the Markov property implies that the stochastic process is memoryless \u2014 we only need the information embedded in the prevailing state S\u209c \u2014 information from the past is irrelevant [5]. As we will shortly see, the filtration is richer and more generic than a state, yet for practical purposes their implications are similar.", "Let\u2019s formalize our stock price problem a bit more. We start with a discrete problem setting, in which the price either goes up (u) or down (-d). Considering an episode horizon of three days, the outcome space \u03a9 may be visualized by a binomial lattice [4]:", "At this point, we need to define the notion of an `event\u2019 A \u2208 \u03a9. Perhaps stated somewhat abstractly, an event is an element of the outcome space. Simply put, we can assign a probability to an event and assert whether it has happened or not. As we will soon show, it is not the same as a realization \u03c9 though.", "A filtration \u2131 is a mathematical model that represents partial knowledge about the outcome. In essence, it tells us whether an event happened or not. The `filtration process\u2019 may be visualized as a sequence of filters, with each filter providing us a more detailed view . Concretely, in an RL context the filtration provides us with the information needed to compute the current state S\u209c, without giving any indication of future changes in the process [2]. Indeed, just like the Markov property.", "Formally, a filtration is a \u03c3-algebra, and although you don\u2019t need to know the ins and outs some background is useful. Loosely defined, a \u03c3-algebra is a collection of subsets of the outcome space, containing a countable number of events as well as all their complements and unions. In measure theory this concept has major implications, for the purpose of this article you only need to remember that the \u03c3-algebra is a collection of events.", "Back to the example, because the filtration only comes alive when seeing it into action. We first need to define the events, using sequences such as \u2018udu\u2019 to describe price movements over time. At t=0 we basically don\u2019t know anything \u2014 all paths are still possible. Thus, the event set A={uuu, uud, udu, udd, ddd, ddu, dud, duu} contains all possible paths \u03c9 \u2208 \u03a9. At t=1, we know a little more: the stock price went either up or down. The corresponding events are defined by A\u1d64={uuu,uud,udu,udd} and A\u2094={ddd,ddu,dud,duu}. If the stock price went up, we can surmise that our sample path \u03c9 will be in A\u1d64 and not in A\u2094 (and vice versa, of course). At t=2, we have four event sets: A\u1d64\u1d64={uuu,uud}, A\u1d64\u2094={udu,udd}, A\u2094\u1d64={duu,dud}, and A\u2094\u2094={ddu,ddd}. Observe that the information is getting increasingly fine-grained; the sets to which \u03c9 might belong are becoming smaller and more numerous. At t=3, we obviously know the exact price path that has been followed.", "Having defined the events, we can define the corresponding filtrations for t=0,1,2,3:", "At t=0, every outcome is possible. We initialize the filtration with the empty set \u2205 and outcome space \u03a9, also known as a trivial \u03c3-algebra.", "At t=1, we can simply add A\u1d64 and A\u2094 to \u2131\u2080 to obtain \u2131\u2081; recall from the definition that each filtration always includes all elements of its predecessor. We can use the freshly revealed information to compute S\u2081. We also get a peek into the future (without actually revealing future information!): if the price went up, we cannot reach the lowest possible price at t=3 anymore. The event sets are illustrated below", "At t=2, we may distinguish between four events depending on the price paths revealed so far. Here things get a bit more involved, as we also need to add the unions and complements (in line with the requirements of the \u03c3-algebra). This was not necessary for \u2131\u2081, as the union of A\u1d64 and A\u2094 equals the outcome space and A\u1d64 is the complement of A\u2094. From an RL perspective, you might note that we have more information than strictly needed. For instance, an up-movement followed by a down-movement yields the same price as the reverse. In RL applications we would typically not store such redundant information, yet you can probably recognize the mathematical appeal.", "At t=3, we already have 256 sets, using the same procedure as before. You can see that filtrations quickly become extremely large. A filtration always contains all elements of the preceding step \u2014 our filtration gets richer and more fine-grained with the passing of time. All this means is that we can more precisely pinpoint the events to which our sample price path may or may not belong.", "We are almost there, but we would be remiss if we only treat discrete problems. In reality, stock prices do not only go \u2018up\u2019 or \u2018down\u2019; they will change within a continuous domain. The same holds for many other RL problems. Although conceptually the same as for the discrete case, providing explicit descriptions for filtrations in continuous settings is difficult. Again, some illustrations might help more than formal definitions.", "Suppose that at every time step, we simulate a return from the real domain [-d,u]. Depending on the time we look ahead, we may then define an interval in which the future stock price will fall, say [329,335] at a given point in time. We can then define intervals within this domain. Any arbitrary interval may constitute an event, for instance:", "The complement of an interval could look like", "Furthermore, a plethora of unions may be defined, such as", "As you may have guessed, there are infinitely many of such events in all shapes and sizes, yet they are still countable and we can assign a probability to each of them [2,3].", "The further we look into the future, the more we can deviate from our current stock price. We might visualize this with a cone shape that expands over time (displayed below for t=50 and t=80). Within the cone, we can define infinitely many intervals. As before, we acquire a more detailed view once more time has passed.", "When encountering filtrations in any RL paper, the basics treated in this article should suffice. Essentially, the only purpose of introducing filtrations \u2131\u209c is to ensure that decisions x\u209c(\u03c9) do not utilize information that has not yet been revealed. When the Markov property holds, a decision x\u209c(S\u209c) that operates on the current state serves the same purpose. The filtration provides a rich description of the past, yet we do not need this information in memoryless problems. Nevertheless, from a mathematical perspective it is an elegant solution with many interesting applications. The reinforcement learning community consists of many researchers and engineers from different backgrounds working in a variety of domains, not everyone speaks the same language. Sometimes it goes a long way to learn another language, even if only a few words.", "[This article is partially based on my ArXiv article \u2018A Gentle Lecture Note on Filtrations in Reinforcement Learning\u2019]", "[2] Shreve, S. E. (2004). Stochastic Calculus for Finance II: Continuous-Time Models, Volume 11. Springer Science & Business Media.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Assistant professor in Financial Engineering and Operations Research. Writing about reinforcement learning, optimization problems, and data science."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F463c93a170d4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481----463c93a170d4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/nl/photos/kopje-koffie-filter-drinken-aroma-2139481/", "anchor_text": "pixel2013"}, {"url": "https://arxiv.org/abs/2008.02622", "anchor_text": "arXiv:2008.02622"}, {"url": "https://arxiv.org/abs/2002.06238", "anchor_text": "arXiv:2002.06238"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----463c93a170d4---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/financial-engineering?source=post_page-----463c93a170d4---------------financial_engineering-----------------", "anchor_text": "Financial Engineering"}, {"url": "https://medium.com/tag/data-science?source=post_page-----463c93a170d4---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----463c93a170d4---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/filtration?source=post_page-----463c93a170d4---------------filtration-----------------", "anchor_text": "Filtration"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----463c93a170d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----463c93a170d4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F463c93a170d4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----463c93a170d4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----463c93a170d4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----463c93a170d4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----463c93a170d4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----463c93a170d4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----463c93a170d4--------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://wvheeswijk.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33f45c9ab481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=post_page-33f45c9ab481--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd27145dcd242&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffiltrations-in-reinforcement-learning-what-they-are-and-why-we-dont-need-them-463c93a170d4&newsletterV3=33f45c9ab481&newsletterV3Id=d27145dcd242&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}