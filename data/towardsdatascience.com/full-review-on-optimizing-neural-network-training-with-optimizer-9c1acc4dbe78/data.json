{"url": "https://towardsdatascience.com/full-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78", "time": 1683009222.411072, "path": "towardsdatascience.com/full-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78/", "webpage": {"metadata": {"title": "Improve neural network with Optimizer | Towards Data Science", "h1": "Full review on optimizing neural network training with Optimizer", "description": "Selecting sufficient optimizer for the neural network is an efficient way to speed up the training speed and achieve optimal result"}, "outgoing_paragraph_urls": [{"url": "https://github.com/geniusnhu/DNN-Improvement/blob/master/Tuning_Optimizer.ipynb", "anchor_text": "here", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/speed-up-training-and-improve-performance-in-deep-neural-net-5732274d51a2", "anchor_text": "here", "paragraph_index": 20}], "all_paragraphs": ["Training a deep neural network is an extremely time-consuming task especially with complex problems. Using a faster optimizer for the network is an efficient way to speed up the training speed, rather than simply using the regular Gradient Descent optimizer. Below, I will discuss and show training results/speed of 5 popular Optimizer approaches: Gradient Descent with momentum and Nesterov Accelerated Gradient, AdaGrad, RMSProp, Adam, and Nadam optimization.", "One of the dangers of using inappropriate optimizers is that the model takes a long time to converge to a global minimum or it will be stuck at a local minimum, resulting in a worse model. Therefore, knowing which Optimizer suits mostly on the problem will save you tons of training hours.", "The main purpose of tuning Optimizer is to speed up the training speed but it also helps to improve the model\u2019s performance.", "Computing the gradient of the associated cost function with regard to each theta and getting the gradient vector pointing uphill, then going in the opposite direction with the vector direction (downhill) using the below equation:", "Therefore, the speed of Gradient Descent optimizer depends solely on the learning rate parameter (eta). With a small learning rate, GD will take small and unchanged steps downward on a gentle surface, and a bit faster steps on a steep surface. Consequently, in a large neural network, it repeats millions of slow steps until it reaches the global minimum (or gets lost in the local minimum). Therefore, the runtime becomes extremely slow.", "There are other versions of Gradient Descent such as Batch Gradient Descent (running on a full dataset), Mini-batch Gradient Descent (running on random subsets of a dataset), Stochastic Gradient Descent \u2014 SGD(picking a random instance at each step), and all have pros and cons. Batch Gradient Descent can reach the global minimum at a terribly slow pace. Mini-batch Gradient Descent gets to the global minimum faster than BGD but it is easier to get stuck in the local minimum, and SGD is usually harder to get to the global minimum compared to the other two.", "Let\u2019s imagine, when a ball rolls from the summit, downward the sloping side to the foot of the hill, it will start slowly then increase the speed as the momentum picks up and eventually reaches a fast pace toward the minimum. This is how Momentum Optimization works. This is enabled by adding a momentum vector m and update the theta parameter with this new weight from momentum vector (m)", "Gradient descent does not take into account the previous gradients. By adding the momentum vector, it updates the weight m after each iteration. The momentum \u03b2 is the parameter controls how fast the terminal velocity is, which is typically set at 0.9 but it should be tuned from 0.5 to 0.9. As a result, Momentum Optimizer converges better and faster than SGD.", "Another variation of Momentum Optimizer is NAG.", "The gradient of the cost function is measured at location \u03b8+\u03b2m (instead of \u03b8 in the original momentum optimization). The reason behind this is that momentum optimization has already pointed toward the right direction, so we should use a slightly ahead location (an approximately next position of the \u03b8) to moderately accelerating the speed of convergence.", "One of the Adaptive learning rate methods, in which the algorithm goes faster down the steep slopes than the gentle slopes. AdaGrad performs well in a simple quadratic problem but not in training a neural network because it tends to slow down a bit too fast and stops before reaching the global minimum. Due to this drawback, I do not usually use AdaGrad for Neural Network but instead apply RMSProp, an alternative of AdaGrad.", "This is one of the most frequently used optimizers, which continues the idea of Adagrad in trying to minimize the vertical movement and updating the model in a horizontal direction toward the global minimum.", "Adagrad sums the gradients from the first iteration and that is why it usually never converges to the global minimum, while RMSProp accumulates the gradients from the previous iterations:", "Adam optimizer is the combination of momentum and RMSProp optimizers. In other words, it takes into account both the exponential decay average of past gradients and the exponential decay average of past squared gradients.", "With these characteristics, Adam is suitable for handling sparse gradients on complex problems with complex data and a large number of features.", "Another variation of Adam is Nadam (using Adam optimizer with Nesterov technique), resulting in a little faster training time than Adam.", "Adagrad, RMSProp, Ada, Nadam, and Adamax are Adaptive learning rate algorithms, which require less tuning on hyperparameters. In case the performance of the model does not meet your expectation, you can try to change back to Momentum optimizer or Nesterov Accelerated Gradient", "In conclusion, most of the time, Adaptive learning rate algorithms outperform Gradient descent and its variants in terms of speed, especially in a deep neural network. However, Adaptive learning rate algorithms do not ensure an absolute convergence to the global minimum.", "If your model is not too complex with a small number of features, and training time is not your priority, using Momentum, Nesterov Accelerated Gradient or SGD is the optimal starting point, then tune the learning rate, activation functions, change Initialization technique to improve the model rather than using Adaptive learning rate Optimizers because the later ones hinder the risk of not converging to the global minimum.", "You can find source code here.", "A detailed explanation of improving Deep neural network performance and training speed by tuning Initialization, Activation functions, and Batch normalization is here.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9c1acc4dbe78&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@hoanganhquynhnhu?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hoanganhquynhnhu?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Nhu Hoang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd0309208c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&user=Nhu+Hoang&userId=fd0309208c7b&source=post_page-fd0309208c7b----9c1acc4dbe78---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@greg_rosenke", "anchor_text": "Greg Rosenke"}, {"url": "https://unsplash.com/photos/0vXelw6zOow", "anchor_text": "Upsplash"}, {"url": "https://github.com/zalandoresearch/fashion-mnist", "anchor_text": "Fashion MNIST"}, {"url": "https://github.com/geniusnhu/DNN-Improvement/blob/master/Tuning_Optimizer.ipynb", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/speed-up-training-and-improve-performance-in-deep-neural-net-5732274d51a2", "anchor_text": "here"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----9c1acc4dbe78---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/optimization?source=post_page-----9c1acc4dbe78---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/tag/tensorflow?source=post_page-----9c1acc4dbe78---------------tensorflow-----------------", "anchor_text": "TensorFlow"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----9c1acc4dbe78---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9c1acc4dbe78---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&user=Nhu+Hoang&userId=fd0309208c7b&source=-----9c1acc4dbe78---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&user=Nhu+Hoang&userId=fd0309208c7b&source=-----9c1acc4dbe78---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9c1acc4dbe78&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9c1acc4dbe78---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9c1acc4dbe78--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hoanganhquynhnhu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@hoanganhquynhnhu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nhu Hoang"}, {"url": "https://medium.com/@hoanganhquynhnhu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "74 Followers"}, {"url": "http://M.Sc", "anchor_text": "M.Sc"}, {"url": "http://linkedin.com/in/nhu-hoang/", "anchor_text": "linkedin.com/in/nhu-hoang/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffd0309208c7b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&user=Nhu+Hoang&userId=fd0309208c7b&source=post_page-fd0309208c7b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7bf4dd50274&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffull-review-on-optimizing-neural-network-training-with-optimizer-9c1acc4dbe78&newsletterV3=fd0309208c7b&newsletterV3Id=e7bf4dd50274&user=Nhu+Hoang&userId=fd0309208c7b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}