{"url": "https://towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978", "time": 1683005832.1211772, "path": "towardsdatascience.com/this-is-how-to-train-better-transformer-models-d54191299978/", "webpage": {"metadata": {"title": "This is how to train better transformer models | by Jonas Vetterle | Towards Data Science", "h1": "This is how to train better transformer models", "description": "For over 2 years now, transformer models, pretrained on large corpora of text, are the state-of-the-art in all things NLP. Researchers and practitioners continue to push boundaries by inventing\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2002.11794", "anchor_text": "Recent research by Berkeley Artificial Intelligence Research (BAIR)", "paragraph_index": 2}, {"url": "https://mlco2.github.io/impact/#compute", "anchor_text": "ML CO\u2082 Calculator", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/2002.11794", "anchor_text": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1910.09700", "anchor_text": "Quantifying the Carbon Emissions of Machine Learning", "paragraph_index": 23}, {"url": "https://www.climatechange.ai/NeurIPS2019_workshop.html", "anchor_text": "Climate Change AI workshop", "paragraph_index": 23}], "all_paragraphs": ["For over 2 years now, transformer models, pretrained on large corpora of text, are the state-of-the-art in all things NLP. Researchers and practitioners continue to push boundaries by inventing better architectures or training larger models on more data. Indeed few would disagree that, all else equal, training larger models on more data increases performance. But what if one is time- or resource-constrained?", "Common wisdom is to take the hit in accuracy, and train smaller models. Not only are smaller models faster to train and to do inference, it\u2019s also cheaper, right?", "Recent research by Berkeley Artificial Intelligence Research (BAIR)\u00b9 suggests this it not the case. Larger models train faster, and can be compressed more efficiently, thereby decreasing inference time. The authors therefore conclude that", "\u201cthe best strategy for resource- constrained training is to train large models and then heavily compress them\u201d", "The authors ran experiments with the following models:", "In each experiment, the authors vary the size of the model in terms of its depth (2\u201324 layers) and width (hidden size 128\u20132024).", "The main results are that larger models:", "1 are more sample-efficient: they obtain better results (lower perplexity on the language modelling task, and higher BLEU score on the translation task) after fewer gradient steps; and", "2 even after adjusting for wall-clock time, larger models train faster. That is, the decrease in training time more than offsets the increase in computational overhead from increasing model size.", "Not only do larger model train faster, they also predict faster. This is because they are more compressible, so you can trim them to the same inference cost as small models while achieving higher accuracy.", "To arrive at this result, the authors compress their models in 2 ways:", "1with both compression methods, larger models provide the better accuracy-efficiency trade off: the drop in accuracy when compressed is lower than for smaller models; and", "2both compression methods can be carried out with little additional computational overhead.", "While there is extensive literature about why bigger models obtain higher test accuracy, less research has been directed towards if and why they also converge faster. The authors offer some explanations for why this might be the case:", "1These results hold on large datasets (where overfitting is less of an issue). Empirically, larger models decrease the training error faster. And since for large datasets the generalization gap (difference between train and test error) is less of an issue, larger models also decrease the test error faster. The authors note that", "\u201cthe challenge in the MLM task is not overfitting, but instead, it is fitting the data \u2014 even 8 billion parameter models do not overfit to large pretraining corpora\u201d", "When decreasing the training dataset to 1% or 5% of its original size, the authors find that the benefit of training large models vanishes.", "2 Larger models use compute more efficiently. The bottleneck when training large models on tasks like Masked Language Modelling is not compute, but memory & storage. Therefore larger models more efficiently use the available compute.", "3Larger models obtain smaller compression errors. The authors show that the parameters resulting from quantization & pruning are closer to the parameters of the original (unpruned) models for large models than for small models.", "This paper constitutes a potential paradigm shift in how models are trained under resource constraints. This has obvious economic benefits: less time and money spent training models, while achieving higher performance.", "It should be noted that in all of the above experiments, the authors did not perform any hyperparameter optimization apart from choosing model depth and width. In a real world setting however good hyperparameters are unknown so a lot of time and resource is spent finding the best setting.", "The findings of this paper are important for another reason. Searching good hyperparameters, or simply just training the final model can take many GPU-hours and can therefore have a substantial carbon footprint.\u00b2 By reducing the time it takes to train models of a certain accuracy it might be possible to reduce emissions. Check out the ML CO\u2082 Calculator tool by the Mila lab if you are interested in finding out more about your carbon footprint.", "[1]: Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein and Joseph E. Gonzalez, Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers (2020).", "[2]: Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt and Thomas Dandres, Quantifying the Carbon Emissions of Machine Learning (2019), Climate Change AI workshop at NeurIPS 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine Learning Engineer. Be curious, stay optimistic."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd54191299978&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jonvet.medium.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": "Jonas Vetterle"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5f495e54b8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&user=Jonas+Vetterle&userId=f5f495e54b8b&source=post_page-f5f495e54b8b----d54191299978---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@samule?utm_source=medium&utm_medium=referral", "anchor_text": "Samule Sun"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/2002.11794", "anchor_text": "Recent research by Berkeley Artificial Intelligence Research (BAIR)"}, {"url": "https://arxiv.org/pdf/2002.11794.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "transformer"}, {"url": "https://arxiv.org/pdf/2002.11794.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.11794.pdf", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/2002.11794.pdf", "anchor_text": "Source"}, {"url": "https://mlco2.github.io/impact/#compute", "anchor_text": "ML CO\u2082 Calculator"}, {"url": "https://arxiv.org/abs/2002.11794", "anchor_text": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"}, {"url": "https://arxiv.org/abs/1910.09700", "anchor_text": "Quantifying the Carbon Emissions of Machine Learning"}, {"url": "https://www.climatechange.ai/NeurIPS2019_workshop.html", "anchor_text": "Climate Change AI workshop"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d54191299978---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d54191299978---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d54191299978---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/tech?source=post_page-----d54191299978---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d54191299978---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&user=Jonas+Vetterle&userId=f5f495e54b8b&source=-----d54191299978---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&user=Jonas+Vetterle&userId=f5f495e54b8b&source=-----d54191299978---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd54191299978&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d54191299978---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d54191299978--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d54191299978--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d54191299978--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d54191299978--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d54191299978--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d54191299978--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d54191299978--------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jonvet.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jonas Vetterle"}, {"url": "https://jonvet.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "37 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff5f495e54b8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&user=Jonas+Vetterle&userId=f5f495e54b8b&source=post_page-f5f495e54b8b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff5f495e54b8b%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthis-is-how-to-train-better-transformer-models-d54191299978&user=Jonas+Vetterle&userId=f5f495e54b8b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}