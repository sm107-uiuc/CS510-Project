{"url": "https://towardsdatascience.com/on-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9", "time": 1682994137.2178292, "path": "towardsdatascience.com/on-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9/", "webpage": {"metadata": {"title": "On integrating symbolic inference into deep neural networks | by Lukas Molzberger | Towards Data Science", "h1": "On integrating symbolic inference into deep neural networks", "description": "Deep neural networks have been a tremendous success story over the last couple of years. Many advances in the field of AI, such as recognizing real world objects, fluently translating natural\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1711.03902.pdf", "anchor_text": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1801.00631.pdf", "anchor_text": "Deep Learning: A Critical Appraisal", "paragraph_index": 12}], "all_paragraphs": ["Deep neural networks have been a tremendous success story over the last couple of years. Many advances in the field of AI, such as recognizing real world objects, fluently translating natural language or playing GO at a world class level, are based on deep neural networks. However, there were only few reports concerning the limitations of this approach. One such limitation is the inability to learn from a small amount of examples. Deep neural networks usually require a huge amount of training examples, whereas humans are able to learn from one single example. If you show a cat to a child who has never seen one before, it can recognize another cat based on this single instance. Deep neural networks on the other hand require hundreds of thousands of images to learn what a cat looks like. Another limitation is the inability to make inferences based on previously learned common knowledge. When reading a text, humans tend to derive wide ranging inferences about possible interpretations of the text. Humans can do this because they can recall knowledge from very different domains and apply it to the text.", "These limitations indicate that something fundamental is missing in deep neural networks. This something is the ability to establish symbolic references to entities in the real world and to put them in relation to each other. Symbolic inference in form of formal logic has been at the core of classic AI for decades, but it has proven to be brittle and complex to work with. Nevertheless is there no way to enhance deep neural networks so that they would become capable of processing symbolic information? Deep neural networks have been inspired by biological neural networks like the human brain. Essentially they are a simplified model of the neurons and synapses that are the basic building blocks of the brain. One such simplification is the omission of the spiking nature of biological neural networks. But what if it is not only important to actually activate a neuron, but also when this neuron is exactly activated. What if the point in time, at which a neuron fires, establishes a relational context to which this activation refers to. Take, for example, a neuron that stands for a particular word. Would it not make sense if that neuron would be triggered every time the word appears in a text? In this case the timing of the spikes would play an important role. And, not only the timing of a single activation, but the timing of all incoming spikes of a neuron relative to each other would be crucial. This timing pattern might be used to establish a relation between these input activations. For example, if a neuron representing a particular word has an input synapse for each letter in this word, it is important that the word neuron is only triggered when the letter neurons have been fired in the right order to each other. Conceptionally these timing differences could be modeled as relations between the input synapses of a neuron. These relations also define the point in time at which the neuron itself fires relative to its input activations. For practical reasons it might be useful to allow the activation of a neuron to have several slots, like the beginning and the end of a word, associated to it. Otherwise the beginning and the end of a word would have to be modeled as two separate neurons. These relations are a very powerful concept. They allow to easily capture the hierarchical structure of text or to relate different ranges within a text to each other. In this case a neuron might refer to a very local information, like a letter, or a very wide ranging information, like the topic of a text.", "Using an activation function is to approximate the firing rate of an individual neuron is another simplification with regard to biological neural networks. For this purpose classical neural networks use the sigmoid function. However, the sigmoid function is symmetric with respect to large positive or negative input values, which makes it very difficult to model logic gate-like operations with neurons using this function. Spiking networks on the other hand, have a clear threshold and ignore all input signals that remain below this threshold. Therefore, the ReLU function or some other asymmetric function could be a better approximation for the firing rate. This asymmetry is also essential for neurons that process relational information. The neuron representing a particular word must always remain completely inactive for all times when the word does not occur.", "Moreover the fact that different types of neurons occur in the cerebral cortex has been neglected in deep neural networks. Two important types are the spiny pyramidal cell, which primarily has an excitatory characteristic, and the aspiny stellate cell, which has an inhibitory one. The inhibitory neurons are special because they allow to build negative feedback loops. These feedback loops are not normally found in a deep neural network because they introduce an inner state to the network. Consider the following network with an inhibitory neuron and two excitatory neurons, representing two different meanings of the word \u2018August\u2019.", "Both meanings are mutually exclusive, meaning that the network now has two stable states. These states may depend on further input synapses of the two excitatory neurons. For example, if the next word after the word \u2018August\u2019 is a potential last name, a corresponding input synapse for the entity neuron August-(first name) could increase the weight of that state. It is now more likely that the word \u2018August\u2019 will be classified as a first name and not a month. But keep in mind that both states need to be evaluated. In larger networks, many neurons may be connected by negative or positive feedback loops, potentially creating a great number of stable states within the network.", "For this reason the network requires an efficient optimization process which determines the best state with regard to some objective function. This objective function could be to minimize the need to suppress strongly activated neurons. However, these states have the tremendous advantage that they allow to consider different interpretations of a given text. It is comparable with a thought process in which different interpretations are evaluated and where the best fitting is the result. Fortunately, the search for an optimal solution state can be optimized quite well.", "We need inhibitory neurons in these feedback loops, because otherwise all mutually suppressive neurons would have to be fully connected to each other. That would lead to a quadratically increasing number of synapses.", "Through the negative feedback loops, that is, by simply connecting a negative synapse to one of its precursor neurons, we have suddenly entered the field of nonmonotonic logic. Nonmonotonic logic is a subfield of formal logic in which implications are not only added to a model but also removed. The assumption is that nonmonotonic logic is needed in order to draw conclusions for many common sense reasoning tasks. One of the main problems of nonmonotonic logic is that it often can not decide which conclusions to draw and which not. It should only draw skeptical or credulous inferences if no other conclusions are more likely. This is where the weighted nature of neural networks comes in handy. Here, more likely states can suppress less likely states.", "Although deep neural networks have come a long way and are now delivering impressive results, it may be worth taking another look at the original, the human brain and its circuitry. If such an inherently complex structure as the human brain is to be used as a blueprint for a neural model, we must make simplifying assumptions. But, this must be done with great care otherwise important aspects of the original may be lost.", "Mark F. Bear, Barry W. Connors, Michael A. Paradiso", "5. Neural-Symbolic Learning and Reasoning: A Survey and Interpretation", "Tarek R. Besold, Artur d\u2019Avila Garcez, Sebastian Bader; Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Kuehnberger, Luis C. Lamb, ; Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, Gerson Zaverucha", "6. Deep Learning: A Critical Appraisal", "Gerhard Brewka, Ilkka Niemela, Miros\u0142aw Truszczynski", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Neural network tinkerer with a background in symbolic AI. Employed @meinestadt.de GmbH in Cologne."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F22ed13ebbba9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://lukasmolzberger.medium.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": ""}, {"url": "https://lukasmolzberger.medium.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Lukas Molzberger"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf39222d9fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&user=Lukas+Molzberger&userId=bf39222d9fe&source=post_page-bf39222d9fe----22ed13ebbba9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://aika.network", "anchor_text": "The Aika Project"}, {"url": "https://towardsdatascience.com/using-meta-neurons-to-learn-facts-from-a-single-training-example-781ca0b7424d", "anchor_text": "Using Meta-Neurons to learn facts from a single training example"}, {"url": "https://towardsdatascience.com/on-adding-negative-recurrent-synapses-to-a-neural-network-25a28409a6f2", "anchor_text": "On adding negative feedback synapses to a neural network"}, {"url": "https://arxiv.org/pdf/1711.03902.pdf", "anchor_text": "Neural-Symbolic Learning and Reasoning: A Survey and Interpretation"}, {"url": "https://arxiv.org/pdf/1801.00631.pdf", "anchor_text": "Deep Learning: A Critical Appraisal"}, {"url": "http://www.informatik.uni-leipzig.de/~brewka/papers/NMchapter.pdf", "anchor_text": "Nonmonotonic Reasoning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----22ed13ebbba9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----22ed13ebbba9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----22ed13ebbba9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://creativecommons.org/licenses/by-nd/4.0/", "anchor_text": "Some rights reserved"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&user=Lukas+Molzberger&userId=bf39222d9fe&source=-----22ed13ebbba9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&user=Lukas+Molzberger&userId=bf39222d9fe&source=-----22ed13ebbba9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F22ed13ebbba9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----22ed13ebbba9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----22ed13ebbba9--------------------------------", "anchor_text": ""}, {"url": "https://lukasmolzberger.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://lukasmolzberger.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Lukas Molzberger"}, {"url": "https://lukasmolzberger.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbf39222d9fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&user=Lukas+Molzberger&userId=bf39222d9fe&source=post_page-bf39222d9fe--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fbf39222d9fe%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fon-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9&user=Lukas+Molzberger&userId=bf39222d9fe&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}