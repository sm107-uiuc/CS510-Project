{"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "time": 1683008151.938733, "path": "towardsdatascience.com/deep-learning-basics-1d26923cc24a/", "webpage": {"metadata": {"title": "Deep Learning Basics. Basic Concepts for Deep Reinforcement\u2026 | by Jordi TORRES.AI | Towards Data Science", "h1": "Deep Learning Basics", "description": "Main concepts of neural networks to allow the reader to understand Deep Learning basics to use it to program an Agent in a Reinforcement Learning problem."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/learning-process-of-a-deep-neural-network-5a9768d7a651", "anchor_text": "Learning Process of a Deep Neural Network", "paragraph_index": 47}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "the next post", "paragraph_index": 52}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "next post", "paragraph_index": 53}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 54}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 54}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 55}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 56}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 59}], "all_paragraphs": ["This is the third post in the series \u201cDeep Reinforcement Learning Explained\u201d . If you have previous knowledge of Deep Learning you can skip this post and go for the following one.", "Or you can read this post if you are interested in getting into the basics of Deep Learning and want to continue with PyTorch in the next post.", "In this post, I will review the main concepts of neural networks to allow the reader to understand Deep Learning basics in order to use it to program an Agent in a Reinforcement Learning problem. In order to ease the explanation, I will base the post in an example that will help to get introduced in the theoretical concepts.", "As a case study, we will create a mathematical model that allows us to identify handwritten digits such as the following ones:", "The goal will be to create a neural network that, given an image, the model identifies the number it represents. For example, if we feed to the model the first image, we would expect it to answer that it is a 5. The next one a 0, next one a 4, an so on.", "Actually, we are dealing with a classification problem, which given an image, the model classifies it between 0 and 9. But sometimes, even we can find ourselves with certain doubts, for example, the first image represents a 5 or a 3?", "For this purpose, the neural network that we will create returns a vector with 10 positions indicating the likelihood of each of the ten possible digits:", "In the next post, we will explain how to code this example using PyTorch. At the moment only mention that we will use the MNIST dataset which contains 60,000 images of hand-made digits to train the model. This dataset of black and white images (images contain gray levels) has been normalized to 28\u00d728 pixels.", "To facilitate the ingest of data into our basic neural network we will make a transformation of the input (image) from 2 dimensions (2D) to a vector of 1 dimension (1D). That is, the matrix of 28\u00d728 numbers can be represented by a vector (array) of 784 numbers (concatenating row by row), which is the format that accepts as input a densely connected neural network like the one we will see in this post.", "We need to represent each label with a vector of 10 positions as we presented before, where the position corresponding to the digit that represents the image contains a 1 and the rest contains 0s. This process of transforming the labels into a vector of as many zeros as the number of different labels, and putting a 1 in the index corresponding to the label, is known as one-hot encoding. For example, the number 7 will be encoded as:", "Now we are ready to start explain the minimum set of basic neural network concepts.", "In order to show how a basic neuronal is, let\u2019s suppose a simple example where we have a set of points in a two-dimensional plane and each point is already labeled \u201csquare\u201d or \u201ccircle\u201d:", "Given a new point \u201c\u00d7\u201c, we want to know what label corresponds to it:", "A common approach is to draw a line that separates the two groups and use this line as a classifier:", "In this case, the input data will be represented by vectors of the form (x1, x2) that indicate their coordinates in this two-dimensional space, and our function will return \u20180\u2019 or \u20181\u2019 (above or below the line) to know if it should be classified as \u201csquare\u201d or \u201ccircle\u201d. It can be defined by", "More generally, we can express the line as:", "To classify input elements X, which in our case are two-dimensional, we must learn a vector of weight W of the same dimension as the input vectors,that is, the vector (w1, w2) and a b bias.", "With these calculated values, we can now construct an artificial neuron to classify a new element X. Basically, the neuron applies this vector W of calculated weights on the values in each dimension of the input element X, and at the end adds the bias b. The result of this will be passed through a non-linear \u201cactivation\u201d function to produce a result of \u20180\u2019 or \u20181\u2019. The function of this artificial neuron that we have just defined can be expressed in a more formal way such as", "Now, we will need a function that applies a transformation to variable z so that it becomes \u20180\u2019 or \u20181\u2019. Although there are several functions ( \u201cactivation functions\u201d), for this example we will use one known as a sigmoid function that returns an actual output value between 0 and 1 for any input value:", "If we analyze the previous formula, we can see that it always tends to give values close to 0 or 1. If the input z is reasonably large and positive, \u201ce\u201d at minus z is zero and, therefore, the y takes the value of 1. If z has a large and negative value, it turns out that for \u201ce\u201d raised to a large positive number, the denominator of the formula will turn out to be a large number and therefore the value of y will be close to 0. Graphically, the sigmoid function presents this form:", "So far we have presented how to define an artificial neuron, the simplest architecture that a neural network can have. In particular, this architecture is named in the literature of the subject as Perceptron, invented in 1957 by Frank Rosenblatt, and visually summarized in a general way with the following scheme:", "A simplified (two) visual representation of the previous neuron (that we will use) can be:", "In the literature of the area, we refer to a Multi-Layer Perceptron (MLP) when we find neural networks that have an input layer, one or more layers composed of perceptrons, called hidden layers and a final layer with several perceptrons called the output layer. In general, we refer to Deep Learning when the model based on neural networks is composed of multiple hidden layers. Visually it can be presented with the following scheme:", "MLPs are often used for classification, and specifically when classes are exclusive, as in the case of the classification of digit images (in classes from 0 to 9). In this case, the output layer returns the probability of belonging to each one of the classes, thanks to a function called softmax. Visually we could represent it in the following way:", "As we mentioned, there are several activation functions in addition to the sigmoid, each with different properties. One of them is the one we just mentioned, the softmax activation function, which will be useful to present an example of simple neural network to classify in more than two classes. For the moment we can consider the softmax function as a generalization of the sigmoid function that allows us to classify more than two classes.", "We will solve the problem in a way that, given an input image, we will obtain the probabilities that it is each of the 10 possible digits. In this way, we will have a model that, for example, could predict a five in an image, but only being sure in 70% that it is a five. Due to the stroke of the upper part of the number in this image, it seems that it could become a three in a 20% chance and it could even give a certain probability to any other number. Although in this particular case we will consider that the prediction of our model is a five since it is the one with the highest probability, this approach of using a probability distribution can give us a better idea of how confident we are of our prediction. This is good in this case, where the numbers are made by hand, and surely in many of them, we cannot recognize the digits with 100% certainty.", "Therefore, for this example of classification, we will obtain, for each input example, an output vector with the probability distribution over a set of mutually exclusive labels. That is, a vector of 10 probabilities each corresponding to a digit and also the sum of all these 10 probabilities results in the value of 1 (the probabilities will be expressed between 0 and 1).", "As we have already advanced, this is achieved through the use of an output layer in our neural network with the softmax activation function, in which each neuron in this softmax layer depends on the outputs of all the other neurons in the layer, since that the sum of the output of all of them must be 1.", "But how does the softmax activation function work? The softmax function is based on calculating \u201cthe evidence\u201d that a certain image belongs to a particular class and then these pieces of evidence are converted into probabilities that it belongs to each of the possible classes.", "An approach to measure the evidence that a certain image belongs to a particular class is to make a weighted sum of the evidence of belonging to each of its pixels to that class. To explain the idea, I will use a visual example.", "Let\u2019s suppose that we already have the model learned for the number zero. For the moment, we can consider a model as \u201csomething\u201d that contains information to know if a number is of a certain class. In this case, for the number zero, suppose we have a model like the one presented below:", "In this case, with a matrix of 28\u00d728 pixels, where the pixels in red represent negative weights (i.e., reduce the evidence that it belongs), while that the pixels in blue represent positive weights (the evidence of which is greater increases). The white color represents the neutral value.", "Let\u2019s imagine that we trace a zero over it. In general, the trace of our zero would fall on the blue zone (remember that we are talking about images that have been normalized to 20\u00d720 pixels and later centered on a 28\u00d728 image). It is quite evident that if our stroke goes over the red zone, it is most likely that we are not writing a zero; therefore, using a metric based on adding if we pass through the blue zone and subtracting if we pass through the red zone seems reasonable.", "To confirm that it is a good metric, let\u2019s imagine now that we draw a three; it is clear that the red zone of the center of the previous model that we used for the zero will penalize the aforementioned metric since, as we can see in the left part of the following figure, when writing a three we pass over:", "But on the other hand, if the reference model is the one corresponding to number 3 as shown in the right part of the previous figure, we can see that, in general, the different possible traces that represent a three are mostly maintained in the blue zone.", "I hope that the reader, seeing this visual example, already intuits how the approximation of the weights indicated above allows us to estimate what number it is.", "Once the evidence of belonging to each of the 10 classes has been calculated, these must be converted into probabilities whose sum of all their components add 1. For this, softmax uses the exponential value of the calculated evidence and then normalizes them so that the sum equates to one, forming a probability distribution. The probability of belonging to class i is:", "Intuitively, the effect obtained with the use of exponentials is that one more unit of evidence has a multiplier effect and one unit less has the inverse effect. The interesting thing about this function is that a good prediction will have a single entry in the vector with a value close to 1, while the remaining entries will be close to 0. In a weak prediction, there will be several possible labels, which will have more or less the same probability.", "For this example, we will define a very simple neural network as a sequence of two layers. Visually we could represent it in the following way:", "In the visual representation, we explicitly express that we have 784 input features of the model (28\u00d728). The first layer of 10 neurons with sigmoid activation function, \u201cdistills\u201d the input data to obtain the desired 10 outputs required as a input in the next layer. The second layer will be a softmax layer of 10 neurons, which means that it will return a matrix of 10 probability values representing the 10 possible digits (as we presented before, where each value will be the probability that the image of the current digit belongs to each one of them).", "Training our neural network, that is, learning the values of our parameters (weights W and b biases) is the most genuine part of Deep Learning and we can see this learning process in a neural network as an iterative process of \u201cgoing and return\u201d by the layers of neurons. The \u201cgoing\u201d is a forward-propagation of the information and the \u201creturn\u201d is a back-propagation of the information.", "The first phase forwardpropagation occurs when the network is exposed to the training data and these cross the entire neural network for their predictions (labels) to be calculated. That is, passing the input data through the network in such a way that all the neurons apply their transformation to the information they receive from the neurons of the previous layer and sending it to the neurons of the next layer. When the data has crossed all the layers, and all its neurons have made their calculations, the final layer will be reached with a result of label prediction for those input examples.", "Next, we will use a loss function to estimate the loss (or error) and to compare and measure how good/bad our prediction result was in relation to the correct result (remember that we are in a supervised learning environment and we have the label that tells us the expected value). Ideally, we want our cost to be zero, that is, without divergence between estimated and expected value. Therefore, as the model is being trained, the weights of the interconnections of the neurons will gradually be adjusted until good predictions are obtained.", "Once the loss has been calculated, this information is propagated backwards. Hence, its name: backpropagation. Starting from the output layer, that loss information propagates to all the neurons in the hidden layer that contribute directly to the output. However, the neurons of the hidden layer only receive a fraction of the total signal of the loss, based on the relative contribution that each neuron has contributed to the original output. This process is repeated, layer by layer, until all the neurons in the network have received a loss signal that describes their relative contribution to the total loss. Now that we have spread this information back, we can adjust the weights of connections between neurons.", "Visually, we can summarize what we have explained with this visual scheme of the stages (based on the previous visual representation of our neural network):", "What we are doing is making the loss as close as possible to zero the next time we go back to using the network for a prediction.", "In general, we can see the learning process as a global optimization problem where the parameters (weights and biases) must be adjusted in such a way that the loss function presented above is minimized. In most cases, these parameters cannot be solved analytically, but in general they can be approached well with an optimizer (iterative optimizing algorithms), such as a technique called gradient descent. This technique changes the weights in small increments with the help of the calculation of the derivative (or gradient) of the loss function, which allows us to see in which direction \u201cto descend\u201d towards the global minimum; this is done in general in batches of data in the successive iterations (epochs) of all the dataset that we pass to the network in each iteration.", "The reader can visit the post Learning Process of a Deep Neural Network for more details about this training loop, although for the purpose of this section I do not think it is necessary.", "We can choose from a a wide range of loss functions for our neural network model. For instance, sure that the reader knows the Mean Squared Error (MSE) loss function commonly used for regression. For classification as the presented in this post, the loss function that is usually used is Cross-Entropy, which allows measure the difference between two probability distributions.", "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Both are slightly different depending on context, but in Deep Learning when calculating error rates between 0 and 1 they resolve to the same thing.", "Cross-entropy loss increases as the predicted probability diverges from the actual label. A perfect model would have a log loss of 0. In binary classification, where the number of classes are 2, cross-entropy can be calculated as:", "In our example, a multiclass classification, we calculate a separate loss for each class label per observation and sum the result:", "In this post, I reviewed the main concepts of neural networks to allow the reader to understand Deep Learning basics in order to use it to program an Agent in a Reinforcement Learning problem. In the next post we will program the example presented in this post using PyTorch, and we will introduce the reader to the basics features of PyTorch, the framework that we will use in this series of posts.", "See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1d26923cc24a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----1d26923cc24a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 03"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci\u00f3n-de-bellman-7b0ebfac2be1", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1", "anchor_text": "3. Funciones de valor y la ecuaci\u00f3n de BellmanAcceso abierto al cap\u00edtulo 3 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/learning-process-of-a-deep-neural-network-5a9768d7a651", "anchor_text": "Learning Process of a Deep Neural Network"}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "the next post"}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----1d26923cc24a---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1d26923cc24a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1d26923cc24a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1d26923cc24a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1d26923cc24a---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----1d26923cc24a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----1d26923cc24a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1d26923cc24a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1d26923cc24a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1d26923cc24a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1d26923cc24a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1d26923cc24a--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-basics-1d26923cc24a&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}