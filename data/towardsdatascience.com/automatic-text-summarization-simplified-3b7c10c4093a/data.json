{"url": "https://towardsdatascience.com/automatic-text-summarization-simplified-3b7c10c4093a", "time": 1682996636.116981, "path": "towardsdatascience.com/automatic-text-summarization-simplified-3b7c10c4093a/", "webpage": {"metadata": {"title": "Automatic Text Summarization : Simplified | by Prakhar Ganesh | Towards Data Science", "h1": "Automatic Text Summarization : Simplified", "description": "In a world where internet is getting exploded with a hulking amount of data every day, being able to automatically summarize is an important challenge. Summaries of long documents, news articles, or\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Judging a book by its cover is not the way to go.. but I guess a summary should do just fine.", "In a world where internet is getting exploded with a hulking amount of data every day, being able to automatically summarize is an important challenge. Summaries of long documents, news articles, or even conversations can help us consume content faster and more efficiently. Automatic Text Summarization is a growing field in NLP and has been getting a lot of attention in the last few years.", "I will not be discussing specific details of any algorithm or\u00a0implementation. This blog is for the curious few who would like to gain a deeper understanding of how these Text Summarization models work. Anyone with absolutely no previous experience in Deep Learning or NLP can superficially follow the blog. Even a rudimentary understanding of commonly used NLP models is enough to fully appreciate the details.", "There are 2 types of text summarization methods, namely extractive and abstractive. Extractive summarization is essentially picking out sentences from the text that can best represent its summary. Extractive summarization techniques have been prevalent for quite some time now, owing to its origin in 1950s. It\u2019s more about learning to understand the importance of each sentence and their relations with each other rather than trying to understand the content of the text.", "Abstractive summarization, on the other hand, is all about trying to understand the content of the text and then providing a summary based on that, which may or may not have the same sentences as present in the original text. Abstractive summarization tries to create its own sentences and is definitely a step towards more human-like summaries.", "The techniques employed to do extractive and abstractive summarization are miles apart from each other. As mentioned earlier, extractive summarization is, crudely speaking, a sentence ranking problem while abstractive summarization involves more complex linguistic models as it generates new sentences.", "I personally believe extractive summarization has run it\u2019s course and now most of the research focus is towards abstractive summarization, which is actually a way more interesting problem (again.. just my opinion!!). So I won\u2019t be talking about extractive summarization, but if you are still interested in reading about it, I would suggest this awesome blog.", "In the last few years, since the arrival of Deep Learning, Abstractive Summarization, Interaction with machines through natural language and Machine Translation have all been getting a lot of success. I have mentioned Machine Translation and interaction here because of the parallelism it follows with Abstractive Summarization. All of these techniques encode an input sentence into features and then tries to generate a different sentence i.e. decode these features.", "A commonly used Deep Learning based Machine Translation model is an LSTM based Encoder Decoder network with Attention. There have been various successful variations of this skeleton, each with their own pros and cons.", "The model starts with an LSTM based Encoder which converts the sentence into a vector of features. The decoder, also made up of an LSTM, is responsible for creating the output, one word at a time. The decoder starts with the vector of features provided by the encoder and then each word is predicted based on the previous word prediction and LSTM output. Attention is placed on the encoder features to make them even more specific to the current word.", "A detailed explanation of the working of an LSTM based Encoder Decoder network with Attention can be found in this blog.", "Unfortunately, no!Generating new sentences is a complex process that the machines have not mastered yet. An issue with Abstractive Summarization is also the length of sentences to be encoded. While LSTMs have the ability to be able to capture both long term and short term contexts, even they have a limit of what can be considered long term. This makes summarizing really long documents difficult.", "Another astronomically important issue for summaries is that it should never contain facts that contradict the input text. Extractive summarization can never face this problem since they pick up sentences directly from the text. But abstractive summarization are prone to such factual incoherence.", "For example, if an abstractive summarization model saw sentences like Germany 3\u20132 France, England 3\u20132 Portugal etc. while training then at testing time, it might predict Spain 3\u20132 Brazil even if the actual score in the input text is, say 1\u20132. That\u2019s because 1\u20132 is not a part of the model\u2019s vocabulary, but 3\u20132 is.", "Extractive summarization suffer with the lack of ability to create their own sentences while abstractive summarization fail against the complexity to create complete sentences on their own. A very creative way from the middle of these two extremes was recently proposed in a network called \u2018Pointer Generator Network\u2019.", "In simpler terms, the authors created the network in such a way that it suggested two different probability distributions on what the next predicted word should be. The first one was based on the model\u2019s vocabulary while the second one based on the vocabulary present in the input text. These two were then combined to get the final distribution. You can read about the model in further detail here.", "If you look at some of the state-of-the-art results of abstractive summarization, you will find that they are doing a respectably good job. But they only work for certain type of documents and fail spectacularly for others. One of the biggest challenges at this point is to be able to create grammatically coherent sentences from encoded features, which is a core part of both abstractive summarization and machine translation.", "This blog is a part of an effort to create simplified introductions to the field of Machine Learning. Follow the complete series here", "Or simply read the next blog in the series", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Computer Vision and Deep Learning enthusiast"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3b7c10c4093a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@prakhargannu?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Prakhar Ganesh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F318f2765b461&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&user=Prakhar+Ganesh&userId=318f2765b461&source=post_page-318f2765b461----3b7c10c4093a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/sciforce/towards-automatic-text-summarization-extractive-methods-e8439cd54715", "anchor_text": "Towards Automatic Text Summarization: Extractive MethodsFor those who had academic writing, summarization \u2014 the task of producing a concise and fluent summary while preserving\u2026medium.com"}, {"url": "https://towardsdatascience.com/attention-based-neural-machine-translation-b5d129742e2c", "anchor_text": "Attention-based Neural Machine TranslationAttention mechanisms are being increasingly used to improve the performance of Neural Machine Translation (NMT) by\u2026towardsdatascience.com"}, {"url": "http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html", "anchor_text": "Taming Recurrent Neural Networks for Better SummarizationThis is a blog post about our latest paper, Get To The Point: Summarization with Pointer-Generator Networks, to appear\u2026www.abigailsee.com"}, {"url": "https://medium.com/@prakhargannu/machine-learning-simplified-1fe22fec0fac", "anchor_text": "Machine Learning : SimplifiedKnow it before you dive inmedium.com"}, {"url": "https://towardsdatascience.com/automated-lip-reading-simplified-c01789469dd8", "anchor_text": "Automated Lip Reading : SimplifiedTake a peek into the world of Automated Lip Reading (ALR)towardsdatascience.com"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----3b7c10c4093a---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/text-summarization?source=post_page-----3b7c10c4093a---------------text_summarization-----------------", "anchor_text": "Text Summarization"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3b7c10c4093a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3b7c10c4093a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/surveys?source=post_page-----3b7c10c4093a---------------surveys-----------------", "anchor_text": "Surveys"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&user=Prakhar+Ganesh&userId=318f2765b461&source=-----3b7c10c4093a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&user=Prakhar+Ganesh&userId=318f2765b461&source=-----3b7c10c4093a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3b7c10c4093a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3b7c10c4093a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3b7c10c4093a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@prakhargannu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prakhar Ganesh"}, {"url": "https://medium.com/@prakhargannu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "649 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F318f2765b461&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&user=Prakhar+Ganesh&userId=318f2765b461&source=post_page-318f2765b461--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6fd6e4d21dc5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fautomatic-text-summarization-simplified-3b7c10c4093a&newsletterV3=318f2765b461&newsletterV3Id=6fd6e4d21dc5&user=Prakhar+Ganesh&userId=318f2765b461&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}