{"url": "https://towardsdatascience.com/multilingual-transformers-ae917b36034d", "time": 1683002994.424808, "path": "towardsdatascience.com/multilingual-transformers-ae917b36034d/", "webpage": {"metadata": {"title": "Multilingual Transformers. Why BERT is not the best choice for\u2026 | by Simone Romano | Towards Data Science", "h1": "Multilingual Transformers", "description": "Last year, we saw rapid improvements in transformer architectures. Being the GLUE benchmark the main reference point for the state-of-the-art in language understanding tasks, most of the research\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gluebenchmark.com/", "anchor_text": "GLUE benchmark", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8", "anchor_text": "BERT, RoBERTa, DistilBERT, XLNet \u2014 which one to use?", "paragraph_index": 0}, {"url": "https://www.nyu.edu/projects/bowman/xnli/", "anchor_text": "cross-lingual Natural Language Inference (XNLI) corpus", "paragraph_index": 2}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 8}, {"url": "https://openreview.net/forum?id=SyxS0T4tvS", "anchor_text": "accepted at ICLR 2020", "paragraph_index": 8}, {"url": "https://openreview.net/forum?id=H1eA7AEtvS", "anchor_text": "ALBERT", "paragraph_index": 9}, {"url": "https://openreview.net/forum?id=BJgQ4lSFPH", "anchor_text": "ALICE", "paragraph_index": 9}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace Transformers code", "paragraph_index": 11}, {"url": "https://github.com/huggingface/transformers/blob/master/LICENSE", "anchor_text": "Apache License 2.0", "paragraph_index": 11}], "all_paragraphs": ["Last year, we saw rapid improvements in transformer architectures. Being the GLUE benchmark the main reference point for the state-of-the-art in language understanding tasks, most of the research efforts focused on English data. BERT, RoBERTa, DistilBERT, XLNet \u2014 which one to use? provides an overview of recent transformer architectures and their pros and cons.", "It is challenging to keep track of the GLUE leader board because the progress on language understanding tasks is so fast-paced. Every month a different team takes the top position.", "At the same time, transformer architectures have been applied to multilingual tasks. To evaluate these tasks, the approaches discussed here use the cross-lingual Natural Language Inference (XNLI) corpus consisting of labelled sentences in 15 languages. Each data point consists of a Premise and a Hypothesis. Premises and Hypotheses have been labelled for textual entailment: i.e. how the Hypothesis is related to the Premise.", "Some other examples of labels are \u201ccontradictory, neutral\u201d.", "Currently, there is no agreed benchmark on multilingual understanding tasks. The XNLI data set seems to be the main reference to keep track of the evolution of multilingual models. In this note, it is presented a brief overview of the evolution of multilingual transformers for multilingual language understanding.", "Very soon after proposing BERT, Google research introduced a multilingual version of BERT capable of working with more than 100 languages.", "Pre-trained on 4 to 16 Cloud TPUs.", "This model was proposed by researchers at Facebook in the beginning of 2019.", "Researchers in Facebook proposed this model at the end of 2019 following the steps of RoBERTa. As for RoBERTa, the main contributions are about choosing a better training setup. (As a side note for RoBERTa, even if this pushes performance up, was not considered to provide enough technical contribution to be accepted at ICLR 2020).", "XLM-R seems to be the best solution to date. It is very possible that the TLM (Translation Language Model) approach to train multilingual transformers will be combined with other technologies. In particular, it is easy to foresee a combination of technologies at the top of the GLUE leader board and TLM. In the machine learning community, there is still much interest in transformers. For example, ALBERT and ALICE have been recently accepted at ICLR 2020.", "The multilingual transformers discussed here can be found pre-trained in Google\u2019s and Facebook\u2019s repository, respectively:", "All the models can also be very easily tested out using HuggingFace Transformers code. Written in PyTorch. License: Apache License 2.0", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fae917b36034d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ialuronico.medium.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": ""}, {"url": "https://ialuronico.medium.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Simone Romano"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17605fb37de1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&user=Simone+Romano&userId=17605fb37de1&source=post_page-17605fb37de1----ae917b36034d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://translatr.varunmalhotra.xyz/", "anchor_text": "https://translatr.varunmalhotra.xyz/"}, {"url": "https://www.wordclouds.com/", "anchor_text": "https://www.wordclouds.com/"}, {"url": "https://gluebenchmark.com/", "anchor_text": "GLUE benchmark"}, {"url": "https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8", "anchor_text": "BERT, RoBERTa, DistilBERT, XLNet \u2014 which one to use?"}, {"url": "https://gluebenchmark.com/leaderboard/", "anchor_text": "https://gluebenchmark.com/leaderboard/"}, {"url": "https://www.nyu.edu/projects/bowman/xnli/", "anchor_text": "cross-lingual Natural Language Inference (XNLI) corpus"}, {"url": "https://arxiv.org/pdf/1809.05053.pdf", "anchor_text": "paper"}, {"url": "https://www.nyu.edu/projects/bowman/xnli/", "anchor_text": "Cross-Lingual NLI Copus (XNLI)"}, {"url": "https://github.com/google-research/bert/blob/master/multilingual.md", "anchor_text": "link"}, {"url": "https://arxiv.org/abs/1906.01502", "anchor_text": "How multilingual is Multilingual BERT?"}, {"url": "https://openreview.net/forum?id=HJeT3yrtDr", "anchor_text": "Cross-Lingual Ability of Multilingual BERT: An Empirical Study"}, {"url": "https://arxiv.org/abs/1906.01502", "anchor_text": "How multilingual is Multilingual BERT?"}, {"url": "https://arxiv.org/abs/1906.01502", "anchor_text": "How multilingual is Multilingual BERT?"}, {"url": "https://github.com/google-research/bert/blob/master/LICENSE", "anchor_text": "Apache License 2.0"}, {"url": "https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf", "anchor_text": "Cross-lingual Language Model Pretraining"}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "link"}, {"url": "https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf", "anchor_text": "Cross-lingual Language Model Pretraining"}, {"url": "https://medium.com/@makcedward/how-subword-helps-on-your-nlp-model-83dd1b836f46", "anchor_text": "link"}, {"url": "https://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf", "anchor_text": "Cross-lingual Language Model Pretraining"}, {"url": "https://github.com/facebookresearch/XLM/blob/master/LICENSE", "anchor_text": "Attribution-NonCommercial 4.0 International"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://openreview.net/forum?id=SyxS0T4tvS", "anchor_text": "accepted at ICLR 2020"}, {"url": "https://arxiv.org/abs/1911.02116", "anchor_text": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"url": "https://arxiv.org/abs/1911.02116", "anchor_text": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"url": "https://arxiv.org/abs/1911.02116", "anchor_text": "Unsupervised Cross-lingual Representation Learning at Scale"}, {"url": "https://github.com/facebookresearch/XLM/blob/master/LICENSE", "anchor_text": "Attribution-NonCommercial 4.0 International"}, {"url": "https://openreview.net/forum?id=H1eA7AEtvS", "anchor_text": "ALBERT"}, {"url": "https://openreview.net/forum?id=BJgQ4lSFPH", "anchor_text": "ALICE"}, {"url": "https://github.com/google-research/bert/blob/master/multilingual.md", "anchor_text": "link"}, {"url": "https://github.com/facebookresearch/XLM", "anchor_text": "link"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "HuggingFace Transformers code"}, {"url": "https://github.com/huggingface/transformers/blob/master/LICENSE", "anchor_text": "Apache License 2.0"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ae917b36034d---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/transformers?source=post_page-----ae917b36034d---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ae917b36034d---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&user=Simone+Romano&userId=17605fb37de1&source=-----ae917b36034d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&user=Simone+Romano&userId=17605fb37de1&source=-----ae917b36034d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fae917b36034d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ae917b36034d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ae917b36034d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ae917b36034d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ae917b36034d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ae917b36034d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ae917b36034d--------------------------------", "anchor_text": ""}, {"url": "https://ialuronico.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ialuronico.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Simone Romano"}, {"url": "https://ialuronico.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "233 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F17605fb37de1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&user=Simone+Romano&userId=17605fb37de1&source=post_page-17605fb37de1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F36f1e5624156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmultilingual-transformers-ae917b36034d&newsletterV3=17605fb37de1&newsletterV3Id=36f1e5624156&user=Simone+Romano&userId=17605fb37de1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}