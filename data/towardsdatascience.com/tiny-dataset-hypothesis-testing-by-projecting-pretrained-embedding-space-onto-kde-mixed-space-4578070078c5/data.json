{"url": "https://towardsdatascience.com/tiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5", "time": 1682995687.794843, "path": "towardsdatascience.com/tiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5/", "webpage": {"metadata": {"title": "Tiny Dataset Hypothesis Testing by Projecting Pretrained-Embedding-Space Onto KDE-Mixed Space | by Natanel Davidovits | Towards Data Science", "h1": "Tiny Dataset Hypothesis Testing by Projecting Pretrained-Embedding-Space Onto KDE-Mixed Space", "description": "Text classification tasks usually demand high sample count and fine-semantic variability for reliable modeling. In many cases, the data at hand is insufficient in both sample count, over-skewness of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjfxP6z4rbhAhWMblAKHdcOCyAQFjAAegQIBBAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSchleicher%2527s_fable&usg=AOvVaw3BWgDfgrlXUpN5Z2fqrWn_", "anchor_text": "Schleicher\u2019s fable", "paragraph_index": 2}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Label_Propagation_Algorithm", "anchor_text": "label-spreading / label-propagation", "paragraph_index": 14}, {"url": "https://medium.com/@cohenori", "anchor_text": "Ori Cohen", "paragraph_index": 20}, {"url": "https://medium.com/@adambali1", "anchor_text": "Adam Bali", "paragraph_index": 20}], "all_paragraphs": ["Text classification tasks usually demand high sample count and fine-semantic variability for reliable modeling. In many cases, the data at hand is insufficient in both sample count, over-skewness of categories, and low variability, i.e., vocabulary variety and semantic meaning. In this post, I will present a novel method to overcome these common hurdles. The purpose of this method is mainly for aiding in quick prototyping, guided topic modeling, hypothesis testing , proof-of-concept (POC), or even when creating a minimal-viable-product (MVP).", "This method is composed of the following steps:", "We start with an extremely small dataset. We are using Schleicher\u2019s fable, at which each sentence will be a document sample.", "Step 2: choosing the best embedding space", "Words are seemingly categorical in nature, but with embedding methods such as Word2Vec & GloVe, they can now be seen as points in a finite-semantic, densely represented space. This pseudo-euclidean space representation can aid significantly in simplifying semantic tasks. Due to the shortage of data in many domains, it is common practice to start with a pre-trained readily-to-use embedding model.", "As a general rule, our domain should be fully represented. Therefore, the chosen embedding should contain as many words as possible that will our data\u2019s / topics\u2019 (topic modeling) vocabulary. In order to prevent out-of-vocabulary (OOV) words, the selected model should contain a very large number of tokens. I usually select the lowest accommodating dimension space, because a higher dimension space can have greater distances between the words from the embedding space and our domain. In other words, this can cause cluster boundaries to skew away from our domain toward the original domain represented by the pre-trained embedding. Lastly, I try to choose an embedding space that is as close as possible to my use-case as seen in Figure 1.", "I use Gensim for training a Word2Vec model. I usually train a model on any dataset. However, It is best, as shown below, to train on a big external dataset, preferably \u2014 mixed with your own. In this implementation, the space dimensionality is manually set, which gives us an advantage with respect to cluster boundaries.", "Once embedding space is chosen, we parse the space guided by our vocabulary. The following assumptions aid us in this task:", "The following code chooses the best encoding space from a list of pretrained embedding spaces, available here and provided by Stanford. Please note that the following procedure uses standard text-preprocessing methodology such as text cleaning, punctuation and stop-word removal, followed by stemming & lemmatizing. As seen in the code below, other embedding files could be created with the Gensim package on any dataset of your choosing, for example:", "With the previous assumptions, suggested in Step2, how do we choose the right clustering algorithm, the number of clusters, and the location of each centroid in the space? The answers to these questions rely heavily on domain. However, if you are not sure how to add your domain\u2019s guiding constraints or enhancements, I suggest a generic and stripped approach. Generally speaking, the number of clusters should be set by observing the dataset, since the semantic separability of any transformation should be with respect to future tasks in the domain itself. The minimal cluster count should be determined by the lowest class count you foresee in future tasks. For example, if in the near future, you see text classification tasks on your data or domain with no more than 10 classes, the minimum cluster count should be set to 10. However, if that number is higher than the dimensionality of the embedding space, then the lower bound should be greater and undefined at this point. In any case, it should not exceed your dataset\u2019s vocabulary or topics count, keeping in mind that in this use-case it is extremely low.", "Points like cluster boundary uncertainty, P-value analysis of each cluster, adaptive thresholding and conditional cluster merge and split are beyond the scope of this post.", "We assume that adjacent words in the embedding space are semantically close enough to be joined to a certain semantic cluster. In order to define clusters, we need to decide on a distance metric. For this task, let\u2019s look at the tokens occupying the embedding space and find the closest two. Let the cosine distance between these two be Ro, then the minimal distance to define cluster word adjacency is R = Ro / 2 - \u03b5, at which cluster count is maximal. In other words, a simple instance-to-instance distance clustering is done to group words. In the case of topic modeling, Ro will be the minimal distance between the closest words from a different topic.", "The following code uses the chosen Glove embedding-space, clusters it using Nearest-Neighbors where K=2 and uses cosine similarity to determine the minimal distance.", "The previous method ensures that clusters will contain at least one word from the dataset, while keeping in mind that there will always be unassigned words in the embedding space.", "The straightforward method for agglomerating the unassigned points (words) into the generated clusters is label-spreading / label-propagation, as seen in Figures 2 & 3.", "However, due to a high run-time complexity (code#5), you may want to use a faster and less accurate method such as linear-SVM (code#6). The following code compares both methods due to the run-time complexity issue. This step is a \u201cbrute force\u201d agglomeration, and will probably yield less than optimal results in future quest, when our dataset is expected to be richer.", "Let\u2019s focus on why core-clustering followed by sample-agglomeration makes sense. Well, we wanted to constrain the embedding to our data-anchors / topics (words). This demands semantic proximity. Once this has been achieved, the most outlying words are assumed to be less probable in future samples. Let me emphasize again \u2014 this use case occurs when we have very little data to begin with, and want to produce a POC or a basic product (MVP).", "Step 4: creating a new embedding space using KDE", "Now that all words are assigned to a cluster, we want a more informative representation that will aid in future unseen samples. Since the embedding space is defined by semantic-proximity, we can encode each sample by the density of each cluster\u2019s probability-density-function (PDF) at that position in space.", "In other words, a word that is located in a dense region in one cluster and less dense at another, will exhibit this behavior using the information projected by using the new density-embedding. Keeping in mind that the embedding dimension is actually the cluster count and the order of the embedding is maintained with respect to the clusters as set forth when this embedding was initialized. The resulting projection, using our tiny dataset, can be seen in Figure 4. Finally, the following code uses KDE to create a new embedding.", "I would like to thank Ori Cohen and Adam Bali for their invaluable critique, proofreading, editing and comments.", "Natanel DavidovitsBizarre-problem solver. Expert in mathematical modeling, optimization, computer vision, NLP/NLU & data science, with a decade of experience in industry-research.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Bizarre-problem solver. Expert in mathematical modeling, optimization, computer vision, NLP/U & data-science, with a decade of industry-research experience."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4578070078c5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4578070078c5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ndor123?source=post_page-----4578070078c5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ndor123?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Natanel Davidovits"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6baa3cd40a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&user=Natanel+Davidovits&userId=d6baa3cd40a4&source=post_page-d6baa3cd40a4----4578070078c5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwjfxP6z4rbhAhWMblAKHdcOCyAQFjAAegQIBBAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FSchleicher%2527s_fable&usg=AOvVaw3BWgDfgrlXUpN5Z2fqrWn_", "anchor_text": "Schleicher\u2019s fable"}, {"url": "https://en.wikipedia.org/wiki/Kernel_(linear_algebra)", "anchor_text": "Ker{}"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Label_Propagation_Algorithm", "anchor_text": "label-spreading / label-propagation"}, {"url": "https://medium.com/@cohenori", "anchor_text": "Ori Cohen"}, {"url": "https://medium.com/@adambali1", "anchor_text": "Adam Bali"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4578070078c5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/embedding?source=post_page-----4578070078c5---------------embedding-----------------", "anchor_text": "Embedding"}, {"url": "https://medium.com/tag/statistics?source=post_page-----4578070078c5---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4578070078c5---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4578070078c5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&user=Natanel+Davidovits&userId=d6baa3cd40a4&source=-----4578070078c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&user=Natanel+Davidovits&userId=d6baa3cd40a4&source=-----4578070078c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4578070078c5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4578070078c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4578070078c5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4578070078c5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4578070078c5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4578070078c5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4578070078c5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4578070078c5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ndor123?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ndor123?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Natanel Davidovits"}, {"url": "https://medium.com/@ndor123/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "26 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6baa3cd40a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&user=Natanel+Davidovits&userId=d6baa3cd40a4&source=post_page-d6baa3cd40a4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fd6baa3cd40a4%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftiny-dataset-hypothesis-testing-by-projecting-pretrained-embedding-space-onto-kde-mixed-space-4578070078c5&user=Natanel+Davidovits&userId=d6baa3cd40a4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}