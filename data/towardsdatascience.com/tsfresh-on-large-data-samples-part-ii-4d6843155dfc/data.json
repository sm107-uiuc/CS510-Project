{"url": "https://towardsdatascience.com/tsfresh-on-large-data-samples-part-ii-4d6843155dfc", "time": 1683017428.972152, "path": "towardsdatascience.com/tsfresh-on-large-data-samples-part-ii-4d6843155dfc/", "webpage": {"metadata": {"title": "tsfresh on Large Data Samples \u2014 Part II | by Nils Braun | Towards Data Science", "h1": "tsfresh on Large Data Samples \u2014 Part II", "description": "In the last post, we have explored how tsfresh automatically extracts many time-series features from your input data. We have also discussed two possibilities to speed up your feature extraction\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@nils-braun/time-series-feature-extraction-on-really-large-data-samples-b732f805ba0e", "anchor_text": "last post", "paragraph_index": 0}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark", "paragraph_index": 3}, {"url": "https://dask.org/", "anchor_text": "dask", "paragraph_index": 3}, {"url": "https://tsfresh.readthedocs.io/en/v0.15.1/api/tsfresh.convenience.html", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://tsfresh.readthedocs.io/en/latest/text/quick_start.html", "anchor_text": "Quickstart", "paragraph_index": 5}, {"url": "https://arrow.apache.org/install/", "anchor_text": "installation instructions", "paragraph_index": 5}, {"url": "https://spark.apache.org/docs/latest/", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://spark.apache.org/docs/latest/cluster-overview.html", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://spark.apache.org/docs/latest/submitting-applications.html", "anchor_text": "documentation", "paragraph_index": 7}, {"url": "https://arrow.apache.org/docs/index.html", "anchor_text": "arrow", "paragraph_index": 8}, {"url": "https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x", "anchor_text": "documentation", "paragraph_index": 9}, {"url": "https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe", "anchor_text": "StackOverflow discussion", "paragraph_index": 13}, {"url": "https://docs.dask.org/en/latest/setup.html", "anchor_text": "dask documentation", "paragraph_index": 23}, {"url": "https://tsfresh.readthedocs.io/en/v0.15.1/text/feature_extraction_settings.html", "anchor_text": "the documentation", "paragraph_index": 26}, {"url": "https://luigi.readthedocs.io/en/stable/", "anchor_text": "documentation", "paragraph_index": 34}, {"url": "https://b2luigi.readthedocs.io/en/stable/", "anchor_text": "b2luigi", "paragraph_index": 40}, {"url": "https://luigi.readthedocs.io/en/stable/running_luigi.html", "anchor_text": "documentation", "paragraph_index": 42}, {"url": "https://twitter.com/dotcsDE", "anchor_text": "@dotcsDE", "paragraph_index": 48}], "all_paragraphs": ["In the last post, we have explored how tsfresh automatically extracts many time-series features from your input data. We have also discussed two possibilities to speed up your feature extraction calculation: using multiple cores on your local machine (which is already turned on by default) or distributing the calculation over a cluster of machines.", "In this post we will go one step further: what happens if your data is so large that loading the data into your scheduling machine is not an option anymore? For many applications, this is not the case (and keeping everything local speeds up the development cycle as well as decreased the number of possible mistakes). However, sometimes you really need to think big.", "There exist multiple solutions to solve the problem of distributing not only the work but also the data \u2014 actually the whole field of big data and data engineering builds around that. The general idea is to only load a fraction of the data into each machine and perform the calculation on this partition of the data \u2014 instead of loading all the data at the same time. The inputs and results are distributed either via network transmissions or via a distributed file system, such as HDFS.", "Two of the best-known examples for this (at least in the Python world) are Apache Spark (with its python bindings PySpark) and dask. We will not cover the basics of Apache Spark nor dask here - they are both wonderful libraries and if you never used them you should definitely check them out! If you do not want to use any of those frameworks, but you still have too much data to handle, have a look into the next section where we will discuss a simplified (but also less powerful) solution.", "Since version 0.15, tsfresh contains convenience functions to input a Spark data frame or a dask data frame into tsfresh (remember: normally you can only use pandas data frames). They are both defined in the tsfresh.convenience.bindings module (with documentation here) and we will cover them in the remainder of this section.", "As an example, we will again use the robot failure data sample from our Quickstart. Even though it is still small enough to fit into your memory, we will treat it as \u201cbig data\u201d and spill it out to multiple parquet files on disk. Make sure to have a pandas parquet binding installed for this (such as pyarrow, see installation instructions) or use a different format.", "Let's start with Apache Spark first.", "Apache Spark is basically the framework for writing and distributing fault-tolerant data pipelines. Even though it is written in Java (and Scala), it has very good and well-documented Python bindings called pyspark. After having installed Apache Spark (see here), you need to create a Spark cluster for distributing the work. As usual, there exist many ways for this (see here for a start), but as an example, we will just use the standalone cluster, which is started when you do not specify differently (also called \u201clocal\u201d mode). We will demonstrate the calculation in the following with a pyspark interactive console, but you can of course also write it into a python script and submit it via spark-submit (check out the documentation).", "Important: Spark leverages the arrow bindings for efficient transformation between pandas and Spark data frames. Therefore, you need to have arrow installed. In a recent arrow version, the internal data format has changed and is now incompatible with Spark. For it to still work, you need to add the line", "to your Spark environment settings file $SPARK_HOME/conf/spark-env.sh as stated in the documentation.", "Now let\u2019s create a data pipeline! We will use Spark\u2019s structured declarative data frame API in the following. Remember that Spark will only build up the computation DAG until you trigger an action in the end.", "So first, let's spin up an interactive pyspark shell and read in the parquet files. As we are running in local mode we do not need to make sure that the data is available for all workers (as there is only one local worker). In a productive environment, you would probably use S3 or HDFS or any other shared data storage.", "This is the format of the robot data. id is the identifier for each time series, time is the time (sorting) parameter, and the F_* and T_* are the different value series (remember: what we call the kind of the time series). Please note that this data format and the pre-processing we will perform in the following is just an example - your data might look differently and other data transformation steps might be necessary. For example, if your column names are different (e.g. the id column is not called \u201cid\u201d), you will need to edit the steps accordingly.", "The feature extraction will later run on each of these series separately \u2014 so we first need to group it both by the time-series identifier id and the kind. However, the data is not easily groupable by kind (data of the same kind are not separated) - so we first need to reshape the data. This transformation is usually called melting and there exist various ways to do this in PySpark. For example, you could follow one of the answers to this StackOverflow discussion. Just make sure to change the column name identifiers if needed.", "After melting, the resulting data frame is a concatenated set of each kind of the time-series, while keeping the id and time columns. When we now group by id and kind", "each grouped chunk of data only contains the data of one kind and one id. We are now ready to use tsfresh! The preprocessing part might look different for your data sample, but you should always end up with a dataset grouped by id and kind before using tsfresh.", "With the given column names in the example, the call to tsfresh looks like this:", "Please remember that Spark will only trigger the calculation once you call an action, so it is still only building up the calculation DAG.", "Internally, tsfresh will call the following on each grouped chunk:", "In the end, it will return a Spark data frame, where each line is the value for a different feature of a different time series id and kind.", "Depending on your use case, you might want to bring them into the usual tabular form (but please note that this might imply heavy shuffling and especially some immediate calculations as the column names need to be known):", "In any case, you can now apply additional transformations or write out the result of the calculation. As long as you have enough workers, this basically scales infinitely.", "In principle interacting with tsfresh from dask follows the same principles as with Spark, so lets quickly walk through them:", "Spin up a dask cluster. There are again multiple ways how to do this depending on your environment. A very good starting point is the dask documentation. If you do not have a cluster to play around, local mode will also work (which means you do not need to setup or import anything). For testing, you can use an interactive python shell or a jupyter notebook.", "Bring the data into the same format as above, where data with different id and kind can be separated easily. Fortunately, dask has already a melt feature, so you just need to call it:", "Now separate the data for different id and kind by grouping", "The data is in the correct format \u2014 we can apply tsfresh! We are again using the full set of feature calculators as an example here, but have a look into the documentation for more information.", "Again, this will internally transform each chunk into a pandas data frame, apply the feature extraction, and transform it back into a dask data frame. The result will be a data frame with one extracted feature for one id and kind per row.", "Continue with your calculation, e.g. transform the results into the usual tabular form (again: this might be computationally very intensive!):", "In case you are wondering: the aggregation function \u201csum\u201d does not really matter here: there is only ever a single value per feature name and identifier.", "You might ask yourself what the difference is between using a ClusterDaskDistributor (which distributes to a dask cluster) as described in the last post and this dask binding.", "The data preprocessing depends a lot on the shape of your data and the way shown here is most likely not the most efficient way for you (because it might lead to a lot of shuffling or re-partitioning). Please think about how you can transform your data efficiently into the input format for tsfresh.", "You have a lot of data which does not fit into memory, but you do not want to add the burden of using a distributed framework (such as dask or PySpark) to your project? Then read on!", "In the following, we will follow the ideas of a distributed framework such as dask or Apache Spark without actually using one (which of course also has some downsides, see below). The idea is:", "For this part, we will use the powers of luigi, a task orchestrator framework. If you have never used luigi have a look into the documentation for a first overview before you continue. We will also try to walk through the basic ideas in the following.", "We will also assume that your data is stored to disk (maybe on a distributed storage like HDFS) and already partitioned by id and kind. For example, it might be in the form", "If you want to generate some test data with the robot dataset, you can use the following python snippet:", "Please note that the data is basically already melted due to the way it is stored. It is also possible to store the data only separated by id and let tsfresh extract the features for all kinds simultaneously. Feel free to adjust the luigi script below to your needs.", "The core building block of luigi are tasks. Tasks can have dependencies among each other, they define which output files they create and of course what to do when the task runs. They are controlled by parameters, which can be used to distinguish different instances of the same task. Let\u2019s define a Task for the cycle of reading, extracting, and writing the data as described above.", "As you can see, the code is quite straightforward. We are again assuming only local setup for this example \u2014 in a real-world application the input and output paths will be on a shared file system (NFS, HDFS, S3) so that both the scheduler and the worker can read/write the files.", "Once we have defined a task for this cycle, we just need to run it. There exist multiple ways to do this \u2014 you could use a local scheduler, a central scheduler, or even a batch system (maybe with the help of my package b2luigi ;-)). As an example, we will just define a list of tasks to run and give it to the luigi.build function. Also, we are only using a single worker and a local scheduler.", "Running this script will give you a happy smiley output and your output data stored in the specified paths. For distributing the work among several machines, start a central scheduler", "And remove the local_scheduler flag. Each call to the luigi script will now start another worker, which will connect to the central scheduler and start processing work (if your code is running on multiple machines, you need to give the scheduler hostname as well). The documentation describes more options. If you want to test it out, make sure to increase the number of ids it needs to process \u2014 otherwise, it will run out of work quickly.", "Using luigi comes with the benefit of a very simple code base and a simplified execution model. But of course, this can not replace a distributed framework such as dask or Apache Spark with all its features:", "But following the KISS principle, I would always prefer a small luigi application over running a complex distributed application (and have done so successfully several times in the past).", "After all these possibilities, the question arises: When should you use what?", "Let\u2019s try to give some usage hints:", "There is a lot more to discover when it comes to distributed computing or distributed data. If you find out about interesting bindings between your framework and tsfresh or of you developed a cool distributor you want to share, we are already happy for pull requests. Happy coding!", "Big thanks to @dotcsDE for great suggestions!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d6843155dfc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nils-braun.medium.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": ""}, {"url": "https://nils-braun.medium.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Nils Braun"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b92b7d7556c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&user=Nils+Braun&userId=3b92b7d7556c&source=post_page-3b92b7d7556c----4d6843155dfc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@nathananderson?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Nathan Anderson"}, {"url": "https://unsplash.com/collections/543019/big-data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@nils-braun/time-series-feature-extraction-on-really-large-data-samples-b732f805ba0e", "anchor_text": "last post"}, {"url": "https://spark.apache.org/", "anchor_text": "Apache Spark"}, {"url": "https://dask.org/", "anchor_text": "dask"}, {"url": "https://tsfresh.readthedocs.io/en/v0.15.1/api/tsfresh.convenience.html", "anchor_text": "here"}, {"url": "https://tsfresh.readthedocs.io/en/latest/text/quick_start.html", "anchor_text": "Quickstart"}, {"url": "https://arrow.apache.org/install/", "anchor_text": "installation instructions"}, {"url": "https://spark.apache.org/docs/latest/", "anchor_text": "here"}, {"url": "https://spark.apache.org/docs/latest/cluster-overview.html", "anchor_text": "here"}, {"url": "https://spark.apache.org/docs/latest/submitting-applications.html", "anchor_text": "documentation"}, {"url": "https://arrow.apache.org/docs/index.html", "anchor_text": "arrow"}, {"url": "https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#compatibiliy-setting-for-pyarrow--0150-and-spark-23x-24x", "anchor_text": "documentation"}, {"url": "https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe", "anchor_text": "StackOverflow discussion"}, {"url": "https://tsfresh.readthedocs.io/en/v0.15.1/text/feature_extraction_settings.html", "anchor_text": "the documentation"}, {"url": "https://docs.dask.org/en/latest/setup.html", "anchor_text": "dask documentation"}, {"url": "https://tsfresh.readthedocs.io/en/v0.15.1/text/feature_extraction_settings.html", "anchor_text": "the documentation"}, {"url": "https://luigi.readthedocs.io/en/stable/", "anchor_text": "documentation"}, {"url": "https://b2luigi.readthedocs.io/en/stable/", "anchor_text": "b2luigi"}, {"url": "https://luigi.readthedocs.io/en/stable/running_luigi.html", "anchor_text": "documentation"}, {"url": "https://github.com/blue-yonder/tsfresh/blob/master/tsfresh/convenience/bindings.py", "anchor_text": "quite simple"}, {"url": "https://twitter.com/dotcsDE", "anchor_text": "@dotcsDE"}, {"url": "https://medium.com/tag/dask?source=post_page-----4d6843155dfc---------------dask-----------------", "anchor_text": "Dask"}, {"url": "https://medium.com/tag/spark?source=post_page-----4d6843155dfc---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/time-series-analysis?source=post_page-----4d6843155dfc---------------time_series_analysis-----------------", "anchor_text": "Time Series Analysis"}, {"url": "https://medium.com/tag/python?source=post_page-----4d6843155dfc---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4d6843155dfc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&user=Nils+Braun&userId=3b92b7d7556c&source=-----4d6843155dfc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&user=Nils+Braun&userId=3b92b7d7556c&source=-----4d6843155dfc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d6843155dfc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d6843155dfc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d6843155dfc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d6843155dfc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d6843155dfc--------------------------------", "anchor_text": ""}, {"url": "https://nils-braun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nils-braun.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nils Braun"}, {"url": "https://nils-braun.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "98 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3b92b7d7556c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&user=Nils+Braun&userId=3b92b7d7556c&source=post_page-3b92b7d7556c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7f5298962150&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftsfresh-on-large-data-samples-part-ii-4d6843155dfc&newsletterV3=3b92b7d7556c&newsletterV3Id=7f5298962150&user=Nils+Braun&userId=3b92b7d7556c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}