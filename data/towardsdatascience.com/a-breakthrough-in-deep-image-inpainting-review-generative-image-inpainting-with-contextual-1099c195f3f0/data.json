{"url": "https://towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0", "time": 1683015582.269742, "path": "towardsdatascience.com/a-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0/", "webpage": {"metadata": {"title": "A Breakthrough in Deep Image Inpainting | by Chu-Tak Li | Towards Data Science", "h1": "A Breakthrough in Deep Image Inpainting", "description": "Welcome back guys! Happy to see you guys:) Last time, we realized that how copy-and-paste is embedded in CNNs for deep image inpainting. Can you get the main idea? If yes, Good! If no, Don\u2019t worry\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "my previous post", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "previous post", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "before", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper of this work", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "previous post", "paragraph_index": 16}, {"url": "https://github.com/JiahuiYu/generative_inpainting/tree/v1.0.0", "anchor_text": "github project page", "paragraph_index": 17}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "previous state-of-the-art", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "Shift-connection layer", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1801.07892", "anchor_text": "Generative Image Inpainting with Contextual Attention", "paragraph_index": 37}, {"url": "http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf", "anchor_text": "Globally and Locally Consistent Image Completion", "paragraph_index": 38}, {"url": "https://chutakcode.wixsite.com/website", "anchor_text": "https://chutakcode.wixsite.com/website", "paragraph_index": 41}], "all_paragraphs": ["Welcome back guys! Happy to see you guys:) Last time, we realized that how copy-and-paste is embedded in CNNs for deep image inpainting. Can you get the main idea? If yes, Good! If no, Don\u2019t worry! Today, we are going to dive into a breakthrough in deep image inpainting, for which contextual attention is proposed. By using contextual attention, we can effectively borrow information from distant spatial locations for reconstructing the local missing pixels. This idea is actually more or less the same as copy-and-paste. Let\u2019s see how they can do that together!", "In my previous post, I have introduced the shift-connection layer in which features from known regions act as references to the generated features inside missing regions to allow us to further refine the generated features for better inpainting results. Here, we assume that the generated features are reasonable estimations of the ground truth and suitable references are determined according to the similarity between features from known regions and the generated features inside missing regions.", "For the task of image inpainting, the structure of CNNs cannot effectively model the long-term correlations between the missing regions and information given by distant spatial locations. If you are familiar with CNNs, you should know that the kernel size and the dilation rate control the receptive field at a convolutional layer and the network has to go deeper and deeper so as to see the entire input image. This means that if we want to capture the context of an image, we have to rely on deeper layers but we lose the spatial information as deeper layers always have smaller spatial size of features. So, we have to find a way to borrow information from distant spatial locations (i.e. understanding the context of an image) without going too deep into a network.", "If you remember what dilated convolution is (We have covered in previous post), you will know that dilated convolution is one way to increase the receptive field at early convolutional layers without adding additional parameters. However, dilated convolution has its limitations. It skips consecutive spatial locations in order to enlarge the receptive field. Note that the skipped consecutive spatial locations are also crucial for filling in the missing regions.", "This work shares similar network architecture, loss function and relevant techniques that we have covered before. For the architecture, the proposed framework consists of two generator networks and two discriminator networks. The two generators follow the fully convolutional networks with dilated convolutions. One generator is for coarse reconstruction and another one is for refinement. This is called standard coarse-to-fine network structure. The two discriminators also look at the completed images both globally and locally. The global discriminator takes the entire image as input while the local discriminator takes the filled region as input.", "For the loss function, simply speaking, they also employ adversarial loss (GAN loss) and L1 loss (for pixel-wise reconstruction accuracy). For the L1 loss, they use a spatially discounted L1 loss in which a weight is assigned to each pixel difference and the weight is based on the distance of a pixel to its nearest known pixel. For GAN loss, they use a WGAN-GP loss instead of the most standard adversarial loss we have introduced. They claim that this WGAN adversarial loss is also based on L1 distance measure, hence the network is easier to train and the training process is more stable.", "In this post, I would like to focus on the proposed contextual attention mechanism. Therefore, I briefly cover the coarse-to-fine network architecture, the WGAN adversarial loss, and the weighted L1 loss in above. Interested readers can refer to my previous posts and the paper of this work for further details.", "Contextual Attention mechanism is proposed to effectively borrow the contextual information from distant spatial locations for reconstructing the missing pixels. The contextual attention is applied to the second refinement network. The first coarse reconstruction network is responsible for a rough estimation of the missing regions. Same as previous, global and local discriminators are used to encourage better local texture details of the generated pixels.", "The most important idea in this paper is the contextual attention which allows us to make use of information from distant spatial locations for reconstructing local missing pixels. Second, the employment of the WGAN adversarial loss and weighted L1 loss improves the training stability. Also, the proposed inpainting framework achieves high-quality inpainting results on various datasets such as natural scene, face and texture as shown in Figure 1 in above.", "Figure 2 shows the network architecture of the proposed inpainting framework. As mentioned, it consists of two generators and two discriminators. If you have read my previous posts, you will find this typical network architecture for deep image inpainting.", "Here is the main focus of this post. Let\u2019s see how the proposed contextual attention layer is designed for borrowing feature information given by known regions at distant spatial locations to generate features inside missing regions.", "Figure 3 shows the graphically illustration of the proposed contextual attention layer. The operation is differentiable and fully convolutional.", "Figure 4 is a more detailed example of the proposed contextual attention layer. As refer to Figure 3, the Foreground means the generated features inside the missing region while the Background means the features extracted from the known region. Similar to copy-and-paste method, we first want to match the generated features inside the missing region to the features outside the missing region.", "Taking Figure 4 as an example, the generated features inside the missing region are with the size of 64\u00d764\u00d764 and assume that the features outside the missing region are divided into 128 small feature patches with size of 64\u00d73\u00d73. Note that the channel size of the features in this example is 64. Then, we perform convolution using the 128 small feature patches and the generated features inside the missing region, and get feature maps with size of 128\u00d764\u00d764. In the paper, this operation is described as,", "where {f_x,y} are the foreground patches (generated feature patches inside the missing region), {b_x\u2019,y\u2019} are the background patches (extracted feature patches outside the missing region). s_x,y,x\u2019,y\u2019 is the similarity between the generated patch centered at missing region (x,y) and the known patch centered at the known region (x\u2019,y\u2019). Actually, this is a standard cosine similarity measure process.", "When we look along the channel dimension, the 128 elements represent the similarities between all the known patches and a particular location inside the missing region. This reflects the contributions of the 128 known patches to the location. We then perform Softmax normalization to the feature maps along the channel dimension as shown in the blue-colored region in Figure 4. After the Softmax normalization, the sum of each location along the channel dimension should be 1.", "Compared with the Shift-Net covered in my previous post, you can see that this time we assign a weight to each known feature patch to indicate its importance for reconstructing each feature location inside the missing region (soft assignment) instead of just keeping the most similar known feature patch to each feature location inside the missing region (hard assignment). This is also the reason why the proposed contextual attention is differentiable.", "Finally, we reconstruct the generated features inside the missing region by means of deconvolution using the attention feature maps as input features and the known patches as kernels. For readers who are interested in the actual implementation, you can visit their github project page for further details.", "The attention propagation can be regarded as a fine-tuning of the attention feature maps. The key idea here is that neighboring pixels usually have closer pixel value. This means that they consider the attention values of the neighborhood to adjust each attention score,", "For example, if we consider the attention values of the left and right neighbors, we can update the current attention value using the equation listed in above. Note that k controls the number of neighbors to be considered.", "The authors claim that this can further improve the inpainting results and this can also be done by convolution with identity matrices as kernels.", "One more point about the attention mechanism is that two techniques were used to control the number of extracted known feature patches. i) Extracting known feature patches with larger strides to reduce the number of kernels. ii) Downsampling the feature map size before the operation and then upsampling after getting the attention maps.", "Figure 5 shows how the authors integrate the proposed contextual attention layer into the second refinement network. You can see that one more branch is introduced to apply the contextual attention and then two branches are concatenated to get the final inpainting results. The attention map color coding is used to visualize the attention map. For example, the middle white color means the pixel focuses on itself, pink on bottom-left region, green on top-right region, etc. You can see that this example has a pink-filled attention map. This means that the filled region borrows much information from the bottom-left region.", "The authors first compared with the previous state-of-the-art that we have introduced before.", "Figure 6 shows the inpainting results by the proposed baseline model and the previous state-of-the-art, GLCIC [2]. The proposed baseline model is the model as shown in Figure 2, without the proposed contextual attention branch. It is clear that the baseline model is better than GLCIC in terms of local texture details. Please zoom in for a better view.", "Figure 7 shows the qualitative results using the baseline model and the full model (with contextual attention) on Places2 dataset. It is obvious that the full model offers better inpainting results with fine local texture details. This reflects that the contextual attention layer can effectively borrow information from distant spatial locations to help reconstructing the missing pixels. Please zoom in for a better view, especially for the attention maps.", "Table 1 lists some objective evaluation metrics for reference. As mentioned before, these metrics cannot fully reflect the quality of the inpainting results as there are many possible solutions to fill in the missing regions. You can see that the proposed full model offers best l1, l2 loss and PSNR. For TV loss, PatchMatch gives lower TV loss as it directly copies raw image patches for filling the holes.", "For reference, the proposed full model has 2.9M parameters. It takes 0.2 second per image on GPU and 1.5 seconds per image on CPU for images with size of 512\u00d7512.", "Attention mechanism is not a new idea and there are several attention modules in the literature. The authors did experiments on using different attention modules.", "The authors compared with two famous attention modules in the literature, namely Spatial Transformer Network [3] and Appearance Flow [4]. Simply speaking, for appearance flow, a convolutional layer is used to replace the contextual attention layer to directly predict 2D pixel offsets as attention. This means that we add a convolutional layer to predict the shifts of the known pixels into the missing pixels. In Figure 8, you can see the results using appearance flow (middle) provide similar attention maps for different testing images. This means that the attention maps are not useful for giving the \u201cattention\u201d we want. You can also observe that the Spatial Transformer Network (left) cannot offer meaningful attention maps for the task of image inpainting. One possible reason is that spatial transformer network predicts parameters of global affine transformation and this is not sufficient to help filling in the missing region which also requires local information. I haven\u2019t go too deep into different attention modules here. Interested readers may refer to the papers for further details.", "Choice of GAN loss for image inpainting. The authors experimented with different GAN losses, such as WGAN loss, typical adversarial loss, and Least Square GAN. They empirically found that WGAN loss provides the best inpainting results.", "Essential Reconstruction loss. The authors trained the refinement network without the L1 loss. They found that L1 loss is necessary to ensure the pixel-wise reconstruction accuracy even L1 loss makes the inpainting results blurry. So, L1 loss is crucial to ensure better content structure of the completed images.", "Perceptual loss, style loss, and TV loss. We will cover perceptual loss and style loss very soon. A simple conclusion here is that these three losses do not bring obvious improvements in inpainting results. So, their model is trained using only weighted L1 loss and WGAN loss.", "Clearly, the key idea of this paper is the contextual attention mechanism. The contextual attention layer is embedded in the second refinement network. Note the role of the first coarse reconstruction network is to have a rough estimation of the missing region. This estimation is used at the contextual attention layer. By matching the generated features inside the missing region and the features outside the missing region, we can know the contributions of all the features outside the missing region to each location inside the missing region. Note that the contextual attention layer is differentiable and fully-convolutional. With the proposed contextual attention, they achieve the state-of-the-art inpainting results.", "You may find that we are going deeper and deeper into the field of deep image inpainting. In my previous post, Shift-connection layer is introduced which embeds the concept of copy-and-paste in CNNs in the form of hard assignment. This paper formulates a contextual attention layer in the form of soft assignment such that this layer is differentiable and can be learned end-to-end without modifying the computation of the gradient.", "I hope that you can grasp the key idea of the contextual attention layer proposed in this paper, especially its formulation as shown in Figure 3 and 4. For readers who want to know more about the network architectures and loss function, please refer to the paper.", "In the future posts, we will look into more task-specific inpainting techniques. Hope that we can learn together and enjoy!", "[1] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang, \u201cGenerative Image Inpainting with Contextual Attention,\u201d Proc. Computer Vision and Pattern Recognition (CVPR), 2018.", "[2] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa, \u201cGlobally and Locally Consistent Image Completion,\u201d ACM Trans. on Graphics, Vol. 36, \u21164, Article 107, Publication date: July 2017.", "Again, many thanks for reading my post! If you have any questions, please feel free to send my an email or leave comments here.", "Actually, I try to shorten the length of the post and focus just one key idea of the paper. I have assumed that readers have basic knowledge of deep image inpainting from my previous posts. By the way, I must keep improving my writing skills to more effectively express my understanding of the papers. Any suggestions are welcome. It is extremely important for us to learn systematically. Thank you very much and see you next time! :)", "DO IT FIRST. ONLY U CAN DEFINE YOURSELF. I have started my PhD journey accidentally. To know more about me at: https://chutakcode.wixsite.com/website"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1099c195f3f0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://lichutak.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Chu-Tak Li"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4c07943b642&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=post_page-f4c07943b642----1099c195f3f0---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1099c195f3f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=-----1099c195f3f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1099c195f3f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=-----1099c195f3f0---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "my previous post"}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "previous post"}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "before"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper of this work"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "previous post"}, {"url": "https://github.com/JiahuiYu/generative_inpainting/tree/v1.0.0", "anchor_text": "github project page"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/a-milestone-in-deep-image-inpainting-review-globally-and-locally-consistent-image-completion-505413c300df", "anchor_text": "previous state-of-the-art"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/how-copy-and-paste-is-embedded-in-cnns-for-image-inpainting-review-shift-net-image-433a2a93c963", "anchor_text": "Shift-connection layer"}, {"url": "https://arxiv.org/pdf/1801.07892.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1801.07892", "anchor_text": "Generative Image Inpainting with Contextual Attention"}, {"url": "http://iizuka.cs.tsukuba.ac.jp/projects/completion/data/completion_sig2017.pdf", "anchor_text": "Globally and Locally Consistent Image Completion"}, {"url": "https://arxiv.org/abs/1506.02025", "anchor_text": "Spatial Transformer Networks"}, {"url": "https://arxiv.org/abs/1605.03557", "anchor_text": "View synthesis by appearance flow"}, {"url": "https://medium.com/tag/image-inpainting?source=post_page-----1099c195f3f0---------------image_inpainting-----------------", "anchor_text": "Image Inpainting"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1099c195f3f0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/image-processing?source=post_page-----1099c195f3f0---------------image_processing-----------------", "anchor_text": "Image Processing"}, {"url": "https://medium.com/tag/convolutional-network?source=post_page-----1099c195f3f0---------------convolutional_network-----------------", "anchor_text": "Convolutional Network"}, {"url": "https://medium.com/tag/generative-adversarial?source=post_page-----1099c195f3f0---------------generative_adversarial-----------------", "anchor_text": "Generative Adversarial"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1099c195f3f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=-----1099c195f3f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1099c195f3f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=-----1099c195f3f0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1099c195f3f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4c07943b642&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=post_page-f4c07943b642----1099c195f3f0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd59b7e8b1294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&newsletterV3=f4c07943b642&newsletterV3Id=d59b7e8b1294&user=Chu-Tak+Li&userId=f4c07943b642&source=-----1099c195f3f0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Written by Chu-Tak Li"}, {"url": "https://lichutak.medium.com/followers?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "122 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chutakcode.wixsite.com/website", "anchor_text": "https://chutakcode.wixsite.com/website"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff4c07943b642&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&user=Chu-Tak+Li&userId=f4c07943b642&source=post_page-f4c07943b642----1099c195f3f0---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd59b7e8b1294&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-breakthrough-in-deep-image-inpainting-review-generative-image-inpainting-with-contextual-1099c195f3f0&newsletterV3=f4c07943b642&newsletterV3Id=d59b7e8b1294&user=Chu-Tak+Li&userId=f4c07943b642&source=-----1099c195f3f0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Chu-Tak Li"}, {"url": "https://medium.com/analytics-vidhya?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Analytics Vidhya"}, {"url": "https://medium.com/analytics-vidhya/4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "4 Steps to install Anaconda and PyTorch onWindows 10Hi guys:) Today, I would like to share how to install Anaconda and PyTorch (with/without GPU) in Windows 10 such that you can run\u2026"}, {"url": "https://medium.com/analytics-vidhya/4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "4 min read\u00b7Nov 19, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fanalytics-vidhya%2F5c9cb0c80dfe&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2F4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe&user=Chu-Tak+Li&userId=f4c07943b642&source=-----5c9cb0c80dfe----0-----------------clap_footer----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://medium.com/analytics-vidhya/4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe?source=author_recirc-----1099c195f3f0----0---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c9cb0c80dfe&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fanalytics-vidhya%2F4-steps-to-install-anaconda-and-pytorch-onwindows-10-5c9cb0c80dfe&source=-----1099c195f3f0----0-----------------bookmark_preview----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----1099c195f3f0----1---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----1099c195f3f0----1-----------------bookmark_preview----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://medium.com/@jacob_marks?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Jacob Marks, Ph.D."}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "How I Turned My Company\u2019s Docs into a Searchable Database with OpenAIAnd how you can do the same with your docs"}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "15 min read\u00b7Apr 25"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&user=Jacob+Marks%2C+Ph.D.&userId=f7dc0c0eae92&source=-----4f2d34bd8736----2-----------------clap_footer----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/how-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736?source=author_recirc-----1099c195f3f0----2---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "20"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f2d34bd8736&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-i-turned-my-companys-docs-into-a-searchable-database-with-openai-4f2d34bd8736&source=-----1099c195f3f0----2-----------------bookmark_preview----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Chu-Tak Li"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "10 Papers You Must Read for Deep Image InpaintingHello! This post can be regarded as a revision of deep image inpainting for my old friends and introductory deep image inpainting for\u2026"}, {"url": "https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": "12 min read\u00b7Nov 30, 2020"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2e41c589ced0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0&user=Chu-Tak+Li&userId=f4c07943b642&source=-----2e41c589ced0----3-----------------clap_footer----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0?source=author_recirc-----1099c195f3f0----3---------------------962b6f0b_957d_459e_b09e_7fa784b44d40-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2e41c589ced0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-papers-you-must-read-for-deep-image-inpainting-2e41c589ced0&source=-----1099c195f3f0----3-----------------bookmark_preview----962b6f0b_957d_459e_b09e_7fa784b44d40-------", "anchor_text": ""}, {"url": "https://lichutak.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "See all from Chu-Tak Li"}, {"url": "https://towardsdatascience.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Jehill Parikh"}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "U-Nets with attentionU-Net are popular NN architecture which are employed for many applications and were initially developed for medical image segmentation."}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b72 min read\u00b7Nov 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&user=Jehill+Parikh&userId=c972081b627e&source=-----c8d7e9bf2416----0-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://jehillparikh.medium.com/u-nets-with-attention-c8d7e9bf2416?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8d7e9bf2416&operation=register&redirect=https%3A%2F%2Fjehillparikh.medium.com%2Fu-nets-with-attention-c8d7e9bf2416&source=-----1099c195f3f0----0-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://youssefraafat57.medium.com/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Youssef Hosni"}, {"url": "https://pub.towardsai.net/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Towards AI"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Getting Started With Stable DiffusionStable Diffusion is a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION. It\u2019s\u2026"}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b712 min read\u00b7Nov 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-artificial-intelligence%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&user=Youssef+Hosni&userId=859af34925b7&source=-----f343639e4931----1-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://pub.towardsai.net/getting-started-with-stable-diffusion-f343639e4931?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff343639e4931&operation=register&redirect=https%3A%2F%2Fpub.towardsai.net%2Fgetting-started-with-stable-diffusion-f343639e4931&source=-----1099c195f3f0----1-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Steins"}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Diffusion Model Clearly Explained!How does AI artwork work? Understanding the tech behind the rise of AI-generated art."}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b77 min read\u00b7Dec 26, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&user=Steins&userId=a36be384d77d&source=-----cd331bd41166----0-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166?source=read_next_recirc-----1099c195f3f0----0---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd331bd41166&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40steinsfu%2Fdiffusion-model-clearly-explained-cd331bd41166&source=-----1099c195f3f0----0-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Using Transformers for Computer VisionAre Vision Transformers actually useful?"}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b713 min read\u00b7Oct 5, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----6f764c5a078b----1-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/using-transformers-for-computer-vision-6f764c5a078b?source=read_next_recirc-----1099c195f3f0----1---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f764c5a078b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fusing-transformers-for-computer-vision-6f764c5a078b&source=-----1099c195f3f0----1-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----2-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----1099c195f3f0----2---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----1099c195f3f0----2-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://jimclydemonge.medium.com/?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://jimclydemonge.medium.com/?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Jim Clyde Monge"}, {"url": "https://medium.com/geekculture?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "A Simple Way To Run Stable Diffusion 2.0 Locally On Your PC \u2014 No Code GuideAn easy and no-code guide on how to run Stable Diffusion 2.0 on local PC with Web UI."}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": "\u00b73 min read\u00b7Dec 1, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2F3beb911e444c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fa-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c&user=Jim+Clyde+Monge&userId=819323b399ac&source=-----3beb911e444c----3-----------------clap_footer----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/a-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c?source=read_next_recirc-----1099c195f3f0----3---------------------b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "9"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3beb911e444c&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fa-simple-way-to-run-stable-diffusion-2-0-locally-on-your-pc-no-code-guide-3beb911e444c&source=-----1099c195f3f0----3-----------------bookmark_preview----b73b92e3_43bb_43af_a5b2_a7f19fc413cf-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----1099c195f3f0--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}