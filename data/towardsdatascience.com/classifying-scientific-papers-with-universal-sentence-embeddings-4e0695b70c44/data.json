{"url": "https://towardsdatascience.com/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44", "time": 1683002522.591136, "path": "towardsdatascience.com/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44/", "webpage": {"metadata": {"title": "Classifying Scientific Papers with Universal Sentence Embeddings | by Riccardo Di Sipio | Towards Data Science", "h1": "Classifying Scientific Papers with Universal Sentence Embeddings", "description": "For several years by now, computers have been achieving an almost human-like ability to identify buildings, animals and other objects thanks to wondrous advancements in computer vision. The\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "convolutional neural networks", "paragraph_index": 0}, {"url": "http://www.robots.ox.ac.uk/~vgg/software/vgg_face/", "anchor_text": "publicly available", "paragraph_index": 0}, {"url": "https://keras.io/applications/#vgg19", "anchor_text": "VGG -19", "paragraph_index": 0}, {"url": "https://keras.io/applications/#mobilenet", "anchor_text": "MobileNet", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Origin_of_language", "anchor_text": "not fully understood", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Noam_Chomsky", "anchor_text": "Noam Chomsky", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Chomsky_hierarchy", "anchor_text": "hierarchical theory of grammars", "paragraph_index": 1}, {"url": "https://nlp.stanford.edu/projects/coref.shtml", "anchor_text": "coreference resolution", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507", "anchor_text": "question answering", "paragraph_index": 3}, {"url": "https://talktotransformer.com/", "anchor_text": "language generation", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da", "anchor_text": "named-entity recognition", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Sparse_matrix", "anchor_text": "sparse-matrix", "paragraph_index": 6}, {"url": "https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4", "anchor_text": "word2vec or skip-gram", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/BERT_(language_model)", "anchor_text": "BERT", "paragraph_index": 10}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 10}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/4", "anchor_text": "Universal Sentence Encoder", "paragraph_index": 11}, {"url": "https://betatim.github.io/posts/analysing-the-arxiv/", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal Component Analysis", "paragraph_index": 17}, {"url": "https://medium.com/@thresholdvc/neurips-2019-entering-the-golden-age-of-nlp-c8f8e4116f9d", "anchor_text": "people argued", "paragraph_index": 21}], "all_paragraphs": ["For several years by now, computers have been achieving an almost human-like ability to identify buildings, animals and other objects thanks to wondrous advancements in computer vision. The breakthrough that made this possible is a combination of theoretical intuitions (i.e. the introduction of convolutional neural networks, back in the 1980s) and hardware (thanks to the widespread and cost-effective availability of GPU cards) that make it possible to execute complex algorithms in a reasonable time. The epitome of this category is probably the VGG class of models, that is publicly available for deployment as most of the pre-trained models are. As most of its siblings, VGG -19 requires a powerful cluster of computers to be trained, but once the initial phase is done, it can be deployed on less powerful machines, in many cases even mobile devices (see for example MobileNet). However, while the field of computer vision has been living in a golden age for quite some time, the same can\u2019t be really said for another task: the interpretation of written text.", "Arguably, humans would not have survived their early stage of biological evolution had they not be able to identify threats such as lions, hyenas and other predators before it was too late. On the other hand, anthropologists believe that human societies became more and more complex as our linguistic abilities improved. While how language became a thing in the first place is not fully understood, it is quite evident that a fully-developed language with syntax is not needed to communicate basic information. This concept was famously expounded by Noam Chomsky in his hierarchical theory of grammars. It is therefore not too far fetched to expect machines to be able to understand human language, or at least interact with us by the means of verbal communication, albeit limited in the beginning. Long story short, we are getting there, but the intricacies of human language are a very hard nut to crack. The most famous problem is probably the coreference resolution, i.e. the task of finding all expressions that refer to the same entity in a text. Take the following sentence:", "\u201cI let an iron ball roll down a slope, and as it hit a glass window, it shattered.\u201d", "Any person can tell that the first pronoun it refers to the ball, while the second points to glass and not vice versa. Grammatically, both options are perfectly valid, but the knowledge of the physical world and some intuition lead us to the correct matching. This is far from being trivial for machines. The current paradigm requires large neural network models to be trained on large bodies of text (called corpora) from which the probability of association between words (n-grams) is estimated. Once the model is trained, it is subsequently fine-tuned to a certain task, e.g. question answering, language generation, named-entity recognition. In the following, we\u2019ll focus on a relatively simple task: document classification. We\u2019re going to train a machine to identify the category of a given scientific paper based on its abstract.", "While it is quite straightforward to encode an image in terms of numbers that can be manipulated by a computer (i.e. by representing its pixels as a matrix), the same can not be said of a text. In fact, different possibilities exist, and the best depends mostly on the task itself. The most basic method consists of creating a dictionary, i.e. an indexed collection of all the words that appear in the corpus. A given text is then represented by a vector, where each element is the index of the corresponding word.", "The problem with this approach is that the length of the vector is variable (in fact equivalent to the number of words in the text), which is not a desirable feature for numerical processing. Another common choice is to represent the text as a matrix, where each row represents a word. In turn, each word is represented by a fixed-length binary vector where all the elements are zero except a single one that uniquely points to the corresponding word in the dictionary, for example:", "This method, called one-hot vector, is not particularly suited either because it requires lots of memory, that is basically wasted as most of the elements are zero. The common solution to this problem is to use a sparse-matrix instead.", "In recent years, people have realized that an even better solution is to represent each word as dense vectors of fixed length, for example:", "The most obvious advantage is that fewer dimensions are needed to encode each word. The dictionary is basically turned into a lookup table of word vectors. The real deal, however, is the ability to \u201cattach\u201d semantic meaning to each dimension of the embedding space. The most glorious example is the following: given four vectors representing king, queen, male, female, one finds that:", "How this is achieved goes beyond the scope of this post, but suffice to say that a neural network (word2vec or skip-gram model) is trained on a corpus to predict the most probable context, i.e. the word that follows a given one, or the two words are more likely to be found before and after a given one.", "In very recent times, this and other similar tricks have been deployed to train huge networks such as BERT and GPT-2 by crawling millions of web pages (including a full dump of Wikipedia). In this, language-processing networks are doing the same as computer vision ones: their training would be impossible without having access to a massive database of examples, which in turn became possible only with the advent of the Internet.", "Finally, and most importantly for our aim, a similar network has been trained by considering not just individual words, but sentences and short paragraphs. We\u2019ll deploy the embedding vectors created by a pre-trained network called Universal Sentence Encoder (USE) to represent documents numerically, and apply some simple classification techniques.", "With sentence embeddings in our hands, we can now turn our attention to the actual classification task. For this example, we\u2019ll create a small database for training/testing by downloading the abstracts of pre-prints that appear on the arXiv server. A good introduction about how to download such documents can be found here.", "Having saved the abstracts in a dataframe, we can later load the file and use the labelled information to train a simple network.", "Pay attention to the following parts of the code:", "Despite its relative simplicity, the code is able to distinguish papers between astro-ph and cs.AI categories with 99% accuracy and very little overtraining (the validation loss plateaus at a higher value 0.0254 than the training loss 0.0139, which keeps falling for a while longer). As a matter of fact, this feat is possible thanks to the USE embeddings that does the heavy lifting. You have just discovered the joy of transfer learning: once a model has been pre-trained on a large database, all you have to do is to fine-tune it for your purpose!", "One aspect of these word or sentence embeddings that can be quite troublesome for practical applications is the sheer size of the embedding space, here 512 dimensions. The most likely problem to arise is the lack of a large database even to perform the fine-tuning step \u2014 connecting the embedding layer to a dense layer with 64 output nodes would have 512*64+64 = 32,832 parameters to be fitted. As a rule of thumb, there should be at least 10 times more training examples, i.e. something in the order of 300k, which may be large for many applications.", "One can easily understand that not all dimensions carry useful information for the task at hand. A simple but effective way to reduce the size of the embedding space while retaining information is by applying a Principal Component Analysis (PCA). The idea is look at variations across all the embedding vectors, and discard the dimensions where there are large variations (representing noise) while retaining those with stronger associations (principal components).", "As it is virtually impossible to visualize what happens in 512 dimensions, a good way to look at it is to compute the cosine similarities (i.e. normalized dot products) between these vectors, and across different categories. The idea behind this operation is that vectors belonging to similar categories should be close to each other and hence have a small angle, while those belonging to different categories should be distant and hence have a large angle. The cosine operation squashes the angles to be between 0 and 1, making a comparison easier. If the PCA is doing its job, one should expect an improvement in the separation between different classes after the dimensional reduction due to the PCA, i.e. a \u201csignal\u201d should appear by discarding noise.", "That\u2019s exactly what the plots below shows. The blue histograms show the distributions of cosine similarities using the full 512 embedding space, while the red ones represent the same after PCA. On the diagonal, documents belonging to the same class should have a cosine very close to one, but this doesn\u2019t appear to be the case (they are mostly distributed around 0.5). Also, by looking at the products between different categories (off-diagonal histograms), one can see little separation, i.e. some histograms peak below 0.5, but the effect is not dramatic. There would be little hope to classify the documents if we used the full embedding space!", "However, after PCA (red histograms), one can see a number of very interesting features. To start with, histograms on the diagonal are pushed toward 1, which is a good sign: papers inside the same category are represented by vectors that are close to each other! Also, looking now at off-diagonal histograms, it\u2019s quite clear that the histograms are pushed toward 0, i.e. the similarity between categories is quite low. Finally, it\u2019s interesting to note that even after PCA, two categories are still hard to distinguish: hep-ex (experimental high-energy physics) and astro-ph (astrophysics). Is this surprising? Is data basically telling us that this two branches of science are more strongly correlated than others such as artificial intelligence (cs.AI) and number theory (math.NT)?", "To summarize, as many people argued at the NeurIPS2019 conference in Vancouver, the field of natural language processing (NLP) is entering a golden age. NLP is basically going through the same revolution that Computer vision had in 2010. Transfer learning and pre-trained models give basically to anyone a chance to fine-tune simple models for specific tasks such as document classification.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "NLP Machine Learning engineer at Ceridian. Formerly physicist at U Toronto, Bologna, CERN LHC/ATLAS."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4e0695b70c44&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://riccardo-disipio.medium.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": ""}, {"url": "https://riccardo-disipio.medium.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Riccardo Di Sipio"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4267d9884285&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&user=Riccardo+Di+Sipio&userId=4267d9884285&source=post_page-4267d9884285----4e0695b70c44---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Convolutional_neural_network", "anchor_text": "convolutional neural networks"}, {"url": "http://www.robots.ox.ac.uk/~vgg/software/vgg_face/", "anchor_text": "publicly available"}, {"url": "https://keras.io/applications/#vgg19", "anchor_text": "VGG -19"}, {"url": "https://keras.io/applications/#mobilenet", "anchor_text": "MobileNet"}, {"url": "https://en.wikipedia.org/wiki/Origin_of_language", "anchor_text": "not fully understood"}, {"url": "https://en.wikipedia.org/wiki/Noam_Chomsky", "anchor_text": "Noam Chomsky"}, {"url": "https://en.wikipedia.org/wiki/Chomsky_hierarchy", "anchor_text": "hierarchical theory of grammars"}, {"url": "https://nlp.stanford.edu/projects/coref.shtml", "anchor_text": "coreference resolution"}, {"url": "https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507", "anchor_text": "question answering"}, {"url": "https://talktotransformer.com/", "anchor_text": "language generation"}, {"url": "https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da", "anchor_text": "named-entity recognition"}, {"url": "https://en.wikipedia.org/wiki/Sparse_matrix", "anchor_text": "sparse-matrix"}, {"url": "https://becominghuman.ai/how-does-word2vecs-skip-gram-work-f92e0525def4", "anchor_text": "word2vec or skip-gram"}, {"url": "https://en.wikipedia.org/wiki/BERT_(language_model)", "anchor_text": "BERT"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/4", "anchor_text": "Universal Sentence Encoder"}, {"url": "https://betatim.github.io/posts/analysing-the-arxiv/", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Principal_component_analysis", "anchor_text": "Principal Component Analysis"}, {"url": "https://medium.com/@thresholdvc/neurips-2019-entering-the-golden-age-of-nlp-c8f8e4116f9d", "anchor_text": "people argued"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4e0695b70c44---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/natural-language-process?source=post_page-----4e0695b70c44---------------natural_language_process-----------------", "anchor_text": "Natural Language Process"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4e0695b70c44---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/principal-component?source=post_page-----4e0695b70c44---------------principal_component-----------------", "anchor_text": "Principal Component"}, {"url": "https://medium.com/tag/sentence-embedding?source=post_page-----4e0695b70c44---------------sentence_embedding-----------------", "anchor_text": "Sentence Embedding"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&user=Riccardo+Di+Sipio&userId=4267d9884285&source=-----4e0695b70c44---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&user=Riccardo+Di+Sipio&userId=4267d9884285&source=-----4e0695b70c44---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4e0695b70c44&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4e0695b70c44---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4e0695b70c44--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4e0695b70c44--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4e0695b70c44--------------------------------", "anchor_text": ""}, {"url": "https://riccardo-disipio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://riccardo-disipio.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Riccardo Di Sipio"}, {"url": "https://riccardo-disipio.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "554 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4267d9884285&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&user=Riccardo+Di+Sipio&userId=4267d9884285&source=post_page-4267d9884285--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb32833879b5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fclassifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44&newsletterV3=4267d9884285&newsletterV3Id=b32833879b5a&user=Riccardo+Di+Sipio&userId=4267d9884285&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}