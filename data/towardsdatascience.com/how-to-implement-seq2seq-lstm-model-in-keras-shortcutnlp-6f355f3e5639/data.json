{"url": "https://towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639", "time": 1682995427.620264, "path": "towardsdatascience.com/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639/", "webpage": {"metadata": {"title": "How to implement Seq2Seq LSTM Model in Keras | by Akira Takezawa | Towards Data Science", "h1": "How to implement Seq2Seq LSTM Model in Keras", "description": "At the first time when I tried to implement seq2seq for Chatbot Task, I got stuck a lot of times especially about the Dimension of Input Data and Input layer of Neural Network Architecture. Now I\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA", "anchor_text": "Machine Learning at Microsoft", "paragraph_index": 7}, {"url": "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html", "anchor_text": "Cornell Movie \u2014 Dialogs Corpus Dataset", "paragraph_index": 9}, {"url": "https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/", "anchor_text": "Understand the Difference Between Return Sequences and Return States for LSTMs in Keras", "paragraph_index": 25}, {"url": "https://machinelearningmastery.com/author/jasonb/", "anchor_text": "Jason Brownlee", "paragraph_index": 25}], "all_paragraphs": ["At the first time when I tried to implement seq2seq for Chatbot Task, I got stuck a lot of times especially about the Dimension of Input Data and Input layer of Neural Network Architecture.", "Now I understand that unless you have a deep understanding of linear algebra concepts such as matrix and tensor, or how Keras API works, you\u2019ll get errors continuously(and that\u2019s so miserable!).", "So in this article, I will explain the complete step-by-step guide of seq2seq for Keras, by checking the inputs and outputs in each process carefully.", "In addition, I will provide some portable original functions which you can use in any other NLP projects for preprocessing.", "Seq2Seq is a type of Encoder-Decoder model using RNN. It can be used as a model for machine interaction and machine translation.", "By learning a large number of sequence pairs, this model generates one from the other. More kindly explained, the I/O of Seq2Seq is below:", "And here we have examples of business applications with Seq2Seq:", "If you want more information about Seq2Seq, here I have a recommendation from Machine Learning at Microsoft on Youtube.", "Now we could understand the versatility of this technique, so let\u2019s take a look at the whole process!", "For training our seq2seq model, we will use Cornell Movie \u2014 Dialogs Corpus Dataset which contains over 220,579 conversational exchanges between 10,292 pairs of movie characters. And it involves 9,035 characters from 617 movies.", "Here one of the conversations from the data set:", "Then we will input these pairs of conversations into Encoder and Decoder. So that means our Neural Network model has two input layers as you can see below.", "This is our Seq2Seq Neural Network Architecture for this time:", "Let's visualize our Seq2Seq by using LSTM:", "The Black Box for \u201cNLP newbie\u201d is I think this:", "How each layer compiles data and change their Dimension of data?", "To make this clear, I will explain how it works in detail. The Layers can be broken down into 5 different parts:", "NOTE: sequence_length is MAX_LEN unified by padding in preprocessing", "NOTE: Input() is used only for Keras tensor instantiations", "NOTE: vocab_size is the number of unique words", "NOTE: Data should be a group of One-Hot Vector", "NOTE: Data is word embedded in 50 dimensions", "The tricky argument of the LSTM layer is these two:", "Whether to return the last state along with the output", "Whether the last output of the output sequence or a complete sequence is returned", "You can find a good explanation from Understand the Difference Between Return Sequences and Return States for LSTMs in Keras by Jason Brownlee.", "NOTE: Data is word embedded in 50 dimensions", "NOTE: Data reshaped by LSTM as a hidden layer in 20 dimensions", "If return_state = False and return_sequences = False :", "NOTE: TimeDistributedDenselayer allows us to apply a layer to every temporal slice of an input", "NOTE: Data reshaped by LSTM as a hidden layer in 20 dimensions", "After Data passed this Fully Connected Layer, we use Reversed Vocabulary which I will explain later to convert from One-Hot Vector into Word Sequence.", "Before jumping on preprocessing of Seq2Seq, I wanna mention about this:", "We need some Variables to define the Shape of our Seq2Seq Neural Network on the way of Data preprocessing", "OK, please put this information on your mind, let\u2019s start to talk about preprocessing. The whole process could be broken down into 8steps:", "I always use this my own function to clean text for Seq2Seq:", "<BOS> means \u201cBegin of Sequence\u201d, <EOS> means \u201cEnd of Sequence\u201d.", "We use Pretraind Word2Vec Model from Glove. We can create embedding layer with Glove with 3 steps:", "Now we finished the entire preprocessing process before input the learning data into our LSTM model.", "After this, you just have to feed them and wait for finishing the learning.", "Thanks for reading, see you in the next article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist, Rakuten / a discipline of statistical causal inference and time-series modeling / using Python and Stan, R / MLOps is my current concern"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F6f355f3e5639&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@akiratakezawa?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akiratakezawa?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Akira Takezawa"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc2291698c8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&user=Akira+Takezawa&userId=bc2291698c8b&source=post_page-bc2291698c8b----6f355f3e5639---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.techleer.com/articles/440-keras-deep-learning-for-python/", "anchor_text": "Keras: Deep Learning for Python"}, {"url": "https://medium.com/u/c3f8c66f5451?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Ravindra Kompella"}, {"url": "https://github.com/samurainote/chatbot_slack_keras", "anchor_text": "GitHub"}, {"url": "https://github.com/samurainote/seq2seq_translate_slackbot", "anchor_text": "GitHub"}, {"url": "https://github.com/samurainote/Text_Summarization_using_Bidirectional_LSTM", "anchor_text": "GitHub"}, {"url": "https://github.com/samurainote/Text_Generation_using_GRU", "anchor_text": "GitHub"}, {"url": "https://www.youtube.com/channel/UCXvHuBMbgJw67i5vrMBBobA", "anchor_text": "Machine Learning at Microsoft"}, {"url": "https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/b71e37fb-5fd9-4094-98c8-04130d5f0771.xhtml", "anchor_text": "https://www.oreilly.com/library/view/deep-learning-essentials/9781785880360/b71e37fb-5fd9-4094-98c8-04130d5f0771.xhtml"}, {"url": "https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html", "anchor_text": "Cornell Movie \u2014 Dialogs Corpus Dataset"}, {"url": "https://bracketsmackdown.com/word-vector.html", "anchor_text": "https://bracketsmackdown.com/word-vector.html"}, {"url": "https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/", "anchor_text": "Understand the Difference Between Return Sequences and Return States for LSTMs in Keras"}, {"url": "https://machinelearningmastery.com/author/jasonb/", "anchor_text": "Jason Brownlee"}, {"url": "https://chunml.github.io/ChunML.github.io/project/Sequence-To-Sequence/", "anchor_text": "Creating A Language Translation Model Using Sequence To Sequence Learning ApproachHello guys. It's been quite a long while since my last blog post. It may sound like an excuse, but I've been struggling\u2026chunml.github.io"}, {"url": "http://sujitpal.blogspot.com/2016/10/deep-learning-models-for-question.html", "anchor_text": "Deep Learning Models for Question Answering with KerasLast week, I was at a (company internal) workshop on Question Answering (Q+A), organized by our Search Guild, of which\u2026sujitpal.blogspot.com"}, {"url": "https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu", "anchor_text": "Machine Translation using Sequence-to-Sequence Learning - NextjournalIn this article we're training a Recurrent Neural Network (RNN) model based on two Long Short-Term Memory (LSTM) layers\u2026nextjournal.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----6f355f3e5639---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/keras?source=post_page-----6f355f3e5639---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/data-science?source=post_page-----6f355f3e5639---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----6f355f3e5639---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----6f355f3e5639---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&user=Akira+Takezawa&userId=bc2291698c8b&source=-----6f355f3e5639---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&user=Akira+Takezawa&userId=bc2291698c8b&source=-----6f355f3e5639---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F6f355f3e5639&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----6f355f3e5639---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----6f355f3e5639--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----6f355f3e5639--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----6f355f3e5639--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akiratakezawa?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@akiratakezawa?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Akira Takezawa"}, {"url": "https://medium.com/@akiratakezawa/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "152 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbc2291698c8b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&user=Akira+Takezawa&userId=bc2291698c8b&source=post_page-bc2291698c8b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff461b1b4c4b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639&newsletterV3=bc2291698c8b&newsletterV3Id=f461b1b4c4b3&user=Akira+Takezawa&userId=bc2291698c8b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}