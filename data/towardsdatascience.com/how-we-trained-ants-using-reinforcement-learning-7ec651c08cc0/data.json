{"url": "https://towardsdatascience.com/how-we-trained-ants-using-reinforcement-learning-7ec651c08cc0", "time": 1683006525.4170501, "path": "towardsdatascience.com/how-we-trained-ants-using-reinforcement-learning-7ec651c08cc0/", "webpage": {"metadata": {"title": "How We Trained Ants Using Reinforcement Learning | Towards Data Science", "h1": "How We Trained Ants Using Reinforcement Learning", "description": "We build an AI that uses Deep-Q learning on a multi-agent system represented by ants. The goal was to train our agents to behave like real ants."}, "outgoing_paragraph_urls": [{"url": "https://antoninduval.github.io/", "anchor_text": "Antonin DUVAL", "paragraph_index": 0}, {"url": "http://thomas-lamson.com/", "anchor_text": "Thomas LAMSON", "paragraph_index": 0}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "Mnih et al., Human-level control through deep reinforcement learning(2015)", "paragraph_index": 22}, {"url": "https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html", "anchor_text": "this great post", "paragraph_index": 22}, {"url": "https://arxiv.org/pdf/1711.08946.pdf", "anchor_text": "Branching Q-learning", "paragraph_index": 23}, {"url": "https://github.com/WeazelDev/AntsRL", "anchor_text": "https://github.com/WeazelDev/AntsRL", "paragraph_index": 44}], "all_paragraphs": ["Written by Antonin DUVAL and Thomas LAMSON", "If you ever observed a colony of ants, you may have noticed how well organised they seem. In order to gather food and defend itself from threats, an average anthill of 250,000 individuals has to cooperate and self-organise. By the use of specific roles and of a powerful tool \u2014 the pheromones \u2014 thousands of somewhat limited ants can cooperate to achieve greater goals.", "We were interested in exploring the emergence of such behaviors in a simulated environment. Giving some \u201cants\u201d the correct tools to use and an environment to explore, would we see these kind of phenomena appear from nothing?", "At cross-roads of Multi-Agent Theory and Reinforcement Learning, we designed a system that would let a colony of very simple ants develop clever strategies to optimize their food supply.", "In order to get your attention and to get you get a better understanding of what we are trying to do here, take a look at this quick video from our beautiful random agent.", "It doesn\u2019t do much, but you will see how we will improve our ants to get them working together ! We will explain further below how the environement works and how the agent can interact with it.", "In order to get a behavior from the ants we trained that is close to real ants, it was important that we first designed their environment in a clever way. So, let\u2019s cover the main hypothesis we assumed to perform our experiment.", "The first and most important hypothesis is the total isolation of agents. It is one of the main hypothesis of the multi-agent paradigm: agents don\u2019t have any direct way of communication that doesn\u2019t pass through the environment itself.", "The second hypothesis is the relativity of perceptions. It is important that our agents doesn\u2019t have any clue what is their location and their orientation in the world. In a general way, they only see the world from their individual perspective.", "As for any RL application, the definition of the environment is at the core of the project.", "The code to generate an environment is made as simple as possible so that anyone can experiment with different map configurations. By using different procedural generators, one can generate walls with various densities, a lot or a few food sources, away or not from the anthill, etc. Finally, a seed can be specified to make sure that the generated environment is controlled if necessary, which is useful to evaluate our different agents on the same map.", "You already saw them in the picture above, but let\u2019s give some more details about our agents: the ants. As explained in the hypotheses, our agents are completely independent from each other and don\u2019t share any direct information. Their internal definition vary with the different models we implemented, but their perception of the world and their ways of acting on it remain the same.", "You can see when an ant has picked up some food: it has a green dot between its mandibles. Isn\u2019t it cute?", "They see in front of them in a certain radius. Here is an image representing the shape of their perception, in terms of discrete slots in the 2D grid. First, we compute the floating point coordinates of every perceived slot, by applying translation and rotation relative to their current position, then we round those coordinates to obtain the real 2D grid slots they perceive. It sometime gives weird results, but we consider that our ants are like real ants: very bad at seeing things! They should however be able to easily detect pheromones in front of them, walls, food or other ants.", "In mathematical terms, their perception is always a square matrix of 7x7, with a mask to make it more round, and with as many layers as necessary to represent every \u2018channel\u2019 of their vision. In our case, each type of object in the world (anthill, other ants, food, walls, pheromones, etc.) is represented on a different layer of perception. This gives very regular perception which is easy to feed to a neural network afterwards.", "Ants have 2 types of actions they can perform:", "They automatically pick up food when their mandibles are empty and there is food on the floor, and they drop it if they are in the anthill. We could have let them learn those actions too, but we thought there was nothing really complicated in these simple rules so we saved some learning time and implemented them manually.", "In the end, our ants have to handle their heading to go in the right direction and to drop pheromones to communicate with other ants in the long term. This is, as we will see, already quite difficult.", "At this stage, we have functional, but completely empty shells of ants. Now comes the time to give them life and make them actually perform actions.", "If you\u2019ve heard of reinforcement learning before, you know that one of the key elements to learn an optimal strategy is to have a well designed reward function. This is how we will teach our agents to perform certain kinds of behaviors. Here, our ants only have one goal: relentlessly supply the anthill with food. This simple task requires long-term planning and social cooperation. Hence, we divided it in a set of different sub-goals: smaller rewards that we will use to tell to our ants \u201cyes, you did great, continue like this!\u201d.", "This is how we wrote our reward function :", "Here, \u03c91, \u03c92, \u03c93 and \u03c94 are arbitrary factors that are set at the beginning of the training. Those are hyper-parameters that we had to tune with great care as they completely change the way our ants learn and behave.", "The goal of this post is not to explain in detail what is the deep Q-learning algorithm (Mnih et al., Human-level control through deep reinforcement learning(2015) [1]). If you are not familiar with this, we invite you to read this great post [2] which explains everything you need to know about reinforcement learning!", "Since ants can perform two actions at the same time (moving and deposit pheromones), we need to output 2 set of Q-value for each action. How to do that ? The paper Branching Q-learning A. Tavakoli, 2019 [3] gives a shared decision module followed by several network branches, one for each action dimension. In the end, we have two separate network heads predicting Q-values for independent actions. To decide the actions to perform, we only have to select the greatest value of each branch, which gives us a combination of actions!", "Since we didn\u2019t have a way to be sure that we were going in the right direction (well, it had to work theoretically, but that\u2019s easy to say), we decided to take a first peak at a simpler problem: teach the ants to explore the map.", "This replay was taken from a random map on evaluation mode. From this replay, we can get two major insights:", "Great, our model works! However, let\u2019s try and be more like rigorous scientists and prove you that our ants improved by displaying the graph of the mean reward and mean loss per episode of training:", "An increasing loss? Well, that is not that bad in reinforcement learning. In our case, it is mainly increasing because the rewards our agents get are getting higher and higher, we also increases the range of the errors the network makes upon predicting those Q-values. What we\u2019re really interested in is the blue curve where we can clearly see the exploration increasing from the random baseline (which is already quite good at exploration only).", "We successfully trained an exploration agent. Now, let\u2019s take a deep breath and dive deep into the real task: make the ants pick up food and bring it back to the anthill.", "To this end, we decided therefore to change the architecture of our network, and took an inspiration from LSTM networks. We added a branch to create some kind of memory information that would be passed from the current state to the next one. In addition, we added a forget gate, which is a stack of layers followed by a sigmoid activation, so that the network could learn when to remember, and what to remember. The complete architecture is given by the diagram below.", "We call \u201cinternal state\u201d two pieces of information that are not directly visible in the perception matrix: how much food the ant is currently holding, and a random seed which gives every ant the opportunity to differentiate itself (the seed remains the same for a given ant during all the episode).", "Finally, the memory is stored in the replay memory as any observation information, giving each ant the ability to be trained to remember things correctly.", "Now that we have a stronger network, a good reward function, we just need to start heating the machine and start training! But wait\u2026 There is still some important hyper-parameters to choose.", "The number of ants The good thing with this kind of multi-agent system is that we can make it work with any number of ants. However, since training with an army of 250,000 ants like in real life doesn\u2019t seem reasonable for the planet -and our poor computers-, we decided to train with only 20 ants.", "The number of epochs This parameter is chosen empirically. Because the environment is coded in Python, the engine is quite slow. We therefore had to limit the number of epochs to 150\u2013200 to limit training time. This is quite a small number of episodes if we compare to other reinforcement learning application. However, results showed it was sufficient to get performant behaviors in our case.", "Decaying epsilon Exploration is key in reinforcement learning. We used a decaying \u03f5 factor. This allows our agents to explore a lot during the first episodes, and then slowly start to pick the best action.", "Finally, it\u2019s time to see some results! Did our ants learn anything? Can we give an interpretation to what has been learned? We will cover this kind of questions from now on.", "Even though we trained our colonies on not more than 1,000 steps for each episode, we made a video of one colony for 20,000 steps to see what was happening. And remember how we trained our model with 20 ants? Well, once trained, we can actually use any number of ants we want! So, because it\u2019s more fun, we ran this simulation with 50 ants. We can finally see our ants gather all the food in the map, relentlessly building their pheromone network throughout the map, changing target as soon as one is exhausted\u2026 We included some pheromones-only parts as we find them beautiful.", "What we observe is that they build some kind of network of pheromone that they use to move around the map. Because they learned to be attracted by a certain type of pheromones, they tend to favor some paths more than others. This creates a hierarchy of paths, with some being real highways, and some others being more like small country lanes.", "Our initial objective was a little ambitious: we were already imagining colonies fighting with each other for food, ants pushing rocks out of pathways or into small rivers to get through obstacles and obtain more food\u2026 Always aim for the stars!", "The results, even after 150 epochs, are very promising and we definitely saw some emerging behavior in the ants society, which was our main concern. Now, we feel like we can\u2019t stop there, but it\u2019s already the end of the time we had for this project. However that won\u2019t keep us from updating this project and this post!", "In further developments, we would let the ants act on their mandibles in order to choose between picking food up and fighting threats like other ants. We would also make the maps more complex and offer the ants a way to communicate more easily through a visible state that other ants can see when close enough. But before all of that, we would re-implement our simulation in C++: we have a scoop, Python is slow.", "We only saw the very beginning of what we expected as emerging behaviors, but we believe that if we add more tools at the disposal of our agents, and more complex problems to solve, we would see more and more clever behaviors appear!", "Well\u2026 A lot of things can be done, starting from where we stopped! Maybe a new chapter will follow? Who knows.", "To get the full Python source-code of the project, please visit our github: https://github.com/WeazelDev/AntsRL", "The main modules we used throughout our project:", "Full article can be seen here :", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I\u2019m student in Artificial Intelligence at CentraleSup\u00e9lec, in Paris. I love doing fun projects and share with others."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7ec651c08cc0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@antonin.duval?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antonin.duval?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Antonin Duval"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8eff7bb7f4fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&user=Antonin+Duval&userId=8eff7bb7f4fa&source=post_page-8eff7bb7f4fa----7ec651c08cc0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://antoninduval.github.io/", "anchor_text": "Antonin DUVAL"}, {"url": "http://thomas-lamson.com/", "anchor_text": "Thomas LAMSON"}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "Mnih et al., Human-level control through deep reinforcement learning(2015)"}, {"url": "https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html", "anchor_text": "this great post"}, {"url": "https://arxiv.org/pdf/1711.08946.pdf", "anchor_text": "Branching Q-learning"}, {"url": "https://github.com/WeazelDev/AntsRL", "anchor_text": "https://github.com/WeazelDev/AntsRL"}, {"url": "https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf", "anchor_text": "Human-level control through deep reinforcement learning,"}, {"url": "https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html", "anchor_text": "A (Long) Peek into Reinforcement Learning (blogpost)"}, {"url": "https://antoninduval.github.io/posts/2020/04/blog-post-2/", "anchor_text": "https://antoninduval.github.io/posts/2020/04/blog-post-2/"}, {"url": "http://thomas-lamson.com/antsrl-multi-agent-reinforcement-learning/", "anchor_text": "http://thomas-lamson.com/antsrl-multi-agent-reinforcement-learning/"}, {"url": "https://antoninduval.github.io/posts/2020/04/blog-post-2/", "anchor_text": "https://antoninduval.github.io/"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----7ec651c08cc0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-agent-systems?source=post_page-----7ec651c08cc0---------------multi_agent_systems-----------------", "anchor_text": "Multi Agent Systems"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7ec651c08cc0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-q-learning?source=post_page-----7ec651c08cc0---------------deep_q_learning-----------------", "anchor_text": "Deep Q Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7ec651c08cc0---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&user=Antonin+Duval&userId=8eff7bb7f4fa&source=-----7ec651c08cc0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&user=Antonin+Duval&userId=8eff7bb7f4fa&source=-----7ec651c08cc0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7ec651c08cc0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7ec651c08cc0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7ec651c08cc0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antonin.duval?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@antonin.duval?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Antonin Duval"}, {"url": "https://medium.com/@antonin.duval/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8eff7bb7f4fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&user=Antonin+Duval&userId=8eff7bb7f4fa&source=post_page-8eff7bb7f4fa--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8eff7bb7f4fa%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-we-trained-ants-using-reinforcement-learning-7ec651c08cc0&user=Antonin+Duval&userId=8eff7bb7f4fa&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}