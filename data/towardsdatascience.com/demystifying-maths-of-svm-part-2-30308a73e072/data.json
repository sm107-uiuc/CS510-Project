{"url": "https://towardsdatascience.com/demystifying-maths-of-svm-part-2-30308a73e072", "time": 1682994891.2999392, "path": "towardsdatascience.com/demystifying-maths-of-svm-part-2-30308a73e072/", "webpage": {"metadata": {"title": "Demystifying Maths of SVM \u2014 Part 2 | by Krishna Kumar Mahto | Towards Data Science", "h1": "Demystifying Maths of SVM \u2014 Part 2", "description": "After paddling through K-NN, Naive Bayes\u2019 Classifier, Logistic Regression and Linear Regression along a rather flat curve, SVM came and affected a sharp dip on it. Having derived the optimization\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "Demystifying Maths of SVM", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article", "paragraph_index": 11}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous part of this article", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "first part", "paragraph_index": 22}, {"url": "https://math.stackexchange.com/questions/174270/what-exactly-is-the-difference-between-a-derivative-and-a-total-derivative", "anchor_text": "stackexchange", "paragraph_index": 41}, {"url": "http://linkedin.com/in/krishna-kumar-mahto", "anchor_text": "linkedin.com/in/krishna-kumar-mahto", "paragraph_index": 53}], "all_paragraphs": ["After paddling through K-NN, Naive Bayes\u2019 Classifier, Logistic Regression and Linear Regression along a rather flat curve, SVM came and affected a sharp dip on it. Having derived the optimization objective of SVM for linearly-separable dataset (I have thoroughly described the derivation in my previous article, which is the part-1 of this mini series), the next halt came with the \u2018slacked\u2019 SVM. In this article, we shall go through step-by-step derivation with detailed discourse about each step.", "Note: This article is in continuation of my previous article- Demystifying Maths of SVM. Reiterating all the terms and concepts that are fundamental blocks to both the linearly classifiable as well as non-linearly classifiable datasets would be redundant as the same have already been explored in the first part of this article. So, I would suggest that if some topics or terms feel unfamiliar, do have a look at that article as well.", "Hypothesis, w.r.t. a machine learning model, is the model itself, which is the classifier (which, is a function) we intend the machine to learn.", "In SVM, the class labels are denoted as -1 for negative class and +1 for positive class.", "The optimization problem that we shall have finally derived at the end of this article and what SVM solves to fit the best function is:", "For reference, here is how the optimization problem of SVM for linearly separable dataset looks like:", "Terms that the readers are expected to be familiar with:", "Visit my previous article to have a better context on these terms.", "Just to make sure we are on the same page, lets discuss how SVM works. There are two ways SVM is interpreted:", "SVM maximizes the margin (as drawn in fig. 1) by learning a suitable decision boundary/decision surface/separating hyperplane.", "SVM maximizes the geometric margin (as already defined, and shown below in figure 2) by learning a suitable decision boundary/decision surface/separating hyperplane.", "In the previous article, we had derived the optimization problem of SVM from scratch. We reached at the following expression:", "It turns out that this optimization problem can learn a reasonable hyperplane only when the dataset is (perfectly) linearly separable (fig. 1). This is because of the set of constraints that defines a feasible region mandating the hyperplane to have a functional margin of atleast 1 w.r.t. each point (values of w and b are to be learned such that functional margin becomes 1. Domain of any hyperplane which has a positive functional margin falls in the feasible region since w and b could simply be scaled to give a functional margin of 1 while still keeping the hyperplane unchanged. However, only some particular (w*, b*) hyperplane can optimize the objective function in the feasible region).", "Therefore, no such hyperplanes can ever be considered during optimization which have a negative functional margin (figure 2) with repsect to any example. A negative functional margin is still an extreme event that the current optimization problem avoids. Apart from this, a separating hyperplane like the dotted yellow one as shown in figure 3, is also not possible. As we can see (figure 3), the blue hyperplane stands more towards the red examples, and is far away from the collection of green examples because of a noise in the dataset. A hyperplane that looks more like the yellow one, although might appear to be doing a better job in forming a decision boundary, it does not fall in the feasible region of the current optimization problem.", "From this, we can also easily derive the fact that situations where points lie on the hyperplane itself are also not possible. Furthermore, the feasible region may be empty, in which case SVM completely fails to classify the examples. Figure 4 depics a situation where SVM cannot learn any separating hyperplane with the current set up.", "Seemingly, the current formulation of SVM is not robust, and any natural dataset is likely to make it of less to no use (because natural datasets are rarely linearly separable). This configuration of SVM is called as Hard-margin SVM, signifying the fact that it has a very strict constraint.", "Before starting with the next section, I would like to reiterate the fact that the name represents SVM\u2019s stringency to learn a decision surface under a set of strict constraints giving off a feasible region that remains invariable w.r.t. the training examples. Recall that the constraints represent a feasible region wherein all hyperplanes have a positive functional margin w.r.t. all data points with emphasis on learning scaled values of w and b (for the same hyperplane) so that functional margin is 1.", "In order to become a better classifier, SVM has to have more lenient constraints so that in the feasible region hyperplanes can be allowed to have a functional margin less than 1. To allow this to happen, a slack variable is introduced to each of the constraints of the optimization problem:", "\u2018\u03be\u2019 is the slack variable vector of \u03bei\u2019s (\u03be subscripted with i). We define \u03bei for each example in the dataset. Each of its elements has to be atleast zero to really allow some points to have a functional margin less than or equal to 1(the second inequality in the above expression). It has a different value for each example in the training dataset and therefore, a hyperplane may have different minimum requirement for the values of functional margin for different points. This also allows the hyperplane to have a functional margin of less than 1 w.r.t. some data points.", "The optimization problem thus, has become:", "This means, we can now be more hopeful to get a separating hyperplane as the one shown below as yellow dotted line in figure 2.", "Does the new optimization problem we have arrived at express what we think it should?", "In the previous part of this article, we established that the optimization function corresponds to maximizing the geometric margin (Maximizing geometric margin in itself is a non-convex optimization problem, so we systematically reduced the maximization of geometric margin problem to an equivalent convex minimization problem). So, the optimization objective says \u201cMaximize the geometric margin subject to the given constraints.\u201d From the constraints, we can observe that since \u03bei can contain any non-negative values, it can take arbitrarily large, positive value for an example. This means functional margin of the hyperplane w.r.t. a point can be any value less than 1, and even negative. This means a hyperplane can make wrong classifications (in the first part of this article, we discussed a negative functional margin w.r.t. a point implies the point has been wrongly classified). Therefore, a hyperplane can have arbitrarily large value of geometric margin as it is free to get pushed anywhere. This is not a guarantee to get us a separating hyperplane!", "Doing this may expand the feasible region in a way that can make us end up with a separating hyperplane that is worse.", "It turns out that we do have a nice choice. We add a new objective to our optimization problem as follows:", "With the introduction of the second objective, the very first thing that can be noticed is the fact that now the optimization problem will not end up with just any value of \u03be. The solution is no more as straightforward as it would have been with one objective. Irrespective of the context for which we intend to solve an optimization problem, it does not very often happen that the same point optimizes two or more objectives simultaneously. The instances of this can be seen everywhere. For e.g., for cars with similar specifications, as we look for higher speeds the handling is likely to go down. It is unlikely that both the speed (objective-1) and handling (objective-2) will have their best values at the same point (i.e., simultaneously). In other words, if we choose a car with the highest speed, then it is quite probable that it shall not have the best possible handling at the same point in the design (feature) space. The converse is true as well(figure 5). In this example, speed and handling are related in a way that if you increase one, then without any additional adjustments the other is likely to decrease. More generally, two functions which may not be having a meaningful relationship also are unlikely to have the same point of optimum.", "This type of Multi Objective Optimization (MOO) problem is not unheard of. Idea of Pareto optimality [1] was introduced to formally address this issue. Pareto optimality defines Pareto optimal points. These points define an envelope called as Pareto front [2]. These points are found to be the best candidates for choosing an optimal point from for a given MOO problem. However, there can be many Pareto solutions for a given MOO problem. \u201cWhich one is the best?\u201d- \u201cNo one is.\u201d. It is impossible to say one point to be better than the other. We can see this evidently in figure 5- if one objective attains an optimum, the other does not necessarily be at its best value.", "Therefore, it is a matter of preference. How high would you like the speed to be? How good the handling should be?", "If it is desired to have a car of speed 70, then the problem is solved. The optimal values of (speed, handling) would then be (70, 30) (figure 5). No individual Pareto solution is universally optimum. User prefernce is needed to pick one out of all possible Pareto solutions.", "With the introduction of one more objective to our optimization problem, we have got into a situation identical to the cars example discussed above. Recall that the first objective maximizes the margin. Given the constraints, the margin could be a large value. However, this corresponds to increasing \u03be indefinitely so as to allow indefintely large and negative functional margin values. Conversely, decreasing \u03be corresponds to a decreasing margin (ideally, we would want \u03be to be a suitable vector of \u03bei\u2019s so that a reasonable separating hyperplane is learned).", "Consequently, Pareto optimality is used to describe the solutions to this MOO problem.", "One of the methods used to solve MOO problems is the weighted-sum method. This method allows to add user preference into the optimization objectives. For SVM, we would like to learn how can we include our preference for a hyperplane that correctly separates more points (a lower value of \u03be is more desirable than a greater value of margin), or a hyperplane with greater margin (a greater value of margin is preferred over a lower value of \u03be) in the form of weights.", "Expression 1 is the final formulation of optimization problem of SVM. The constant c\u22650 is there to account for user preference to identify a single suitable solution from the many Pareto optimal solutions. A suitable value of c should be used if the preference is to learn a hyperplane that does the most reasonable separation on the training set. A relatively larger c would mean that we are incorporating more preference to the sum_of_\u03bei terms, which means we shall get a more optimal value for it and a less optimal value for the ||w||\u00b2.", "How do weights account for user preference, and how does it influence the optimal points?", "Part 1: How user preferences map to weights?", "Let us now comprehend how do the weights describe preference for a particular objective function. For notation convenience, we shall write the two objective functions as given below:", "Our optimization problem is then (not writing the constraints, nonetheless they must hold),", "This optimization problem with multiple objectives can be restated as an optimization problem with single objective. The single objective is the weighted sum of the individual objective functions and is often called as Utility function [3]:", "The weights w1 and w2 are always taken to be positive. At this point, it is still not clear how weights correspond to user preferences. Also, note that the user preferences are relative to each other and hold meaning only locally for the given problem.", "Note that if the optimization problem is convex and the weights are positive, then it is ensured that solving the weighted sum function gives a solution which is pareto optimal [3].", "We could represent user preferences for the objectives as a mathematical function. Needless to say, the preference function would be a function of f1 and f2. Let\u2019s call it P(f1, f2). The gradient of P(f) would then be:", "Equation-2 is the total derivative of Preference function (for more on total derivative I would suggest you read the first answer on this stackexchange post).", "The gradient of the utility function (equation- 1) is:", "Each component of Equation-2 represents the rate of change of preference of a user with a change in design point value (on which functions f1 and f2 are defined).", "Comparing Equation-2 and Equation-3 suggests that if w1 and w2 are selected properly, then the gradient of utility function can become parallel to the gradient of the preference function. Simply put, the rate of change of the utility function should approximate the rate of change of user preferences. As preferences are locally significant and are relative to each other, so talking about how preferences change w.r.t. design points makes sense while deliberating upon w1 and w2 representing preferences in some absolute sense is immaterial (because preferences themselves are not defined absolutely/universally). A choice of appropriate weights can make the gradient of utility function parallel to that of the preference function.", "Part 2- How the weights influence optimal points?", "The utility function, U has an optimum value", "If one of the weights say, w2, is significantly greater than w1 then each partial derivative of f1 will be much greater than partial derivative of f2 (since LHS = RHS ). For a convex function, this corresponds to points where f1 has much higher values as compared to f2. Similar arguments explain how it would work out to be when w1 is much greater than w2.", "In SVM, a single constant c is used instead of two weights because of the same reason that the preference for a hyperplane is relative and will be local to a specific problem. So, c is used to represent how one objective (maximizing margin) is preferred over the other (minimizing sum_of_\u03bei\u2019s). Because a problem is likely to be different from all other problems, hypertuning the parameter c is required so that a suitable preference can be incorporated with the optimization problem to model our preference for a robust classifier. If c is set to be too high, the objective ||w||\u00b2/2 will also be high at the computed Pareto optimal point. This corresponds to a classifier with small margin and small values of \u03bei\u2019s, i.e., we may end up with a hard margin SVM. Consequently, if the dataset is not linearly separable, we may get a less-desired hyperplane- something like shown in figure 3.", "I found it really difficult to make sense out of Expression 1. It is not that I could not find an answer- I read on quora, stackexchange and other platforms. Everyone had been maintaining the same statement \u201cc balances the trade off between margin width and amount of misclassifications you want to avoid\u201d. But just knowing this did not seem to be complete. But after referring to more sources and around 10 papers, I found a smooth passage that led to the quoted statement. I have cited only two most relevant papers here. In this article, I attempted to set down a smooth discussion of what I learned. I hope the article did not kink its way from one concept to another. If it felt so somewhere, do let me know in the comment section. The purpose with which I started to write was not just to explain a topic, it was to take a smooth journey- something which I personally like.", "Thank you for reading. Looking forward to getting feedbacks and suggestions.", "[1] Bui, L. T., & Alam, S. (2008). An introduction to multi-objective optimization. In Multi-Objective Optimization in Computational Intelligence: Theory and Practice (pp. 1\u201319). IGI Global.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer-Intern at EIG | Machine Learning Enthusiast | Python | VIT | linkedin.com/in/krishna-kumar-mahto"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F30308a73e072&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----30308a73e072--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@krish.thorcode?source=post_page-----30308a73e072--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Krishna Kumar Mahto"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1dae62fc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=post_page-e1dae62fc758----30308a73e072---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "Demystifying Maths of SVM"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous article"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "previous part of this article"}, {"url": "https://towardsdatascience.com/demystifying-maths-of-svm-13ccfe00091e", "anchor_text": "first part"}, {"url": "https://math.stackexchange.com/questions/174270/what-exactly-is-the-difference-between-a-derivative-and-a-total-derivative", "anchor_text": "stackexchange"}, {"url": "https://www.igi-global.com/dictionary/pareto-front/21878", "anchor_text": "https://www.igi-global.com/dictionary/pareto-front/21878"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----30308a73e072---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----30308a73e072---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----30308a73e072---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/svm?source=post_page-----30308a73e072---------------svm-----------------", "anchor_text": "Svm"}, {"url": "https://medium.com/tag/optimization?source=post_page-----30308a73e072---------------optimization-----------------", "anchor_text": "Optimization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=-----30308a73e072---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=-----30308a73e072---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----30308a73e072--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F30308a73e072&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----30308a73e072---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----30308a73e072--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----30308a73e072--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----30308a73e072--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----30308a73e072--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----30308a73e072--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@krish.thorcode?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Krishna Kumar Mahto"}, {"url": "https://medium.com/@krish.thorcode/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "211 Followers"}, {"url": "http://linkedin.com/in/krishna-kumar-mahto", "anchor_text": "linkedin.com/in/krishna-kumar-mahto"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1dae62fc758&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=post_page-e1dae62fc758--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fe1dae62fc758%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdemystifying-maths-of-svm-part-2-30308a73e072&user=Krishna+Kumar+Mahto&userId=e1dae62fc758&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}