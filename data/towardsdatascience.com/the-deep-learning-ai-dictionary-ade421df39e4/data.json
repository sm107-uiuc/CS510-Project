{"url": "https://towardsdatascience.com/the-deep-learning-ai-dictionary-ade421df39e4", "time": 1682993307.411951, "path": "towardsdatascience.com/the-deep-learning-ai-dictionary-ade421df39e4/", "webpage": {"metadata": {"title": "The Deep Learning(.ai) Dictionary | by Jan Zawadzki | Towards Data Science", "h1": "The Deep Learning(.ai) Dictionary", "description": "Surviving in the Coursera Deep Learning world means understanding and navigating through the jungle of technical terms. You\u2019re not\ud83d\udcaf% sure what AdaGrad, Dropout, or Xavier Initialization mean? Use\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.coursera.org/courses?languages=en&query=deep+learning", "anchor_text": "Coursera Deep Learning", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6", "anchor_text": "Activation Function", "paragraph_index": 3}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "Adam Optimization", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "inventors", "paragraph_index": 4}, {"url": "https://www.youtube.com/watch?v=8NgKbjFbwJg", "anchor_text": "Adaptive Gradient Algorithm", "paragraph_index": 5}, {"url": "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf", "anchor_text": "here", "paragraph_index": 5}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/Pooling/", "anchor_text": "Average Pooling", "paragraph_index": 6}, {"url": "http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf", "anchor_text": "AlexNet", "paragraph_index": 7}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "here", "paragraph_index": 7}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "Backpropagation", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1", "anchor_text": "Batch Gradient Descent", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c", "anchor_text": "Batch Normalization", "paragraph_index": 10}, {"url": "https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted", "anchor_text": "Bias", "paragraph_index": 11}, {"url": "https://medium.com/fuzz/machine-learning-classification-models-3040f71e2529", "anchor_text": "Classification", "paragraph_index": 12}, {"url": "http://colah.github.io/posts/2014-07-Understanding-Convolutions/", "anchor_text": "Convolution", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220", "anchor_text": "Cost Function", "paragraph_index": 14}, {"url": "https://deeplearning4j.org/neuralnet-overview", "anchor_text": "Deep Neural Network", "paragraph_index": 15}, {"url": "https://stackoverflow.com/questions/14829785/why-derivative-of-a-function-is-used-to-calculate-local-minimum-instead-of-the-a", "anchor_text": "Derivative", "paragraph_index": 16}, {"url": "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/", "anchor_text": "Dropout", "paragraph_index": 17}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://www.quora.com/What-is-end-to-end-learning-in-machine-learning?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa", "anchor_text": "End-to-End Learning", "paragraph_index": 18}, {"url": "https://arxiv.org/abs/1604.07316", "anchor_text": "end-to-end driving", "paragraph_index": 18}, {"url": "http://neuralnetworksanddeeplearning.com/chap1.html", "anchor_text": "Epoch", "paragraph_index": 19}, {"url": "https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426", "anchor_text": "Forward Propagation", "paragraph_index": 20}, {"url": "http://cs231n.github.io/convolutional-networks/#fc", "anchor_text": "Fully-Connected layer", "paragraph_index": 21}, {"url": "https://www.quora.com/Are-GRU-Gated-Recurrent-Unit-a-special-case-of-LSTM", "anchor_text": "Gated Recurrent Unit", "paragraph_index": 22}, {"url": "https://arxiv.org/abs/1406.1078", "anchor_text": "here", "paragraph_index": 22}, {"url": "http://datalya.com/blog/2017/machine-learning-versus-human-level-performance", "anchor_text": "Human-Level Performance", "paragraph_index": 23}, {"url": "https://www.quora.com/What-are-hyperparameters-in-machine-learning", "anchor_text": "Hyperparameters", "paragraph_index": 24}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet", "paragraph_index": 25}, {"url": "https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks", "anchor_text": "Iteration", "paragraph_index": 26}, {"url": "https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html", "anchor_text": "Gradient Descent", "paragraph_index": 27}, {"url": "http://ruder.io/optimizing-gradient-descent/", "anchor_text": "This post", "paragraph_index": 27}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/", "anchor_text": "Layer", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1", "anchor_text": "Learning Rate Decay", "paragraph_index": 29}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/Pooling/", "anchor_text": "Maximum Pooling", "paragraph_index": 30}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Long Short-Term Memory", "paragraph_index": 31}, {"url": "http://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/", "anchor_text": "Mini-Batch Gradient Descent", "paragraph_index": 32}, {"url": "http://www.cs.bham.ac.uk/~jxb/NN/l8.pdf", "anchor_text": "Momentum", "paragraph_index": 33}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "Neural Network", "paragraph_index": 34}, {"url": "https://www.youtube.com/watch?v=SnYMimFnKuY", "anchor_text": "Non-Max Suppression", "paragraph_index": 35}, {"url": "https://arxiv.org/abs/1705.02950", "anchor_text": "here", "paragraph_index": 35}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Recurrent Neural Networks", "paragraph_index": 36}, {"url": "https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions", "anchor_text": "ReLU", "paragraph_index": 37}, {"url": "https://machinelearningmastery.com/linear-regression-for-machine-learning/", "anchor_text": "Regression", "paragraph_index": 38}, {"url": "https://www.quora.com/What-is-an-intuitive-explanation-of-RMSProp", "anchor_text": "Root Mean Squared Propagation", "paragraph_index": 39}, {"url": "https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/", "anchor_text": "Parameters", "paragraph_index": 40}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/", "anchor_text": "Softmax", "paragraph_index": 41}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/", "anchor_text": "Stochastic Gradient Descent", "paragraph_index": 42}, {"url": "https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab", "anchor_text": "Supervised Learning", "paragraph_index": 43}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "Transfer Learning", "paragraph_index": 44}, {"url": "https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294", "anchor_text": "Unsupervised Learning", "paragraph_index": 45}, {"url": "http://www.fast.ai/2017/11/13/validation-sets/", "anchor_text": "Validation Set", "paragraph_index": 46}, {"url": "http://neuralnetworksanddeeplearning.com/chap5.html", "anchor_text": "Vanishing Gradients", "paragraph_index": 47}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321", "anchor_text": "here", "paragraph_index": 47}, {"url": "https://elitedatascience.com/overfitting-in-machine-learning", "anchor_text": "Variance", "paragraph_index": 48}, {"url": "https://stackoverflow.com/questions/38379905/what-is-vector-in-terms-of-machine-learning", "anchor_text": "Vector", "paragraph_index": 49}, {"url": "https://www.quora.com/What-is-the-VGG-neural-network", "anchor_text": "VGG-16", "paragraph_index": 50}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "here", "paragraph_index": 50}, {"url": "http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "Xavier Initialization", "paragraph_index": 51}, {"url": "https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006", "anchor_text": "YOLO", "paragraph_index": 52}, {"url": "https://arxiv.org/pdf/1612.08242v1.pdf", "anchor_text": "here", "paragraph_index": 52}, {"url": "https://medium.com/@janzawadzki", "anchor_text": "Medium", "paragraph_index": 54}, {"url": "https://www.linkedin.com/in/jan-zawadzki/", "anchor_text": "LinkedIn", "paragraph_index": 54}, {"url": "https://medium.com/machine-learning-world/netflix-or-coursera-how-to-finish-andrew-ngs-1st-deep-learning-course-in-7-days-6fa293ee83d8", "anchor_text": "check", "paragraph_index": 54}, {"url": "https://towardsdatascience.com/https-medium-com-janzawadzki-applying-andrew-ngs-1st-deep-neural-network-on-the-titanic-survival-data-set-b77edbc83816", "anchor_text": "these", "paragraph_index": 54}, {"url": "https://towardsdatascience.com/structuring-your-machine-learning-project-course-summary-in-1-picture-and-22-nuggets-of-wisdom-95b051a6c9dd", "anchor_text": "posts", "paragraph_index": 54}], "all_paragraphs": ["Surviving in the Coursera Deep Learning world means understanding and navigating through the jungle of technical terms. You\u2019re not\ud83d\udcaf% sure what AdaGrad, Dropout, or Xavier Initialization mean? Use this guide as a reference to freshen up your memory when you stumble upon a term that you safely parked in a dusty corner in the back of your mind.", "This dictionary aims to briefly explain the most important terms of the Coursera Deep Learning Specialization from Andrew Ng\u2019s deeplearning.ai. It contains short explanations of the terms, accompanied by links to follow-up posts, images, and original papers. The post aims to be equally useful for Deep Learning beginners and practitioners.", "Let\u2019s open the encyclopedia of deep learning. \ud83d\udcda", "Activation Function\u2014 Used to create a non-linear transformation of the input. The inputs are multiplied by weights and added to a bias term. Popular Activation functions include ReLU, tanh or sigmoid.", "Adam Optimization \u2014 Can be used instead of stochastic gradient descent optimization methods to iteratively adjust network weights. Adam is computationally efficient, works well with large data sets, and requires little hyperparameter tuning, according to the inventors. Adam uses an adaptive learning rate \u03b1, instead of a predefined and fixed learning rate. Adam is currently the default optimization algorithm in deep learning models.", "Adaptive Gradient Algorithm \u2014 AdaGrad is a gradient descent optimization algorithm that features an adjustable learning rate for every parameter. AdaGrad adjusts the parameters on frequently updated parameters in smaller steps than for less frequently updated parameters. It thus fares well on very sparse data sets, e.g. for adapting word embeddings in Natural Language Processing tasks. Read the paper here.", "Average Pooling \u2014 Averages the results of a convolutional operation. It is often used to shrink the size of an input. Average pooling was primarily used in older Convolutional Neural Networks architectures, while recent architectures favor maximum pooling.", "AlexNet \u2014 A popular CNN architecture with eight layers. It is a more extensive network architecture than LeNet and takes longer to train. AlexNet won the 2012 ImageNet image classification challenge. Read the paper here.", "Backpropagation \u2014The general framework used to adjust network weights to minimize the loss function of a neural network. The algorithm travels backward through the network and adjusts the weights through a form of gradient descent of each activation function.", "Batch Gradient Descent \u2014 Regular gradient descent optimization algorithm. Performs parameter updates for the entire training set. The algorithm needs to calculate the gradients of the whole training set before completing a step of parameter updates. Thus, batch gradient can be very slow for large training sets.", "Batch Normalization \u2014 Normalizes the values in a neural network layer to values between 0 and 1. This helps train the neural network faster.", "Bias \u2014Occurs when the model does not achieve a high accuracy on the training set. It is also called underfitting. When a model has a high bias, it will generally not yield high accuracy on the test set.", "Classification \u2014 When the target variable belongs to a distinct class, not a continuous variable. Image classification, fraud detection or natural language processing are examples of deep learning classification tasks.", "Convolution \u2014 A mathematical operation which multiplies an input with a filter. Convolutions are the foundation of Convolutional Neural Networks, which excel at identifying edges and objects in images.", "Cost Function \u2014 Defines the difference between the calculated output and what it should be. Cost functions are one of the key ingredients of learning in deep neural networks, as they form the basis for parameter updates. The network compares the outcome of its forward propagation with the ground-truth and adjusts the network weights accordingly to minimize the cost function. The root mean squared error is a simple example of a cost function.", "Deep Neural Network \u2014 A neural network with many hidden layers, usually more than five. It is not defined how many layers minimum a deep neural network has to have. Deep Neural Networks are a powerful form of machine learning algorithms which are used to determine credit risk, steer self-driving cars and detect new planets in the universe.", "Derivative \u2014 The derivative is the slope of a function at a specific point. Derivatives are calculated to let the gradient descent algorithm adjust weight parameters towards the local minimum.", "Dropout \u2014 A regularization technique which randomly eliminates nodes and its connections in deep neural networks. Dropout reduces overfitting and enables faster training of deep neural networks. Each parameter update cycle, different nodes are dropped during training. This forces neighboring nodes to avoid relying on each other too much and figuring out the correct representation themselves. It also improves the performance of certain classification tasks. Read the paper here.", "End-to-End Learning \u2014 An algorithm is able to solve the entire task by itself. Additional human intervention, like model switching or new data labeling, is not necessary. For example, end-to-end driving means that the neural network figures out how to adjust the steering command just by evaluating images.", "Epoch \u2014Encompasses a single forward and backward pass through the training set for every example. A single epoch touches every training example in an iteration.", "Forward Propagation \u2014 A forward pass in deep neural networks. The input travels through the activation functions of the hidden layers until it produces a result at the end. Forward propagation is also used to predict the result of an input example after the weights have been properly trained.", "Fully-Connected layer \u2014 A fully-connected layer transforms an input with its weights and passes the result to the following layer. This layer has access to all inputs or activations from the previous layer.", "Gated Recurrent Unit \u2014A Gated Recurrent Unit (GRU) conducts multiple transformations on the given input. It is mostly used in Natural Language Processing Tasks. GRUs prevent the vanishing gradients problem in RNNs, similar to LSTMs. In contrast to LSTMs, GRUs don\u2019t use a memory unit and are thus more computationally efficient while achieving a similar performance. Read the paper here.", "Human-Level Performance \u2014 The best possible performance of a group of human experts. Algorithms can exceed human-level performance. Valuable metric to compare and improve neural network against.", "Hyperparameters \u2014 Determine performance of your neural network. Examples of hyperparameters are, e.g. learning rate, iterations of gradient descent, number of hidden layers, or the activation function. Not to be confused with parameters or weights, which the DNN learns itself.", "ImageNet \u2014 Collection of thousands of images and their annotated classes. Very useful resource for image classification tasks.", "Iteration \u2014 Total number of forward and backward passes of a neural network. Every batch counts as one pass. If your training set has 5 batches and trains 2 epochs, then it will run 10 iterations.", "Gradient Descent \u2014 Helps Neural Network decide how to adjust parameters to minimize the cost function. Repeatedly adjust parameters until the global minimum is found. This post contains a well-explained, holistic overview of different gradient descent optimization methods.", "Layer \u2014 A set of activation functions which transform the input. Neural networks use multiple hidden layers to create output. You generally distinguish between the input, hidden, and output layers.", "Learning Rate Decay \u2014 A concept to adjust the learning rate during training. Allows for flexible learning rate adjustments. In deep learning, the learning rate typically decays the longer the network is trained.", "Maximum Pooling \u2014 Only selects the maximum values of a specific input area. It is often used in convolutional neural networks to reduce the size of the input.", "Long Short-Term Memory \u2014 A special form of RNN which is able to learn the context of an input. While regular RNNs suffer from vanishing gradients when corresponding inputs are located far away from each other, LSTMs can learn these long-term dependencies. Read the paper here.", "Mini-Batch Gradient Descent\u2014 An optimization algorithm which runs gradient descent on smaller subsets of the training data. The method enables parallelization as different workers separately iterate through different mini-batches. For every mini-batch, compute the cost and update the weights of the mini-batch. It\u2019s an efficient combination of batch and stochastic gradient descent.", "Momentum \u2014 A gradient descent optimization algorithm to smooth the oscillations of stochastic gradient descent methods. Momentum calculates the average direction of the direction of the previously taken steps and adjusts the parameter update in this direction. Imagine a ball rolling downhill and using this momentum when adjusting to roll left or right. The ball rolling downhill is an analogy to gradient descent finding the local minimum.", "Neural Network \u2014 A machine learning model which transforms inputs. A vanilla neural network has an input, hidden, and output layer. Neural Networks have become the tool of choice for finding complex patterns in data.", "Non-Max Suppression \u2014 Algorithm used as a part of YOLO. It helps detect the correct bounding box of an object by eliminating overlapping bounding boxes with a lower confidence of identifying the object. Read the paper here.", "Recurrent Neural Networks \u2014 RNNs allow the neural network to understand the context in speech, text or music. The RNN allows information to loop through the network, thus persisting important features of the input between earlier and later layers.", "ReLU\u2014 A Rectified Linear Unit, is a simple linear transformation unit where the output is zero if the input is less than zero and the output is equal to the input otherwise. ReLU is the activation function of choice because it allows neural networks to train faster and it prevents information loss.", "Regression \u2014Form of statistical learning where the output variable is a continuous instead of a categorical value. While classification assigns a class to the input variable, regression assigns a value that has an infinite number of possible values, typically a number. Examples are the prediction of house prices or customer age.", "Root Mean Squared Propagation \u2014 RMSProp is an extension of the stochastic gradient descent optimization method. The algorithm features a learning rate for every parameter, but not a learning rate for the entire training set. RMSProp adjusts the learning rate based on how quickly the parameters changed in previous iterations. Read the paper here.", "Parameters \u2014 Weights of a DNN which transform the input before applying the activation function. Each layer has its own set of parameters. The parameters are adjusted through backpropagation to minimize the loss function.", "Softmax \u2014 An extension of the logistic regression function which calculates the probability of the input belonging to every one of the existing classes. Softmax is often used in the final layer of a DNN. The class with the highest probability is chosen as the predicted class. It is well-suited for classification tasks with more than two output classes.", "Stochastic Gradient Descent \u2014 An optimization algorithm which performs a parameter update for every single training example. The algorithm converges usually much faster than batch gradient descent, which performs a parameter update after calculating the gradients for the entire training set.", "Supervised Learning \u2014 Form of Deep Learning where an output label exists for every input example. The labels are used to compare the output of a DNN to the ground-truth values and minimize the cost function. Other forms of Deep Learning tasks are semi-supervised training and unsupervised training.", "Transfer Learning \u2014 A technique to use the parameters from one neural network for a different task without retraining the entire network. Use weights from a previously trained network and remove output layer. Replace the last layer with your own softmax or logistic layer and train network again. Works because lower layers often detect similar things like edges which are useful for other image classification tasks.", "Unsupervised Learning \u2014 A form of machine learning where the output class is not known. GANs or Variational Auto Encoders are used in unsupervised Deep Learning tasks.", "Validation Set \u2014 The validation set is used to find the optimal hyperparameters of a deep neural network. Generally, the DNN is trained with different combinations of hyperparameters are tested on the validation set. The best performing set of hyperparameters is then applied to make the final prediction on the test set. Pay attention to balancing the validation set. If lots of data is available, use as much as 99% for the training, 0.5% for the validation and 0.5% the test set.", "Vanishing Gradients \u2014 The problem arises when training very deep neural networks. In backpropagation, weights are adjusted based on their gradient, or derivative. In deep neural networks, the gradients of the earlier layers can become so vanishingly small, that the weights are not updated at all. The ReLU activation function is suited to address this problem because it doesn\u2019t squash the input as much as other functions. Read the paper here.", "Variance \u2014Occurs when the DNN overfits to the training data. The DNN fails to distinguish noise from pattern and models every variance in the training data. A model with high variance usually fails to accurately generalize to new data.", "Vector \u2014 A combination of values that are passed as inputs into an activation layer of a DNN.", "VGG-16 \u2014 A popular network architecture for CNNs. It simplifies the architecture of AlexNet and has a total of 16 layers. There are many pretrained VGG models which can be applied to novel use cases through transfer learning. Read the paper here.", "Xavier Initialization \u2014 Xavier initialization assigns the start weights in the first hidden layer so that the input signals reach deep into the neural network. It scales the weights based on the number of neurons and outputs. This way, it prevents the signal from either becoming too small or too large later in the network.", "YOLO \u2014 You Only Look Once, is an algorithm to identify objects in an image. Convolutions are used to determine the probability of an object being in a part of an image. Non-max suppression and anchor boxes are then used to correctly locate the objects. Read the paper here.", "I hope this dictionary helped you get a clearer understanding of the terms used in the deep learning world. Keep this guide handy when taking the Coursera Deep Learning Specialization to quickly look up terms and concepts.", "If you think this post was helpful, don\u2019t forget to show your \ud83d\udc9b through \ud83d\udc4f \ud83d\udc4f \ud83d\udc4f and follow me on Medium and LinkedIn to hear more about Deep Learning, Online Courses, Autonomous Cars, and Life. Also, check these posts about the Deep Learning Specialization out. Please comment to share your opinion. Cheers! \ud83d\ude47", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Head of Artificial Intelligence @CARIAD SE (VW Group) | Business & Data Science"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fade421df39e4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ade421df39e4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@janzawadzki?source=post_page-----ade421df39e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@janzawadzki?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Jan Zawadzki"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe746a0fcb7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&user=Jan+Zawadzki&userId=e746a0fcb7b6&source=post_page-e746a0fcb7b6----ade421df39e4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.coursera.org/courses?languages=en&query=deep+learning", "anchor_text": "Coursera Deep Learning"}, {"url": "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6", "anchor_text": "Activation Function"}, {"url": "https://bit.ly/2GBeocg", "anchor_text": "https://bit.ly/2GBeocg"}, {"url": "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/", "anchor_text": "Adam Optimization"}, {"url": "https://arxiv.org/abs/1412.6980", "anchor_text": "inventors"}, {"url": "https://www.youtube.com/watch?v=8NgKbjFbwJg", "anchor_text": "Adaptive Gradient Algorithm"}, {"url": "http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf", "anchor_text": "here"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/Pooling/", "anchor_text": "Average Pooling"}, {"url": "http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf", "anchor_text": "AlexNet"}, {"url": "https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf", "anchor_text": "here"}, {"url": "https://goo.gl/BVXbhL", "anchor_text": "https://goo.gl/BVXbhL"}, {"url": "http://neuralnetworksanddeeplearning.com/chap2.html", "anchor_text": "Backpropagation"}, {"url": "https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1", "anchor_text": "Batch Gradient Descent"}, {"url": "https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c", "anchor_text": "Batch Normalization"}, {"url": "https://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted", "anchor_text": "Bias"}, {"url": "https://goo.gl/htKsQS", "anchor_text": "https://goo.gl/htKsQS"}, {"url": "https://medium.com/fuzz/machine-learning-classification-models-3040f71e2529", "anchor_text": "Classification"}, {"url": "http://colah.github.io/posts/2014-07-Understanding-Convolutions/", "anchor_text": "Convolution"}, {"url": "https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220", "anchor_text": "Cost Function"}, {"url": "https://deeplearning4j.org/neuralnet-overview", "anchor_text": "Deep Neural Network"}, {"url": "https://goo.gl/HqKdeg", "anchor_text": "https://goo.gl/HqKdeg"}, {"url": "https://stackoverflow.com/questions/14829785/why-derivative-of-a-function-is-used-to-calculate-local-minimum-instead-of-the-a", "anchor_text": "Derivative"}, {"url": "https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/", "anchor_text": "Dropout"}, {"url": "http://jmlr.org/papers/v15/srivastava14a.html", "anchor_text": "here"}, {"url": "https://goo.gl/obY4L5", "anchor_text": "https://goo.gl/obY4L5"}, {"url": "https://www.quora.com/What-is-end-to-end-learning-in-machine-learning?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa", "anchor_text": "End-to-End Learning"}, {"url": "https://arxiv.org/abs/1604.07316", "anchor_text": "end-to-end driving"}, {"url": "http://neuralnetworksanddeeplearning.com/chap1.html", "anchor_text": "Epoch"}, {"url": "https://towardsdatascience.com/under-the-hood-of-neural-network-forward-propagation-the-dreaded-matrix-multiplication-a5360b33426", "anchor_text": "Forward Propagation"}, {"url": "http://cs231n.github.io/convolutional-networks/#fc", "anchor_text": "Fully-Connected layer"}, {"url": "https://www.quora.com/Are-GRU-Gated-Recurrent-Unit-a-special-case-of-LSTM", "anchor_text": "Gated Recurrent Unit"}, {"url": "https://arxiv.org/abs/1406.1078", "anchor_text": "here"}, {"url": "https://goo.gl/dUPtdV", "anchor_text": "https://goo.gl/dUPtdV"}, {"url": "http://datalya.com/blog/2017/machine-learning-versus-human-level-performance", "anchor_text": "Human-Level Performance"}, {"url": "https://www.quora.com/What-are-hyperparameters-in-machine-learning", "anchor_text": "Hyperparameters"}, {"url": "http://www.image-net.org/", "anchor_text": "ImageNet"}, {"url": "https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks", "anchor_text": "Iteration"}, {"url": "https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html", "anchor_text": "Gradient Descent"}, {"url": "http://ruder.io/optimizing-gradient-descent/", "anchor_text": "This post"}, {"url": "https://bit.ly/2JnOeLR", "anchor_text": "https://bit.ly/2JnOeLR"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/", "anchor_text": "Layer"}, {"url": "https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1", "anchor_text": "Learning Rate Decay"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/Pooling/", "anchor_text": "Maximum Pooling"}, {"url": "http://colah.github.io/posts/2015-08-Understanding-LSTMs/", "anchor_text": "Long Short-Term Memory"}, {"url": "http://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "here"}, {"url": "https://bit.ly/2GlKyMF", "anchor_text": "https://bit.ly/2GlKyMF"}, {"url": "https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/", "anchor_text": "Mini-Batch Gradient Descent"}, {"url": "https://bit.ly/2Iz7uob", "anchor_text": "https://bit.ly/2Iz7uob"}, {"url": "http://www.cs.bham.ac.uk/~jxb/NN/l8.pdf", "anchor_text": "Momentum"}, {"url": "https://www.youtube.com/watch?v=aircAruvnKk", "anchor_text": "Neural Network"}, {"url": "https://www.youtube.com/watch?v=SnYMimFnKuY", "anchor_text": "Non-Max Suppression"}, {"url": "https://arxiv.org/abs/1705.02950", "anchor_text": "here"}, {"url": "https://bit.ly/2H303sF", "anchor_text": "https://bit.ly/2H303sF"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "Recurrent Neural Networks"}, {"url": "https://goo.gl/nr7Hf8", "anchor_text": "https://goo.gl/nr7Hf8"}, {"url": "https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions", "anchor_text": "ReLU"}, {"url": "https://machinelearningmastery.com/linear-regression-for-machine-learning/", "anchor_text": "Regression"}, {"url": "https://www.quora.com/What-is-an-intuitive-explanation-of-RMSProp", "anchor_text": "Root Mean Squared Propagation"}, {"url": "https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/", "anchor_text": "Parameters"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/", "anchor_text": "Softmax"}, {"url": "https://bit.ly/2HdWZHL", "anchor_text": "https://bit.ly/2HdWZHL"}, {"url": "http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/", "anchor_text": "Stochastic Gradient Descent"}, {"url": "https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab", "anchor_text": "Supervised Learning"}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294", "anchor_text": "Unsupervised Learning"}, {"url": "http://www.fast.ai/2017/11/13/validation-sets/", "anchor_text": "Validation Set"}, {"url": "http://neuralnetworksanddeeplearning.com/chap5.html", "anchor_text": "Vanishing Gradients"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.7321", "anchor_text": "here"}, {"url": "https://elitedatascience.com/overfitting-in-machine-learning", "anchor_text": "Variance"}, {"url": "https://stackoverflow.com/questions/38379905/what-is-vector-in-terms-of-machine-learning", "anchor_text": "Vector"}, {"url": "https://www.quora.com/What-is-the-VGG-neural-network", "anchor_text": "VGG-16"}, {"url": "https://arxiv.org/pdf/1409.1556.pdf", "anchor_text": "here"}, {"url": "http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization", "anchor_text": "Xavier Initialization"}, {"url": "https://towardsdatascience.com/yolo-you-only-look-once-real-time-object-detection-explained-492dc9230006", "anchor_text": "YOLO"}, {"url": "https://arxiv.org/pdf/1612.08242v1.pdf", "anchor_text": "here"}, {"url": "https://medium.com/@janzawadzki", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/jan-zawadzki/", "anchor_text": "LinkedIn"}, {"url": "https://medium.com/machine-learning-world/netflix-or-coursera-how-to-finish-andrew-ngs-1st-deep-learning-course-in-7-days-6fa293ee83d8", "anchor_text": "check"}, {"url": "https://towardsdatascience.com/https-medium-com-janzawadzki-applying-andrew-ngs-1st-deep-neural-network-on-the-titanic-survival-data-set-b77edbc83816", "anchor_text": "these"}, {"url": "https://towardsdatascience.com/structuring-your-machine-learning-project-course-summary-in-1-picture-and-22-nuggets-of-wisdom-95b051a6c9dd", "anchor_text": "posts"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ade421df39e4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----ade421df39e4---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/dictionary?source=post_page-----ade421df39e4---------------dictionary-----------------", "anchor_text": "Dictionary"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----ade421df39e4---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/andrew-ng?source=post_page-----ade421df39e4---------------andrew_ng-----------------", "anchor_text": "Andrew Ng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&user=Jan+Zawadzki&userId=e746a0fcb7b6&source=-----ade421df39e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&user=Jan+Zawadzki&userId=e746a0fcb7b6&source=-----ade421df39e4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fade421df39e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ade421df39e4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ade421df39e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ade421df39e4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ade421df39e4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ade421df39e4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ade421df39e4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@janzawadzki?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@janzawadzki?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jan Zawadzki"}, {"url": "https://medium.com/@janzawadzki/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.3K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe746a0fcb7b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&user=Jan+Zawadzki&userId=e746a0fcb7b6&source=post_page-e746a0fcb7b6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6a3e9814aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-deep-learning-ai-dictionary-ade421df39e4&newsletterV3=e746a0fcb7b6&newsletterV3Id=6a3e9814aa0&user=Jan+Zawadzki&userId=e746a0fcb7b6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}