{"url": "https://towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d", "time": 1682997500.165811, "path": "towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Generalisation of On-policy Function Approximation | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Generalisation of On-policy Function Approximation", "description": "In previous posts, we have extended the idea of reinforcement learning from discrete state space to continuous state space, and a 1000 state random walk example was implemented, in which case a\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b", "anchor_text": "post", "paragraph_index": 2}, {"url": "https://github.com/MJeremy2017/reinforcement-learning-an-introduction/blob/master/chapter10/TileCoding.py", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/MountainCar/MountainCar.py", "anchor_text": "full implementation", "paragraph_index": 18}], "all_paragraphs": ["In previous posts, we have extended the idea of reinforcement learning from discrete state space to continuous state space, and a 1000 state random walk example was implemented, in which case a policy is given, as at all states the action of going left or right always has equal probability, and the only problem is to measure the value function of each state based on the given policy(we call these sort of problems a predict problem).", "In this article, let\u2019s extend the idea to more general cases called control problem, where a policy is not given and our goal is to use Q function to learn the best action at each state. In this post, I will:", "(Note: we are going to apply tile coding as the approximate function, if you are not familiar with it, please refer to my last post)", "Unlike predict problem, where action distribution is already know, in control problem the agent is tasked to explore the environment and learn the best action at each state, consequently, when formulating the problem a Q(s, a) function is considered instead of value function V(s) .", "On the other hand, like predict problem, the algorithm of parametric value function can be naturally extended to parametric Q function, simply by replacing the value function with Q function. Let\u2019s get right into the algorithm we are going to apply:", "If you have been following my previous posts, once again you will see this algorithm very familiar. Alike n-step Sarsa in discrete state space, this semi-gradient n-step Sarsa is exactly the same with that except that at each time step, the parameter w is being updated rather than directly updating Q function. The idea is that at each update time \u03c4 , the accumulated value up to \u03c4+n is used to correct current estimation, and according to gradient descent idea, the parameter weight w is updated slightly towards the actual value proportionally to its derivative and delta.", "If you still have some confusions in mind, that is totally fine. I believe the best way to understand every detail of an algorithm is to implement it and apply it on an actual problem. Let\u2019s do it on one of the classic reinforcement learning problems, mountain car.", "Consider the task of driving an underpowered car up a steep mountain road, as suggested by the diagram. The difficulty is that gravity is stronger than the car\u2019s engine, and even at full throttle the car cannot accelerate up the steep slope. The only solution is to first move away from the goal and up the opposite slope on the left.", "The reward in this problem is -1 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (-1), and zero throttle (0). The car moves according to a simplified physics. Its position, x_t, and velocity, x_ \u0307t, are updated by:", "where the bound operation enforces -1.2 <= x_t+1 <= 0.5 and -0.07 <= x_ \u0307t+1 <= 0.07. In addition, when x_t+1 reached the left bound, x_ \u0307t+1 was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position x_t in [-0.6, -0.4) and zero velocity. (Sorry for the unformatted syntax editing, I am not sure how to edit mathematic syntax on medium)", "In a nutshell, the car starts at a position between -0.6 and 0.4 with 0 velocity, and it has 3 discrete actions, stay(0), going forward(1) and backward(-1), and the goal is to reach the mountain top at the rightmost. So the states in this problem includes (position, velocity) , action set is (-1, 0, 1) , and reward is -1 everywhere except the goal state(notice that the states are continuous and action is discrete).", "Understood the problem setting, we now need a Q value function to keep track of values of all states and update value estimation along the exploration. Remember in last post, we have gone through the basics of using tile coding to encode a continuous state space and applying it in the update process. In this post, we will continue to use tile coding in our value function, but the code is slightly different:", "The tile coding function being used here refers to here. The reason I didn\u2019t use the tile coding introduced in last post is that the one I introduced is a bit basic with uniformly distributed offset, and I personally tested the result is not as good as this one, and also this seems to be tile coding function Sutton introduced in his book.", "Anyway, let\u2019s get back on track. The idea here is exactly the same, each continuous state is being encoded into some indices of tiles, and at each step of the episode, its weights of corresponding tiles are being updated.", "Let\u2019s get a closer look at the function. The numberOfTilings = 8 and maxSize = 2048 , meaning that we set 8 tilings, each with 16*16 number of grids, so that 16*16*8 = 2048 . More specifically, the state of (position, velocity) is represented by a 3D cube of dimension 16*16*8 , and the tiles function, from the files I referred to, is able to get the active tiles given a state and action. You might have noticed that inside the init function, the state is rescaled by:", "To be honest, I don\u2019t know why it is scaled in this way, but looks like to use this tile coding function, continuous value of state has always to be rescaled.", "Like all Q value functions, it has at least two functions:", "The costToGo function is used for visualisation in later steps, and it simply returns the most negative value of that state and action(as the reward is always -1, the result is given a - sign).", "Till now, we actually have gone through the most tricky part, and the rest are things we have been repeatedly doing. (full implementation)", "As usual, in the init function, we specify the action and state range, and some general elements in all reinforcement learning problems.", "The takeAction takes in an action and determines next state of the car. The update rule follows the formula I introduced above. The chooseAction function still uses \u03f5-greedy method, and in the greedy part, it chooses the action results in the most estimate value, giving by the valueFunc we described above.", "Besides the goal state getting a reward of 0, all other states get a reward of -1 .", "This is the longest function of all, but no need to worry, in fact, everything is the same as it was in discrete setting(I actually copied and pasted from my previous code), except the value function is changed to the function we introduced just now. The target G is the accumulated n-step value, including the value in step tau+n , and in the update process, the state, action and G are sent to the valueFunction.update() function together.", "In this session, we will use the costToGo function defined inside theValueFunction to measure the cost to reach the goal given a state and action. The code of plot is listed below:", "The lower the cost, the closer it is to reach the goal state. In fact, the lower cost appears either at the leftmost position or the rightmost position, which verifies that only by getting to the opposite direction of the goal can the agent be able to reach the final goal state.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2f47576f772d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://meatba11.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----2f47576f772d---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f47576f772d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----2f47576f772d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f47576f772d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=-----2f47576f772d---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/reinforcement-learning-tile-coding-implementation-7974b600762b", "anchor_text": "post"}, {"url": "https://github.com/MJeremy2017/reinforcement-learning-an-introduction/blob/master/chapter10/TileCoding.py", "anchor_text": "here"}, {"url": "https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/MountainCar/MountainCar.py", "anchor_text": "full implementation"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2f47576f772d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----2f47576f772d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f47576f772d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----2f47576f772d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2f47576f772d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----2f47576f772d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2f47576f772d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----2f47576f772d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----2f47576f772d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Written by Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----2f47576f772d---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-on-policy-function-approximation-2f47576f772d&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----2f47576f772d---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/geekculture?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Geek Culture"}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Using ChatGPT in PythonPractical Examples of Using ChatGPT SDK"}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "\u00b74 min read\u00b7Dec 20, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgeekculture%2Feeaed9847e72&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fusing-chatgpt-in-python-eeaed9847e72&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----eeaed9847e72----0-----------------clap_footer----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://medium.com/geekculture/using-chatgpt-in-python-eeaed9847e72?source=author_recirc-----2f47576f772d----0---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Feeaed9847e72&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fgeekculture%2Fusing-chatgpt-in-python-eeaed9847e72&source=-----2f47576f772d----0-----------------bookmark_preview----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----2f47576f772d----1---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----2f47576f772d----1-----------------bookmark_preview----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----2f47576f772d----2---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----2f47576f772d----2-----------------bookmark_preview----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Jeremy Zhang"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "UNet Line by Line ExplanationExample UNet Implementation"}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": "\u00b74 min read\u00b7Oct 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b191c76baf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funet-line-by-line-explanation-9b191c76baf5&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----9b191c76baf5----3-----------------clap_footer----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5?source=author_recirc-----2f47576f772d----3---------------------b75d5af5_d1e7_4d61_884b_6d928aecf558-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "5"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b191c76baf5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funet-line-by-line-explanation-9b191c76baf5&source=-----2f47576f772d----3-----------------bookmark_preview----b75d5af5_d1e7_4d61_884b_6d928aecf558-------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "See all from Jeremy Zhang"}, {"url": "https://towardsdatascience.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&user=Andrew+Austin&userId=42d388912d13&source=-----6c3b3d623a2d----0-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6c3b3d623a2d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40MoneyAndData%2Fai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d&source=-----2f47576f772d----0-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&user=Anand+Mishra&userId=86f86a9a5573&source=-----383190b14464----1-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383190b14464&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40anandmishra.kanha%2Fdeep-reinforcement-learning-current-state-of-art-383190b14464&source=-----2f47576f772d----1-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&user=Steve+Roberts&userId=6b6735266652&source=-----f3e00f3c1a50----0-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----2f47576f772d----0---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff3e00f3c1a50&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40tinkertytonk%2Fstate-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50&source=-----2f47576f772d----0-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&user=Wouter+van+Heeswijk%2C+PhD&userId=33f45c9ab481&source=-----abed1952457b----1-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----2f47576f772d----1---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fabed1952457b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fproximal-policy-optimization-ppo-explained-abed1952457b&source=-----2f47576f772d----1-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@upendravijay2/reinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@upendravijay2?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@upendravijay2?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Upendra Vijay"}, {"url": "https://medium.com/@upendravijay2/reinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Reinforcement Learning is the closest to human learningJust like we humans learn from the dynamic environment we live in and our actions determine whether we are rewarded or punished, so do\u2026"}, {"url": "https://medium.com/@upendravijay2/reinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "2 min read\u00b7Nov 30, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2Ff0086f3dc03d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40upendravijay2%2Freinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d&user=Upendra+Vijay&userId=9fbf46e0dda4&source=-----f0086f3dc03d----2-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@upendravijay2/reinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d?source=read_next_recirc-----2f47576f772d----2---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff0086f3dc03d&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40upendravijay2%2Freinforcement-learning-is-the-closest-to-human-learning-f0086f3dc03d&source=-----2f47576f772d----2-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@manubotija/day-13-my-trip-into-reinforcement-learning-5d6898aa17d6?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@manubotija?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@manubotija?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Manu Botija"}, {"url": "https://medium.com/@manubotija/day-13-my-trip-into-reinforcement-learning-5d6898aa17d6?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "Day 13 \u2014 My trip into Reinforcement LearningToday was about discovering what already exists out there. This is something that I was postponing as much as I could but the time has\u2026"}, {"url": "https://medium.com/@manubotija/day-13-my-trip-into-reinforcement-learning-5d6898aa17d6?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": "2 min read\u00b7Jan 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F5d6898aa17d6&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40manubotija%2Fday-13-my-trip-into-reinforcement-learning-5d6898aa17d6&user=Manu+Botija&userId=242036de3a1a&source=-----5d6898aa17d6----3-----------------clap_footer----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/@manubotija/day-13-my-trip-into-reinforcement-learning-5d6898aa17d6?source=read_next_recirc-----2f47576f772d----3---------------------9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5d6898aa17d6&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40manubotija%2Fday-13-my-trip-into-reinforcement-learning-5d6898aa17d6&source=-----2f47576f772d----3-----------------bookmark_preview----9167b232_6024_4b7f_86c6_2cc2fd6af4f3-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2f47576f772d--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----2f47576f772d--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}