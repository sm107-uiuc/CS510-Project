{"url": "https://towardsdatascience.com/electra-developments-in-masked-language-modelling-3cf1c25fc61a", "time": 1683014920.742003, "path": "towardsdatascience.com/electra-developments-in-masked-language-modelling-3cf1c25fc61a/", "webpage": {"metadata": {"title": "ELECTRA: Developments in Masked Language Modelling | by Alec Robinson | Towards Data Science", "h1": "ELECTRA: Developments in Masked Language Modelling", "description": "With BERT and the BERT-derived transformers (XLNet, RoBERTa, ALBERT, any transformer named after a Sesame Street character), it became clear to me that as an individual hobbyist with a single GPU and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ai.science/", "anchor_text": "AISC", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=Mgck4XFR9GA&t=2166s", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "BERT", "paragraph_index": 2}, {"url": "https://mccormickml.com/2019/11/05/GLUE/", "anchor_text": "here", "paragraph_index": 10}], "all_paragraphs": ["With BERT and the BERT-derived transformers (XLNet, RoBERTa, ALBERT, any transformer named after a Sesame Street character), it became clear to me that as an individual hobbyist with a single GPU and no industry backing, it would be impossible for me to train a deep learning language model myself. When I was preparing a live presentation for AISC on XLNet (on YouTube here), I came across this tweet by Elliot Turner:", "Well, there went my vague dreams of training my own model. This was back in 2019, ie, the Before Times. By that, I mean before GPT-3, which uses such a staggering amount of computational resources that it would only be possible to train, currently, with eight figures behind you. This model intrigued me with the claim, in the abstract, that ELECTRA-small could outperform GPT-1 after being trained on a single GPU for four days. They argue that the full model (ELECTRA-base) performs comparably to XLNet and RoBERTa while using approximately a quarter of their resources. Intriguing. What is this methodology?", "The architecture and most hyperparameters for this model are the same as in BERT, so won\u2019t be outlined here.", "Replaced Token Detection is their pre-training methodology of choice. Whereas BERT selectively replaces tokens in a sequence with [MASK], ELECTRA replaces tokens with a plausible alternative word using a generator.", "The input sequences are lists of tokens x = [x1, \u2026 xn]. MLM seeks to replace k of those tokens with alternative plausible words, arriving at a list of k masks [m1,\u2026,mk]. For this paper, they recommend masking out about 15% of input tokens.", "This process yields x^corrupt, a list of tokens with plausible alternative words inserted into m places.", "It is then the discriminator\u2019s job to determine whether a word in the sentence is the original, or the altered word.", "The loss functions for the MLM and Discriminator are as follows:", "Although this looks mightily like a GAN, the generator is trained with maximum likelihood as opposed to adversarially. Another difference is that if the generator generates a token that is identical to the original token, that token is marked as original, rather than generated.", "The pretraining data here for the base model is the same as BERT, which is 3.3 billion tokens from Wikipedia and BooksCorpus. Electra-large is trained on the XLNet data, which adds to the BERT data with ClueWeb, CommonCrawl, and GigaWord.", "The evaluation of the models is done on the GLUE benchmark, as well as SQuAD, both explained here.", "There are some changes to Electra that improve on the model accuracy.", "Weight Sharing: The embeddings used in the generator and discriminator are shared. This yields a small boost in accuracy while training for the same number of epochs, and with the same model parameters as there are with no weight tying. Only the embedding weights are shared, sharing all of the weights has the significant drawback of requiring the two networks to be the same size. A theory for why this works so well here is that with masked language modelling, the input tokens and the corrupted tokens to be discriminated reside in the same vector space. In that case, it doesn\u2019t make sense for the generator to learn an embedding space, and the discriminator to effectively learn an embedding space and the transformation between embedding spaces.", "This is largely the corollary to the previous extension, in that if the generator and discriminator share all weights and are the same size, the model needs twice as much computation per training step as it would with just pure [MASK] tokens. With this model, they explore using a unigram generator to generate the masking tokens. With various sizes of generator being used, what they settle on is a generator that is about 0.25\u20130.5 the size of the discriminator.", "Clark et al. tried multiple high-level training algorithms for ELECTRA as a whole, and settled on the following:", "To be compared to typically-sized SOTA models, the base ELECTRA has to be comparably large. They train their own BERT-Large model using the same hyperparameters and training time as ELECTRA-400k.", "A point that I am curious about is that in the paper, it is a bit of a puzzle how long models take to train relative to each other. What they said is that ELECTRA-Large is the same size as BERT-Large, but they also trained their own BERT with the same hyperparameters and training time as ELECTRA-400k. The selling point for ELECTRA that they describe is using much less physical computational resources, but I wish they outlined those physical and time resources more explicitly beside the other large models, for comparison purposes. They do list the train FLOPs (Floating Point Operation Per Second), which typically could be taken as a measure of how fast their GPU is computing \u2014 but in the appendix they clarify that an \u201coperation\u201d is counted as a mathematical operation, rather than a machine instruction (as is typical). A comparison of the resources used and the time it took them to pre-train and train would have been appreciated.", "Their own model was trained with the same hyperparameters used in ELECTRA-400k.", "Their suspicion is that having to analyze all of the tokens in a string, rather than fill in the gap created by an explicit mask, led to the efficiency gains in Electra. They created Electra 15%, which only calculates the discriminator loss over the 15% of tokens that were masked out. Base Electra scored 85% on the GLUE task, while Electra 15% scored 82.4%, which is comparable to BERT\u2019s 82.2%.", "In this study, they tried many training paradigms, like model size, generator architecture, weight tying, and some were so generally unsuccessful that they didn\u2019t appear in the main body of the paper.", "Though this is not trained adversarially, it is an example of how contrastive learning can be applied to language with great effect. In broad terms, contrastive learning involves discriminating between observed data points and fictitious examples. BERT, Ernie, XLNet, ELMo, RoBERTa, and SpanBERT have all introduced or extended the masked language modelling paradigm, where the model guesses the correct token for a single masked-out token, and Electra extends it further by introducing the possibility that the mask might be any token in the series received by the discriminator. ELECTRA is now a part of Hugging-Face and simpletransformers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep Learning aficionado, musician, book lover"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3cf1c25fc61a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://alecrobinson.medium.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": ""}, {"url": "https://alecrobinson.medium.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Alec Robinson"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57dff17ca481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&user=Alec+Robinson&userId=57dff17ca481&source=post_page-57dff17ca481----3cf1c25fc61a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/2003.10555.pdf", "anchor_text": "https://arxiv.org/pdf/2003.10555.pdf"}, {"url": "https://ai.science/", "anchor_text": "AISC"}, {"url": "https://www.youtube.com/watch?v=Mgck4XFR9GA&t=2166s", "anchor_text": "here"}, {"url": "https://twitter.com/eturner303/status/1143174828804857856?lang=en", "anchor_text": "this"}, {"url": "https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270", "anchor_text": "BERT"}, {"url": "https://mccormickml.com/2019/11/05/GLUE/", "anchor_text": "here"}, {"url": "https://medium.com/tag/electra?source=post_page-----3cf1c25fc61a---------------electra-----------------", "anchor_text": "Electra"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----3cf1c25fc61a---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/transformers?source=post_page-----3cf1c25fc61a---------------transformers-----------------", "anchor_text": "Transformers"}, {"url": "https://medium.com/tag/paper-review?source=post_page-----3cf1c25fc61a---------------paper_review-----------------", "anchor_text": "Paper Review"}, {"url": "https://medium.com/tag/nlp?source=post_page-----3cf1c25fc61a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&user=Alec+Robinson&userId=57dff17ca481&source=-----3cf1c25fc61a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&user=Alec+Robinson&userId=57dff17ca481&source=-----3cf1c25fc61a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3cf1c25fc61a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3cf1c25fc61a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3cf1c25fc61a--------------------------------", "anchor_text": ""}, {"url": "https://alecrobinson.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://alecrobinson.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alec Robinson"}, {"url": "https://alecrobinson.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F57dff17ca481&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&user=Alec+Robinson&userId=57dff17ca481&source=post_page-57dff17ca481--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F57dff17ca481%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Felectra-developments-in-masked-language-modelling-3cf1c25fc61a&user=Alec+Robinson&userId=57dff17ca481&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}