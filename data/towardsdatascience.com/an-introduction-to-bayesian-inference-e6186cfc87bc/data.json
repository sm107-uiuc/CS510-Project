{"url": "https://towardsdatascience.com/an-introduction-to-bayesian-inference-e6186cfc87bc", "time": 1682996718.971691, "path": "towardsdatascience.com/an-introduction-to-bayesian-inference-e6186cfc87bc/", "webpage": {"metadata": {"title": "An Introduction to Bayesian Inference | by Neel Parekh | Towards Data Science", "h1": "An Introduction to Bayesian Inference", "description": "In data science we are often interested in understanding how data was generated, mainly because it allows us to answer questions about new and/or incomplete observations. More specifically, we want\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/parekhneel/", "anchor_text": "https://www.linkedin.com/in/parekhneel/", "paragraph_index": 28}], "all_paragraphs": ["Foreword: The following post is intended to be an introduction with some math. It assumes some familiarity with statistics. Although it includes math, I\u2019m by no means an expert. I struggled to learn the basics of probabilistic programming, and hopefully this helps someone else make a little more sense out of the world. If you find any errors, please drop a comment and help us learn. Cheers!", "In data science we are often interested in understanding how data was generated, mainly because it allows us to answer questions about new and/or incomplete observations. More specifically, we want to model the process by which and the degree to which inputs in observed data influenced the output data. But for any given dataset, there are an infinite number of possible model structures that could reasonably explain how the dataset was generated. But not all model structures are made equally \u2014 that is, certain model structures are more realistic than others based on our assumptions of how the world works. It is up to the modeler to choose the one that best describes the world they are observing.", "GOAL: Find the model M that best explains how a dataset D was generated", "Even after we have chosen which assumptions about influence we\u2019ve made about the world (i.e. a model structure), the model structure itself so far is abstract. For example, we might believe that a model that predicts the probability of me eating gelato would be appropriate if we were to set it up so that the date and my location influence the temperature, which in turn influences the chances of me wolfing down dark chocolate gelato (the best kind). But this only gives us the model structure. Ideally, we would love to have a function that can assign probabilities to any possible gelato outcome for a given date and location. In order to do that, we also need to know how much each input influences each level of the model \u2014 these are captured by the model parameters \u03b8.", "REVISED GOAL: For a given model structure M that encompasses our assumptions about the structure of the world (i.e. the process of influence by which we believe the data were generated), what are the values of the model\u2019s parameters \u03b8 that best explain the observed data D if it had been generated under this chosen model?", "In other words, we might be interested in finding the values of \u03b8 that maximize the likelihood of having observed the ys we did given the Xs we did (in other words: maximize the likelihood of the observed data D). This is called the maximum likelihood estimator.", "Back to the gelato: imagine you recorded every single day where I was and whether I ate gelato or not (weird, but maybe my doctor is concerned I\u2019m eating too much gelato). The maximum likelihood estimator would output the \u03b8s that maximize the likelihood function. The likelihood function is the probability that we would have observed the ys (my gelato consumption) and Xs (the date and my location) if those ys were truly generated with those \u03b8s and those Xs. Maximizing the likelihood gives us the combination of \u03b8s that seemingly worked the best at explaining the data.", "Often, we try to search through every single combination of values for model parameters \u03b8s and calculate the probability that the model with that specific set of model parameter values generated our observed data D. Of course, it\u2019s impossible to search through EVERY combination of \u03b8 in most practical scenarios, so in traditional machine learning we instead optimize this search with algorithms like gradient descent, which choose which values of \u03b8 to test next given how well this current combination of values did at explaining the observed data. We then select and report the combination of \u03b8s that maximized the likelihood of our data.", "One issue with this approach is that we are reporting a single combination of \u03b8s as our best estimate, but someone else who sees this report will have no idea how confident we are for each reported parameter value. This is an issue because observed data is almost always a sample of the true population and inherently noisy, and no matter how much data we collect; we should never be 100% confident in any point estimate of a model parameter. Consider if we had observed a different or limited sample of the population (e.g. what if I only remembered to record my gelato consumption 75% of the time, or only when I was in NYC?), would our reported \u03b8s change drastically or minimally? Knowing the credible interval for each \u03b8 value embeds information about how representative we think our estimate is for a given model parameter based on the data we\u2019ve seen, which may often represent the effect of a true physical phenomenon. We also know very little about how knowledge of my gelato consumption might effect the probability of it having been a certain temperature or me being in a certain city (i.e. inference in the reverse direction).", "In order to address these issues, I\u2019d like to introduce Bayesian Inference, where Bayes Rule is our mantra:", "Let\u2019s see how it can be applied to our goal. Recall that it would be ideal to know the probability distribution of a parameter value \u03b8 under the model structure we\u2019ve chosen, given the data we\u2019ve observed: P_M (\u03b8 | D). Plugging this into Bayes\u2019 Rule:", "We can now rewrite Eqn 2 as:", "If we choose values of \u03b8 to test for our pre-selected model, it\u2019s really easy to calculate P_M (D | \u03b8) because we know all the inputs/output/parameters to this modeling function (recall this is called the likelihood), and we also know the prior P(\u03b8) because it represents our beliefs about a specific \u03b8 (e.g. for a seemingly fair coin the \u03b8 is likely a normal centered on 0.5, and depending on our beliefs of how fair the coin is, we might increase or decrease the variance of that normal). The numerator of EQN 4 is quite simple to calculate for a chosen \u03b8 .", "Ask us for those quantities for every single possible \u03b8 and we quickly realize we run into the same problem as before in traditional machine learning: we can\u2019t sample every single \u03b8! In other words, the denominator of EQN 4 is nearly impossible to compute.", "Markov Chain Monte Carlo (MCMC) techniques were developed in order to intelligently sample \u03b8s rather than directly sum the likelihood and prior for every possible \u03b8 . The main idea behind these techniques is similar to the \u03b8 update techniques we saw in traditional machine learning: we \u201cupdate\u201d with a new set of \u03b8 values based on our evaluation of the likelihood of the current set of \u03b8 values. The big difference here is that we are sampling rather than updating \u2014 in other words, we are interested in the history of values we have explored. This is important because we are no longer simply interested in knowing the single best estimate of each of the parameters (which is what we do in gradient descent), but rather we are interested in knowing the collection of \u201cgood\u201d estimates for each of the parameters and how likely they are. (i.e. the probability distribution of \u03b8 given D).", "Let\u2019s delve into a high level overview of MCMC algorithms and why they work here. MCMC cares about tracking two things:", "\u03b8_current: a single \u03b8 we are currently interested intrace_\u03b8: a list of all \u03b8_currents in order", "MCMC begins by choosing a random initial value of \u03b8:", "and calculates the likelihood and prior (i.e. the numerator of Eqn 4). The point of creating MCMC was that although the denominator is constant across all choices of \u03b8, we can\u2019t calculate the sum in the denominator directly, so let\u2019s put that aside for now. At this point, we have no idea whether our chosen \u03b8_0 is a good choice. We instead choose a proposal \u03b8_1 (at this point, consider it magically chosen, but we\u2019ll get back to that really soon) and if it produces a larger numerator in EQN 4 , we can agree it\u2019s a better choice for \u03b8. This is because in EQN 4 the denominator is constant, no matter our choice of \u03b8 to plug into EQN 4. Since we know \u03b8_1 is better, let\u2019s add it to our trace, update our current \u03b8, and calculate the likelihood and prior (i.e. numerator of EQN 4):", "We can now choose a new \u03b8_2 to propose (still a magical process, for now) and if it\u2019s not as good at explaining the data (i.e. it produces a smaller numerator of Eqn 4) then we don\u2019t immediately accept it this time, instead we accept it into our \u03b8_current and trace_\u03b8 with probability \ud835\udefc:", "The reason for this is because we can reason that \u03b8_2 is only \ud835\udefc as good as \u03b8_current. This means that in our sampling history, we should expect the number of times we sample \u03b8_2 to be a factor of \ud835\udefc of the number of times we sample \u03b8_current:", "In this regime, after sufficient sampling, you should have an intuition that we will have explored a lot of \u03b8 values and have saved them in the trace variable with frequency proportional to how well the \u03b8 value explains the data relative to other \u03b8s. That\u2019s exactly the distribution we were interested in!", "And lastly we return to the question of how to generate good proposal \u03b8s. If we have no method and choose \u03b8s to propose at random among all possible values of \u03b8, we will reject far too many samples (considering the true density of \u03b8 is probably relatively narrowly distributed). Instead we draw \u03b8s from a proposal distribution q(\u03b8_proposed | \u03b8_current). For example we might choose to make q a Normal with a fixed variance and mean \u03b8_current.", "This is the essence of one of the most popular MCMC algorithms called Metropolis-Hastings (MH). We incorporate this into our acceptance ratio \ud835\udefc in the following way:", "If we choose a symmetric proposal distribution (e.g. the Normal distribution), then:", "and it follows that \ud835\udefc_r = \ud835\udefc. This is called the Random Walk MH algorithm, and you\u2019ll end up with plots like this:", "The left is a KDE plot (essentially a smoothed histogram) for an \u201cintercept_mu\u201d parameter, and the right is the trace over time for each sampling chain. We would be able to infer that although the most likely value of the parameter (the mode of the trace, a.k.a. the MAP) is 0.16, the credible interval of the parameter is likely between 0.1 and 0.21.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Nielsen in NYC. LinkedIn: https://www.linkedin.com/in/parekhneel/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe6186cfc87bc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@parekhneel?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@parekhneel?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Neel Parekh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda7e178912b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&user=Neel+Parekh&userId=da7e178912b3&source=post_page-da7e178912b3----e6186cfc87bc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.youtube.com/watch?v=OTO1DygELpY", "anchor_text": "this video"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e6186cfc87bc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e6186cfc87bc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----e6186cfc87bc---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/gelato?source=post_page-----e6186cfc87bc---------------gelato-----------------", "anchor_text": "Gelato"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----e6186cfc87bc---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&user=Neel+Parekh&userId=da7e178912b3&source=-----e6186cfc87bc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&user=Neel+Parekh&userId=da7e178912b3&source=-----e6186cfc87bc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe6186cfc87bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e6186cfc87bc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e6186cfc87bc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@parekhneel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@parekhneel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Neel Parekh"}, {"url": "https://medium.com/@parekhneel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "35 Followers"}, {"url": "https://www.linkedin.com/in/parekhneel/", "anchor_text": "https://www.linkedin.com/in/parekhneel/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fda7e178912b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&user=Neel+Parekh&userId=da7e178912b3&source=post_page-da7e178912b3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fda7e178912b3%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-introduction-to-bayesian-inference-e6186cfc87bc&user=Neel+Parekh&userId=da7e178912b3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}