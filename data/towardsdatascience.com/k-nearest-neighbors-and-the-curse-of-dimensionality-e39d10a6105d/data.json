{"url": "https://towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d", "time": 1682997356.494825, "path": "towardsdatascience.com/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d/", "webpage": {"metadata": {"title": "k-Nearest Neighbors and the Curse of Dimensionality | by Peter Grant | Towards Data Science", "h1": "k-Nearest Neighbors and the Curse of Dimensionality", "description": "In my last article, we learned about the k-nearest neighbors modeling algorithm. That algorithm makes classification predictions about your desired data points based on the assumption that nearby\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/introducing-k-nearest-neighbors-7bcd10f938c5", "anchor_text": "my last article", "paragraph_index": 0}, {"url": "https://www.amazon.com/gp/product/1492041130/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1492041130&linkCode=as2&tag=petergrantpub-20&linkId=48ae2069a141e1f8e6cdb548b21c2eda", "anchor_text": "Data Science from Scratch", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d", "anchor_text": "the next one", "paragraph_index": 9}, {"url": "https://towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d", "anchor_text": "a future article", "paragraph_index": 12}], "all_paragraphs": ["In my last article, we learned about the k-nearest neighbors modeling algorithm. That algorithm makes classification predictions about your desired data points based on the assumption that nearby data points are similar to your test point. In order to function it needs two things:", "This means that your success when using the k-nearest neighbors algorithm is very dependent on having a dense data set. This makes it especially vulnerable to the \u201cCurse of Dimensionality\u201d.", "The \u201cCurse of Dimensionality\u201d is a tongue in cheek way of stating that there\u2019s a ton of space in high-dimensional data sets. The size of the data space grows exponentially with the number of dimensions. This means that the size of your data set must also grow exponentially in order to keep the same density. If you don\u2019t, then data points start getting farther and farther apart.", "At face value it doesn\u2019t seem that k-nearest neighbors would be especially sensitive to this problem. Every machine learning algorithm needs a dense data set in order to accurately predict over the entire data space. Errors arise in all algorithms if there are gaps between the data. So what makes k-nearest neighbors special?", "The special challenge with k-nearest neighbors is that it requires a point to be close in every single dimension. Some algorithms can create regressions based on single dimensions, and only need points to be close together along that axis. k-nearest neighbors doesn\u2019t work that way. It needs all points to be close along every axis in the data space. And each new axis added, by adding a new dimension, makes it harder and harder for two specific points to be close to each other in every axis.", "Joel Grus does a good job of describing this issue in Data Science from Scratch. In that book he calculates the average and minimum distances between two points in a dimension space as the number of dimensions increases. He calculated 10,000 distances between points, with the number of dimensions ranging from 0 to 100. He then proceeds to plot the average and minimum distance between two points, as well as the ratio of the closest distance to the average distance (Distance_Closest / Distance_Average).", "In those plots, Joel showed that the ratio of the closest distance to the average distance increased from 0 at 0 dimensions, up to ~0.8 at 100 dimensions. And this shows the fundamental challenge of dimensionality when using the k-nearest neighbors algorithm; as the number of dimensions increases and the ratio of closest distance to average distance approaches 1 the predictive power of the algorithm decreases. If the nearest point is almost as far away as the average point, then it has only slightly more predictive power than the average point.", "Think of how this problems applies to our apple example from above. It\u2019s a bit counter-intuitive, since the apple example is inherently in 3 dimensions, but imagine that the distance to the next closest item in the produce section is about the same as the average distance. Suddenly you can\u2019t be sure if the nearest item is another gala apple, or a fuji apple, or an orange, or a bunch of parsley. As far as the k-nearest neighbors algorithm would be concerned, the entire produce section would be jumbled together.", "The problem is fundamentally that there isn\u2019t enough data available for the number of dimensions. As the number of dimensions increases the size of the data space increases, and the amount of data needed to maintain density also increases. Without dramatic increases in the size of the data set, k-nearest neighbors loses all predictive power. And this makes one possible solution for the issue straightforward: Add more data. It\u2019s entirely possible to add more and more data to ensure that you have enough data density even as you add more dimensions. And if you have the hardware to handle that volume of data, it\u2019s a perfectly legitimate solution.", "Of course, you can\u2019t always have the hardware necessary to add that much data. Not every data scientist can afford access to a supercomputer. And even then, it\u2019s possible to have a large enough data set that not even a super computer could handle it in a reasonable amount of time. This is where the concept of dimensionality reduction comes into play. Dimensionality reduction is a topic beyond the scope of this article, but I\u2019ll cover it in the next one. Essentially, it refers to identifying trends in the data set that operate along dimensions that are not explicitly called out in the data set. You can then create new dimensions matching those axes and remove the original axes, thus reducing the total number of axes in your data set.", "As an example, say that you plot the location of garden gnomes in a city. You get the GPS co-ordinates of each garden gnome, and plot them on a map. You have the two dimensions of North-South and East-West. Now imagine that, for some reason, all of the garden gnomes are placed in a near diagonal line running across the town. There\u2019s one in the Southeast, one in the Northwest corner, and a nearly straight line between the two. Now you can create a new axis called the Southeast-Northeast axis (Or, if you want to be silly, \u201cThe Garden Gnome Demarcation Line\u201d) and remove the North-South and East-West axis. In this way you\u2019ve reduced your data set from two dimensions two one, and made it easier to the k-nearest neighbors algorithm to succeed.", "The k-nearest neighbors algorithm hinges on data points being close together. This becomes challenging as the number of dimensions increases, referred to as the \u201cCurse of Dimensionality.\u201d It\u2019s especially hard for the k-nearest neighbors algorithm it requires two points to be very close on every axis, and adding a new dimension creates another opportunity for points to be farther apart. As the number of dimensions increases, the closest distance between two points approaches the average distance between points, eradicating the ability of the k-nearest neighbors algorithm to provide valuable predictions.", "To overcome this challenge, you can add more data to the data set. By doing so you add density to the data space, bringing the nearest points closer together and returning the ability of the k-nearest neighbors algorithm to provide valuable predictions. This is a valuable solution so long as you have the hardware needed to perform computations on your data set. As your data set gets larger and larger, you need more and more computing power to process it. Eventually the size of your data set will surpass your computing power. At that point, you need to use dimensionality reduction to present all of the valuable information in fewer dimensions. That topic will be described in more detail in a future article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Scientist at Lawrence Berkeley National Laboratory who also teaches skills you need to build a fulfilling career."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe39d10a6105d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://petergrant-81989.medium.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Peter Grant"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F985f2ff02845&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&user=Peter+Grant&userId=985f2ff02845&source=post_page-985f2ff02845----e39d10a6105d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/introducing-k-nearest-neighbors-7bcd10f938c5", "anchor_text": "my last article"}, {"url": "https://www.amazon.com/gp/product/1492041130/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1492041130&linkCode=as2&tag=petergrantpub-20&linkId=48ae2069a141e1f8e6cdb548b21c2eda", "anchor_text": "Data Science from Scratch"}, {"url": "https://towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d", "anchor_text": "the next one"}, {"url": "https://towardsdatascience.com/an-introduction-to-dimenstionality-reduction-8c7d1c80737d", "anchor_text": "a future article"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e39d10a6105d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e39d10a6105d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e39d10a6105d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----e39d10a6105d---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&user=Peter+Grant&userId=985f2ff02845&source=-----e39d10a6105d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&user=Peter+Grant&userId=985f2ff02845&source=-----e39d10a6105d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe39d10a6105d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e39d10a6105d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e39d10a6105d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e39d10a6105d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e39d10a6105d--------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://petergrant-81989.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Peter Grant"}, {"url": "https://petergrant-81989.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "969 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F985f2ff02845&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&user=Peter+Grant&userId=985f2ff02845&source=post_page-985f2ff02845--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F90e3ec001185&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d&newsletterV3=985f2ff02845&newsletterV3Id=90e3ec001185&user=Peter+Grant&userId=985f2ff02845&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}