{"url": "https://towardsdatascience.com/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182", "time": 1683002764.56447, "path": "towardsdatascience.com/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182/", "webpage": {"metadata": {"title": "Reinforcement Learning Policy for Developers | by Ziad SALLOUM | Towards Data Science", "h1": "Reinforcement Learning Policy for Developers", "description": "Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com Policy is somehow a tricky concept, mainly for Reinforcement Learning beginners. This article\u2026"}, "outgoing_paragraph_urls": [{"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com", "paragraph_index": 0}, {"url": "https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac", "anchor_text": "Dynamic Programming in this article", "paragraph_index": 14}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Based Reinforcement Learning", "paragraph_index": 21}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step", "paragraph_index": 21}], "all_paragraphs": ["Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com", "Policy is somehow a tricky concept, mainly for Reinforcement Learning beginners. This article will try to clarify the topic in plain and simple English, away from mathematical notions. It is written with developers in mind.", "If you have ever heard of best practices or guidelines then you have heard about policy. Consider, for example, fire safety guidelines for people living in high buildings. Probably the most important guideline is not to use the elevator during a fire. People should close doors, stock water, use wet sheets, and make their position known to firefighters.These series of actions are issued by guidelines that are the result of a compilation of best practices that have been accumulated throughout time and experience.So the policy tells you how to act when facing a certain situation. It does not tell you the result or value of your action. However, you implicitly expect to have the best possible outcome when following this policy, for example escaping the fire unharmed.", "Action value is when you take action and assess its result or value.For example, a person trapped on a high floor in a building on fire considers his/her options (the actions available), then tries each one of them to assess its outcome. Of course, in reality, this is not possible, because once hurt there is no coming back. But in a simulation, sports, games, there is a possibility to play many scenarios and spot the actions that give the best results.", "It goes without saying that a policy is the compilation of the best practices based on actions with the highest score (or value).", "meaning that at state s we take action a such that a gives the best result.Where q(s,a) is the value of the action a taken at state s.", "But there is a catch. You need to really understand that this is a stochastic environment, full of uncertainty. This means that the same action at the same state may lead to different results each time it is executed.", "For example, a basketball player practicing three points throws, will score some times and misses other times, even if he is shooting from the same spot (or angle) using the same hand. He/she will have to compute an average (score/number of throws) to find out the efficiency of the combination of the spot (or angle) and hand.", "To discover the best angle and hand to use during a match, he/she will have to practice shooting from different angles and different hands, then average the results in order to deduce which combination works best for him/her.", "Of course, one way to solve this issue is to perform a big number of iterations. At each iteration perform all available actions at that state, then take the average of q(s,a) for each action, and update policy \ud835\uded1(s). We say that we are updating the policy on each iteration. So after each iteration, we have a new policy \ud835\uded1 that reflects the computed q-values.", "After sufficiently large number of iterations, when the average of q(s,a) starts to vary slowly enough to consider it stable, we say that we have reached a stable phase and the policy has become optimal (meaning this is the best that can be reached), we call it optimal policy and we write it \ud835\uded1*", "There are different ways to approach the topic of policy and different ways to improve it (i.e. update)", "The optimal policy is when the current policy being trained has reached a stage after which it can\u2019t be substantially improved.The importance of optimal strategy is that it allows an agent that follows it to achieve the \u201cbest possible\u201d results.", "This is the easiest scenario, and also the least realistic one. All the environment dynamics are known, we just have to iteratively compute the q-values, and after each iteration assign the action with the highest q(s,a) value to the policy \ud835\uded1(s).In Dynamic Programming, we know all the states and all the transition probabilities, so there is no need to discover anything, simply to compute the values.", "More on Dynamic Programming in this article.", "In Monte Carlo, we don\u2019t know much about the inner workings of the environment, so we need to discover it by playing episodes. We play each episode to the end, till we reach a terminal state, then we compute the q- value q(s,a) for every state accessed and every action performed.", "However, unlike Dynamic Programming we can\u2019t compute the q-value for all states because we simply don\u2019t know them beforehand. We have to discover them as we play the episodes.Now the question, is how do we play these episodes, in a meaningful way so we can extract the optimal policy?Of course, there is the random walk policy, that consists of picking at each state a random action to perform. This will work at the beginning, but later on, when we start discovering that some actions are better than others, it becomes quickly inefficient to keep the random walk because it won\u2019t be learning anything. The reason for this inefficiency is because the policy is not being updated with the new values of q.", "So it is better to use a policy that plays the best possible action at each state. But the problem of such a policy is that we will be executing the same action every time, and we won\u2019t be able to discover new states, add to that we are not sure that the chosen action is truly the best one. Remember that the environment is full of uncertainty and there is nothing that guarantees that if one action that yielded good result at time t, will have the same good result at time t+1. So in order to have the best of the two approaches, we adopt a policy called \ud835\udf00-greedy.In the \ud835\udf00-greedy policy, we follow the greedy policy (going for the best action that we know of so far) 1- \ud835\udf00 part of the time, and we pick a random action \ud835\udf00 part of the time. This will allow discovering new states through random walk some of the time and exploit the best action most of the time.", "Follow these links to learn more about Monte Carlo and Temporal Difference methods.", "Not everything is about picking the action that results in the best q-value. To really grasp this idea, do the following experiment:Play Rock-Scissor-Paper with someone, and pay attention to the fact that playing the same action in a row is a bad idea (even if it worked the first few times), because your opponent will countermeasure it. For example, if you frequently play Rock, your opponent will play paper and win.So the policy is no more about the best result of an action at a certain state, but about the probability distribution of actions that will let you win.In other words, it is about how unpredictable your actions should be.", "Note that stochastic policy does not mean it is stochastic in all states. It suffices to be for some of them. The states in which the policy acts deterministically, its actions probability distribution (on those states) would be 100% for one action and 0% for all the other ones.", "Policy Based Reinforcement Learning and Policy Gradient Step by Step explain stochastic policies in more detail.", "What does it mean to evaluate a policy?Well, it is like testing anything else. You try to find out if it delivers what it promises.In the same way, policy evaluation consists of asking a policy to provide an action given a certain state, then perform this action and computes the result (q-value for example)Eventually, not all actions will yield the expected result, and thus the policy needs some fine tuning.This is the job of the Policy Control also called Policy Improvement", "Policy control or improvement is about giving feedback to the policy after executing an action at a certain state. The result of the action is fed into the policy so it updates its internal behaviour, in a way to make that action more likely to be used (or not) next time the same state is reached. This feedback is called the update.", "Consider again the example of the basketball player throwing at the three-point line. Starting with the left hand he/she scored a few times, so the policy is updated to use the left hand. As the practicing session goes on, he/she comes to be successful 30% of the time when using the left hand, but 70% of the time using the right hand. The policy should then be updated to use the right hand rather than the left hand when on this spot (or angle).", "To conceptually implement a policy, let\u2019s consider the following interface called IPolicy. It contains two methods:", "Each specific policy implements this interface in a way to reflect its behaviour.For example, the RandomPolicy implements getAction(s) so that it returns random action at state s. It does not implement the update method since there is no need for any update when the behaviour is always random.", "The GreedyPolicy returns the action at state s that gives the best q-value. For this reason, it has to store these values in an appropriate structure.On the other hand, the update method updates the q-value computed at state s and action a. This is usually done by averaging with previously computed values.", "The EpsilonGreedyPolicy returns a random action at state s , \ud835\udf00 part of the time, and the action that has the best q-value (1- \ud835\udf00) part of the time. As for the update method, it has the same logic as GreedyPolicy.", "StochasticPolicy does not directly rely on q-value (it might indirectly do that), but it returns the action following the probability distribution of their success. For example, if action A has been successful 60% of the time, action B 30% of the time and action C 10% of the time. Then the action returned by the method follows the distribution 60% A, 30% B, 10% C.", "As for the update method, it updates the underlying probability distribution of the actions.", "To put everything together and reach an optimal policy, we consider a high level algorithm that does the following:", "This article focuses on policies in Reinforcement Learning in a practical way with a descriptive explanation rather than a mathematical one. It serves as a building block in the whole RL structure, and it profits best for developers.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F43cd2b713182&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://zsalloum.medium.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2----43cd2b713182---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@philipwyers?utm_source=medium&utm_medium=referral", "anchor_text": "Philip Wyers"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://rl-lab.com/", "anchor_text": "http://rl-lab.com"}, {"url": "https://unsplash.com/@chrismoore_?utm_source=medium&utm_medium=referral", "anchor_text": "Chris Moore"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac", "anchor_text": "Dynamic Programming in this article"}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "Temporal Difference"}, {"url": "https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083", "anchor_text": "Policy Based Reinforcement Learning"}, {"url": "https://towardsdatascience.com/policy-gradient-step-by-step-ac34b629fd55", "anchor_text": "Policy Gradient Step by Step"}, {"url": "https://towardsdatascience.com/understanding-reinforcement-learning-math-for-developers-b538b6ef921a", "anchor_text": "Understanding Reinforcement Learning Math, for Developers"}, {"url": "https://medium.com/p/9350e1523031", "anchor_text": "Q vs V in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/p/1b7ed0c030f4", "anchor_text": "Math Behind Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac", "anchor_text": "Dynamic Programming in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511", "anchor_text": "Monte Carlo in Reinforcement Learning, the Easy Way"}, {"url": "https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce", "anchor_text": "TD in Reinforcement Learning, the Easy Way"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----43cd2b713182---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----43cd2b713182---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----43cd2b713182---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----43cd2b713182---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&user=Ziad+SALLOUM&userId=1f2b933522e2&source=-----43cd2b713182---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F43cd2b713182&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----43cd2b713182---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----43cd2b713182--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----43cd2b713182--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----43cd2b713182--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----43cd2b713182--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----43cd2b713182--------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://zsalloum.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ziad SALLOUM"}, {"url": "https://zsalloum.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "845 Followers"}, {"url": "https://rl-lab.com", "anchor_text": "https://rl-lab.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1f2b933522e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&user=Ziad+SALLOUM&userId=1f2b933522e2&source=post_page-1f2b933522e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F408fc441c93b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frevisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182&newsletterV3=1f2b933522e2&newsletterV3Id=408fc441c93b&user=Ziad+SALLOUM&userId=1f2b933522e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}