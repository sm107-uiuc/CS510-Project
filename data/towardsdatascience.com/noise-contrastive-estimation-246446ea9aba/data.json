{"url": "https://towardsdatascience.com/noise-contrastive-estimation-246446ea9aba", "time": 1682996337.912008, "path": "towardsdatascience.com/noise-contrastive-estimation-246446ea9aba/", "webpage": {"metadata": {"title": "Noise Contrastive Estimation. A Gentle Introduction | by Zak Jost | Towards Data Science", "h1": "Noise Contrastive Estimation", "description": "I have recently worked to understand Noise Contrastive Estimation (NCE), and it was a tough journey. This post is meant to make it easier for other people by being less formal, and working through\u2026"}, "outgoing_paragraph_urls": [{"url": "https://blog.zakjost.com/post/nce-intro/", "anchor_text": "blog.zakjost.com", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "word2vec", "paragraph_index": 2}, {"url": "https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf", "anchor_text": "it\u2019s been shown", "paragraph_index": 23}], "all_paragraphs": ["This article originally appeared on blog.zakjost.com", "I have recently worked to understand Noise Contrastive Estimation (NCE), and it was a tough journey. This post is meant to make it easier for other people by being less formal, and working through the particulars of implementation. Before digging into the details of how it works, let\u2019s first talk about the problem it sets out to solve.", "Let\u2019s say you want to learn vector embeddings of words to help with Natural Language tasks. The famous word2vec approach does something clever:", "By doing this, the network learns the statistics of how often words occur together in sentences. Since the weights of the embedding lookup table are learnable, it tends to put similar words closer together in the embedding space than dissimilar words. Cool.", "But let\u2019s think about the computation in Step 4 above. Your network\u2019s dense layer has a weight matrix of size (embedding_dim, vocab_dim), which transforms the context embedding into numbers that help you predict the probability of each vocabulary word. This requires two steps: 1) get a value for each word from the output of the dense layer (we'll call the values z_i, where i indexes a particular word in the vocabulary), and then 2) turn those values into probabilities by using a Softmax transformation:", "Notice that the denominator requires a sum over the entire vocabulary size |V|. If you have a huge vocabulary size, then it becomes expensive and slow to normalize each and every training example by summing over the outputs of every vocabulary word.", "In Step 5, we calculate the cross-entropy loss:", "Even though the loss function is expressed as a sum over the entire vocabulary, the only term that is non-zero is where the label y = 1; i.e. the term corresponding to the actual target word. The problem is that every p_i term divides by the same denominator, which itself is a sum over the entire vocabulary. This makes our loss function depend on every output in the network, when means every network parameter will have a non-zero gradient and therefore needs updating for every training example.", "One idea to solve this problem is: instead of summing over the probabilities of every incorrect vocabulary word in the denominator, just pick a few. These chosen non-target words are called Negative Samples. So: follow the same steps as above, but in Step 4, only make predictions for the target word and a random sample of the other words, and pretend that represents the entire vocabulary.", "This will clearly not given the right normalization since you\u2019re not summing over the vast majority of the vocabulary, but it\u2019s an approximation that turns out to work well. It has the added advantage that you don\u2019t need to update the weights for every vocabulary word, which can be millions of parameters, but instead only the weights for the target word and the negative samples. Since this probability is normalized using just the target word and a few negative samples instead of the entire vocabulary, there\u2019s many fewer variables involved. The number of gradients/updates therefore goes from |embed|*|V| to |embed| * |samples + 1|. This makes sense intuitively as well. Should we really use/update the parameters for the word \u201czebra\u201d for every slice of every sentence available?", "NCE is very similar to Negative Sampling in implementation, but it adds some theoretical teeth. Let\u2019s first discuss how they frame the problem and then go through the tweaks to the implementation.", "In Negative Sampling, we labeled the true target word with 1, and random samples of the incorrect target words with 0's. This is sort of like training a model to predict, \u201cWhich of these is real, and which is noise?\u201d NCE uses a Logistic Regression (LogReg) model to answer this question directly.", "Recall that LogReg models the log-odds ratio that the input came from one class instead of another:", "We want the log-odds that a class came from the true word-distribution P instead of a noise distribution Q:", "This compares the data distribution, which we\u2019re trying to learn, to a reference noise distribution \u2014 hence the name Noise Contrastive Estimation. We don\u2019t know the real distribution P, but we\u2019re free to specify Q to be whatever we want. The Q distribution is what we use to generate our Negative Samples. For instance, maybe we sample all vocabulary words with equal probability, or in a way that takes into account how rare a word is in the training data. The point is: it\u2019s up to us, and that makes calculating the log(Q) part straight-forward.", "To be super-clear, I think it\u2019s worth looking one more time at the word2vec network and thinking about how Q is used:", "We use a context vector as input into our dense layer. But instead of calculating the outputs of every single word in the vocabulary, we do the following: Randomly sample words from a distribution we\u2019ve specified: Q. Then only calculate the network output values for the true target word and for the words we randomly sampled from our noise distribution. If we pulled 5 random samples, then we would only evaluate the network for 6 outputs (samples + target), and ignore the rest of the vocabulary.", "Now, since we define the Noise Distribution that determines how we pull the Negative Samples, we can analytically calculate any particular word\u2019s probability according to this distribution, Q. For instance, if we define \u201cword-1\u201d to have probability 10% and \u201cword-2\u201d with probability 90%, and we happen to pull a sample of \u201cword-1\u201d, then Q = 0.10; it\u2019s just a reference to the distribution we defined. So we can get the log(Q) part in", "That\u2019s what we use our neural network for \u2014 to predict P given a context. We then use the network output and the analytically calculated Q to calculate the logit equation above. The network is trained by treating this as a normal Logistic Regression task where the target word is labeled with 1 and the negative samples are all labeled with 0.", "Using this framework, we transformed the unsupervised task of learning the data\u2019s probability distribution P into a supervised LogReg problem, in which we invent the labels by merely indicating if a word is the true target or came from the noise distribution. By learning a model to tell the difference between the real thing and a thing we invented, Q, it learns to understand the real thing. Pretty clever framework.", "Now that we understand we want to create a Logistic Regression model to predict the log-odds of being real data vs from the noise distribution, let\u2019s describe how we need to change the word2vec implementation. Steps 1\u20133 are identical: we create the same (context, target) pairs, and average the embeddings of the context words to get a context vector.", "Here, N = k + 1 (number of negative samples plus the target word), l_i are the labels for if it\u2019s the target or a negative sample, and p_i-hats are the outputs of the sigmoid as defined above.", "The only real implementation difference from Negative Sampling is that there\u2019s a correction added, which takes into account the probability that the word is sampled, and then it\u2019s cast as a Logistic Regression/binary classification task. By using our network to output P and minimizing the Binary Cross Entropy, our network learns P.", "Note: In this form, the network is technically not outputting normalized values-there\u2019s nothing inherently enforcing that the outputs sum to 1. Adding explicit normalization would require summing over the entire vocabulary, which defeats the entire purpose. The original paper suggests that the normalization constants for each input context be a learnable parameter, and this is shown to work. However, it\u2019s been shown that, surprisingly, just assuming the network outputs normalized values gives equivalent performance, and the network learns to mostly self-normalize, since this is the optimal solution.", "Noise Contrastive Estimation is a way of learning a data distribution by comparing it against a noise distribution, which we define. This allows us to cast an unsupervised problem as a supervised logistic regression problem. It\u2019s implementation is similar to Negative Sampling, which is an approximation mechanism that was invented to reduce the computational cost of normalizing network outputs by summing over the entire vocabulary. The primary difference in implementation between NCE and Negative Sampling is that in NCE, the probability that a sample came from the noise distribution is explicitly accounted for and the problem is cast as a formal estimate of the log-odds ratio that a particular sample came from the real data distribution instead of the noise distribution.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F246446ea9aba&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----246446ea9aba--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@zjost85?source=post_page-----246446ea9aba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@zjost85?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Zak Jost"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b5658f69b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&user=Zak+Jost&userId=31b5658f69b6&source=post_page-31b5658f69b6----246446ea9aba---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://blog.zakjost.com/post/nce-intro/", "anchor_text": "blog.zakjost.com"}, {"url": "https://en.wikipedia.org/wiki/Word2vec", "anchor_text": "word2vec"}, {"url": "https://www.tensorflow.org/guide/embedding", "anchor_text": "lookup embedding layer"}, {"url": "https://www.cs.toronto.edu/~amnih/papers/ncelm.pdf", "anchor_text": "it\u2019s been shown"}, {"url": "https://blog.zakjost.com/post/nce-intro/", "anchor_text": "https://blog.zakjost.com"}, {"url": "http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf", "anchor_text": "original NCE paper"}, {"url": "http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf", "anchor_text": "NCE for learning word embeddings"}, {"url": "https://arxiv.org/pdf/1410.8251.pdf", "anchor_text": "set of notes"}, {"url": "https://www.tensorflow.org/extras/candidate_sampling.pdf", "anchor_text": "Candidate Sampling reference"}, {"url": "http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/", "anchor_text": "Part 1"}, {"url": "http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/", "anchor_text": "Part 2"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----246446ea9aba---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----246446ea9aba---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----246446ea9aba---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----246446ea9aba---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&user=Zak+Jost&userId=31b5658f69b6&source=-----246446ea9aba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&user=Zak+Jost&userId=31b5658f69b6&source=-----246446ea9aba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F246446ea9aba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----246446ea9aba---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----246446ea9aba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----246446ea9aba--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----246446ea9aba--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----246446ea9aba--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----246446ea9aba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@zjost85?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@zjost85?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Zak Jost"}, {"url": "https://medium.com/@zjost85/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "555 Followers"}, {"url": "https://www.youtube.com/channel/UCxw9_WYmLqlj5PyXu2AWU_g", "anchor_text": "https://www.youtube.com/channel/UCxw9_WYmLqlj5PyXu2AWU_g"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F31b5658f69b6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&user=Zak+Jost&userId=31b5658f69b6&source=post_page-31b5658f69b6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fac53ca1e86fa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnoise-contrastive-estimation-246446ea9aba&newsletterV3=31b5658f69b6&newsletterV3Id=ac53ca1e86fa&user=Zak+Jost&userId=31b5658f69b6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}