{"url": "https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4", "time": 1683013288.798203, "path": "towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4/", "webpage": {"metadata": {"title": "Under the Hood \u2014 Logistic Regression | by Saurabh Singh | Towards Data Science", "h1": "Under the Hood \u2014 Logistic Regression", "description": "This is the second article in a series of articles where we will understand the \u201cunder the hood\u201d workings of various ML algorithms, using their base math equations. With so many optimized\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/introduction-to-machine-learning-f41aabc55264", "anchor_text": "this article", "paragraph_index": 3}, {"url": "https://towardsdatascience.com/linear-regression-under-the-hood-583003d0bf38", "anchor_text": "Linear Regression", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/under-the-hood-decision-tree-454f8581684e", "anchor_text": "Decision Trees", "paragraph_index": 53}], "all_paragraphs": ["This is the second article in a series of articles where we will understand the \u201cunder the hood\u201d workings of various ML algorithms, using their base math equations.", "With so many optimized implementations out there, we sometimes focus too much on the library and the abstraction it provides, and too little on the underlying calculations that go into the model. Understanding these calculations can often be the difference between a good and a great model.", "In this series, I focus on implementing the algorithms by hand, to understand the math behind it, which will hopefully help us train and deploy better models.", "Note \u2014 This series assumes you know the basics of Machine Learning and why we need it. If not, do give this article a read to get up to speed on why and how we utilize ML.", "Logistic Regression builds on the concepts of Linear Regression, where the model produces a linear equation relating the input features(X) to the target variable (Y).", "The two major differentiating features of the Logistic Regression algorithm are \u2014", "Consider the following data, with two input features \u2014 X1, X2, and one binary (0/1) target feature \u2014 Y", "Logistic Regression will try to find the optimum values for the parameters w1, w2, and b, such that \u2014", "Here, the function H, also known as the activation function, converts the continuous output values of y to a discrete value. This will ensure that the equation is able to output a 1 or 0 similar to the input data.", "The algorithm finds these optimum values using the following steps \u2014", "2. Pick one instance of the data and calculate the continuous output (z).", "3. Calculate the discrete output (\u0177) using the activation function H().", "4. Calculate loss \u2014 Did our assumptions lead us close to a 1, when the actual target was 1?", "5. Calculate the gradient for w1, w2 and b \u2014 How should we change the parameters to move closer to the actual output?", "7. Repeat steps 2\u20136 until convergence.", "Let\u2019s look at each of these steps in detail.", "We start with assigning random values to the model parameters \u2014", "Let\u2019s start with the first row of our data \u2014", "Putting in our assumed parameter values, we get \u2014", "If you\u2019ve been following the \u201cUnder the Hood\u201d series, you would have noticed that Steps 1 and 2 are exactly the same as in Linear Regression. This point will come up often, as Linear Regression forms the foundation for most algorithms out there \u2014 from simple ML algorithms to Neural Networks.", "It\u2019s all powered by Y = MX + c", "With our foundation laid out, we now focus on what makes Logistic Regression different from Linear Regression, and this is where the \u201cactivation function\u201d comes in.", "The activation function forms the bridge between the linear world with continuous target values and the \u201clogistic\u201d world (like the one we are working in right now!) with discrete target values.", "A simple form of an activation function would be a thresholding function (also known as the \u201cStep function\u201d) \u2014", "But, as with all things ML, \u201cit can\u2019t be that simple\u201d.", "One major flaw with a simple thresholding function like this, is that we have to manually select the right threshold(based on the range of output) every time we build a classification model. Our values could have any arbitrary range depending on the input variables and weights.", "Another fact working against a simple function like this, is that it is not differentiable at z=threshold. We need to follow the pipeline of loss -> gradient -> weight-update, hence we will need to make things a bit more complex here such that our life is simpler when we calculate the derivative(gradient) of our loss.", "This is where a slightly modified version of the threshold \u2014 the Sigmoid function, comes in \u2014", "Just like thresholding, the sigmoid function converts real number values, to a value between 0 and 1 with a midpoint at z=0 (thus solving our first problem). As evident from the graph, the function is smooth at all points, which will be of benefit when calculating gradients(thus solving our second problem).", "Another advantage of the sigmoid function is that it tells us how close our estimate is, to 0 or 1. This helps us get a good understanding of the model loss(error) \u2014 a prediction of 0.9 for a row that has an actual 1 is better than a prediction of 0.7. Such intricacies are lost when using a step function.", "Let\u2019s use the sigmoid function to calculate our estimated output \u2014", "Building on Linear Regression, we could choose squared error to represent our loss, but that would always make the error small, as our values lie in the 0\u20131 range. We need a function that outputs a large loss when our assumptions provide a value close to 0, while the actual is 1, and vice-versa.", "One such function is the log-loss, which uses log transformation of our predicted output(z) to calculate loss.", "We define the loss function as \u2014", "The output of the log function approaches negative infinity as the input reaches close to 0, and it is 0 when the input is 1. The negative sign inverts the log values to ensure our loss lies between 0 to infinity.", "We can write the function as a single equation \u2014", "We can now calculate the error our assumptions lead us to \u2014", "We now calculate the impact each of our parameter has on the predicted output(and loss) by calculating the gradient of each of our parameters vs the loss.", "This is where having differential functions in our pipeline make our life easy, giving us derivatives** for each of our parameters as \u2014", "The gradients tell us how much we should change each of our parameter assumptions to reduce the loss and move our predicted output closer to the actual output.", "As we are \u201ctraining\u201d the model on one instance at a time, we want to limit the impact the loss on this individual instance has on our parameters. Thus, we scale the gradients to a tenth of their value, before updating the parameters, using \u201clearning rate (\u03b7)\u201d \u2014", "This completes one iteration of training.", "To get the optimum parameter values, we repeat the above steps either for a fixed number of times, or until our loss stops reducing i.e., convergence.", "Let\u2019s work through another iteration, using the updated parameters \u2014", "Looks like our loss has increased on this instance. This shows how the binary instances are pulling the weights in opposite direction to generate a decision boundary between the two classes.", "Repeating the steps for another few iterations*, sampling data randomly from the first 4 rows, we get the optimum weights as \u2014", "Using these values on our last row of data (which we did not use while training), we get \u2014", "That is really close to our actual class \u20181\u2019.", "Choosing a cutoff at 0.5(mid-point of our sigmoid curve), gives us a prediction of 1 for this instance.", "And that\u2019s it. At its core, this is all that the Logistic Regression algorithm does.", "\u201cUnder the Hood\u201d being the focus of this series, we took a look at the foundation of Logistic Regression taking one sample at a time and updating our parameters to fit the data.", "While it\u2019s true that this is what Logistic Regression does at its core, there is a lot more that goes into a good Logistic Regression model, like \u2014", "I will cover these concepts in a parallel series focused on the various intricacies of the different ML algorithms we cover in this series.", "In the next article in this series, we will continue with the classification task, and look under the hood of another class of algorithms \u2014 Decision Trees, which work a lot like how you and I would use reasoning to arrive at a conclusion regarding the data.", "*Over 5 iterations, this is how our loss, gradients, and the parameter values changed \u2014", "** I have not covered how we get the derivatives of our loss function w.r.t our parameters in this article, as the derivation is extensive and warrants an article in its own right (It will be covered in another article :))", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A problem solver first, a data scientist second,\u2026 and a writer (n-1)th"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F407c0276c0b4&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@saurabhsingh0424?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saurabhsingh0424?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Saurabh Singh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890c4657d91c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&user=Saurabh+Singh&userId=890c4657d91c&source=post_page-890c4657d91c----407c0276c0b4---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@joel_rohland?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Joel Rohland"}, {"url": "https://unsplash.com/s/photos/repair?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/linear-regression-under-the-hood-583003d0bf38", "anchor_text": "Under the Hood \u2014 Linear Regression"}, {"url": "https://towardsdatascience.com/under-the-hood-logistic-regression-407c0276c0b4", "anchor_text": "Under the Hood \u2014 Logistic Regression"}, {"url": "https://towardsdatascience.com/under-the-hood-decision-tree-454f8581684e", "anchor_text": "Under the Hood \u2014 Decision Tree"}, {"url": "https://towardsdatascience.com/introduction-to-machine-learning-f41aabc55264", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/linear-regression-under-the-hood-583003d0bf38", "anchor_text": "Linear Regression"}, {"url": "https://towardsdatascience.com/under-the-hood-decision-tree-454f8581684e", "anchor_text": "Decision Trees"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----407c0276c0b4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/logistic-regression?source=post_page-----407c0276c0b4---------------logistic_regression-----------------", "anchor_text": "Logistic Regression"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----407c0276c0b4---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/classification?source=post_page-----407c0276c0b4---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----407c0276c0b4---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&user=Saurabh+Singh&userId=890c4657d91c&source=-----407c0276c0b4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&user=Saurabh+Singh&userId=890c4657d91c&source=-----407c0276c0b4---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F407c0276c0b4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----407c0276c0b4---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----407c0276c0b4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----407c0276c0b4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----407c0276c0b4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saurabhsingh0424?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@saurabhsingh0424?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Saurabh Singh"}, {"url": "https://medium.com/@saurabhsingh0424/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "52 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890c4657d91c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&user=Saurabh+Singh&userId=890c4657d91c&source=post_page-890c4657d91c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fb4e01e65ca96&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funder-the-hood-logistic-regression-407c0276c0b4&newsletterV3=890c4657d91c&newsletterV3Id=b4e01e65ca96&user=Saurabh+Singh&userId=890c4657d91c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}