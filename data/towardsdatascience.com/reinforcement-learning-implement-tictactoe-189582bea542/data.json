{"url": "https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542", "time": 1682996294.318364, "path": "towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542/", "webpage": {"metadata": {"title": "Reinforcement Learning \u2014 Implement TicTacToe | by Jeremy Zhang | Towards Data Science", "h1": "Reinforcement Learning \u2014 Implement TicTacToe", "description": "We have implemented grid world game by iteratively updating Q value function, which is the estimating value of (state, action) pair. This time let\u2019s look into how to leverage reinforcement learning\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@zhangyue9306/implement-grid-world-with-q-learning-51151747b455", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py", "anchor_text": "code here", "paragraph_index": 4}, {"url": "https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py", "anchor_text": "code here", "paragraph_index": 27}], "all_paragraphs": ["We have implemented grid world game by iteratively updating Q value function, which is the estimating value of (state, action) pair. This time let\u2019s look into how to leverage reinforcement learning in adversarial game \u2014 tic-tac-toe, where there are more states and actions and most importantly, there is an opponent playing against our agent.(Check out previous post)", "Whereas in general game theory methods, say min-max algorithm, the algorithm always assume a perfect opponent who is so rational that each step it takes is to maximise its reward and minimise our agent reward, in reinforcement learning it does not even presume a model of the opponent and the result could be surprisingly well.", "By considering the opponent as part of the environment which the agent can interact with, after certain amount iterations, the agent is able to planning ahead without any model of the agent or environment, or conducting any search of possible future actions or states. The advantage is obvious in that the method saves the effort of complex mathematical deduction or exploration of large number of search space, but it is able to achieve state-of-art skill by simply trial and learn.", "In the following sessions, we will:", "Firstly, we need a State class to act as both board and judger. It has functions recording board state of both players and update state when either player takes an action. Meanwhile, it is able to judge the end of game and give reward to players accordingly. (check out code here)", "To formulate this reinforcement learning problem, the most important thing is to be clear about the 3 major components \u2014 state, action, and reward. The state of this game is the board state of both the agent and its opponent, so we will initialise a 3x3 board with zeros indicating available positions and update positions with 1 if player 1 takes a move and -1 if player 2 takes a move. The action is what positions a player can choose based on the current board state. Reward is between 0 and 1 and is only given at the end of the game.", "In the init function, we initialise a vacant board and two players p1 and p2 (we initialise p1to play first). Each player has a playSymbol , when a player takes an action, its playerSymbol will be filled in the board and the board state will be updated.", "The getHash function hashes the current board state so that it can be stored in the state-value dictionary.", "When a player takes an action, its corresponding symbol will be filled in the board. And after the state being updated, the board will also update the current vacant positions on the board and feed it back to the next player in turn.", "After each action being taken by the player, we need a function to continuously check if the game has ended and if end, to judge the winner of the game and give reward to both players.", "The winner function checks sum of rows, columns and diagonals, and return 1 if p1 wins, -1 if p2 wins, 0 if draw and None if the game is not yet ended. At the end of game, 1 is rewarded to winner and 0 to loser. One thing to notice is that we consider draw is also a bad end, so we give our agent p1 0.1 reward even the game is tie(one can try out different reward to see how the agents act).", "We need a player class which represents our agent, and the player is able to:", "We will initialise a dict storing state-value pair and update the estimates at the end of each game.", "In the init function, we keep track of all positions the player\u2019s been taken during each game in a list self.states and update the corresponding states in self.states_value dict. In terms of choosing action, we use \u03f5-greedy method to balance between exploration and exploitation. Here we set exp_rate=0.3 , which means \u03f5=0.3 , so 70% of the time our agent will take greedy action, which is choosing action based on current estimation of states-value, and 30% of the time our agent will take random action.", "We store the hash of board state into state-value dict, and while exploitation, we hash the next board state and choose the action that returns the maximum value of next state.", "To update value estimation of states, we will apply value iteration which is updated based on the formula below", "The formula simply tells us that the updated value of state t equals the current value of state t adding the difference between the value of next state and the value of current state, which is multiplied by a learning rate \u03b1 (Given the reward of intermediate state is 0).", "The logic is that we update the current value slowly based on our latest observation.", "As I mentioned above, the positions of each game is stored in self.states and when the agent reach the end of the game, the estimates are updated in reversed fashion.", "Now that our agent is able to learn by updating value estimation and our board is all set up, it is time to let two players play against each other (This part is placed inside State Class).", "During training, the process for each player is:", "At the end of the training(playing after certain amount of rounds), our agent is able to learn its policy which is stored in the state-value dict. We need to save this policy to play against a human player.", "Now our agent is all set up, in the last step we need a human class to manage to play against the agent.", "This class includes only 1 usable function chooseAction which requires us to input the board position we hope to take.", "And we also need to modify a bit on the play function inside State:", "Mostly the same, we let player 1(which is our agent) to play first, and at each step, the board is printed.", "The play2 function we show the board state and ask you to input your position during game playing.", "Go try it yourself and have fun! (check out code here)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Hmm\u2026I am a data scientist looking to catch up the tide\u2026"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F189582bea542&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://meatba11.medium.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26----189582bea542---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/@zhangyue9306/implement-grid-world-with-q-learning-51151747b455", "anchor_text": "previous post"}, {"url": "https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py", "anchor_text": "code here"}, {"url": "https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py", "anchor_text": "code here"}, {"url": "http://incompleteideas.net/book/the-book-2nd.html", "anchor_text": "http://incompleteideas.net/book/the-book-2nd.html"}, {"url": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction", "anchor_text": "https://github.com/JaeDukSeo/reinforcement-learning-an-introduction"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----189582bea542---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----189582bea542---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----189582bea542---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----189582bea542---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&user=Jeremy+Zhang&userId=f37783fc8c26&source=-----189582bea542---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F189582bea542&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----189582bea542---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----189582bea542--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----189582bea542--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----189582bea542--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----189582bea542--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----189582bea542--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----189582bea542--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----189582bea542--------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://meatba11.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeremy Zhang"}, {"url": "https://meatba11.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff37783fc8c26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&user=Jeremy+Zhang&userId=f37783fc8c26&source=post_page-f37783fc8c26--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcdbd8b83c584&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-implement-tictactoe-189582bea542&newsletterV3=f37783fc8c26&newsletterV3Id=cdbd8b83c584&user=Jeremy+Zhang&userId=f37783fc8c26&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}