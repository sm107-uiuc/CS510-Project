{"url": "https://towardsdatascience.com/robustly-optimized-bert-pretraining-approaches-537dc66522dd", "time": 1682997530.32696, "path": "towardsdatascience.com/robustly-optimized-bert-pretraining-approaches-537dc66522dd/", "webpage": {"metadata": {"title": "Robustly optimized BERT Pretraining Approaches | by Vikas Bhandary | Towards Data Science", "h1": "Robustly optimized BERT Pretraining Approaches", "description": "Natural language processing (NLP) has been on the rise recently. All due to the language model (lm) pretraining method. A careful comparison between lm pretraining methods is near impossible because\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "paper", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet", "paragraph_index": 4}, {"url": "https://arxiv.org/abs/1907.10529", "anchor_text": "SpanBERT", "paragraph_index": 4}, {"url": "https://neptune.ai/blog/unmasking-bert-transformer-model-performance", "anchor_text": "this", "paragraph_index": 8}, {"url": "https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf", "anchor_text": "Radford et al. (2019)", "paragraph_index": 11}, {"url": "https://bit.ly/37gMxwb", "anchor_text": "https://bit.ly/37gMxwb", "paragraph_index": 13}], "all_paragraphs": ["Natural language processing (NLP) has been on the rise recently. All due to the language model (lm) pretraining method. A careful comparison between lm pretraining methods is near impossible because of various reasons (training data, computational costs, and hyperparameter choices). In this post, I will summarize some of the changes made in the BERT\u2019s pretraining method which impacted the final results so that it achieves state-of-the-art results in 4 out of 9 GLUE tasks. For a detailed understanding please read the following paper.", "We almost thought BooksCorpus and English Wikipedia datasets (a total of 16 GB ) were enough for a language model to get a basic understanding of language, but it wasn't the case. Facebook and University of Washington researchers (authors of the paper) trained RoBERTa on a total of 160 GB uncompressed English dataset on DGX-1 machines, each with 8 x 32GB Nvidia V100.", "In the above table, we can see in similar settings RoBERTa outperformed BERT large model in SQuAD and MNLI-m tasks in just 100k steps( as compared to BERT with 1M).", "BERT uses two different types of training objectives one is Masked Language Model (MLM) and another is Next Sentence Prediction (NSP). In MLM BERT selects 15% of tokens for replacements, out of which 80% of tokens are replaced with [MASK] and 10% are left unchanged and 10% are replaced with randomly selected vocabulary tokens.", "NSP is a binary classification task that predicts if two input segments appear together. This objective was designed to improve the downstream tasks as it would require the model to understand the relationship and context of two given sequences. As observed in the paper removing NSP hurts performance in Natural Language Understanding (NLU) tasks. But in recent models like XLNet, SpanBERT, etc the necessity of NSP loss is questioned.", "Therefore experiments were performed under various settings (SEGMENT_PAIR, SENTENCE-PAIR, FULL-SENTENCES, DOC-SENTENCES) to compare the effect of NSP loss on model performance. In SEGMENT_PAIR (with NSP loss), setting original BERT\u2019s original input format is used, similarly in SENTENCE-PAIR (with NSP loss) setting input is sampled from either contiguous portion from the same document or from a different document. But in FULL-SENTENCES(without NSP loss) setting input consists of complete sentences from one or more documents similarly in DOC-SENTENCES setting input is packed with complete sentences from the same documents. The input size for these experiments is maintained at 512.", "The table shows, that the model in DOC-SENTENCES (without NSP loss) setting outperforms every other model. As in other settings, the model is not able to learn long-range dependencies. Despite that, RoBERTa uses FULL-SENTENCES setting for training objectives.", "In MLM training objective, BERT performs masking only once during data preprocessing which means the same input masks are fed to the model on every single epoch. This is referred to as static masking. To avoid using the same mask for every epoch, training data was duplicated 10 times. If the masking is performed every time a sequence is fed to the model, the model sees different versions of the same sentence with masks on different positions. Here this type of masking is referred to as dynamic masking.", "The performance of the model trained with dynamic masking is slightly better or at least comparable to the original approach used in BERT (i.e., static masking) model so RoBERTa is trained with dynamic masking. You can check this link if you are interested in understanding how masking works and how it improves the BERT model\u2019s overall performance.", "Training with large mini-batches can improve the optimization speed and end task performance if the learning speed is increased accordingly. This is shown in previous work(show paper link) done in machine translation. The computational cost of training a model for 1M steps with a batch size of 256 is equivalent to training 31K steps with a batch size of 8K. Training a model with large mini-batches improves the perplexity of MLM objectives, likewise, it is easier to parallelize via distributed data-parallel training. Even without large-scale parallelization efficiency can be improved by gradient accumulation.", "Byte-Pair Encoding (BPE) is a hybrid between character and word-level representations, which solely relies on subword units. These subword units can be extracted by performing a statistical analysis of the training dataset. Generally, the BPE vocabulary size range from 10K -100K subword units.", "BERT uses a character level BPE vocabulary size of 30K which is learned after preprocessing with heuristic tokenization rules. RoBERTa uses the encoding method discussed in the paper by Radford et al. (2019). Here BPE subword vocabulary is reduced to 50K (still bigger than BERT\u2019s vocab size) units with the capability to encode any text without any unknown tokens and no need for preprocessing or tokenization rules. Using this encoding degraded the performance of end-task performance in some cases. Still, this method was used for encoding as it is a universal encoding scheme that doesn't need any preprocessing and tokenization rules.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Full-stack developer, Deep learning & NLP Enthusiast, #dreamer https://bit.ly/37gMxwb"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F537dc66522dd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----537dc66522dd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vksbhandary?source=post_page-----537dc66522dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Vikas Bhandary"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d3c13d219b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&user=Vikas+Bhandary&userId=3d3c13d219b3&source=post_page-3d3c13d219b3----537dc66522dd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jhonkasalo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Joakim Honkasalo"}, {"url": "https://unsplash.com/@jhonkasalo?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa: A Robustly Optimized BERT Pretraining ApproachLanguage model pretraining has led to significant performance gains but careful comparison between different approaches\u2026arxiv.org"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1906.08237", "anchor_text": "XLNet"}, {"url": "https://arxiv.org/abs/1907.10529", "anchor_text": "SpanBERT"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "paper"}, {"url": "https://neptune.ai/blog/unmasking-bert-transformer-model-performance", "anchor_text": "this"}, {"url": "https://www.techbooky.com/wp-content/uploads/2019/02/Better-Language-Models-and-Their-Implications.pdf", "anchor_text": "Radford et al. (2019)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----537dc66522dd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----537dc66522dd---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----537dc66522dd---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/nlp?source=post_page-----537dc66522dd---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/data-science?source=post_page-----537dc66522dd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&user=Vikas+Bhandary&userId=3d3c13d219b3&source=-----537dc66522dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&user=Vikas+Bhandary&userId=3d3c13d219b3&source=-----537dc66522dd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F537dc66522dd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----537dc66522dd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----537dc66522dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----537dc66522dd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----537dc66522dd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----537dc66522dd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----537dc66522dd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vksbhandary?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vikas Bhandary"}, {"url": "https://medium.com/@vksbhandary/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "69 Followers"}, {"url": "https://bit.ly/37gMxwb", "anchor_text": "https://bit.ly/37gMxwb"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3d3c13d219b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&user=Vikas+Bhandary&userId=3d3c13d219b3&source=post_page-3d3c13d219b3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcf4958572aa0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frobustly-optimized-bert-pretraining-approaches-537dc66522dd&newsletterV3=3d3c13d219b3&newsletterV3Id=cf4958572aa0&user=Vikas+Bhandary&userId=3d3c13d219b3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}