{"url": "https://towardsdatascience.com/deep-learning-for-nlp-word-embeddings-4f5c90bcdab5", "time": 1683009149.624841, "path": "towardsdatascience.com/deep-learning-for-nlp-word-embeddings-4f5c90bcdab5/", "webpage": {"metadata": {"title": "Deep Learning for NLP: Word Embeddings | by James Thorn | Towards Data Science", "h1": "Deep Learning for NLP: Word Embeddings", "description": "Word embeddings have become one of the most used tools and main drivers of the amazing achievements of Artificial Intelligence tasks that require processing natural languages like speech or texts. In\u2026"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Wikipedia or Twitter", "paragraph_index": 32}, {"url": "https://twitter.com/Jaimezorno", "anchor_text": "Twitter at @jaimezorno", "paragraph_index": 37}, {"url": "https://medium.com/@jaimezornoza?source=post_page---------------------------", "anchor_text": "here", "paragraph_index": 37}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "follow me on Medium", "paragraph_index": 38}, {"url": "https://howtolearnmachinelearning.com/", "anchor_text": "this repository", "paragraph_index": 38}], "all_paragraphs": ["Word embeddings have become one of the most used tools and main drivers of the amazing achievements of Artificial Intelligence tasks that require processing natural languages like speech or texts.", "In this post, we will unveil the magic behind them, see what they are, why they have become a standard in the Natural Language Processing (NLP hereinafter) world, how they are built, and explore some of the most used word embedding algorithms.", "Everything will be explained in a simple and intuitive manner, avoiding complex maths and trying to make the content of the post as accessible as possible.", "It will be broken down in the following subsections:", "Once you are ready, let's start by seeing what word embeddings are.", "Computers break everything down to numbers. Bits (zeros and ones) more specifically. What happens when a software inside a computer (like a Machine Learning algorithm for example) has to operate or process a word? Simple, this word needs to be given to the computer as the only thing it can understand: as numbers.", "In NLP, the most simple way to do this is by creating a vocabulary with a huge amount of words (100.000 words let\u2019s say), and assigning a number to each word in the vocabulary.", "The first word in our vocabulary (\u2018apple\u2019 maybe) will be number 0. The second word (\u2018banana\u2019) will be number 1, and so on up to number 99.998, the previous to last word (\u2018king\u2019) and 999.999 being assigned to the last word (\u2018queen\u2019).", "Then we represent every word as a vector of length 100.000, where every single item is a zero except one of them, corresponding to the index of the number that the word is associated with.", "This is called one-hot encoding for words.", "The one-hot encoding have various different issues related with efficiency and context, that we will see in just a moment.", "Word embeddings are just another form representing words through vectors, that successfully solve many of the issues derived from using a one-hot encoding by somehow abstracting the context or high-level meaning of each word.", "The main takeaway here is that word embeddings are vectors that represent words, so that similar meaning words have similar vectors.", "Consider the previous example but with only three words in our vocabulary: \u2018apple\u2019, \u2018banana\u2019 and \u2018king\u2019. The one hot encoding vector representations of these words would be the following.", "If we then plotted these word vectors in a 3 dimensional space, we would get a representation like the one shown in the following figure, where each axis represents one of the dimensions that we have, and the icons represent where the end of each word vector would be.", "As we can see, the distance from any vector (position of the icons) to all the other ones is the same: two size 1 steps in different directions. This would be the same if we expanded the problem to 100.000 dimensions, taking more steps but maintaining the same distance between all the word vectors.", "Ideally, we would want vectors for words that have similar meanings or represent similar items to be close together, and far away from those that have completely different meanings: we want apple to be close to banana but far away from king.", "Also, one hot encodings are very inefficient. If you think about it, they are huge empty vectors with only one item having a value different than zero. They are very sparse, and can greatly slow down our calculations.", "In conclusion: one hot encodings don\u2019t take into account the context or meaning of the words, all the words vectors have the same distance in between them, and are highly inefficient.", "Word embeddings solve these problems by representing each word in the vocabulary by a fairly small (150, 300, 500 dimensional) fixed size vector, called an embedding, which is learned during the training.", "These vectors are created in a manner so that words that appear in similar contexts or have similar meaning are close together, and they are not sparse vectors like the ones derived from one-hot embeddings.", "If we had a 2 dimensional word embedding representation of our previous 4 words, and plotted it on a 2D grid, it would look something like the following figure.", "As we can clearly see from the previous image, the word embedding representations of the words \u2018apple\u2019 and \u2018banana\u2019 are closer together in between them than to the words \u2018king\u2019 and \u2018queen\u2019, where this applies in the opposite way: words with similar meanings are close together when we use word embeddings.", "This fact also allows us to do something very very cool. We can operate with word embeddings, using representations of words to go from a known word to another one.", "The following image shows how if we subtract the word embedding of the word \u2018royal\u2019 from the embedding of the word \u2018king\u2019 we arrive somewhere near the embedding of the word \u2018man\u2019. In a similar manner, if we subtract the embedding of \u2018royal\u2019 from the embedding of queen, we arrive somewhere near the embedding of the word \u2018woman\u2019. Cool right?", "Lastly, as we can see in the word embedding vectors, they usually have a smaller size (2 in our example, but most times they have 150, 200, 300, or 500 dimensions) and are not sparse, making calculations with them much more efficient than with one-hot vectors.", "As you have probably guessed, like many elements in the Machine Learning ecosystem, word embeddings are built by learning. Learning from data.", "There are many algorithms that can learn word embeddings, and we will see them in just a bit, but the general goal is to build a matrix E, that can translate a one-hot vector representing a word, to a fixed sized vector that is the embedding of such word.", "Let's see a very high-level example of one way this could be done.", "Consider the sentence \u201cI love drinking apple smoothies\u2019. If I remove the word \u2018apple\u2019 we are left with the following, incomplete sentence: \u2018I love drinking __ smoothies\u2019. If I then gave you this incomplete sentence, and told you to guess the missing word, you will probably say words like \u2018banana\u2019, \u2018strawberry\u2019, or \u2018apple\u2019, which all have a similar meaning, and usually appear in similar contexts.", "One of the main ways to learn word embeddings, is by a very similar process to this: the algorithms learn similar word embedding for words that appear many times in similar contexts by guessing missing words in a huge corpus of text sentences.", "An embedding matrix E (the matrix that translates a one hot embedding into a word embedding vector) is calculated by training something similar to a language model (a model that tries to predicts missing words in a sentence) using an Artificial Neural Network to predict this missing word, in a similar manner to how the weights and biases of the network are calculated.", "In practice, you can avoid training your own word embeddings, as there are publicly available word embeddings built from various corpuses (like Wikipedia or Twitter GloVe Word embeddings), saving you time and effort.", "To end, lets briefly see some of the most popular word embedding algorithms.", "The two most used Word embedding algorithms are Word2Vec and GloVe. Let's see how they work.", "That is it! As always, I hope you enjoyed the post, and that I managed to help you understand what word embeddings are, how they work, and why they are so powerful.", "Here you can find some additional resources in case you want to learn more about the topic:", "If you liked this post then feel free to follow me on Twitter at @jaimezorno. Also, you can take a look at my other posts on Data Science and Machine Learning here. Have a good read!", "If you want to learn more about Machine Learning and Artificial Intelligence follow me on Medium, and stay tuned for my next posts! Also, you can check out this repository for more resources on Machine Learning and AI!", "Lastly, check out my other posts on Deep Learning for NLP:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4f5c90bcdab5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://james-thorn.medium.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "James Thorn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14----4f5c90bcdab5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "Wikipedia or Twitter"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "A Neural probabilistic model."}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "Bengio et al."}, {"url": "https://www.youtube.com/watch?v=xtPXjvwCt64", "anchor_text": "Lecture on Learning word embeddings by Andrew Ng."}, {"url": "https://machinelearningmastery.com/what-are-word-embeddings/", "anchor_text": "Machine Learning Mastery post on Word Embeddings."}, {"url": "https://blog.aylien.com/overview-word-embeddings-history-word2vec-cbow-glove/", "anchor_text": "Overview of Word Embeddings and their use on semantic models."}, {"url": "https://twitter.com/Jaimezorno", "anchor_text": "Twitter at @jaimezorno"}, {"url": "https://medium.com/@jaimezornoza?source=post_page---------------------------", "anchor_text": "here"}, {"url": "https://medium.com/@jaimezornoza", "anchor_text": "follow me on Medium"}, {"url": "https://howtolearnmachinelearning.com/", "anchor_text": "this repository"}, {"url": "https://towardsdatascience.com/deep-learning-for-nlp-anns-rnns-and-lstms-explained-95866c1db2e4", "anchor_text": "Deep Learning for NLP: ANNs, RNNs and LSTMs explained!Learn about Artificial Neural Networks, Deep Learning, Recurrent Neural Networks and LSTMs like never before and use\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/deep-learning-for-nlp-creating-a-chatbot-with-keras-da5ca051e051", "anchor_text": "Deep Learning for NLP: Creating a Chatbot with Keras!Learn how to use Keras to build a Recurrent Neural Network and create a Chatbot! Who doesn\u2019t like a friendly-robotic\u2026towardsdatascience.com"}, {"url": "https://unsplash.com/", "anchor_text": "Unsplash"}, {"url": "https://www.flaticon.com/", "anchor_text": "FlatIcon"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4f5c90bcdab5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4f5c90bcdab5---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4f5c90bcdab5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/startup?source=post_page-----4f5c90bcdab5---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/tag/technology?source=post_page-----4f5c90bcdab5---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&user=James+Thorn&userId=1fd70d25ff14&source=-----4f5c90bcdab5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&user=James+Thorn&userId=1fd70d25ff14&source=-----4f5c90bcdab5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4f5c90bcdab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4f5c90bcdab5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4f5c90bcdab5--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Thorn"}, {"url": "https://james-thorn.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3K Followers"}, {"url": "https://howtolearnmachinelearning.com/", "anchor_text": "https://howtolearnmachinelearning.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ad84c2cef18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-for-nlp-word-embeddings-4f5c90bcdab5&newsletterV3=1fd70d25ff14&newsletterV3Id=5ad84c2cef18&user=James+Thorn&userId=1fd70d25ff14&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}