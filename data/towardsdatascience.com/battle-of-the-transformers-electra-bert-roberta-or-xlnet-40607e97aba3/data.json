{"url": "https://towardsdatascience.com/battle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3", "time": 1683007033.336141, "path": "towardsdatascience.com/battle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3/", "webpage": {"metadata": {"title": "Battle of the Transformers: ELECTRA, BERT, RoBERTa, or XLNet | by Thilina Rajapakse | Towards Data Science", "h1": "Battle of the Transformers: ELECTRA, BERT, RoBERTa, or XLNet", "description": "One of the \u201csecrets\u201d behind the success of Transformer models is the technique of Transfer Learning. In Transfer Learning, a model (in our case, a Transformer model) is pre-trained on a gigantic\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 4}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 4}, {"url": "https://www.wandb.com/", "anchor_text": "Weights & Biases", "paragraph_index": 4}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/text_classification/yelp_reviews_polarity", "anchor_text": "examples directory", "paragraph_index": 5}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "FastAI", "paragraph_index": 6}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "docs", "paragraph_index": 8}, {"url": "https://simpletransformers.ai/docs/classification-specifics/", "anchor_text": "docs", "paragraph_index": 19}, {"url": "https://app.wandb.ai/thilina/Classification%20Model%20Comparison?workspace=user-thilina", "anchor_text": "here", "paragraph_index": 20}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "leaderboard", "paragraph_index": 27}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 41}], "all_paragraphs": ["One of the \u201csecrets\u201d behind the success of Transformer models is the technique of Transfer Learning. In Transfer Learning, a model (in our case, a Transformer model) is pre-trained on a gigantic dataset using an unsupervised pre-training objective. This same model is then fine-tuned (typically supervised training) on the actual task at hand. The beauty of this approach is that the fine-tuning dataset can be as small as 500\u20131000 training samples! A number small enough to be potentially scoffed out of the room if one were to call it Deep Learning. This also means that the expensive and time-consuming part of the pipeline, pre-training, only needs to be done once and the pre-trained model can be reused for any number of tasks thereafter. Since pre-trained models are typically made publicly available \ud83d\ude4f, we can grab the relevant model, fine-tune it on a custom dataset, and have a state-of-the-art model ready to go in a few hours!", "If you are interested in learning how pre-training works and how you can train a brand new language model on a single GPU, check out my article linked below!", "ELECTRA is one of the latest classes of pre-trained Transformer models released by Google and it switches things up a bit compared to most other releases. For the most part, Transformer models have followed the well-trodden path of Deep Learning, with larger models, more training, and bigger datasets equalling better performance. ELECTRA, however, bucks this trend by outperforming earlier models like BERT while using less computational power, smaller datasets, and less training time. (In case you are wondering, ELECTRA is the same \u201csize\u201d as BERT).", "In this article, we\u2019ll look at how to use a pre-trained ELECTRA model for text classification and we\u2019ll compare it to other standard models along the way. Specifically, we\u2019ll be comparing the final performance (Matthews correlation coefficient (MCC)) and the training times for each model listed below.", "As always, we\u2019ll be doing this with the Simple Transformers library (based on the Hugging Face Transformers library) and we\u2019ll be using Weights & Biases for visualizations.", "You can find all the code used here in the examples directory of the library.", "We\u2019ll be using the Yelp Review Polarity dataset which is a binary classification dataset. The script below will download it and store it in the data directory. Alternatively, you can manually download the data from FastAI.", "Once the data is in the data directory, we can start training our models.", "Simple Transformers models can be configured extensively (see docs), but we\u2019ll just be going with some basic, \u201cgood enough\u201d hyperparameter settings. This is because we are more interested in comparing the models to each other on an equal footing, rather than trying to optimize for the absolute best hyperparameters for each model.", "With that in mind, we\u2019ll increase the train_batch_size to 128 and we\u2019ll increase the num_train_epochs to 3 so that all models will have enough training to converge.", "One caveat here is that the train_batch_size is reduced to 64 for XLNet as it cannot be trained on an RTX Titan GPU with train_batch_size=128. However, any effect of this discrepancy is minimized by setting gradient_accumulation_steps to 2, which changes the effective batch size to 128. (Gradients are calculated and the model weights are updated only once for every two steps)", "All other settings which affect training are unchanged from their defaults.", "Setting up the training process is quite simple. We just need the data loaded into Dataframes and the hyperparameters defined and we are off to the races!", "For convenience, I\u2019m using the same script to train all models as we only need to change the model names between each run. The model names are supplied by a shell script which also automatically runs the training script for each model.", "The training script is given below:", "Note that the Yelp Reviews Polarity dataset uses the labels [1, 2] for positive and negative, respectively. I\u2019m changing this to [0, 1] for negative and positive, respectively. Simple Transformers requires the labels to start from 0 (duh!) and a label of 0 for negative sentiment is a lot more intuitive (in my opinion).", "The bash script which can automate the entire process:", "Note that you can remove the saved models at each stage by adding rm -r outputs to the bash script. This might be a good idea if you don\u2019t have much disk space to spare.", "The training script will also log the evaluation scores to Weights & Biases, letting us compare models easily.", "For more information on training classification models, check out the Simple Transformers docs.", "You can find all my results here. Try playing around with the different graphs and information available!", "Let\u2019s go through the important results.", "These are the final MCC scores obtained by each model. As you can see, the scores are quite close to each other for all the models.", "To get a better view of the differences, the chart below zooms into the X-axis and shows only the range 0.88\u20130.94.", "Note that a zoomed-in view, while helpful for spotting differences, can distort the perception of the results. Therefore, the chart below is for illustrative purposes only. Beware the graph that hides its zeros!", "The roberta-base model leads the pack with xlnet-base close behind. The distilroberta-base and the electra-base models follow next, with barely anything between them. Honestly, the difference between the two is probably more due to random chance than anything else in this case. Bringing up the rear, we have bert-base-cased, distilbert-base-cased, and electra-small respectively.", "Looking at the actual values shows close they are.", "In this experiment, RoBERTa seems to outperform the other models. However, I\u2019m willing to bet that with some tricks like hyperparameter tuning and ensembling, the ELECTRA model is capable of making up the difference. This is confirmed by the current GLUE benchmark leaderboard where ELECTRA is sitting above RoBERTa.", "It is important to keep in mind that the ELECTRA model required substantially less pre-training resources (about a quarter) compared to RoBERTa. This is true for distilroberta-base as well. Even though the distilroberta-base model is comparatively smaller, you need the original roberta-base model before you can distil it into distilroberta-base.", "The XLNet model is nearly keeping pace with the RoBERTa model but it requires far more computational resources than all other models shown here (see training time graph).", "The venerable (although less than two years old) BERT model is starting to show its age and is outperformed by all but the electra-small model.", "The electra-small model, although not quite matching the standards of the other models, still performs admirably. As might be expected, it trains the fastest, has the smallest memory requirements and is the fastest at inference.", "The speed of training is determined mostly by the size (number of parameters) of the model, except in the case of XLNet. The training algorithm used with XLNet makes it significantly slower than the comparative BERT, RoBERTa, and ELECTRA models, despite having roughly the same number of parameters. The GPU memory requirement for XLNet is also higher compared to the other models tested here, necessitating the use of a smaller training batch size as noted earlier (64 compared to 128 for the other models).", "The inference times (not tested here) should also follow this general trend.", "Finally, another important consideration is how quickly each of the models converges. All these model were trained for 3 full epochs without using early stopping.", "Evidently, there is no discernible difference between the models with regard to how many training steps are required for convergence. All the models seem to be converging around 9000 training steps. Of course, the time taken to converge would vary due to the difference in training speed.", "It\u2019s a tough call to choose between different Transformer models. However, we can still gain some valuable insights from the experiment we\u2019ve seen.", "Based on these insights, I can offer the following recommendations (although they should be taken with a grain of salt as results may vary between different datasets).", "It would be interesting to see if the large models also follow this trend. I hope to test this out in a future article (where T5 might also be thrown into the mix)!", "If you would like to see some more in-depth analysis regarding the training and inference speeds of different models, check out my earlier article (sadly, no ELECTRA) linked below.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F40607e97aba3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----40607e97aba3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/users/272447-272447/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=569288", "anchor_text": "272447"}, {"url": "https://pixabay.com/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=569288", "anchor_text": "Pixabay"}, {"url": "https://towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d", "anchor_text": "Understanding ELECTRA and Training an ELECTRA Language ModelHow does a Transformer Model learn a language? What\u2019s new in ELECTRA? How do you train your own language model on a\u2026towardsdatascience.com"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://www.wandb.com/", "anchor_text": "Weights & Biases"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/text_classification/yelp_reviews_polarity", "anchor_text": "examples directory"}, {"url": "https://www.anaconda.com/distribution/", "anchor_text": "here"}, {"url": "https://github.com/NVIDIA/apex", "anchor_text": "here"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "FastAI"}, {"url": "https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model", "anchor_text": "docs"}, {"url": "https://simpletransformers.ai/docs/classification-specifics/", "anchor_text": "docs"}, {"url": "https://app.wandb.ai/thilina/Classification%20Model%20Comparison?workspace=user-thilina", "anchor_text": "here"}, {"url": "https://gluebenchmark.com/leaderboard", "anchor_text": "leaderboard"}, {"url": "https://imgflip.com/i/40s1sv", "anchor_text": "https://imgflip.com/i/40s1sv"}, {"url": "https://openreview.net/pdf?id=r1xMH1BtvB", "anchor_text": "paper"}, {"url": "https://towardsdatascience.com/to-distil-or-not-to-distil-bert-roberta-and-xlnet-c777ad92f8", "anchor_text": "To Distil or Not To Distil: BERT, RoBERTa, and XLNetTransformers are the undisputed kings of Natural Language Processing. But with so many different models around it can\u2026towardsdatascience.com"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----40607e97aba3---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----40607e97aba3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----40607e97aba3---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----40607e97aba3---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----40607e97aba3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----40607e97aba3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F40607e97aba3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----40607e97aba3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----40607e97aba3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----40607e97aba3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----40607e97aba3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----40607e97aba3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----40607e97aba3--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbattle-of-the-transformers-electra-bert-roberta-or-xlnet-40607e97aba3&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}