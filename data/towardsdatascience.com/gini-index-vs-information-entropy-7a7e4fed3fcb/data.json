{"url": "https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb", "time": 1682997093.266783, "path": "towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb/", "webpage": {"metadata": {"title": "Gini Index vs Information Entropy | by Andrew Hershy | Towards Data Science", "h1": "Gini Index vs Information Entropy", "description": "Measures of impurity/information-gain, particularly Gini Index and Entropy, are interesting and practical concepts for data scientists to know. We will deep-dive into these concepts with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "Wikipedia", "paragraph_index": 8}, {"url": "http://web.mit.edu/6.02/www/f2011/handouts/2.pdf", "anchor_text": "notes", "paragraph_index": 15}, {"url": "https://github.com/ahershy/Gini-Index-and-Entropy-Exercise/blob/master/Entropy%2Bvs%2BGini%2BAnalysis.ipynb", "anchor_text": "here", "paragraph_index": 29}, {"url": "https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4", "anchor_text": "Is Random Forest better than Logistic Regression? (a comparison)", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/excel-vs-sql-a-conceptual-comparison-dcfbee640c83", "anchor_text": "Excel vs SQL: A Conceptual Comparison", "paragraph_index": 33}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "Predicting Cancer with Logistic Regression in Python", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/portfolio-linear-optimization-breakdown-f519546ed1ff", "anchor_text": "Optimize your Investments using Math and Python", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/r-squared-recipe-5814995fa39a", "anchor_text": "Calculating R-squared from scratch (using python)", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/word-clouds-in-python-comprehensive-example-8aee4343c0bf", "anchor_text": "Word Clouds in Python: Comprehensive Example", "paragraph_index": 37}], "all_paragraphs": ["Measures of impurity/information-gain, particularly Gini Index and Entropy, are interesting and practical concepts for data scientists to know. We will deep-dive into these concepts with easy-to-understand examples below.", "Decision trees recursively split features with regard to their target variable\u2019s purity. The algorithm is designed to find the optimal point of the most predictive feature in order to split 1 dataset into 2. These 2 new datasets\u2019 target variable will be more pure than the original dataset\u2019s.", "\u201cPure\u201d is the key word here, however. What does that word mean, exactly? In a general sense \u201cpurity\u201d can be thought of as how homogenized a group is. But homogeneity can mean different things depending on which mathematical backbone your decision tree runs on. The 2 most popular backbones for decision tree\u2019s decisions are Gini Index and Information Entropy.", "These 3 examples below should get the point across:", "If we have 4 red gumballs and 0 blue gumballs, that group of 4 is 100% pure.", "If we have 2 red and 2 blue, that group is 100% impure.", "If we have 3 red and 1 blue, that group is either 75% or 81% pure, if we use Gini or Entropy respectively.", "Why does this matter? Depending on which impurity measurement is used, tree classification results can vary. This can make small (or sometimes large) impact on your model.", "Let\u2019s start with Gini Index, as it\u2019s a bit easier to understand. According to Wikipedia, the goal is to \u201cmeasure how often a randomly chosen element from the set would be incorrectly labeled\u201d[1].", "To visualize this, let\u2019s go back to the gumball examples. If we decided to arbitrarily label all 4 gumballs as red, how often would one of the gumballs be incorrectly labeled?", "The impurity measurement is 0 because we would never incorrectly label any of the 4 red gumballs here. If we arbitrarily chose to label all the balls \u2018blue\u2019, then our index would still be 0, because we would always incorrectly label the gumballs.", "The gini score is always the same no matter what arbitrary class you take the probabilities of because they always add to 0 in the formula above.", "A gini score of 0 is the most pure score possible.", "The impurity measurement is 0.5 because we would incorrectly label gumballs wrong about half the time. Because this index is used in binary target variables (0,1), a gini index of 0.5 is the least pure score possible. Half is one type and half is the other. Dividing gini scores by 0.5 can help intuitively understand what the score represents. 0.5/0.5 = 1, meaning the grouping is as impure as possible (in a group with just 2 outcomes).", "The impurity measurement here is 0.375. If we divide this by 0.5 for more intuitive understanding we will get 0.75, which is the probability of incorrectly/correctly labeling.", "Entropy is more computationally heavy due to the log in the equation. Like gini, The basic idea is to gauge the disorder of a grouping by the target variable. Instead of utilizing simple probabilities, this method takes the log base2 of the probabilities (you can use any log base, however, as long as you\u2019re consistent). The entropy equation uses logarithms because of many advantageous properties. The Main advantage is the additive property it provides. These MIT lecture notes will help grasp the concept more clearly (pg8) [2].", "Let\u2019s visualize how entropy works with the same gumball scenarios:", "Unsurprisingly, the impurity measurement is 0 for entropy as well. This is the max purity score using information entropy.", "The impurity measurement is 1 here, as it\u2019s the maximum impurity obtainable.", "The purity/impurity measurement is 0.811 here, a bit worse than the gini score.", "Let\u2019s visualize both the Gini and Entropy curves with some code in python:", "Below we are making a function to automate gini calculations.", "Keeping consistent with our gumball theme, let\u2019s make a loop that calculates the gini score of any conceivable combination of red and blue gumball floats, adding to 4. We will run 10,000 iterations of the gini function above so we can graph the gini curve later.", "The beginning of the dataframe is below. The other 9,994 rows couldn\u2019t fit.", "Now we will plot our curve:", "Gini is blue, Entropy is orange. You will see how these differences are exemplified in information gain in the next section!", "Information gain is why impurity is so important. Once we derive the impurity of the dataset, we can see how much information is gained as we go down the tree and measure the impurity of the nodes.", "In the example below, we are splitting gumball preference by a particular attribute (let\u2019s say size of the gumball). It gives us the parent/child node relationship:", "Gini has a higher information gain measurement, for this example.", "*All the code in this publication can be found on my github here", "[3] Provost, F., & Fawcett, T. (2013). Data Science for Business: What you need to know about data mining and data-analytic thinking. K\u00f6ln, CA: O`Reilly.", "Please subscribe if you found this helpful. If you enjoy my content, please check out a few other projects:", "Is Random Forest better than Logistic Regression? (a comparison)", "Excel vs SQL: A Conceptual Comparison", "Predicting Cancer with Logistic Regression in Python", "Optimize your Investments using Math and Python", "Calculating R-squared from scratch (using python)", "Word Clouds in Python: Comprehensive Example", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a7e4fed3fcb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andrewhershy.medium.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Andrew Hershy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b7b4b5294a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=post_page-f2b7b4b5294a----7a7e4fed3fcb---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/photos/WahfNoqbYnM", "anchor_text": "Source"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "Wikipedia"}, {"url": "http://web.mit.edu/6.02/www/f2011/handouts/2.pdf", "anchor_text": "notes"}, {"url": "https://github.com/ahershy/Gini-Index-and-Entropy-Exercise/blob/master/Entropy%2Bvs%2BGini%2BAnalysis.ipynb", "anchor_text": "here"}, {"url": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity", "anchor_text": "https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity"}, {"url": "http://web.mit.edu/6.02/www/f2011/handouts/2.pdf", "anchor_text": "http://web.mit.edu/6.02/www/f2011/handouts/2.pdf"}, {"url": "https://andrewhershy.medium.com/membership", "anchor_text": "https://andrewhershy.medium.com/membership"}, {"url": "https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4", "anchor_text": "Is Random Forest better than Logistic Regression? (a comparison)"}, {"url": "https://towardsdatascience.com/excel-vs-sql-a-conceptual-comparison-dcfbee640c83", "anchor_text": "Excel vs SQL: A Conceptual Comparison"}, {"url": "https://towardsdatascience.com/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc", "anchor_text": "Predicting Cancer with Logistic Regression in Python"}, {"url": "https://towardsdatascience.com/portfolio-linear-optimization-breakdown-f519546ed1ff", "anchor_text": "Optimize your Investments using Math and Python"}, {"url": "https://towardsdatascience.com/r-squared-recipe-5814995fa39a", "anchor_text": "Calculating R-squared from scratch (using python)"}, {"url": "https://towardsdatascience.com/word-clouds-in-python-comprehensive-example-8aee4343c0bf", "anchor_text": "Word Clouds in Python: Comprehensive Example"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a7e4fed3fcb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/impurity?source=post_page-----7a7e4fed3fcb---------------impurity-----------------", "anchor_text": "Impurity"}, {"url": "https://medium.com/tag/classification?source=post_page-----7a7e4fed3fcb---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----7a7e4fed3fcb---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a7e4fed3fcb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=-----7a7e4fed3fcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=-----7a7e4fed3fcb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a7e4fed3fcb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a7e4fed3fcb---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a7e4fed3fcb--------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andrewhershy.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrew Hershy"}, {"url": "https://andrewhershy.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "496 Followers"}, {"url": "https://www.linkedin.com/in/andrew-hershy-a7779199/", "anchor_text": "https://www.linkedin.com/in/andrew-hershy-a7779199/"}, {"url": "https://andrewhershy.medium.com/membership", "anchor_text": "https://andrewhershy.medium.com/membership"}, {"url": "https://github.com/ahershy", "anchor_text": "https://github.com/ahershy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2b7b4b5294a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=post_page-f2b7b4b5294a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4d03304fb3bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgini-index-vs-information-entropy-7a7e4fed3fcb&newsletterV3=f2b7b4b5294a&newsletterV3Id=4d03304fb3bb&user=Andrew+Hershy&userId=f2b7b4b5294a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}