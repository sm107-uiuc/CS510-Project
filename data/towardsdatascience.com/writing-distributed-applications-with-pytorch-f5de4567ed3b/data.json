{"url": "https://towardsdatascience.com/writing-distributed-applications-with-pytorch-f5de4567ed3b", "time": 1683010658.027859, "path": "towardsdatascience.com/writing-distributed-applications-with-pytorch-f5de4567ed3b/", "webpage": {"metadata": {"title": "Writing distributed data parallel applications with PyTorch | by shreeraman karikalan | Towards Data Science", "h1": "Writing distributed data parallel applications with PyTorch", "description": "The primary motive of this article is to elucidate in simple steps how to train a deep learning model on multiple GPUs utilizing distributed computing techniques implemented with PyTorch. The article\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/shreeraman96/distributed_training", "anchor_text": "here", "paragraph_index": 1}], "all_paragraphs": ["The primary motive of this article is to elucidate in simple steps how to train a deep learning model on multiple GPUs utilizing distributed computing techniques implemented with PyTorch. The article assumes that you are familiar with training deep learning networks. The tutorial starts with an introduction to some key concepts about distributed computing and then dives into writing a python script using PyTorch\u2019s distributed data parallel functionality to train a model with 4 GPUs", "Note: The tutorial here will cover only the key steps associated with distributed training. The entire code can be found in the repository here", "Distributed computing \u2014 what is distributed computing?", "A distributed system is a collection of distinct independent nodes that communicate over a network to achieve a common goal. Each machine is termed as a Node, and a bunch of nodes connected over a single network form a cluster. The nodes utilize the network to communicate with its peers.", "Distributed computing associates to the style of writing programs that makes full utility of the computing power of machines spread-out across the architecture", "Deep learning and distributed systems are two of the constantly evolving systems gaining popularity in the recent times. The computing power has nearly seen an exponential rise lately, and with Deep Neural Networks(DNN), research enthusiasts are making wonders across the world. Hence, it is very vital to reap benefits from both. The training time of DNNs can be greatly reduced with improved performance if we harness the computing potential of large-scale distributed systems", "1.Distribute computing across the multiple nodes with systematic and coherent processing", "The primary focus here is to explain in simple steps how to utilize the PyTorch distributed module to conduct masked language modelling training for the BERT model through data parallelism technique. The same steps can be followed for training other models as well", "Herein, data is distributed across multiple nodes to achieve faster training times. Each node is supposed to have its own dedicated replica of the model, optimizer and other essentials. We will use the distributed API of the PyTorch module to achieve this.", "PyTorch supports synchronous distributed training by providing a wrapper class around any PyTorch model.", "Each process gets its own replica of python interpreter, optimizer and model and performs a complete optimization at each step reducing time spent on overheads", "PyTorch also supports different communication backends to gather and collect the results of the forward and backward pass at each step of the iteration across multiple nodes.", "Now let\u2019s dive into the actual implementation", "Step 1: Get the list of all the GPUs available with device_ids and total number of GPUs available", "Step 2: Load sentences for masked language modelling from any source and clean and process the data.", "Step 3:Load a pretrained BertTokenizer and encode all the sentences using the same to create target sentences. Mask certain words of the input sentences to create masked sentences", "Step 4: Set different configuration parameters required for distributed training and load them into a single entity, preferably a class", "World size: Total number of processes equivalent to the number of GPUs available", "Master address: IP address of the machine that will host the process with rank 0", "Master port: free available port for communication between the nodes", "Step 5: Defining the train() function:", "This is the function which will be distributed across multiple nodes. Hence it is very important to ensure that each of the node has its own dedicated copy of the required variables and setting", "Multiple different arguments and variables will be collected over a single entity and passed as a value parameter into the training function.", "In addition to this, the training function will also receive a parameter rank through which the processor decides if it is a worker node or master node.", "The node with rank 0 will be the master node. PyTorch takes care of assigning the ranks for each GPUs so we don\u2019t have to worry about it", "Step 5.1: setting up the process group with appropriate backend system and the model", "Init_process allows processes to communicate and coordinate with each other by sharing their locations. PyTorch gives two methods to specify the process group configurations and Initiate the process group", "1. Specify the world size, rank and store (optional)", "2. Specify a URL string with rank and world size encoded in the URL or other ways to indicate where/how to communicate with the peers. By default, it will be \u201cenv://\u201d", "The init_process takes in 4 parameters:", "1. Backend: communication backend to be used. Options available : Gloo, NCCL, MPI. NCCL is suitable for GPU training while Gloo is suited more for CPU training", "3. World_size: Number of process in the job. Generally equivalent to number of GPUs available", "4. Rank: rank of the current process", "Load the required model. Here in our case, load the BERT model for masked language modelling( BertForMaskedLM )", "Step 5.4: set-GPU device & load model into device", "Step 5.6: Wrap the model with Distributed Data Parallel class to distribute the model across nodes", "This container provides a wrapper around our PyTorch model and parallelizes the application of the given modules and splits the input across the specified devices. This module is replicated on each machine and each device on the cluster and each replica handles a fraction of the input.", "1. Module: module to be parallelized. Herein our case, model to be used in our case", "2. Device_ids: CUDA devices as list", "Step 5.7: Setting up data loader for each replica", "we setup training_sampler using the DistributedDataSampler() wrapper class to sample and distribute the input data for each replica.", "2. Number_of_replicas: equal to world_size(4) in our case", "the next step will be to setup Dataloader with our defined distributed sampler.", "Step 5.8 define the rest of the training process & save the model", "Step 6: Initiate the Spawning process according to the number of GPUs available. PyTorch will now spawn the process across the specified world size and the device list", "Hopefully, this article provided you with a fair idea on how to utilize PyTorch\u2019s Distributed APIs to achieve distributed training over multiple GPUs. The entire code can be found in the repository here. If you wish to gain an in-depth understanding of the modules, I suggest the following links.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff5de4567ed3b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://shreeraman-ak.medium.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": ""}, {"url": "https://shreeraman-ak.medium.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "shreeraman karikalan"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8107c46e46c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&user=shreeraman+karikalan&userId=8107c46e46c8&source=post_page-8107c46e46c8----f5de4567ed3b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/shreeraman96/distributed_training", "anchor_text": "here"}, {"url": "https://pytorch.org/docs/stable/distributed.html#basics", "anchor_text": "Distributed communication package - torch.distributed - PyTorch master documentationtorch.distributed supports three backends, each with different capabilities. The table below shows which functions are\u2026pytorch.org"}, {"url": "https://pytorch.org/tutorials/intermediate/dist_tuto.html", "anchor_text": "Writing Distributed Applications with PyTorch - PyTorch Tutorials 1.5.1 documentationAuthor: S\u00e9b Arnold In this short tutorial, we will be going over the distributed package of PyTorch. We'll see how to\u2026pytorch.org-"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----f5de4567ed3b---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/bert?source=post_page-----f5de4567ed3b---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/pre-training?source=post_page-----f5de4567ed3b---------------pre_training-----------------", "anchor_text": "Pre Training"}, {"url": "https://medium.com/tag/distributed-applications?source=post_page-----f5de4567ed3b---------------distributed_applications-----------------", "anchor_text": "Distributed Applications"}, {"url": "https://medium.com/tag/distributeddataparallel?source=post_page-----f5de4567ed3b---------------distributeddataparallel-----------------", "anchor_text": "Distributeddataparallel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&user=shreeraman+karikalan&userId=8107c46e46c8&source=-----f5de4567ed3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&user=shreeraman+karikalan&userId=8107c46e46c8&source=-----f5de4567ed3b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff5de4567ed3b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f5de4567ed3b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f5de4567ed3b--------------------------------", "anchor_text": ""}, {"url": "https://shreeraman-ak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://shreeraman-ak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "shreeraman karikalan"}, {"url": "https://shreeraman-ak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "6 Followers"}, {"url": "https://www.linkedin.com/in/shreeraman-karikalan/", "anchor_text": "https://www.linkedin.com/in/shreeraman-karikalan/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8107c46e46c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&user=shreeraman+karikalan&userId=8107c46e46c8&source=post_page-8107c46e46c8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8107c46e46c8%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwriting-distributed-applications-with-pytorch-f5de4567ed3b&user=shreeraman+karikalan&userId=8107c46e46c8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}