{"url": "https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28", "time": 1682994799.1140559, "path": "towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28/", "webpage": {"metadata": {"title": "Handling imbalanced datasets in machine learning | by Baptiste Rocca | Towards Data Science", "h1": "Handling imbalanced datasets in machine learning", "description": "Suppose that you are working in a given company and you are asked to create a model that, based on various measurements at your disposal, predicts whether a product is defective or not. You decide to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/u/b17ebd108358?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Joseph Rocca", "paragraph_index": 0}, {"url": "https://medium.com/u/b17ebd108358?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Joseph Rocca", "paragraph_index": 54}], "all_paragraphs": ["This post was co-written with Joseph Rocca.", "Suppose that you are working in a given company and you are asked to create a model that, based on various measurements at your disposal, predicts whether a product is defective or not. You decide to use your favourite classifier, train it on the data and voila : you get a 96.2% accuracy ! Your boss is astonished and decides to use your model without any further tests. A few weeks later he enters your office and underlines the uselessness of your model. Indeed, the model you created has not found any defective product from the time it has been used in production.After some investigations, you find out that there is only around 3.8% of the product made by your company that are defective and your model just always answers \u201cnot defective\u201d, leading to a 96.2% accuracy. The kind of \u201cnaive\u201d results you obtained is due to the imbalanced dataset you are working with. The goal of this article is to review the different methods that can be used to tackle classification problems with imbalanced classes.", "First we will give an overview of different evaluation metrics that can help to detect \u201cnaive behaviours\u201d. We will then discuss a whole bunch of methods consisting in reworking the dataset and show that these methods can be misleading. Finally, we will show that reworking the problem is, most of the time, the best way to proceed.", "Some subsections indicated by a (\u221e) symbol contains more mathematical details and can be skipped without impacting the overall comprehension of this post. Notice also that in most of what follows, we will consider two classes classification problems but reasoning can easily be extended to multi-classes cases.", "In this first section, we would like to remind different ways to evaluate a trained classifier in order to be sure to detect any kind of \u201cnaive behaviour\u201d. As we saw in the introduction\u2019s example, accuracy, while being an important and unavoidable metric, can be misleading and therefore should be used cautiously and alongside other metrics. Let\u2019s see what other tools can then be used.", "A good and yet simple metric that should always be used when dealing with classification problem is the confusion matrix. This metric gives an interesting overview of how well a model is doing. Thus, it is a great starting point for any classification model evaluation. We summarise most of the metrics that can be derived from the confusion matrix in the following graphic", "Let us give a short description of these metrics. The accuracy of the model is basically the total number of correct predictions divided by total number of predictions. The precision of a class define how trustable is the result when the model answer that a point belongs to that class. The recall of a class expresses how well the model is able to detect that class. The F1 score of a class is given by the harmonic mean of precision and recall (2\u00d7precision\u00d7recall / (precision + recall)), it combines precision and recall of a class in one metric.", "For a given class, the different combinations of recall and precision have the following meanings :", "In our introductory example, we have the following confusion matrix for 10000 products.", "The accuracy is 96.2% as said earlier. The non defective class precision is 96.2% and the defective class precision is not computable. The recall of the non defective class is 1.0 which is perfect (all the non defective products have been labelled as such). But the recall of the defective class is 0.0 which is the worse case (no defective products were detected). Thus, we can conclude our model is not doing well for this class. The F1 score is not computable for the defective products and is 0.981 for the non defective products. In this example, looking at the confusion matrix could have led to re-think our model or our goal (as we will see in the following sections). It could have prevented using a useless model.", "Another interesting metric is the ROC curve (standing for Receiver Operating Characteristic), defined with respect to a given class (that we will denote C in the following).", "Suppose that for a given point x, we have a model that outputs the probability that this point belongs to C: P(C | x). Based on this probability, we can define a decision rule that consists in saying that x belongs to class C if and only if P(C | x)\u2265T, where T is a given threshold defining our decision rule. If T=1, a point is labelled as belonging to C only if the model is 100% confident it does. If T=0, every points are labelled as belonging to C.", "Each value of the threshold T generates a point (false positive, true positive) and, then, the ROC curve is the curve described by the ensemble of points generated when T varies from 1 to 0. This curve starts at point (0,0), ends at point (1,1) and is increasing. A good model will have a curve that increases quickly from 0 to 1 (meaning that only a little precision has to be sacrificed to get a high recall).", "Based on the ROC curve, we can build another metric, easier to use, to evaluate the model: the AUROC which is the Area Under the ROC curve. AUROC acts a little bit as a scalar value that summarises the entire ROC curve. As it can be seen, the AUROC tend towards 1.0 for the best case and towards 0.5 for the worst case.Here again, a good AUROC score means that the model we are evaluating does not sacrifice a lot of precision to get a good recall on the observed class (often the minority class).", "Before trying to tackle the problem, let\u2019s try to better understand it. For this, we are going to consider a very simple example that will allow us both to review quickly some basic aspects of a two classes classification and to better grasp the fundamental problem of imbalanced dataset. This example will also be used in the following sections.", "Let\u2019s suppose that we have two classes: C0 and C1. Points from the class C0 follow a one dimensional Gaussian distribution of mean 0 and variance 4. Points from the class C1 follow a one dimensional Gaussian distribution of mean 2 and variance 1. Suppose also that in our problem the class C0 represent 90% of the dataset (and, so, the class C1 represent the remaining 10%). In the following picture, we have depicted a representative dataset containing 50 points along with the theoretical distributions of both classes in the right proportions", "In this example we can see that the curve of the C0 class is always above the curve of the C1 class and, so, for any given point the probability that this point was drawn from class C0 is always greater than the probability it was drawn from class C1. Mathematically, using basic Bayes rule, we can write", "where we can clearly see the effect of the priors and how it can lead to a situation where a class is always more likely than the other.", "All this implies that even from a perfect theoretical point of view we know that if we had to train a classifier on these data, the accuracy of the classifier would be maximal when always answering C0. So, if the goal is to train a classifier to get the best possible accuracy then it should not be seen as a problem but just as a fact: with these features, the best we can do (in terms of accuracy) is to always answer C0. We have to accept it.", "In the given example, we can observe that the two classes are not well separable (they are not far apart from each other). However, we can notice that facing an imbalanced dataset doesn\u2019t necessarily mean that the two classes are not well separable and, so, that the classifier can\u2019t do a pretty good job on the minority class. For example, consider that we still have two classes C0 (90%) and C1 (10%). Data in C0 follow a one dimensional Gaussian distribution of mean 0 and variance 4 whereas data in C1 follow a one dimensional Gaussian distribution of mean 10 and variance 1. If we plot the data as before, we then have", "Here we see that contrarily to the previous case the C0 curve is not always above the C1 curve and, so, there are points that are more likely to be drawn from class C1 than from class C0. In this case, the two classes are separated enough to compensate the imbalance: a classifier will not necessarily answer C0 all the time.", "Finally, we should keep in mind that a classifier has a theoretical minimal error probability. For a classifier of this kind (one feature, two classes), we can mention that, graphically, the theoretical minimal error probability is given by the area under the minimum of the two curves.", "We can recover this intuition mathematically. Indeed, from a theoretical point of view, the best possible classifier will choose for each point x the most likely of the two classes. It naturally implies that for a given point x, the best theoretical error probability is given by the less likely of these two classes", "Then we can express the overall error probability", "Which is the area under the min of the two curves represented above.", "To begin, the very first possible reaction when facing an imbalanced dataset is to consider that data are not representative of the reality: if so, we assume that real data are almost balanced but that there is a proportions bias (due to the gathering method, for example) in the collected data. In this case, it is almost mandatory to try collecting more representative data.Let\u2019s see, now, what can be done when the dataset is imbalanced because reality is so. In the the next two subsections we present some methods that are often mentioned to tackle imbalanced classes and that deal with the dataset itself. In particular, we discuss the risks related to undersampling, oversampling and generating synthetic data as well as the benefits of getting more features.", "These methods are often presented as great ways to balance the dataset before fitting a classifier on it. In a few words, these methods act on the dataset as follows:", "All these approaches aim at rebalancing (partially or fully) the dataset. But should we rebalance the dataset to have as much data of both classes ? Or should the majority class stay the most represented ? If so, in what proportions should we rebalance ?", "When using a resampling method (for example to get as much data from C0 than from C1), we show the wrong proportions of the two classes to the classifier during the training. The classifier learned this way will then have a lower accuracy on the future real test data than the classifier trained on the unchanged dataset. Indeed, the true proportions of classes are important to know for classifying a new point and that information has been lost when resampling the dataset.", "So, if these methods have not to be completely rejected, they should be used cautiously: it can lead to a relevant approach if new proportions are chosen with purpose (we will see that in the next section), but it can also be a nonsense to just rebalance the classes without any further thoughts about the problem. To conclude this subsection, let\u2019s say that modifying the dataset with resampling-like methods is changing the reality, so it requires to be careful and to have in mind what it means for the outputted results of our classifier.", "We discussed in the previous subsection the fact that resampling the training dataset (modifying the classes proportions) can be or not a good idea depending on the real purpose of the classifier. We saw in particular that if the two classes are imbalanced, not well separable and that we target a classifier with the best possible accuracy, then getting a classifier that always answer the same class is not necessarily a problem but just a fact: there is nothing better to do with these variables.", "However, it remains possible to obtain better results in terms of accuracy by enriching the dataset with an additional feature (or more). Let\u2019s go back to our first example where classes were not well separable: maybe can we find a new additional feature that can help distinguish between the two classes and, so, improve the classifier accuracy.", "Compared to the approaches mentioned in the previous subsection that suggest to change the reality of data, this approach that consists in enriching data with more information from the reality is a far better idea when it is possible.", "Up to now the conclusion is pretty disappointing: if the dataset is representative of the true data, if we can\u2019t get any additional feature and if we target a classifier with the best possible accuracy, then a \u201cnaive behaviour\u201d (answering always the same class) is not necessarily a problem and should just be accepted as a fact (if the naive behaviour is not due to the limited capacity of the chosen classifier, of course).", "So what if we are still unhappy with these results? In this case, it means that, in one way or another, our problem is not well stated (otherwise we should accept results as they are) and that we should rework it in order to get more satisfying results. Let\u2019s see an example.", "The feeling that obtained results are not good can come from the fact that the objective function was not well defined. Up to now, we have assumed that we target a classifier with high accuracy, assuming at the same time that both kinds of errors (\u201cfalse positive\u201d and \u201cfalse negative\u201d) have the same cost. In our example it means we assumed that predicting C0 when true label is C1 is as bad as predicting C1 when true label is C0. Errors are then symmetric.", "Let\u2019s consider our introductory example with defective (C1) and not defective (C0) products. In this case, we can imagine that not detecting a defective product will cost more to the company (customer service costs, possible juridical costs if dangerous defects, \u2026) than wrongly labelling a not defective product as defective (production cost lost). Now, predicting C0 when true label is C1 is far worse than predicting C1 when true label is C0. Errors are no longer symmetric.", "Consider then more particularly that we have the following costs:", "Then, we can redefine our objective function: we don\u2019t target the best accuracy anymore but we look for the lower prediction cost instead.", "From a theoretical point of view, we don\u2019t want to minimise the error probability defined above but the expected prediction cost given by", "where C(.) defines the classifier function. So, if we want to minimise the expected prediction cost, the theoretical best classifier C(.) minimises", "or equivalently, dividing by the density of x, C(.) minimises", "So, with this objective function, the best classifier from a theoretical point of view will then be such that:", "Notice that we recover the expression of the \u201cclassic\u201d classifier (focus on accuracy) when costs are equal.", "One first possible way to take into account the cost in our classifier is to do it after the training. The idea is, first, to train a classifier the basic way to output the following probabilities", "without assuming any costs. Then, the predicted class will be C0 if", "Here, it doesn\u2019t matter which classifier we are using as long as it outputs the probability of each class for a given point. In our main example, we can fit a Bayes classifier on our data and we can then reweight the obtained probabilities to adjust the classifier with the costs errors as described.", "The idea of class reweight is to take into account the asymmetry of cost errors directly during the classifier training. Doing so, the outputted probabilities for each class will already embed the cost error information and could then be used to define a classification rule with a simple 0.5 threshold.", "For some models (for example Neural Network classifiers), taking the cost into account during the training can consist in adjusting the objective function. We still want our classifier to output", "but this time it is trained such as to minimise the following cost function", "For some other models (for example Bayes classifier), resampling methods can be used to bias the classes proportions such that to enter the cost error information inside the classes proportions. If we consider the costs P01 and P10 (such that P01 > P10), we can either:", "The main takeaways of this article are:", "We should notice that we have not discussed at all techniques like \u201cstratified sampling\u201d that can be useful when batch training a classifier. When facing an imbalanced classes problem, such techniques ensure more stability during the training (by removing the proportions variance inside batches).", "Finally, let\u2019s say that the main keyword of this article is \u201cgoal\u201d. Knowing exactly what you want to obtain will help overcome imbalanced dataset problems and will ensure having the best possible results. Defining the goal perfectly should always be the first thing to do and is the starting point of any choice that have to be done in order to create a machine learning model.", "Last article written with Joseph Rocca:", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7a0e84220f28&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@baptiste.rocca?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.rocca?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Baptiste Rocca"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20ad1309823a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&user=Baptiste+Rocca&userId=20ad1309823a&source=post_page-20ad1309823a----7a0e84220f28---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/getting-started", "anchor_text": "Getting Started"}, {"url": "https://medium.com/u/b17ebd108358?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Joseph Rocca"}, {"url": "https://medium.com/u/b17ebd108358?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Joseph Rocca"}, {"url": "https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29", "anchor_text": "Understanding Generative Adversarial Networks (GANs)Building, step by step, the reasoning that leads to GANs.towardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7a0e84220f28---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7a0e84220f28---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7a0e84220f28---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data?source=post_page-----7a0e84220f28---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/tag/getting-started?source=post_page-----7a0e84220f28---------------getting_started-----------------", "anchor_text": "Getting Started"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&user=Baptiste+Rocca&userId=20ad1309823a&source=-----7a0e84220f28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&user=Baptiste+Rocca&userId=20ad1309823a&source=-----7a0e84220f28---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7a0e84220f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7a0e84220f28---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7a0e84220f28--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7a0e84220f28--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7a0e84220f28--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.rocca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@baptiste.rocca?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Baptiste Rocca"}, {"url": "https://medium.com/@baptiste.rocca/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F20ad1309823a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&user=Baptiste+Rocca&userId=20ad1309823a&source=post_page-20ad1309823a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ffea591ab16a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhandling-imbalanced-datasets-in-machine-learning-7a0e84220f28&newsletterV3=20ad1309823a&newsletterV3Id=fea591ab16a4&user=Baptiste+Rocca&userId=20ad1309823a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}