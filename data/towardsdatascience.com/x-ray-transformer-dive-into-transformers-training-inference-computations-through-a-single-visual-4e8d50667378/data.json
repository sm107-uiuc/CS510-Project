{"url": "https://towardsdatascience.com/x-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378", "time": 1683013877.7148259, "path": "towardsdatascience.com/x-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378/", "webpage": {"metadata": {"title": "X-Ray Transformer Infographic. An all in one x-ray of the model that\u2026 | by Javier Ideami | Towards Data Science", "h1": "X-Ray Transformer Infographic", "description": "An all in one x-ray of the model that gave rise to GPT-3. Explore computations and transformations end to end during training and inference"}, "outgoing_paragraph_urls": [{"url": "https://github.com/javismiles/X-Ray-Transformer", "anchor_text": "github repo", "paragraph_index": 80}], "all_paragraphs": ["The transformer architecture has produced a revolution in the NLP field and in deep learning. A multitude of applications are benefiting from the capacity of these models to process sequences in parallel while achieving a deeper understanding of their context through the attention mechanisms they implement. And GPT-3 is a hot topic right now in the deep learning community.", "Understanding how the transformer processes sequences can be a bit challenging at first. When tackling a complex model, many people like to study how the computations of the model change the shapes of the tensors that travel through it.", "To that purpose, I created the X-Ray Transformer infographic, that allows you to make the journey from the beginning to the end of the transformer\u2019s computations in both the training and inference phases. Its objective is to achieve a quick and deep understanding of the inner computations of a transformer model through the analysis and exploration of a single visual asset.", "A link to download a higher resolution version of the full infographic below is available at the end of this article.", "When looking at this infographic, the first thing to consider, shown at the bottom right of the graphic, is the colors that denote different important stages.", "Once clarified the color codes, the next is to notice the pink circles with numbers inside them. Those help us see the general path of execution, first moving through the encoder and then the decoder.", "The two large arrows on both sides are a reminder of some of the key stages of the execution of both the encoder and decoder phases.", "To generate this infographic, I used a small transformer model that generates a chatbot. The chatbot is trained with pairs of questions and answers. This specific model is trained on questions and answers related to movies and series, especially science fiction ones. Examples of questions and answers:", "Below the title of the infographic, we can review the most important parameters to consider when studying the shapes of the computations.", "We begin the journey on the bottom left of the infographic as we begin to train the model.", "We obtain a batch from our dataloader. We use a batch size of 8, so each batch contains 8 input sequences and 8 output sequences.", "The 8 input sequences are padded as necessary (adding padding tokens) so that they all have the same length, in the case of this specific batch, 10 (the length of the longest sequence in that batch). The same is done with the output sequences of the batch.", "These sequences have been tokenized and numericalized to prepare them to be ingested by the model. By the time the training loop extracts a new batch, the sequences are numericalized and structured in a tensor of dimensions 8x10 (BS x SeqLen).", "Next we need to create a mask that will help us ensure that the additional padding elements in the sequence are not taken into account by the attention mechanisms. So we set to False or 0 those positions in the mask belonging to padding tokens in the input sequences.", "Now we have to create our embeddings, so we send the 8x10 tensor to the embed module and get back a 8x10x32 (BS x SeqLen x EmbedSize) tensor because our embedding size is 32 in this small example (512 is a typical embedding size for transformer models).", "To that, we add the result of the positional encoding module, which will help the model take into account the differences in positioning across the sequence.", "The first layer of our encoder is ready to ingest this 8x10x32 tensor.", "The first thing the encoder does is to create three copies of the 8x10x32 tensor to produce the Q, K and V elements of the model, that is, the query, keys and values.", "These 3 tensors are passed through 3 linear modules first (one for each of the tensors). In this small example, these linear modules don\u2019t change the dimensionality (but they could if we wish them to).", "After passing these linear modules, we arrive to the point of having to split the computation into our 4 heads (8 is a typical value for the number of heads. In this small example I use 4). Using 4 heads will allow the attention mechanism to interpret the sequences from different perspectives.", "Computationally we can prepare this stage in two simple steps.", "Our objective now is to calculate the attention scores. The encoder performs what is called self-attention. Self-attention helps us compare different parts of each input sequence to themselves and the rest of the sequence itself.", "Conceptually, we are exploring how much attention should different parts of our sequence pay to different parts of itself.", "To find this out, we will multiply the Query and Keys tensors.", "The next thing we will do is to apply a mask. This is because, remember, we made sure that all the sequences in the batch would have the very same length. And to do that we had to add padding tokens to the sequences that were shorter than the largest one of that batch.", "So we now should mask (make really small or very negative) those parts of the tensor that refer to parts of the sequence that had the padding token. So we apply the mask and eliminate the influence of those parts of the sequences that correspond to those padding tokens.", "Now we will apply a softmax module to the 10x10 matrix of the tensor, so that all the numbers of each row sum to 1, converting each row into a probability distribution.", "Those are our soft self attention scores. For each sequence of each batch and within each head, how strong is the connection between each part of that sequence and each part of itself, with the sum of all the influences on each part of the sequence adding to one.", "Now that we have the attention scores, we should apply them to the values, to the V tensor. We want to transform the values of the encoder according to the results of the self-attention computations.", "Our attention scores have the shape of 8x4x10x10 (BS x Heads x SeqLen x SeqLen). And our V tensor has the shape of 8x4x10x8 (BS x Heads x SeqLen x dimK). Remember that we are really multiplying the last 2 dimensions, so we are multiplying 10x10 * 10x8. This produces a new tensor of dimension 8x4x10x8 (BS x Heads x SeqLen x dimK).", "At this point, we have concluded the self-attention stage. We found out the attention scores by multiplying the queries and the keys. And then applied those attention scores to the values to obtain the final attention tensor.", "It\u2019s the moment to unify the 4 heads into one. To do that, we do the inverse of before, combining transposition and reconfiguration to obtain a new shape of 8x10x32 (BS x Heads x EmbSize).", "After passing the resulting tensor through a linear module, we arrive at our first skip connection. We will add our current tensor to the original one that entered the encoder layer. And then we will apply layer normalization to keep the data values within a good range.", "Next, we pass our 8x10x32 tensor through a feedforward layer and then apply another skip connection, adding the resulting tensor to the one that entered the feed forward layer (and normalizing the result as before).", "We can optionally apply a dropout module at different stages of the previous computations, for example when performing the skip connection additions or at the end of the attention phases.", "Wonderful! That was one layer of the encoder. The very same computations will be applied x number of times corresponding to the number of layers we have in the encoder.", "Notice that the tensor that entered the encoder layer and the one that exits the encoder layer have the very same shape: 8x10x32. That\u2019s why we can chain as many encoder layers as we like one after the other.", "Once we arrive at the final encoder layer, we obtain our final 8x10x32 tensor. This encoder output tensor will be used later in the encoder-decoder attention mechanism (present at the decoder layers) to provide the keys and values that will interact with the questions tensor of the decoder.", "But before we go there, let\u2019s move to the next step. The bottom part of the decoder.", "At the bottom of the decoder we have a 8x14 tensor (BS x SeqLen) that contains 8 sequences of reply phrases. As usual, these phrases have been tokenized and numericalized when creating the dataset and data loaders (and they contain padding tokens as needed).", "Something important to note is that in the decoder, we shift the sequences to the right by one position. So the first token will be a start-of-sentence token rather than the first word of the sentence. Why do we do this?", "We do it because we don\u2019t want our model to just copy and paste the decoder\u2019s input into its output. We want it to predict the next word (or character, but in this example we are predicting words). So if we don\u2019t shift everything to the right by one, the prediction for position N will be the current word at position N in the decoder\u2019s input, which we can access directly. To prevent this from happening, we shift the decoder\u2019s input to the right by one position. In this way, at each stage, the decoder has to predict the position N but can only see up to position N-1 of the existing phrase.", "We also create the decoder mask, which contains True above the diagonal and False below it. This mask helps prevent the decoder from considering parts of the sentence that it hasn\u2019t yet seen.", "Let\u2019s go deeper into this point because it\u2019s crucial. The decoder\u2019s self attention mask ensures that each self-attention vector doesn\u2019t pay attention to positions that are in the future.", "So If I am calculating the self-attention scores for the word in position 3 of the sequence, I will mask out all positions after that one. This is necessary because when we are building our output phrase we need to perform our calculations based on the words generated so far and we shouldn\u2019t be able to know the future words that will be coming later.", "In a way we are preventing the decoder from cheating during the training process. If we want, for example, to predict the second word of a phrase, we should take into consideration only the first position of the output phrase. If we want to predict the fifth word, we should consider just the first, second, third and fourth positions.", "Notice that the decoder\u2019s mask is also masking the padding tokens that may exist in the output sequences. So the decoder\u2019s mask adds the masking of the padding tokens to the masking of future positions in the sequence.", "As in the encoder, the 8x14 tensor is sent to the embed module which outputs a 8x14x32 (BS x Heads x SeqLen x EmbSize) tensor , because the embedding size is 32. Next, the result of the positional encoding module is added to it.", "At this point we arrive at the first decoder layer, which will be repeated as many times as the number of decoder layers we wish to have.", "In the decoder layer we enter into two consecutive attention stages.", "The first self-attention stage of the decoder is identical to the encoder\u2019s one, except for using the output sequences as the data, and using the decoder\u2019s mask.", "In the second attention stage, the encoder-decoder attention, a similar process happens with some key differences:", "And the mask that we use in the masking stage is the one that was used in the encoder, the input sequence one. This way we make sure to only consider the connections between the output sequence and the parts of the input sequence that don\u2019t have padding tokens. The output sequence itself has already been masked by the first stage of the decoder layer.", "The attention scores are not a square matrix anymore. We obtain a 14x10 matrix within the 8x4x14x10 (BS x Heads x InSeqLen x OutSeqLen) tensor, reflecting that we are obtaining the relationships between the different parts of the output sequences with the different parts of the input sequences.", "As usual, after performing the attention computations, we concatenate the results to obtain in this case a 8x14x32 (BS x SeqLen x EmbSize) tensor.", "After we perform the self-attention and encoder-decoder attention stages of the decoder layer, we move to a final stage within the same layer, passing firstly the 8x14x32 tensor through a feedforward module, and then, as we did in the encoder, adding the result of that computation to the input of that module (the feedforward module), applying as well a layer normalization module to the result. (the use of dropout in this process as well as in others, as mentioned in the encoder\u2019s section, is a potential optional addition.)", "This decoder layer process is then repeated over the existing number of decoder layers. As before, the input and outputs of each decoder layer have identical shapes, 8x14x32 (BS x Heads x EmbSize), which makes it easy to chain a few of these layers/processes.", "Once we have iterated through all the decoder layers, we obtain a final 8x14x32 tensor, which we then pass through a linear layer whose output has the shape 8x14x950 (BS x SeqLen x OutVocabSize), 950 being the vocabulary size of the outputs of the chatbot.", "This 8x14x950 tensor contains our predictions for this iteration. For each sequence of the batch, and for each of the 14 parts of each sequence, we obtain 950 values corresponding to the potential 950 words that are candidates for the next position of the output phrase.", "It\u2019s time to calculate the loss, the difference between our objectives and our current predictions.", "We take that predictions tensor into a cross entropy loss module, which also receives our 8x14 target tensor. The result of that cross entropy module is the loss value of this iteration of the training process.", "That loss value is then back propagated through the network, weights are updated, and the process restarts again with the encoder processing a new batch.", "We continue the training process for as many epochs as we like until we reach our objective (in terms of accuracy, loss value, validation loss, etc). Every x number of iterations or epochs we proceed to save the weights of the network in case we want to restart the training some other time, or to have the latest trained weights ready to perform inference at any time.", "So that was the training process. Let\u2019s now quickly look at the inference process. Once the transformer is trained, how do we execute it and run it?", "For that, we have to focus on the green parts of the infographic.", "At the bottom left of the graphic, we see the green Inference column that begins the inference process.", "When we run the trained transformer we will enter a single input phrase, for example: What\u2019s your favourite character in the expanse series?", "That\u2019s why our batch size will be 1. We still need to specify a batch size because the computations require it. The phrase is tokenized and numericalized, obtaining a 1x9 (BS x SeqLen) tensor because this example phrase has 9 tokens ([\u201cwhat\u2019s\u201d, \u2018your\u2019, \u2018favourite\u2019, \u2018character\u2019, \u2018in\u2019, \u2018the\u2019, \u2018expanse\u2019, \u2018series\u2019,\u2019?\u2019]. Notice that we can tokenize phrases in many different ways, and there are many tokenizers available that you can use. This small example uses a simple way of tokenizing the phrases.", "We also create our input mask, which at this inference stage will have True in every position.", "Next, we pass that input tensor to the embed module and add to it the output of the positional encoding module to get a 1x9x32 (BS x Heads x EmbSize) tensor.", "The first layer of the encoder begins with similar computations to the ones done during the training iterations, but this time using this 1x9x32 tensor. The encoder layers repeat until we arrive at the final one, where we obtain a 1x9x32 tensor which will be used by the decoder to provide the keys and values of the encoder-decoder attention stage.", "We move to the decoder, where things get a little different.", "So our input to the decoder will initially have a shape of 1x1. In the next iteration it will become 1x2, then 1x3, etc until it reaches 1xN with N being the number of iterations of the decoder loop until we obtain the end-of-sentence token.", "At each point in the loop, we create a new mask that adapts to each iteration. Initially has a shape of 1x1x1 (BS x SeqLen x SeqLen). In the next iteration it becomes 1x2x2, then 1x3x3 until it reaches 1xNxN when we reach the end-of-sentence token. As before, this mask helps us prevent the model from paying attention to future positions in the sequence (beyond the current one it is considering) when calculating the attention scores.", "We then go through a number of decoder layers, with each doing the same computations we saw before:", "At the end of the decoder layers we will obtain a 1xNx950 (BS x SeqLen x OutVocabSize) tensor, with N being the position in the decoder loop in which we are. The first iteration we obtain a 1x1x950 tensor, the second time a 1x2x950 tensor, etc.", "We pass the resulting tensor through a softmax module to obtain a probability distribution. This distribution gives us the probabilities of obtaining each of the elements of the output vocabulary for each word of the output phrase. We will consider the probabilities of the last part of that tensor, the ones belonging to the next word we want to predict.", "We can sample in a variety of ways from this probability distribution to obtain a 1x1 tensor that contains the new word that will be added to the end of the current output sentence.", "We then continue to loop and add new words to the output sentence until we find the end-of-sentence token.", "And that\u2019s it, we have a cool transformer chatbot whose computations have been revealed to us through this x-ray transformer visualization.", "You may download a larger version (10488 x 14000 pixels) of the x-ray transformer visualization from the dedicated github repo:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A multidisciplinary engineer, researcher, creative director, artist and entrepreneur, from augmented reality to deep learning, filmmaking, 3D and beyond."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4e8d50667378&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e8d50667378--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ideami?source=post_page-----4e8d50667378--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84----4e8d50667378---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/javismiles/X-Ray-Transformer", "anchor_text": "github repo"}, {"url": "https://github.com/javismiles/X-Ray-Transformer", "anchor_text": "javismiles/X-Ray-TransformerDive into transformers training & inference computations through a single visual. The X-Ray Transformer infographic\u2026github.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4e8d50667378---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4e8d50667378---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4e8d50667378---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4e8d50667378---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4e8d50667378---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&user=Javier+Ideami&userId=7f7b5d730c84&source=-----4e8d50667378---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&user=Javier+Ideami&userId=7f7b5d730c84&source=-----4e8d50667378---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4e8d50667378&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4e8d50667378---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4e8d50667378--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4e8d50667378--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4e8d50667378--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4e8d50667378--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4e8d50667378--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/@ideami/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c3e5da2cfc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fx-ray-transformer-dive-into-transformers-training-inference-computations-through-a-single-visual-4e8d50667378&newsletterV3=7f7b5d730c84&newsletterV3Id=4c3e5da2cfc1&user=Javier+Ideami&userId=7f7b5d730c84&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}