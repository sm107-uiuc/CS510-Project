{"url": "https://towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571", "time": 1683014126.9590158, "path": "towardsdatascience.com/monte-carlo-dropout-7fd52f8b6571/", "webpage": {"metadata": {"title": "Monte Carlo Dropout | Towards Data Science", "h1": "Monte Carlo Dropout", "description": "Improve your neural network for free with one small trick, getting model uncertainty estimate as a bonus."}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28", "anchor_text": "distribution", "paragraph_index": 7}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Gal & Ghahramani (2016)", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/the-gentlest-of-introductions-to-bayesian-data-analysis-74df448da25", "anchor_text": "Bayesian", "paragraph_index": 8}, {"url": "https://github.com/ageron/handson-ml2", "anchor_text": "jupyter notebooks", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Gal & Ghahramani (2016)", "paragraph_index": 14}, {"url": "https://michaloleszak.medium.com/subscribe", "anchor_text": "subscribe for email updates", "paragraph_index": 21}, {"url": "https://michaloleszak.medium.com/membership", "anchor_text": "becoming a Medium member", "paragraph_index": 21}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "here", "paragraph_index": 22}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles", "paragraph_index": 23}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com", "paragraph_index": 25}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal", "paragraph_index": 25}], "all_paragraphs": ["There ain\u2019t no such thing as a free lunch, at least according to the popular adage. Well, not anymore! Not when it comes to neural networks, that is to say. Read on to see how to improve your network\u2019s performance with an incredibly simple yet clever trick called the Monte Carlo Dropout.", "The magic trick we are about to introduce only works if your neural network has dropout layers, so let\u2019s kick off with briefly introducing these. Dropout boils down to simply switching-off some neurons at each training step. At each step, a different set of neurons are switched off. Mathematically speaking, each neuron has some probability p of being ignored, called the dropout rate. The dropout rate is typically set to be between 0 (no dropout) and 0.5 (approximately 50% of all neurons will be switched off). The exact value depends on the network type, layer size, and the degree to which the network overfits the training data.", "But why do this? Dropout is a regularization technique, that is, it helps prevent overfitting. With little data and/or a complex network, the model might memorize the training data and, as a result, work great on the data it has seen during training but deliver terrible results on new, unseen data. This is called overfitting, and dropout seeks to alleviate it.", "How? There are two ways to understand why switching off some parts of the model might be beneficial. First, the information spreads out more evenly across the network. Think about a single neuron somewhere inside the network. There are a couple of other neurons that provide it with inputs. With dropout, each of these input sources can disappear at any time during training. Hence, our neuron cannot rely on one or two inputs only, it has to spread out its weights and pay attention to all inputs. As a result, it becomes less sensitive to input changes which results in the model generalizing better.", "The other explanation of dropout\u2019s effectiveness is even more important from the point of view of our Monte Carlo trick. Since in every training iteration you randomly sample the neurons to be dropped out in each layer (according to that layer\u2019s dropout rate), a different set of neurons are being dropped out each time. Hence, each time the model\u2019s architecture is slightly different and you can think of the outcome as an averaging ensemble of many different neural networks, each trained on one batch of data only.", "A final detail: dropout is only used during training. At inference time, that is when we make predictions with our network, we typically don\u2019t apply any dropout \u2014 we want to use all the trained neurons and connections.", "Now that we have dropout out of the way, what is Monte Carlo? If you\u2019re thinking about a neighborhood in Monaco, you\u2019re right! But there is more to it.", "In statistics, Monte Carlo refers to a class of computational algorithms that rely on repeated random sampling to obtain a distribution of some numerical quantity.", "Monte Carlo Dropout, proposed by Gal & Ghahramani (2016), is a clever realization that the use of the regular dropout can be interpreted as a Bayesian approximation of a well-known probabilistic model: the Gaussian process. We can treat the many different networks (with different neurons dropped out) as Monte Carlo samples from the space of all available models. This provides mathematical grounds to reason about the model\u2019s uncertainty and, as it turns out, often improves its performance.", "How does it work? We simply apply dropout at test time, that's all! Then, instead of one prediction, we get many, one by each model. We can then average them or analyze their distributions. And the best part: it does not require any changes in the model\u2019s architecture. We can even use this trick on a model that has already been trained! To see it working in practice, let\u2019s train a simple network to recognize digits from the MNIST dataset.", "After training for 30 epochs, this model scores the accuracy of 96.7% on the test set. To turn on dropout at prediction time, we simply need to set training=True to ensure training-like behavior, that is dropping out some neurons. This way, each prediction will be slightly different and we may generate as many as we like.", "Let\u2019s create two useful functions: predict_proba() generates the desired number num_samples of predictions and averages the predicted class probability for each of the 10 digits in the MNIST dataset, while predict_class() simply chooses the highest predicted probability to pick the most likely class. This and some of the following code snippets are inspired by the ones from Geron (2019). The book is accompanied by a set of excellent jupyter notebooks.", "Now, let\u2019s make 100 predictions and evaluate accuracy on the test set.", "This yields an accuracy of 97.2%. Compared to the previous result, we have decreased the error rate from 3.3% to 2.8%, which is by 15%, without changing or retraining the model at all!", "Let\u2019s take a look at prediction uncertainty. In classification tasks, class probabilities obtained from the softmax output are often erroneously interpreted as model confidence. However, Gal & Ghahramani (2016) show that a model can be uncertain in its predictions even with a high softmax output. We can see it in our MNIST predictions as well. Let\u2019s compare the softmax output with the Monte Carlo Dropout-predicted probabilities for a single test example.", "Both agree that the test example is most likely from the 3rd class. However, the softmax is 100% sure that\u2019s the case, which should already alert you that something is not right. Probability estimates of 0% or 100% are usually dangerous. Monte Carlo Dropout provides us with much more information about the prediction uncertainty: most likely it\u2019s class 3, but there is a small chance it might be class 4, and 5, although unlikely, is still more probable than 1, for instance.", "So far, we have talked about a classification task. Let\u2019s now turn to a regression problem to see how Monte Carlo Dropout provides us with prediction uncertainty. Let\u2019s fit a regression model to predict house prices using the Boston housing dataset.", "For a classification task, we have defined functions to predict class probabilities and the most likely class. Similarly, for the regression problem, we need functions to get the predictive distribution and a point estimate (let\u2019s use the mean for this).", "Let\u2019s again make 100 predictions for one test example and plot their distribution, marking its mean, which is our point estimate, or best guess.", "For this particular test example, the mean of the predictive distribution amounts to 18, but we can see that other values are not unlikely \u2014 the model is not very certain about its predictions.", "Just one final remark: we have been implementing Monte Carlo Dropout by setting the model\u2019s training mode to true throughout this article. This works well, but it might affect other parts of the model that behave differently at training and inference time, such as batch normalization, for instance. To make sure we only switch on dropout without affecting anything else, we should create a custom MonteCarloDropout layer that inherits from the regular dropout, and has its training parameter set to true by default (the following piece of code has been adapted form Geron (2019)).", "If you liked this post, why don\u2019t you subscribe for email updates on my new articles? And by becoming a Medium member, you can support my writing and get unlimited access to all stories by other authors and myself.", "Need consulting? You can ask me anything or book me for a 1:1 here.", "You can also try one of my other articles. Can\u2019t choose? Pick one of these:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML Engineer & Data Science Instructor | Top Writer in AI & Statistics | michaloleszak.com | Book 1:1 @ hiretheauthor.com/michal"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7fd52f8b6571&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://michaloleszak.medium.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8----7fd52f8b6571---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@brooksieg?utm_source=medium&utm_medium=referral", "anchor_text": "Geoff Brooks"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/6-useful-probability-distributions-with-applications-to-data-science-problems-2c0bee7cef28", "anchor_text": "distribution"}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Gal & Ghahramani (2016)"}, {"url": "https://towardsdatascience.com/the-gentlest-of-introductions-to-bayesian-data-analysis-74df448da25", "anchor_text": "Bayesian"}, {"url": "https://github.com/ageron/handson-ml2", "anchor_text": "jupyter notebooks"}, {"url": "https://arxiv.org/abs/1506.02142", "anchor_text": "Gal & Ghahramani (2016)"}, {"url": "https://michaloleszak.medium.com/subscribe", "anchor_text": "subscribe for email updates"}, {"url": "https://michaloleszak.medium.com/membership", "anchor_text": "becoming a Medium member"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "here"}, {"url": "https://michaloleszak.github.io/blog/", "anchor_text": "my other articles"}, {"url": "https://towardsdatascience.com/svm-kernels-what-do-they-actually-do-56ce36f4f7b8", "anchor_text": "SVM Kernels: What Do They Actually Do?An intuitive visual explanationtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/calibrating-classifiers-559abc30711a", "anchor_text": "Calibrating classifiersAre you sure your model returns probabilities? \ud83c\udfb2towardsdatascience.com"}, {"url": "https://towardsdatascience.com/a-comparison-of-shrinkage-and-selection-methods-for-linear-regression-ee4dd3a71f16", "anchor_text": "A Comparison of Shrinkage and Selection Methods for Linear RegressionA detailed look at 7 popular shrinkage & selection methods.towardsdatascience.com"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----7fd52f8b6571---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7fd52f8b6571---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----7fd52f8b6571---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/bayesian-statistics?source=post_page-----7fd52f8b6571---------------bayesian_statistics-----------------", "anchor_text": "Bayesian Statistics"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----7fd52f8b6571---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----7fd52f8b6571---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=-----7fd52f8b6571---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7fd52f8b6571&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7fd52f8b6571---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7fd52f8b6571--------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://michaloleszak.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Micha\u0142 Oleszak"}, {"url": "https://michaloleszak.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.7K Followers"}, {"url": "http://michaloleszak.com", "anchor_text": "michaloleszak.com"}, {"url": "http://hiretheauthor.com/michal", "anchor_text": "hiretheauthor.com/michal"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc58320fab2a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=post_page-c58320fab2a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F38bf302f5b56&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmonte-carlo-dropout-7fd52f8b6571&newsletterV3=c58320fab2a8&newsletterV3Id=38bf302f5b56&user=Micha%C5%82+Oleszak&userId=c58320fab2a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}