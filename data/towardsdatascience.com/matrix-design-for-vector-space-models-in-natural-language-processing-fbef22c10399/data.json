{"url": "https://towardsdatascience.com/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399", "time": 1683011088.075479, "path": "towardsdatascience.com/matrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399/", "webpage": {"metadata": {"title": "Matrix Design for Vector Space Models in Natural Language Processing | by Nabanita Roy | Towards Data Science", "h1": "Matrix Design for Vector Space Models in Natural Language Processing", "description": "I remember when I studied algebra for the first time, I had this weird urge of representing these alphabets into words. I had a few scribbles of random alphabets being used as numbers and then just\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Big_Five_personality_traits", "anchor_text": "big five scores", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Openness_to_experience", "anchor_text": "openness", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Conscientiousness", "anchor_text": "conscientiousness", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Extraversion_and_introversion", "anchor_text": "extraversion", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Agreeableness", "anchor_text": "agreeableness", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Neuroticism", "anchor_text": "neuroticism", "paragraph_index": 30}], "all_paragraphs": ["I remember when I studied algebra for the first time, I had this weird urge of representing these alphabets into words. I had a few scribbles of random alphabets being used as numbers and then just playing around.", "Well, as it turns out, when I studied information theory and then word vectors, I was like \u201cI knew it!!!\u201d", "One of the fundamentals of computer science is representing your knowledge in the form of numbers and even better \u2014 with a combination of 0s and 1s. Machine Learning models are no exceptions and hence, converting texts to numeric representations (or like my professor would call it \u2014 proxies) is of the building blocks of any kind of text analytics task.", "There are two primary sources of human knowledge for comprehending human language:", "I would call the dictionary a structured learning artifact designed to aid natural human learning while knowledge gathered from reading books is a sort of unstructured and even unforeseen process. These are two key approaches to understanding natural language which has been systematically attempted to digitize for an artificial being to learn (I mean the computer \ud83d\ude05).", "Dictionaries are implemented as WordNet; the wealth of knowledge that serves as a digital dictionary ready to be consumed by an NLP program, much like how we refer to a dictionary to learn the meaning of a particular word.", "The other way of gaining knowledge is by learning from artifacts such as books, novels, item reviews, short messages, tweets, and so on. This knowledge can be effectively represented using vectors to capture semantic knowledge embedded in them. This is all the fuss about that I am going to briefly discuss here.", "Vectors are one-dimensional matrices used to represent a collection of numbers in a one-dimensional space. In Machine Learning, a feature vector is a one-dimensional vector used to represent all numeric encodings of features for one particular instance of data. As the number of instances expands, the matrix also grows. Similarly, in recommender systems, matrices are used to relate between users and the purchased or viewed item(s) where each vector represents the choice of each user. In psychology, vectors are used to assess the psychometric features and a feature vector would comprise of the points of each psychometric traits being assessed per person.", "Thus, the usage of vectors and matrices to quantify data into machine-interpretable formats is quite prevalent. In Natural Language Processing as well words are represented in a vector space where each vector corresponds to the distribution of a particular word. This makes the computation of predictions by generalization, which is mostly deciphered by calculating vector similarity, much like a supervised classification problem.", "\u201cYou shall know a word by the company it keeps\u201d", "So as we have established already, we will be converting texts to their numeric representations, and we will do it by representing them as dense vectors of real numbers. These vectors, which collectively form a matrix can be designed in such a way that lets us conserve the meaning we expect to derive from the piece of text.", "A sentence, for example \u2014 \u201c I love text analytics.\u201d, the occurrence of the word \u2018love\u2019 can be converted into a vector of 4 x 1 where 4 is the vocabulary size (denoted by V).", "Mathematically, the vector to represent \u2018love\u2019 could be written as :", "This is a form of one-hot encoding.", "If there are multiple sentences then the simplest way to scale this is to add the vocabulary into the vocabulary dimension of the vector and keep on expanding this \u201coccurrence\u201d vector.", "Then, if there are M tokens, you could create a V x M matrix that encodes the presence and absence of the tokens in the vocabulary.", "Now, let\u2019s retrospect about this way of encoding and jot the caveats. The foremost important thing in languages is that a word itself does not give out predictive signals until it is put into a context for a holistic understanding. For example, if I say \u201cGood\u201d \u2014 Yes, good is a positive word, but what is good? Or is there a \u201cnot\u201d before \u201cgood\u201d? This is why I labeled this as the \u201coccurrence\u201d vector. It is a simple yes or no situation. This is not contextual.", "The second problem is the massive vocabulary that could be generated if the word vector representation is not controlled. This would lead to expensive computations and time-consuming processes.", "To some-what capture the meaning of a group of words occurring together, we have two most widely used matrix designs discussed below:", "In this vector representation method, each word is represented as the frequency of its occurrence per document. The vector size would hence be|V|x D, where D is the number of documents. This vector is comprised of real numbers and scales up with the addition of more documents. In other words, for the ith word, the term frequency (tf) of that word in the jth document is placed at ijth element of the matrix.", "To demonstrate this, consider these three documents:", "Document 1:\u201c Listen, Harry can I have a go on it? Can I?\u201d", "Document 2: \u201cI don\u2019t think anyone should ride that broom just yet!\u201d said Hermione shrilly.", "Document 3: Harry and Ron looked at her.", "The word-document vector representation would look something like this:", "2. Window-based Word x Word Matrix:", "This is also called a co-occurrence matrix where how many times a word has appeared in the vicinity of another word is measured. For example, if a window-size of 3 is being assessed, then how many times a word (i) in the vocabulary has occurred within the range j+3 to j-3 of the jth word in the vocabulary is measured and placed at the position ij of the co-occurrence matrix. If the vocabulary size is |V| then the final matrix would be of shape |V| x |V|.", "To demonstrate this for window-size 3, the co-occurrence matrix for the previous example would be:", "Had I used more examples, Harry, Ron and Hermione would be more prominent than others in terms of the frequency counts as their names would occur much more frequently together (within a span of 3 words, forwards and backward) which establishes semantic meaning in these stand-alone words.", "Defining the dimension of the vector is contextual to the problem being solved using NLP techniques. Consider if you only want to know, in the context of user-ratings as \u201cVery Good\u201d, \u201cGood\u201d, \u201cAverage\u201d, \u201cNot Good\u201d, \u201cWorse\u201d. The dimension for this would be a one-hot encoder, where 1 would correspond to the user rating. This is also a form of capturing user-sentiment in the context of a specific item. The size of the dimension for this problem is 5. Every instance will be a 1 x 5 matrix.", "Now, in terms of psychometric analysis, if you are encoding the big five scores per user, then there will be real numbers in the range of 0 and 50 per big five personality trait, namely, openness, conscientiousness, extraversion, agreeableness, and neuroticism. This vector is also of shape 1 x 5. Depending on the problem you are solving, these values could be scaled as lower values of each of these personality tests indicate the opposite behavior of that particular trait. For example, [15, 12, 34, 44, 29] could be scaled to [-1, -1, 1 , 1 , 0].", "If you are working on a gender classification task, then your vector representation could possibly be of 1x2 and if you have 10k documents then your dimension would be of the same order unless you strategize the length of the word vector.", "The choice of dimensionality for word vectors has a huge influence on the performance of the model. A smaller dimension of vectors would be inefficient to capture all features (under-fitting) while a large dimension will lead to over-fitting. A large dimension directly increases model complexity, computational costs as well as training time by adding latency. Besides, the features computed from the vectors would be a linear or a quadratic function of the dimension which further impacts training time and computational costs.", "The meaning embedded in words can be identified by using count statistics and representing a word\u2019s association with adjacent words or the document in which it resides and representing them in the form of a vector. More the vocabulary and documents, the larger is the matrix, which leads to the curse of dimensionality. Since these encodings strategies result in sparse matrices, they also have associated scalability issues as well as they are computationally expensive. An MxN matrix has the computational cost of O(mn\u00b2) which is undesirable. However, these two representations are the most widely used matrix design strategies and have proved to be quite efficient in solving NLP problems such as text classification, non-contextual sentiment analysis, and document similarity.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ EY (UK & Ireland) | Education Lead @ Women in AI Ireland | \u2764 NLP"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffbef22c10399&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://nroy0110.medium.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": ""}, {"url": "https://nroy0110.medium.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Nabanita Roy"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd36a8b28c928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&user=Nabanita+Roy&userId=d36a8b28c928&source=post_page-d36a8b28c928----fbef22c10399---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jontyson?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Tyson"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@omgitsyeshi?utm_source=medium&utm_medium=referral", "anchor_text": "Yeshi Kangrang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@paan_azam13?utm_source=medium&utm_medium=referral", "anchor_text": "Farhan Azam"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://en.wikipedia.org/wiki/Big_Five_personality_traits", "anchor_text": "big five scores"}, {"url": "https://en.wikipedia.org/wiki/Openness_to_experience", "anchor_text": "openness"}, {"url": "https://en.wikipedia.org/wiki/Conscientiousness", "anchor_text": "conscientiousness"}, {"url": "https://en.wikipedia.org/wiki/Extraversion_and_introversion", "anchor_text": "extraversion"}, {"url": "https://en.wikipedia.org/wiki/Agreeableness", "anchor_text": "agreeableness"}, {"url": "https://en.wikipedia.org/wiki/Neuroticism", "anchor_text": "neuroticism"}, {"url": "http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf", "anchor_text": "http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf"}, {"url": "http://web.stanford.edu/class/cs224u/materials/cs224u-2020-vsm-handout.pdf", "anchor_text": "http://web.stanford.edu/class/cs224u/materials/cs224u-2020-vsm-handout.pdf"}, {"url": "https://medium.com/tag/natural-language-processi?source=post_page-----fbef22c10399---------------natural_language_processi-----------------", "anchor_text": "Natural Language Processi"}, {"url": "https://medium.com/tag/vector-space-model?source=post_page-----fbef22c10399---------------vector_space_model-----------------", "anchor_text": "Vector Space Model"}, {"url": "https://medium.com/tag/word-representation?source=post_page-----fbef22c10399---------------word_representation-----------------", "anchor_text": "Word Representation"}, {"url": "https://medium.com/tag/word-vectors?source=post_page-----fbef22c10399---------------word_vectors-----------------", "anchor_text": "Word Vectors"}, {"url": "https://medium.com/tag/matrix-design?source=post_page-----fbef22c10399---------------matrix_design-----------------", "anchor_text": "Matrix Design"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&user=Nabanita+Roy&userId=d36a8b28c928&source=-----fbef22c10399---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&user=Nabanita+Roy&userId=d36a8b28c928&source=-----fbef22c10399---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffbef22c10399&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fbef22c10399---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fbef22c10399--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fbef22c10399--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fbef22c10399--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fbef22c10399--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fbef22c10399--------------------------------", "anchor_text": ""}, {"url": "https://nroy0110.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://nroy0110.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nabanita Roy"}, {"url": "https://nroy0110.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "222 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd36a8b28c928&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&user=Nabanita+Roy&userId=d36a8b28c928&source=post_page-d36a8b28c928--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa0eba44b2636&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmatrix-design-for-vector-space-models-in-natural-language-processing-fbef22c10399&newsletterV3=d36a8b28c928&newsletterV3Id=a0eba44b2636&user=Nabanita+Roy&userId=d36a8b28c928&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}