{"url": "https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1", "time": 1683010641.4893491, "path": "towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1/", "webpage": {"metadata": {"title": "Knowledge Graphs in Natural Language Processing @ ACL 2020 | by Michael Galkin | Towards Data Science", "h1": "Knowledge Graphs in Natural Language Processing @ ACL 2020", "description": "This post commemorates the first anniversary of the series where we examine advancements in NLP and Graph ML powered by knowledge graphs! \ud83c\udf82 1\ufe0f\u20e3 The feedback of the audience drives me to continue, so\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.aclweb.org/anthology/2020.acl-main.412.pdf", "anchor_text": "Saxena et al", "paragraph_index": 5}, {"url": "http://proceedings.mlr.press/v48/trouillon16.pdf", "anchor_text": "ComplEx", "paragraph_index": 5}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.91.pdf", "anchor_text": "Lan et al", "paragraph_index": 7}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.677.pdf", "anchor_text": "Wang et al", "paragraph_index": 10}, {"url": "https://yale-lily.github.io/spider", "anchor_text": "Spider", "paragraph_index": 10}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.187.pdf", "anchor_text": "Elgohary et al", "paragraph_index": 11}, {"url": "https://github.com/MSR-LIT/Splash", "anchor_text": "SPLASH", "paragraph_index": 11}, {"url": "https://www.aclweb.org/anthology/2020.acl-demos.24.pdf", "anchor_text": "Zeng et al", "paragraph_index": 11}, {"url": "http://www.naturalsql.com/", "anchor_text": "Photon", "paragraph_index": 11}, {"url": "https://medium.com/mlreview/machine-learning-on-graphs-neurips-2019-875eecd41069", "anchor_text": "recent hot", "paragraph_index": 12}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.617.pdf", "anchor_text": "Chami et al", "paragraph_index": 13}, {"url": "https://papers.nips.cc/paper/8696-multi-relational-poincare-graph-embeddings.pdf", "anchor_text": "MurP", "paragraph_index": 14}, {"url": "https://exascale.info/assets/pdf/rosso2020www.pdf", "anchor_text": "Rosso et al", "paragraph_index": 15}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.546.pdf", "anchor_text": "Guan et al", "paragraph_index": 16}, {"url": "https://www.researchgate.net/profile/Saiping_Guan/publication/333060086_Link_Prediction_on_N-ary_Relational_Data/links/5cdda342299bf14d959f3863/Link-Prediction-on-N-ary-Relational-Data.pdf", "anchor_text": "NaLP", "paragraph_index": 16}, {"url": "https://exascale.info/assets/pdf/rosso2020www.pdf", "anchor_text": "Rosso et al", "paragraph_index": 18}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.489.pdf", "anchor_text": "Sun, Vashishth, Sanyal et al", "paragraph_index": 19}, {"url": "https://arxiv.org/pdf/2006.13365", "anchor_text": "\u201cBringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework\u201d", "paragraph_index": 20}, {"url": "https://github.com/pykeen/pykeen", "anchor_text": "PyKEEN 1.0", "paragraph_index": 20}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.238.pdf", "anchor_text": "Sachan", "paragraph_index": 21}, {"url": "https://github.com/facebookresearch/PyTorch-BigGraph", "anchor_text": "PyTorch-BigGraph", "paragraph_index": 22}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.241.pdf", "anchor_text": "Tang et al", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1902.10197.pdf", "anchor_text": "RotatE", "paragraph_index": 23}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.358.pdf", "anchor_text": "Xu et al", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1412.6575.pdf", "anchor_text": "DistMult", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1606.06357.pdf", "anchor_text": "ComplEx", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1510.04935.pdf", "anchor_text": "HolE", "paragraph_index": 23}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.526.pdf", "anchor_text": "Xie et al", "paragraph_index": 23}, {"url": "https://arxiv.org/pdf/1512.00567.pdf", "anchor_text": "Inception", "paragraph_index": 23}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.313.pdf", "anchor_text": "Nguyen et al", "paragraph_index": 23}, {"url": "https://webnlg-challenge.loria.fr/challenge_2020/", "anchor_text": "WebNLG 2020", "paragraph_index": 25}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.18.pdf", "anchor_text": "Chen et al", "paragraph_index": 28}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.708.pdf", "anchor_text": "Chen et al", "paragraph_index": 29}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.712.pdf", "anchor_text": "Song et al", "paragraph_index": 30}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.448.pdf", "anchor_text": "everybody should stop using BLEU for evaluating NLG quality", "paragraph_index": 31}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.704.pdf", "anchor_text": "BLEURT", "paragraph_index": 31}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.224.pdf", "anchor_text": "Zhao et al", "paragraph_index": 32}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.457.pdf", "anchor_text": "Huang et al", "paragraph_index": 33}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.12.pdf", "anchor_text": "Campagna et al", "paragraph_index": 37}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.444.pdf", "anchor_text": "Yu et al", "paragraph_index": 38}, {"url": "https://pdfs.semanticscholar.org/0d3c/68c207fc83fb402b7217811af22066300fc9.pdf", "anchor_text": "OpenDialKG", "paragraph_index": 39}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.635.pdf", "anchor_text": "Zhou et al", "paragraph_index": 39}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.6.pdf", "anchor_text": "Lin et al", "paragraph_index": 40}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.565.pdf", "anchor_text": "Qin et al", "paragraph_index": 40}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.184.pdf", "anchor_text": "Zhang et al", "paragraph_index": 40}, {"url": "https://sites.google.com/view/2ndnlp4convai/home", "anchor_text": "NLP for ConvAI", "paragraph_index": 40}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.521.pdf", "anchor_text": "Kolluru et al", "paragraph_index": 42}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.209.pdf", "anchor_text": "Broscheit et al", "paragraph_index": 44}, {"url": "https://www.akbc.ws/2020/papers/", "anchor_text": "AKBC 2020", "paragraph_index": 46}, {"url": "https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html", "anchor_text": "TaPas", "paragraph_index": 47}, {"url": "https://ai.facebook.com/blog/tabert-a-new-model-for-understanding-queries-over-tabular-data/", "anchor_text": "TABERT", "paragraph_index": 47}], "all_paragraphs": ["This post commemorates the first anniversary of the series where we examine advancements in NLP and Graph ML powered by knowledge graphs! \ud83c\udf82 1\ufe0f\u20e3The feedback of the audience drives me to continue, so fasten your seatbelts (and maybe brew some \u2615\ufe0f): in this episode, we are looking at the KG-related ACL 2020 proceedings!", "ACL 2020 went fully virtual this year and I can\u2019t imagine how hard was it for the chairs to organize such an enormous event online catering for multiple time zones and 700+ accepted papers. Thanks to all involved as well as speakers and participants, the conference went smoothly given its size \ud83d\udc4f", "So did something change in the KG & NLP field comparing to ACL 2019? Yes! I would summarize this year\u2019s contribution as:", "Knowledge graphs demonstrate better capabilities to reveal higher-order interdependencies in otherwise unstructured data", "In this context, questions are sent against structured sources like SPARQL-based KGs or SQL databases (other query languages are not that prominent).This year, we can observe increasing efforts incorporating complex (also known as multi-hop) questions.", "For example, Saxena et al tackle the problem of complex KGQA coupling KG embeddings with a question embedding vector in their EmbedKGQA. 1\ufe0f\u20e3 First, an underlying KG is embedded with some algorithm (the authors choose ComplEx), so that each entity and relation is associated with a specific vector. In some cases, the authors freeze them, or keep fine-tuning depending on the KG size. 2\ufe0f\u20e3 The input question is encoded via RoBERTA ([CLS] token from the last layer) and passed through 4 FC layers (well, if you ask why exactly 4, I don\u2019t have an answer, looks like a magic number \ud83e\uddd9\u200d\u2642\ufe0f) that are supposed to project the question into the complex space. 3\ufe0f\u20e3 The crucial part happens in scoring where the authors adopt the KG embeddings framework and build a triple (head entity, question, candidate entity). The scoring function is the same as in ComplEx. Here, the head is the main entity in the question, the question itself is considered a relation (might seem a bit stretched though), and candidate entities are either all entities in the KG (if small) or 2-hop subgraph around the head (when pruning is required). Yes, it does resemble a typical 1-N scoring mechanism used for training KGE algorithms. \u2702\ufe0f A candidate space can be further pruned by computing and thresholding a dot product (h_q, h_r) between the question embedding h_q and each relation embedding h_r .", "\ud83e\uddea In the experiments performed over MetaQA and WebQuestionsSP, the authors probe an additional scenario of an incomplete KG randomly removing 50% of the edges, so that the system has to learn to infer such missing links. In the full scenario, EmbedKGQA performs on par with PullNet (slightly better on 3-hop questions) and 10\u201340% better in absolute Hits@1 scores compared to baselines when not using additional text to augment the KG. \ud83d\udc4f Still, it is interesting to check how EmbedKGQA would process questions that require aggregations or have several grounded entities \ud83e\udd14.", "Somewhat on the other side of the spectrum, Lan et al propose to use iterative an RL-based query generation approach (KG embeddings-free \ud83d\ude09 ). Based on the topic entity (obtained via some entity linker, the authors resort to Google KG API for linking to Freebase), there is a set of 3 operations, namely, extend, connect, and aggregate that are applied to the seed entity building a query pattern. Naturally, the operations allow for complex multi-hop patterns with min/max aggregations. \ud83d\udd26 At each step, the authors use beam search to keep K best patterns and rank them by deriving a 7d feature vector followed by a feed-forward net with a softmax. \ud83e\udd28 You\u2019d ask: \u201cWait, but where is BERT? Everybody\u2019s using BERT now!\u201d. Well, no panic, here you go: the surface forms of entities and relations participating in a query graph are linearized, concatenated with the input question and fed into BERT to get a [CLS] representation of the last layer (which is one of 7d features).", "\ud83c\udf21 The authors evaluate the approach on ComplexWebQuestions, WebQuestionsSP, and ComplexQuestions (looks a bit tailored to Freebase, isn\u2019t it?), and find a noticeable improvement over the baselines. Further ablations show the importance of the 3 chosen operators. And here is the cliffhanger: it is a short paper! \ud83d\udc40 I would recommend this paper as a good example of a short paper that conveys the main idea, shows experiments, and demonstrates the validity of the approach with ablations \ud83d\udc4d", "\ud83d\udcd1 Structured QA also includes semantic parsing over SQL tables, and many new complex datasets drive the research in SQLandia.", "Among others, I\u2019d outline Wang et al and their RAT-SQL (relation-aware transformer, no \ud83d\udc00) where they define explicit edges between columns and tables to encode the database schema. The authors also define an initial schema and value linking to obtain candidate columns and tables. Further, columns, tables, and question tokens are jointly passed through modified self-attention layers. \u27a1\ufe0f Finally, the tree-structured decoder builds an SQL query. RAT-SQL shows drastic improvements in Spider with bigger gains when using BERT for initial embeddings of tokens \ud83d\udcc8", "Often, interacting with a semantic parsing system you\u2019d want to fix small problems on the fly, or point \ud83d\udd34 the parser at its mistake. Elgohary et al tackle this exact problem and propose SPLASH, a dataset for correcting SQL parses with natural language feedback. The correction scenario is different from conversational text2SQL tasks, so even recent SOTA models like EditSQL exhibit a large gap in the correction task compared to human annotators, i.e., 25% against 81%. That\u2019s quite a gap \ud83d\udc40. Along the same dimension, Zeng et al develop Photon which is fully-fledged text-to-SQL system capable of performing some query correction as well \ud83d\ude09", "Hyperbolic spaces are among recent hot topics in ML. Refraining from \ud83e\udd2f, in simpler terms, in a hyperbolic space \ud83d\udd2e (thanks to its properties) you can represent hierarchies and tree-like structures more efficiently and at the same time use fewer dimensions!", "With that motivation, Chami et al propose AttH, a hyperbolic KG embedding algorithm that uses rotations, reflections, and translations to model logical and hierarchical patterns in the KG. Att comes from the hyperbolic attention that is applied to rotated and reflected vectors. \ud83c\udfa9The trick to bypass shaky Riemannian optimization is to use tangent spaces, to which every point of the d-dimensional Poincare ball can be mapped. In this obviously non-trivial setup, each relation is associated not only to one vector, but also to the parameters that describe relation-specific reflections and rotations. Still, in real-world KGs R << V , so the overhead is not that big.", "\u2696\ufe0f In the experiments, AttH performs especially well on WN18RR and Yago 3\u201310 that exhibit some hierarchical structure, with lesser margins on FB15k-237. More importantly, just 32-dimensional AttH shows huge margins compared to 32-d models in real and complex planes. Moreover, 32-d scores are just 0.02\u20130.03 MRR points smaller than SOTA 500-d embedding models on WN18RR and FB15k-237. Ablation studies demonstrate the importance of having learnable curvatures whereas its closest match, MurP, has them fixed.", "Another growing trend in graph representation learning is to go beyond simple KGs consisting of triples and learn representations for more complex, hyper-relational KGs (as coined in the work by Rosso et al), when every triple might have a set of key-value attribute pairs that give fine-grained details on the validity of the triple in various contexts. In fact, Wikidata adopts the hyper-relational model in its Wikidata Statement Model where attributes are called qualifiers. It\u2019s important not to mix the model with n-ary facts (that generate redundant predicates) and hypergraphs. That is, if you work with Wikidata only on the triple level, you lose a good half of the content \ud83d\ude03", "Guan et al do not want to lose a good half of Wikidata and propose NeuInfer, an approach for learning embeddings of hyper-relational KGs (their previous work, NaLP, was more suited to n-ary facts).", "The idea of NeuInfer is to compute a validity and compatibility score of a hyper-relational fact (cf the illustration). First, (h,r,t) embeddings are fed into a fully-connected net (FCN) to estimate the likelihood of this triple (validity). Second, for each key-value pair a quintuple (h,r,t,k,v) is constructed and passed through another set of FCNs. Having m pairs, m vectors are min-pooled and the result represents the compatibility score, i.e., how well those qualifiers live with the main triple. Finally, the authors use a weighted sum of two scores to get the final score.", "The authors evaluate NeuInfer on standard benchmarks JF17K (extracted from Freebase) and WikiPeople (from Wikidata) and report significant improvement in JF17K compared to NaLP when predicting heads, tails, and attribute values. \ud83d\udcc8 I would encourage the authors to compare their numbers with HINGE (from Rosso et al) as both approaches are conceptually similar.", "\ud83d\udca1 And now we need to talk. We need to talk about reproducibility of KG embedding algorithms published even at top conferences like ACL 2019. Sun, Vashishth, Sanyal et al find that several recent KGE models that reported SOTA results (drastically better than existing baselines) suffer from test set leakages, or have unusually many zeroified neurons after ReLU activations scoring valid triples \u2622\ufe0f. Further, they show that their performance metric scores (like Hits@K and MRR) depend on the position of the valid triple among sampled negatives (which should not happen, actually). On the other hand, existing strong baselines perform exactly the same despite any position. The take-away message is to use the evaluation protocol that places a valid triple at a random position among negatives.", "[start of a shameless self-promotion \ud83d\ude0a] Well, our team also has something to say about this issue: in our new paper \u201cBringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework\u201d we performed 65K+ experiments and spent 21K+ GPU hours evaluating 19 models spanning from RESCAL first published in 2011 to the late 2019\u2019s RotatE and TuckER, 5 loss functions, various training strategies with/without negative sampling, and many more hyper-parameters that turn out to be important to consider. We are also releasing the best found hyperparameters for all the models for you folks and our beloved community \ud83e\udd17. In addition, we are releasing PyKEEN 1.0, a PyTorch library for training and benchmarking KG embeddings models! [end of the self-promotion]", "\ud83d\udd25 Several other works I\u2019d encourage you to read thoroughly: Sachan studies the problem of compression of KG entity embeddings by discretization, e.g., \u201cBarack Obama\u201d instead of a 200d float32 vector would be encoded as \u201c2\u20131\u20133\u20133\u201d and \u201cMichelle Obama\u201d as \u201c2\u20131\u20133\u20132\u201d.", "That is, you only need a D-long vector of K values (here, D=4, K=3). For discretization, tempered softmax is found to work better. And as a reverse function from the KD code back to N-dimensional vector of floats the author suggests using a simple Bi-LSTM. Experimental results are astonishing \ud83d\udc40 \u2014 compression rates for FB15k-237 and WN18RR reach 100\u20131000x with a negligible (max 2% MRR) performance drop and computation overhead at inference time (when a KD code has to be decoded back). \ud83e\udd14 I suggest we all sit down for a minute and re-think our KGE pipelines (especially in production scenarios). For example, 200d embeddings of 78M Wikidata entities obtained via PyTorch-BigGraph require \ud83d\udc49 110 GB \ud83d\udc48 of space. Just imagine what would be possible with a gentle 100x compression \ud83d\ude0f.", "\u2795 There is also a lineup of works that improve popular KGE models:* Tang et al generalize RotatE from 2D rotations to high-dimensional spaces with orthogonal relation transforms which works better for 1-N and N-N relations.* Xu et al generalize bilinear models to multi-linear by chunking dense vectors in K parts. It is then shown that if K=1 the approach is equal to DistMult, if K=2 the approach reduces to ComplEx and HolE, and the authors experiment with K=4 and K=8.* Xie et al extend ConvE by replacing standard conv filters with those from the Inception network famous in the Computer Vision domain.* Nguyen et al apply a self-attention style encoder and a CNN decoder for triple classification and search personalization tasks.", "As KGs (and structured data in general) become widely adopted in NLP, in 2020, we can observe a surge of natural language generation (NLG) approaches that take a set of RDF triples / an AMR graph / a set of table cells and produce a coherent human-readable text like description or question.", "By the way, current RDF-to-text approaches are only evaluated on WebNLG 2017 data, but there is a new run of the challenge, WebNLG 2020! \ud83c\udf89 If you are into NLG, be sure to participate \ud83d\ude09", "Well, the NLG trend of the year is exhaustively summarized in one tweet:", "Sophisticated planners and executors? Some structural alignment? NO \ud83d\ude05. Just spin up your favorite pre-trained LM.", "\ud83e\udd14 In fact, plugging in a pre-trained LM and feeding it a few examples indeed works. Chen et al demonstrate this phenomenon on tables coupled with the GPT-2 decoder. That is, table cells are first passed through a learnable LSTM encoder to get a hidden state for the copy mechanism. On the other hand, the text goes into GPT-2 with frozen weights. \u270d\ufe0fThe copy mechanism on top helps to retain rare tokens from table cells. The experiments on WikiBio show that as few as 200 training examples are enough to generate texts much better than sophisticated strong baselines. Guess how many would GPT-3 need \ud83d\ude0f", "Continuing with tables, Chen et al build a new dataset, LogicNLG, that requires using additional logic on top of standard text generation. For example (cf the illustration), some comparative and counting operations are needed to include parts like \u201c1 more gold medal\u201d or \u201cmost gold medals\u201d which make the text much more natural and lively \ud83c\udf3c. Baseline models for the dataset use pre-traind GPT-2 and BERT, but looks like there is still some room for LMs to improve.", "In the graph-to-text domain, Song et al apply a slightly modified Transformer encoder that explicitly processes relation surface forms. The input is just a linearized graph (which you can build, say, with DFS). The decoder is kept intact though. \ud83c\udfa9 The key component of the approach is to add (along with the standard LM loss) two autoencoding losses that are designed to better capture structure of the verbalized graphs. The first loss reconstructs triple relations, whereas another reconstructs nodes and edge labels of linearized input graphs. \ud83e\uddea Experiments conducted on AMR and RDF graphs (WebNLG) suggest that just adding those two losses could yield about 2 BLEU points.", "\ud83d\uddd2 At this point I should make a small remark that everybody should stop using BLEU for evaluating NLG quality (one of the best ACL\u201920 paper nominees, I\u2019d trust them). The organizers of WebNLG 2020 pretty much share this opinion as they officially measure chrF++ and BertScore in addition to the classic (or shall we say outdated?) metrics. Furhermore, here at ACL\u201920 a new metric, BLEURT, was proposed and shown to better correlate with human judgement. Let\u2019s invest into those new evaluation metrics and let ol\u2019 BLEU go for some rest \ud83c\udfd6", "\ud83c\udf33 Still, there is a vivid life in the world without (or at least not that much of) transformers! Applied to the graph-to-text task, Zhao et al propose DualEnc, an encoder-planner-decoder model. 1\ufe0f\u20e3 First, the input graph is pre-processed to transform a relation into an explicit node. Hence, there are several induced labeled edges s->p, p->s, p->o, o->p. The graph is then encoded via R-GCN to obtain entity and relation embeddings. 2\ufe0f\u20e3 The same graph is encoded through another R-GCN with additional features showing whether a relation has been used already or not. The plan is constructed in the following manner: while there are unvisited relations, softmax selects the most probable relation, the relation is then appended to the plan. Once the sequence is ready, it is extended with subjects and objects of the those relations. Finally, the resulting sequence is encoded via LSTM. Both graph encoding and plan encoding are fed into the decoder to generate the output.\ud83d\udccf The experiments show: 1) that DualEnc shows very good generalization on the unseen test set in plan building, 2) text generation quality outperforms straightly applied transformers, 3) great speedup of the planning stage, i.e., 2019 SOTA needs 250 seconds to solve one 7-triple instance, whereas DualEnc solves all 4928 examples in 10 seconds \ud83d\ude80", "Finally, let\u2019s move from data-to-text to summarization. In the domain of abstractive summarization Huang et al employ KGs built from a document to enhance the generation procedure in their ASGARD approach.", "Concretely, the encoder consists of two parts. 1\ufe0f\u20e3 RoBERTa is used to encode input paragraphs. The final layer embeddings are fed into a BiLSTM to obtain hidden states. 2\ufe0f\u20e3 OpenIE is used to extract triples and induce a graph from the input document. Tokens of relations are transformed into explicit nodes similar to DualEnc, and initial node states are taken from the BiLSTM from step 1. Graph Attention Net (GAT) is then used to update the node states with a readout function to get a graph context vector. 3\ufe0f\u20e3 The generation process is conditioned by both vectors obtained at steps 1 and 2.", "\ud83e\uddd9\u200d\u2642\ufe0fSome magic happens in training: ASGARD resorts to reinforcement learning where thee reward function depends on ROUGE and cloze score. The cloze part consists in extracting OpenIE graphs from human-written summaries and generating cloze-style questions based on them for the system to better learn the meaning of the summarized document. \ud83d\udce6 So you kinda have a QA model inside! The authors generate 1M+ cloze questions for CNN and NYT datasets. \ud83d\udccf Experiments report improvements over previous baselines! However, the unanimous winner is a pre-trained BART fine-tuned on the target dataset \ud83d\ude05 Well, looks like the \u201cTPUs go brrr\u201d strategy works here, too.", "In the ConvAI domain, I am a bit biased towards goal-oriented systems as KGs and structured data naturally extend their capabilities.", "\ud83d\udd25 First, Campagna et al propose a method to synthesize goal-oriented dialogues as additional training data for the dialogue state tracking (DST) task. The authors create an abstract model (one could name it an ontology, too) that defines basic states, actions, and transitions. Why this is cool: 1\ufe0f\u20e3 the model can be applied to various domains like restaurant booking or train connection search with any slots and values; 2\ufe0f\u20e3 synthesized data allows for a zero-shot transfer in domains where you have very limited supervised data. 3\ufe0f\u20e3 In fact, the experiments show that using only the synthesized corpus for training (and evaluating on the real MultiWoz 2.1 test) reaches about 2/3 of the accuracy of the original full training set \ud83d\udcaa I believe the method could be used as a general data augmentation practice in developing dialogue systems in specific domains or where annotated training data is limited.", "Focusing on relation extraction in dialogues, Yu et al develop DialogRE, a new dataset comprised of 36 relations taken from about 2k dialogues from Friends. Although the relations are not annotated with Wikidata or DBpedia URIs, the dataset still poses a considerable challenge even for BERT. Furthermore, the authors propose a new metric that shows how many turns a system needs to extract a certain relation.", "OpenDialKG was one of the best paper award nominees at ACL 2019 for the efforts promoting graph-based reasoning in dialogue systems in a new dataset. Zhou et al did a great job adopting the main ideas of OpenDialKG in their new KdConv dataset suited for Chinese \ud83d\udc4f", "There is also a lineup of works studying how to incorporate external knowledge in end-to-end dialogue systems. If your background knowledge is expressed as textual triples or table cells (or even plain text), Lin et al suggest using Transformer as knowledge encoder, while Qin et al rely on memory network-like encoder. If you have a commonsense KG like ConceptNet, Zhang et al extract concepts from utterances to build a local graph and then employ a GNN encoder to encode the \u201ccentral concept\u201d of the conversation which will affect the decoder. If you are interested in even more recent ConvAI goodies, be sure to check the proceedings of the NLP for ConvAI workshop co-located (well, virtually) with ACL!", "If your work happens to be about building KGs from raw text documents, probably you already know that OpenIE is a de-facto standard to start from. As we have seen in previous sections, rule-based frameworks like OpenIE4 or OpenIE 5 are still actively used. That is, increasing the quality of OpenIE extractions could alleviate many problems in KG construction. A small note: KGs obtained after Open IE are also called Open KGs.", "Kolluru et al propose a generative OpenIE approach, IMoJIE (Iterative Memory-based Joint Information Extraction). Inspired by the CopyAttention paradigm, the authors propose an iterative generative seq2seq IE algorithm: at each iteration, the original sequence is concatenated with previous extractions and passed through BERT to obtain final embeddings. The LSTM decoder with copy and attention mechanisms is then tasked to generate a new extraction (tokens that would comprise a triple, for instance). \ud83e\udd16To further improve the training set, the authors aggregate and rank outputs of OpenIE 3, OpenIE 4, and other systems as \u201csilver labels\u201d for generation. Although the architecture looks pretty simple, it does bring significant improvements \ud83d\udcc8 compared to existing baselines. The ablation study reports that BERT is pretty crucial for the overall quality, so I\u2019d hypothesize that quality could be further improved plugging in a bigger Transformer, or using a domain-specific pre-trained LM, e.g., if your text is from legal or biomed field.", "While link prediction (LP) on RDF-like KGs has an established track record and several milestones, we could not say the same about LP on open KGs.", "But now we can! \ud83e\udd29 Broscheit et al define the task of open link prediction given the challenges of open KGs:", "The authors propose a methodology how to build and clean the dataset, an evaluation protocol, and the benchmark itself. The OLPBench is one of the largest datasets for LP with KG embeddings: it contains 30M triples, 1M distinct open relations, and 2.5M mentions of 800K unique entities \ud83d\udc40. For experiments, the authors use ComplEx, where multi-token mentions are aggregated via LSTM. The open LP task turns out to be very difficult \ud83d\ude2f: even mighty 768d ComplEx yields only 3.6 MRR, 2 Hits@1, and 6.6 Hits@10. Clearly, this is a very challenging dataset: it is quite interesting to see the approaches that would be not only scalable to such a large graph but also increase the performance to FB15k-237-like numbers (FYI, currently it is about 35 MRR points and 55 Hits@10).", "By the way, if you are further interested in building KGs from text, I\u2019d encourage you to check the proceedings of the recent AKBC 2020 conference which attracted a great lineup of speakers and publications \ud83d\udc4f", "This year at ACL\u201920 we see less KG-augmented language models (but do have a look at TaPas and TABERT that were designed to work over tables) and maybe a bit less of NER. On the other hand, graph-to-text NLG is on the rise!Still, you made it to the end, dear reader, and you deserve some applause :)", "Let me know in the comments what you liked and what is to be improved. Thanks for reading and stay tuned for more publications \ud83d\ude0c", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Research Scientist @ Intel Labs. Working on Graph ML, Geometric DL, and Knowledge Graphs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Febb1f0a6e0b1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://mgalkin.medium.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": ""}, {"url": "https://mgalkin.medium.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Michael Galkin"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----ebb1f0a6e0b1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.412.pdf", "anchor_text": "Saxena et al"}, {"url": "http://proceedings.mlr.press/v48/trouillon16.pdf", "anchor_text": "ComplEx"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.91.pdf", "anchor_text": "Lan et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.91.pdf", "anchor_text": "Lan et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.677.pdf", "anchor_text": "Wang et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.677.pdf", "anchor_text": "Wang et al"}, {"url": "https://yale-lily.github.io/spider", "anchor_text": "Spider"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.187.pdf", "anchor_text": "Elgohary et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.187.pdf", "anchor_text": "Elgohary et al"}, {"url": "https://github.com/MSR-LIT/Splash", "anchor_text": "SPLASH"}, {"url": "https://www.aclweb.org/anthology/2020.acl-demos.24.pdf", "anchor_text": "Zeng et al"}, {"url": "http://www.naturalsql.com/", "anchor_text": "Photon"}, {"url": "https://medium.com/mlreview/machine-learning-on-graphs-neurips-2019-875eecd41069", "anchor_text": "recent hot"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.617.pdf", "anchor_text": "Chami et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.617.pdf", "anchor_text": "Chami et al"}, {"url": "https://papers.nips.cc/paper/8696-multi-relational-poincare-graph-embeddings.pdf", "anchor_text": "MurP"}, {"url": "https://exascale.info/assets/pdf/rosso2020www.pdf", "anchor_text": "Rosso et al"}, {"url": "https://exascale.info/assets/pdf/rosso2020www.pdf", "anchor_text": "Rosso et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.546.pdf", "anchor_text": "Guan et al"}, {"url": "https://www.researchgate.net/profile/Saiping_Guan/publication/333060086_Link_Prediction_on_N-ary_Relational_Data/links/5cdda342299bf14d959f3863/Link-Prediction-on-N-ary-Relational-Data.pdf", "anchor_text": "NaLP"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.546.pdf", "anchor_text": "Guan et al"}, {"url": "https://exascale.info/assets/pdf/rosso2020www.pdf", "anchor_text": "Rosso et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.489.pdf", "anchor_text": "Sun, Vashishth, Sanyal et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.489.pdf", "anchor_text": "Sun, Vashishth, Sanyal et al"}, {"url": "https://arxiv.org/pdf/2006.13365", "anchor_text": "\u201cBringing Light Into the Dark: A Large-scale Evaluation of Knowledge Graph Embedding Models Under a Unified Framework\u201d"}, {"url": "https://github.com/pykeen/pykeen", "anchor_text": "PyKEEN 1.0"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.238.pdf", "anchor_text": "Sachan"}, {"url": "https://github.com/facebookresearch/PyTorch-BigGraph", "anchor_text": "PyTorch-BigGraph"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.241.pdf", "anchor_text": "Tang et al"}, {"url": "https://arxiv.org/pdf/1902.10197.pdf", "anchor_text": "RotatE"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.358.pdf", "anchor_text": "Xu et al"}, {"url": "https://arxiv.org/pdf/1412.6575.pdf", "anchor_text": "DistMult"}, {"url": "https://arxiv.org/pdf/1606.06357.pdf", "anchor_text": "ComplEx"}, {"url": "https://arxiv.org/pdf/1510.04935.pdf", "anchor_text": "HolE"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.526.pdf", "anchor_text": "Xie et al"}, {"url": "https://arxiv.org/pdf/1512.00567.pdf", "anchor_text": "Inception"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.313.pdf", "anchor_text": "Nguyen et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.224.pdf", "anchor_text": "Zhao et al"}, {"url": "https://webnlg-challenge.loria.fr/challenge_2020/", "anchor_text": "WebNLG 2020"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.18.pdf", "anchor_text": "Chen et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.18.pdf", "anchor_text": "Chen et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.708.pdf", "anchor_text": "Chen et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.708.pdf", "anchor_text": "Chen et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.712.pdf", "anchor_text": "Song et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.712.pdf", "anchor_text": "Song et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.448.pdf", "anchor_text": "everybody should stop using BLEU for evaluating NLG quality"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.704.pdf", "anchor_text": "BLEURT"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.224.pdf", "anchor_text": "Zhao et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.224.pdf", "anchor_text": "Zhao et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.457.pdf", "anchor_text": "Huang et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.12.pdf", "anchor_text": "Campagna et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.12.pdf", "anchor_text": "Campagna et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.444.pdf", "anchor_text": "Yu et al"}, {"url": "https://pdfs.semanticscholar.org/0d3c/68c207fc83fb402b7217811af22066300fc9.pdf", "anchor_text": "OpenDialKG"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.635.pdf", "anchor_text": "Zhou et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.6.pdf", "anchor_text": "Lin et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.565.pdf", "anchor_text": "Qin et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.184.pdf", "anchor_text": "Zhang et al"}, {"url": "https://sites.google.com/view/2ndnlp4convai/home", "anchor_text": "NLP for ConvAI"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.521.pdf", "anchor_text": "Kolluru et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.521.pdf", "anchor_text": "Kolluru et al"}, {"url": "https://www.aclweb.org/anthology/2020.acl-main.209.pdf", "anchor_text": "Broscheit et al"}, {"url": "https://www.akbc.ws/2020/papers/", "anchor_text": "AKBC 2020"}, {"url": "https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html", "anchor_text": "TaPas"}, {"url": "https://ai.facebook.com/blog/tabert-a-new-model-for-understanding-queries-over-tabular-data/", "anchor_text": "TABERT"}, {"url": "https://medium.com/tag/nlp?source=post_page-----ebb1f0a6e0b1---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/knowledge-graph?source=post_page-----ebb1f0a6e0b1---------------knowledge_graph-----------------", "anchor_text": "Knowledge Graph"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ebb1f0a6e0b1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ebb1f0a6e0b1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----ebb1f0a6e0b1---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----ebb1f0a6e0b1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----ebb1f0a6e0b1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Febb1f0a6e0b1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ebb1f0a6e0b1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ebb1f0a6e0b1--------------------------------", "anchor_text": ""}, {"url": "https://mgalkin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://mgalkin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Galkin"}, {"url": "https://mgalkin.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3697bb9022&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fknowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&newsletterV3=4d4f8ddd1e68&newsletterV3Id=c3697bb9022&user=Michael+Galkin&userId=4d4f8ddd1e68&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}