{"url": "https://towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646", "time": 1683013379.8461719, "path": "towardsdatascience.com/fine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646/", "webpage": {"metadata": {"title": "High accuracy text classification with Python | Towards Data Science", "h1": "High accuracy text classification with Python", "description": "High accuracy of text classification can be achieved with fine-tuning the best NLP models like BERT. Several methods to increase the accuracy are listed."}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "GPT 3", "paragraph_index": 0}, {"url": "https://openai.com/blog/language-unsupervised/", "anchor_text": "pre-training", "paragraph_index": 0}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://www.datacamp.com/community/tutorials/transfer-learning?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9071353&gclid=EAIaIQobChMIrpSllo2n6wIVgsEWBR1QXwx2EAAYASAAEgJ2ZfD_BwE", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT", "paragraph_index": 1}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa", "paragraph_index": 1}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face", "paragraph_index": 1}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "transformer architecture", "paragraph_index": 1}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/", "anchor_text": "Attention", "paragraph_index": 1}, {"url": "https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e", "anchor_text": "aligned vectors", "paragraph_index": 1}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html", "anchor_text": "cosine similarity", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "fine-tuning BERT", "paragraph_index": 2}, {"url": "https://www.kaggle.com/nopdev/real-and-fake-news-dataset", "anchor_text": "Fake News Dataset", "paragraph_index": 2}, {"url": "https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20for%20text%20classification.ipynb", "anchor_text": "code", "paragraph_index": 2}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "paper", "paragraph_index": 4}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface team", "paragraph_index": 14}, {"url": "https://www.linkedin.com/in/alexander-kozlov-65a039a5/", "anchor_text": "https://www.linkedin.com/in/alexander-kozlov-65a039a5/", "paragraph_index": 17}], "all_paragraphs": ["As of the time of writing this piece, state-of-the-art results on NLP and NLU tasks are obtained with Transformer models. There is a trend of performance improvement as models become deeper and larger, GPT 3 comes to mind. Training small versions of such models from scratch takes a significant amount of time, even with GPU. This problem can be solved via pre-training when a model is trained on a large text corpus using a high-performance cluster. Later it can be fine-tuned for a specific task in a much shorter amount of time. During fine tuning stage, additional layers can be added to the model for specific tasks, which can be different from those for which the model was initially trained. This technique is related to transfer learning, a concept applied to areas of machine learning beyond NLP (see here and here for a quick intro).", "In this post, I would like to share my experience of fine-tuning BERT and RoBERTa, available from the transformers library by Hugging Face, for a document classification task. Both models share a transformer architecture, which consists of at least two distinct blocks \u2014 encoder and decoder. Both encoder and decoder consist of multiple layers based around Attention mechanism. Encoder processed the input token sequence into a vector of floating point numbers \u2014 a hidden state, which is picked up by the decoder. It is the hidden state that encompasses the information content of the input sequence. This enables to represent an entire sequence of tokens with a single dense vector of float point numbers. Two texts or documents, which have similar meaning are represented by closely aligned vectors. Comparing vectors using a metric of choice, for example, cosine similarity, enables to quantify the similarity of original text pieces.", "While researching this topic I found an article on fine-tuning BERT to classify Fake News Dataset, available on Kaggle. Running the code from the article as is yields F1 score that is considerably lower than the claimed 96.99%. After a weekend of reading and adding a few things I have managed to squeeze out 99.05% F1 score on a test set for RoBERTa model with two additional Linear layers (code).", "To start, let's have a brief look at the Fake News dataset. It consists of 6299 items with text exceeding 5 words, 3128 fake and 3171 real. The plot below shows the histogram of the text length distribution, cropped at 5000 tokens. Documents with a token count as high as 20000 are present in the dataset.", "Batch size and sequence length trade-off. Both BERT and RoBERTa are limited to 512 token sequences in their base configuration. GPU memory limitations can further reduce the maximum sequence length. It is possible to trade batch size for sequence length. In \u201cLanguage Models are Few-Shot Learners\u201d paper authors mention the benefit of higher batch size at later stages of training. As fine-tuning picks up where pre-training ends, higher batch size lead to better results and somewhat reduce over-fitting.", "Sequences in a training batch can have different lengths. This requires appending padding tokens to each sequence to get them to a common length. It can be done using a dedicated tokenizer, generously supplied by Hugging Face with corresponding models:", "Torchtext library provides several easy to use Swiss army knife iterators. Among other things, they are capable of grouping sequences of similar lengths into batches and padding them, splitting a dataset into train, validation and test sets, stratified if necessary, shuffling after every epoch.", "Attention mask for batch training. A padded training batch is passed to RoBERTa, which outputs a batch of hidden state vectors, one per training batch sequence. Padding indexes do not represent any useful information. The end of each sequence in a batch is denoted by a special end-of-string token. Indeed, a batch of size 1 does not require any padding at all. The padding indexes, therefore, should be excluded from attention weights calculation. This is achieved with the help of an attention mask tensor:", "The mask tensor has values of 0 (False) for padding tokens and 1 (True) for all other tokens. It is applied via an element-wise multiplication to the Key input of an Attention layer, which reduces to 0 contributions from padding tokens. Attention mask should also be applied during validation and testing if it is performed in batches.", "Pre-fine-tuning and learning rate. During training the output of RoBERTa is a batch of hidden states, which is passed to classifier layers:", "When the above model is initialised, RoBERTa is assigned pre-trained parameters. For this reason, fine-tuning should be performed with a small learning rate, of the order of 1e-5. However, the classifier layers are assigned random untrained values of their parameters. For this reason, I ran a few training epochs with frozen RoBERTa parameters and higher learning rate of 1e-4, while adjusting only classifier layer parameters. Next, the whole model is trained with all parameters updated at the same time.", "In this final step of the training, a linear learning rate scheduler, which updates the optimiser learning rate at each training step, turned out to be quite beneficial. During the first two epochs optimiser is warming up \u2014 the learning rate increases to its maximum value of 2e-6, which enables the model to explore local parameter space. In the following epochs, the learning rate is gradually reduced to zero.", "Huggingface library provides out-of-the-box sequence classifiers. These models have a name ending with \u201cForSequenceClassification\u201d, which speaks for itself. It is the same model as above, but with a single Linear layer, preceded by a Dropout. I trained my models with BERT and RoBERTa, as well as two out of the box models, \u201cBertForSequenceClassification\u201d and \u201cRobertaForSequenceClassification\u201d. For all BERT models cased configuration was used. Below table shows a brief summary of results on a test set.", "Training set contained 70% of the data (4410 items), 10% (629 items) in validation set and 20% (1260 items) in test set. It may seem that RoBERTa performed only marginally better. However, \u201cROBERTAClassifier\u201d was wrong almost 3 times less often, 1% of the test samples, than \u201cBERTClassifier\u201d, which got it wrong almost 3% of the time.", "In summary, an exceptionally good accuracy for text classification, 99% in this example, can be achieved by fine-tuning the state-of-the-art models. For the latter, a shout-out goes to Huggingface team!", "As is, all models read only first 256 tokens. To paraphrase an old proverb, it may not be very accurate to judge a news piece by the first few hundred tokens, including title. An obvious way to improve the result is to get the model to read some more of the text. One way to overcome the text size limitation is to split a text into chunks of manageable length. Encoding several chunks with RoBERTa will yield an array of hidden states, which together contain more information about the text than just a single first chunk. To combine hidden states into a single vector one can use a range of techniques, such as a simple averaging or an RNN cell. The resulting aggregated vector can be passed to subsequent layers. Such a model can potentially make a more informed decision on whenever the news piece is fake or real.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Researcher and Data Scientist at the University of Melbourne. Theoretical Physics PhD. https://www.linkedin.com/in/alexander-kozlov-65a039a5/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc9e63cf64646&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@alexx.kozloff?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexx.kozloff?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Alexander Kozlov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b9790be23db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&user=Alexander+Kozlov&userId=8b9790be23db&source=post_page-8b9790be23db----c9e63cf64646---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com", "anchor_text": "pixabay.com"}, {"url": "https://arxiv.org/abs/2005.14165", "anchor_text": "GPT 3"}, {"url": "https://openai.com/blog/language-unsupervised/", "anchor_text": "pre-training"}, {"url": "https://machinelearningmastery.com/transfer-learning-for-deep-learning/", "anchor_text": "here"}, {"url": "https://www.datacamp.com/community/tutorials/transfer-learning?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034349&utm_targetid=dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=9071353&gclid=EAIaIQobChMIrpSllo2n6wIVgsEWBR1QXwx2EAAYASAAEgJ2ZfD_BwE", "anchor_text": "here"}, {"url": "https://arxiv.org/abs/1810.04805", "anchor_text": "BERT"}, {"url": "https://arxiv.org/abs/1907.11692", "anchor_text": "RoBERTa"}, {"url": "https://huggingface.co/", "anchor_text": "Hugging Face"}, {"url": "http://nlp.seas.harvard.edu/2018/04/03/attention.html", "anchor_text": "transformer architecture"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/", "anchor_text": "Attention"}, {"url": "https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e", "anchor_text": "aligned vectors"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html", "anchor_text": "cosine similarity"}, {"url": "https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b", "anchor_text": "fine-tuning BERT"}, {"url": "https://www.kaggle.com/nopdev/real-and-fake-news-dataset", "anchor_text": "Fake News Dataset"}, {"url": "https://github.com/aramakus/ML-and-Data-Analysis/blob/master/RoBERTa%20for%20text%20classification.ipynb", "anchor_text": "code"}, {"url": "https://arxiv.org/pdf/2005.14165.pdf", "anchor_text": "paper"}, {"url": "https://huggingface.co/", "anchor_text": "Huggingface team"}, {"url": "https://medium.com/tag/nlp?source=post_page-----c9e63cf64646---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c9e63cf64646---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c9e63cf64646---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----c9e63cf64646---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/text-classification?source=post_page-----c9e63cf64646---------------text_classification-----------------", "anchor_text": "Text Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&user=Alexander+Kozlov&userId=8b9790be23db&source=-----c9e63cf64646---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&user=Alexander+Kozlov&userId=8b9790be23db&source=-----c9e63cf64646---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc9e63cf64646&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c9e63cf64646---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c9e63cf64646--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c9e63cf64646--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c9e63cf64646--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexx.kozloff?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@alexx.kozloff?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alexander Kozlov"}, {"url": "https://medium.com/@alexx.kozloff/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "39 Followers"}, {"url": "https://www.linkedin.com/in/alexander-kozlov-65a039a5/", "anchor_text": "https://www.linkedin.com/in/alexander-kozlov-65a039a5/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8b9790be23db&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&user=Alexander+Kozlov&userId=8b9790be23db&source=post_page-8b9790be23db--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F58ff3265a8a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-bert-and-roberta-for-high-accuracy-text-classification-in-pytorch-c9e63cf64646&newsletterV3=8b9790be23db&newsletterV3Id=58ff3265a8a4&user=Alexander+Kozlov&userId=8b9790be23db&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}