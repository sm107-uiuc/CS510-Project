{"url": "https://towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc", "time": 1682993618.669813, "path": "towardsdatascience.com/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc/", "webpage": {"metadata": {"title": "A \u201cData Science for Good\u201c Machine Learning Project Walk-Through in Python: Part One | by Will Koehrsen | Towards Data Science", "h1": "A \u201cData Science for Good\u201c Machine Learning Project Walk-Through in Python: Part One", "description": "Data science is an immensely powerful tool in our data-driven world. Call me idealistic, but I believe this tool should be used for more than getting people to click on ads or spend more time\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "the sequel", "paragraph_index": 1}, {"url": "https://www.kdnuggets.com/2015/07/guide-data-science-good.html", "anchor_text": "Data Science for Good", "paragraph_index": 1}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "Jupyter Notebook both on Kaggle", "paragraph_index": 3}, {"url": "https://github.com/WillKoehrsen/data-science-for-good/blob/master/costa-rican-poverty/A%20Complete%20Walkthrough.ipynb", "anchor_text": "on GitHub", "paragraph_index": 3}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction", "anchor_text": "Costa Rican Household Poverty Level Prediction", "paragraph_index": 4}, {"url": "https://www.iadb.org/en", "anchor_text": "Inter-American Development Bank", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420", "anchor_text": "machine learning problem is", "paragraph_index": 6}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "available in the notebooks", "paragraph_index": 8}, {"url": "https://www.kaggle.com/willkoehrsen/featuretools-for-good", "anchor_text": "experimenting with new techniques", "paragraph_index": 8}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "part two", "paragraph_index": 9}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/kernels", "anchor_text": "work shared by other data scientists", "paragraph_index": 11}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion", "anchor_text": "discussion platform", "paragraph_index": 11}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data", "anchor_text": "data definitions", "paragraph_index": 14}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html", "anchor_text": "Macro F1 Score", "paragraph_index": 19}, {"url": "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin", "anchor_text": "\u201cMacro\u201d is one of the averaging options for multi-class problems.", "paragraph_index": 19}, {"url": "https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428", "anchor_text": "appropriate measure", "paragraph_index": 21}, {"url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis", "anchor_text": "Exploratory Data Analysis (EDA)", "paragraph_index": 22}, {"url": "https://towardsdatascience.com/data-cleaning-101-948d22a92e4", "anchor_text": "Data cleaning", "paragraph_index": 22}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Scikit-Learn random forest classifier", "paragraph_index": 24}, {"url": "https://stats.stackexchange.com/a/250742/157316", "anchor_text": "stratified sampling", "paragraph_index": 24}, {"url": "https://pandas.pydata.org/pandas-docs/stable/basics.html#basics-dtypes", "anchor_text": "different column data types", "paragraph_index": 25}, {"url": "https://en.wikipedia.org/wiki/Statistical_data_type", "anchor_text": "different statistical types of data", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96", "anchor_text": "automated feature engineering", "paragraph_index": 38}, {"url": "https://www.kaggle.com/willkoehrsen/featuretools-for-good", "anchor_text": "another notebook with automated feature engineering", "paragraph_index": 38}, {"url": "https://www.featuretools.com/", "anchor_text": "automated feature engineering (with Featuretools)", "paragraph_index": 38}, {"url": "https://en.wikipedia.org/wiki/Collinearity#Usage_in_statistics_and_econometrics", "anchor_text": "called collinear", "paragraph_index": 42}, {"url": "https://medium.com/mindorks/what-is-feature-engineering-for-machine-learning-d8ba3158d97a", "anchor_text": "calculate features one at a time using domain knowledge", "paragraph_index": 43}, {"url": "https://github.com/Featuretools/Automated-Manual-Comparison", "anchor_text": "manual vs automated feature engineering", "paragraph_index": 45}, {"url": "https://github.com/Featuretools/featuretools", "anchor_text": "Featuretools", "paragraph_index": 47}, {"url": "https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219", "anchor_text": "Here\u2019s an article", "paragraph_index": 47}, {"url": "http://scikit-learn.org/stable/modules/feature_selection.html", "anchor_text": "feature selection", "paragraph_index": 48}, {"url": "https://stats.stackexchange.com/a/100272/157316", "anchor_text": "highly correlated with one another are called collinear.", "paragraph_index": 49}, {"url": "https://machinelearningmastery.com/feature-selection-machine-learning-python/", "anchor_text": "methods for feature selection", "paragraph_index": 52}, {"url": "http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html", "anchor_text": "univariate", "paragraph_index": 52}, {"url": "https://github.com/danielhomola/mifs", "anchor_text": "multivariate", "paragraph_index": 52}, {"url": "http://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection", "anchor_text": "model-based feature importances for feature selection", "paragraph_index": 52}, {"url": "https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166", "anchor_text": "pairsplot", "paragraph_index": 53}, {"url": "https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf", "anchor_text": "highly readable paper", "paragraph_index": 58}, {"url": "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/", "anchor_text": "Gradient Tree Boosting", "paragraph_index": 59}, {"url": "https://www.quora.com/What-does-the-No-Free-Lunch-theorem-mean-for-machine-learning-In-what-ways-do-popular-ML-algorithms-overcome-the-limitations-set-by-this-theorem", "anchor_text": "one model is always the best", "paragraph_index": 59}, {"url": "http://scikit-learn.org/", "anchor_text": "Scikit-Learn", "paragraph_index": 60}, {"url": "http://scikit-learn.org/stable/supervised_learning.html", "anchor_text": "many machine learning models", "paragraph_index": 60}, {"url": "https://blog.easysol.net/wp-content/uploads/2017/03/ME-Explained-600x486.png", "anchor_text": "range of complexity", "paragraph_index": 61}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api", "anchor_text": "Light GBM", "paragraph_index": 61}, {"url": "https://www.openml.org/a/estimation-procedures/1", "anchor_text": "cross validation", "paragraph_index": 62}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "model hyperparameter optimization", "paragraph_index": 65}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "second part", "paragraph_index": 66}, {"url": "http://shop.oreilly.com/product/0636920052289.do", "anchor_text": "machine learning", "paragraph_index": 67}, {"url": "https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a", "anchor_text": "we\u2019ve communicated our work", "paragraph_index": 68}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "see part two", "paragraph_index": 69}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "the notebook", "paragraph_index": 69}, {"url": "https://github.com/WillKoehrsen/data-science-for-good/blob/master/costa-rican-poverty/A%20Complete%20Walkthrough.ipynb", "anchor_text": "GitHub", "paragraph_index": 69}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction", "anchor_text": "solving the problem for yourself", "paragraph_index": 69}, {"url": "http://twitter.com/koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 70}], "all_paragraphs": ["Data science is an immensely powerful tool in our data-driven world. Call me idealistic, but I believe this tool should be used for more than getting people to click on ads or spend more time consumed by social media.", "In this article and the sequel, we\u2019ll walk through a complete machine learning project on a \u201cData Science for Good\u201d problem: predicting household poverty in Costa Rica. Not only do we get to improve our data science skills in the most effective manner \u2014 through practice on real-world data \u2014 but we also get the reward of working on a problem with social benefits.", "It turns out the same skills used by companies to maximize ad views can also be used to help relieve human suffering.", "The full code is available as a Jupyter Notebook both on Kaggle (where it can be run in the browser with no downloads required) and on GitHub. This is an active Kaggle competition and a great project to get started with machine learning or to work on some new skills.", "The Costa Rican Household Poverty Level Prediction challenge is a data science for good machine learning competition currently running on Kaggle. The objective is to use individual and household socio-economic indicators to predict poverty on a household basis. IDB, the Inter-American Development Bank, developed the problem and provided the data with the goal of improving upon traditional methods for identifying families at need of aid.", "The poverty labels fall into four levels making this a supervised multi-class classification problem:", "The general approach to a machine learning problem is:", "While these steps may seem to present a rigid structure, the machine learning process is non-linear, with parts repeated multiple times as we get more familiar with the data and see what works. It\u2019s nice to have an outline to provide a general guide, but we\u2019ll often return to earlier parts of the process if things aren\u2019t working out or as we learn more about the problem.", "We\u2019ll go through the first four steps at a high-level in this article, taking a look at some examples, with the full details available in the notebooks. This problem is a great one to tackle both for beginners \u2014 because the dataset is manageable in size \u2014 and for those who already have a firm footing because Kaggle offers an ideal environment for experimenting with new techniques.", "The last two steps, plus an experimental section, can be found in part two.", "In an ideal situation, we\u2019d all be experts in the problem subject with years of experience to inform our machine learning. In reality, we often work with data from a new field and have to rapidly acquire knowledge both of what the data represents and how it was collected.", "Fortunately, on Kaggle, we can use the work shared by other data scientists to get up to speed relatively quickly. Moreover, Kaggle provides a discussion platform where you can ask questions of the competition organizers. While not exactly the same as interacting with customers at a real job, this gives us an opportunity to figure out what the data fields represent and any considerations we should keep in mind as we get into the problem.", "Some good questions to ask at this point are:", "For example, after engaging in discussions with the organizers, the community found out the text string \u201cyes\u201d actually maps to the value 1.0 and that the maximum value in one of the columns should be 5 which can be used to correct outliers. We would have been hard-pressed to find out this information without someone who knows the data collection process!", "Part of data understanding also means digging into the data definitions. The most effective way is literally to go through the columns one at a time, reading the description and making sure you know what the data represents. I find this a little dull, so I like to mix this process with data exploration, reading the column description and then exploring the column with stats and figures.", "For example, we can read that meaneduc is the average amount of education in the family, and then we can plot it distributed by the value of the label to see if it has any noticeable differences between the poverty level .", "This shows that families the least at risk for poverty \u2014 non-vulnerable \u2014 tend to have higher average education levels than those most at risk. Later in feature engineering, we can use this information by building features from the education since it seems to show a different between the target labels.", "There are a total of 143 columns (features), and while for a real application, you want to go through each with an expert, I didn\u2019t exhaustively explore all of these in the notebook. Instead, I read the data definitions and looked at the work of other data scientists to understand most of the columns.", "Another point to establish from the problem and data understanding stage is how we want to structure our training data. In this problem, we\u2019re given a single table of data where each row represents an individual and the columns are the features. If we read the problem definition, we are told to make predictions for each household which means that our final training dataframe (and also testing) should have one row for each house. This point informs our entire pipeline, so it\u2019s crucial to grasp at the outset.", "Finally, we want to make sure we understanding the labels and the metric for the problem. The label is what we want to predict, and the metric is how we\u2019ll evaluate those predictions. For this problem, the label is an integer, from 1 to 4, representing the poverty level of a household. The metric is the Macro F1 Score, a measure between 0 and 1 with a higher value indicating a better model. The F1 score is a common metric for binary classification tasks and \u201cMacro\u201d is one of the averaging options for multi-class problems.", "Once you know the metric, figure out how to calculate it with whatever tool you are using. For Scikit-Learn and the Macro F1 score, the code is:", "Knowing the metric allows us to assess our predictions in cross validation and using a hold-out testing set, so we know what effect, if any, our choices have on performance. For this competition, we are given the metric to use, but in a real-world situation, we\u2019d have to choose an appropriate measure ourselves.", "Data exploration, also called Exploratory Data Analysis (EDA), is an open-ended process where we figure out what our data can tell us. We start broad and gradually hone in our analysis as we discover interesting trends / patterns that can be used for feature engineering or find anomalies. Data cleaning goes hand in hand with exploration because we need to address missing values or anomalies as we find them before we can do modeling.", "For an easy first step of data exploration, we can visualize the distribution of the labels for the training data (we are not given the testing labels).", "Right away this tells us we have an imbalanced classification problem, which can make it difficult for machine learning models to learn the underrepresented classes. Many algorithms have ways to try and deal with this, such as setting class_weight = \"balanced\" in the Scikit-Learn random forest classifier although they don\u2019t work perfectly. We also want to make sure to use stratified sampling with cross validation when we have an imbalanced classification problem to get the same balance of labels in each fold.", "To get familiar with the data, it\u2019s helpful to go through the different column data types which represent different statistical types of data:", "I\u2019m using statistical type to mean what the data represents \u2014 for example a Boolean that can only be 1 or 0 \u2014 and data type to mean the actual way the values are stored in Python such as integers or floats. The statistical type informs how we handle the columns for feature engineering.", "(I specified usually for each data type / statistical type pairing because you may find that statistical types are saved as the wrong data type.)", "If we look at the integer columns for this problem, we can see that most of them represent Booleans because there are only two possible values:", "Going through the object columns, we are presented with a puzzle: 2 of the columns are Id variables (stored as strings), but 3 look to be numeric values.", "This is where our earlier data understanding comes into play. For these three columns, some entries are \u201cyes\u201d and some are \u201cno\u201d while the rest are floats. We did our background research and thus know that a \u201cyes\u201d means 1 and a \u201cno\u201d means 0. Using this information, we can correct the values and then visualize the variable distributions colored by the label.", "This is a great example of data exploration and cleaning going hand in hand. We find something incorrect with the data, fix it, and then explore the data to make sure our correction was appropriate.", "A critical data cleaning operation for this data is handling missing values. To calculate the total and percent of missing values is simple in Pandas:", "In some cases there are reasons for missing values: the v2a1 column represents monthly rent and many of the missing values are because the household owns the home. To figure this out, we can subset the data to houses missing the rent payment and then plot the tipo_ variables (I\u2019m not sure where these column names come from) which show home ownership.", "Based on the plot, the solution is to fill in the missing rent payments for households that own their house with 0 and leave the others to be imputed. We also add a boolean column that indicates if the rent payment was missing.", "The other missing values in the columns are dealt with the same way: using knowledge from other columns or about the problem to fill in the values, or leaving them to be imputed. Adding a boolean column to indicate missing values can also be useful because sometimes the information that a value was missing is important. Another crucial point to note is that for missing values, we often want to think about using information in other columns to fill in missing values such as we did with the rent payment.", "Once we\u2019ve handled the missing values, anomalies, and incorrect data types, we can move on to feature engineering. I usually view data exploration as an ongoing process rather than one set chunk. For example, as we get into feature engineering, we might want to explore the new variables we create.", "The data science process is non-linear: while we have a general outline, we often go back and redo previous steps as we get deeper into the problem.", "If you follow my work, you\u2019ll know I\u2019m convinced automated feature engineering \u2014 with domain expertise \u2014 will take the place of traditional manual feature engineering. For this problem, I took both approaches, doing mostly manual work in the main notebook, and then writing another notebook with automated feature engineering. Not surprisingly, the automated feature engineering took one tenth the time and achieved better performance! Here I\u2019ll show the manual version, but keep in mind that automated feature engineering (with Featuretools) is a great tool to learn.", "In this problem, our primary objective for feature engineering is to aggregate all the individual level data at the household level. That means grouping together the individuals from one house and then calculating statistics such as the maximum age, the average level of education, or the total number of cellphones owned by the family.", "Fortunately, once we have separated out the individual data (into the ind dataframe), doing these aggregations is literally one line in Pandas (with idhogar the household identifier used for grouping):", "After renaming the columns, we have a lot of features that look like:", "The benefit of this method is that it quickly creates many features. One of the drawbacks is that many of these features might not be useful or are highly correlated (called collinear) which is why we need to use feature selection.", "An alternative method to aggregations is to calculate features one at a time using domain knowledge based on what features might be useful for predicting poverty. For example, in the household data, we create a feature called warning which adds up a number of household \u201cwarning signs\u201d ( house is a dataframe of the household variables):", "We can also calculate \u201cper capita\u201d features by dividing one value by another ( tamviv is the number of household members):", "When it comes to manual vs automated feature engineering, I think the optimal answer is a blend of both. As humans, we are limited in the features we build both by creativity \u2014 there are only so many features we can think to make \u2014 and time \u2014 there is only so much time for us to write the code. We can make a few informed features like those above by hand, but where automated feature engineering excels is when doing aggregations that can automatically build on top of other features.", "The best approach is to spend some time creating a few features by hand using domain knowledge, and then hand off the process to automated feature engineering to generate hundreds or thousands more.", "(Featuretools is the most advanced open-source Python library for automated feature engineering. Here\u2019s an article to get you started in about 10 minutes.)", "Once we have exhausted our time or patience making features, we apply feature selection to remove some features, trying to keep only those that are useful for the problem. \u201cUseful\u201d has no set definition, but there are some heuristics (rules of thumb) that we use to select features.", "One method is by determining correlations between features. Two variables that are highly correlated with one another are called collinear. These are a problem in machine learning because they slow down training, create less interpretable models, and can decrease model performance by causing overfitting on the training data.", "The tricky part about removing correlated features is determining the threshold of correlation for saying that two variables are too correlated. I generally try to stay conservative, using a correlation coefficient in the 0.95 or above range. Once we decide on a threshold, we use the below code to remove one out of every pair of variables with a correlation above this value:", "We are only removing features that are correlated with one another. We want features that are correlated with the target(although a correlation of greater than 0.95 with the label would be too good to be true)!", "There are many methods for feature selection (we\u2019ll see another one in the experimental section near the end of the article). These can be univariate \u2014 measuring one variable at a time against the target \u2014 or multivariate \u2014 assessing the effects of multiple features. I also tend to use model-based feature importances for feature selection, such as those from a random forest.", "After feature selection, we can do some exploration of our final set of variables, including making a correlation heatmap and a pairsplot.", "One point we get from the exploration is the relationship between education and poverty: as the education of a household increases (both the average and the maximum), the severity of poverty tends to decreases (1 is most severe):", "On the other hand, as the level of overcrowding \u2014 the number of people per room \u2014 increases, the severity of the poverty increases:", "These are two actionable insights from this competition, even before we get to the machine learning: households with greater levels of education tend to have less severe poverty, and households with more people per room tend to have greater levels of poverty. I like to think about the ramifications and larger picture of a data science project in addition to the technical aspects. It can be easy to get overwhelmed with the details and then forget the overall reason you\u2019re working on this problem.", "The ultimate goal of this project is to figure out how to predict poverty to most effectively get help to those in need.", "The following graph is one of my favorite results in machine learning: it displays the performance of machine learning models on many datasets, with the percentages showing how many times a particular method beat any others. (This is from a highly readable paper by Randal Olson.)", "What this shows is that there are some problems where even a simple Logistic Regression will beat a Random Forest or Gradient Boosting Machine. Although the Gradient Tree Boosting model generally works the best, it\u2019s not a given that it will come out on top. Therefore, when we approach a new problem, the best practice is to try out several different algorithms rather than always relying on the same one. I\u2019ve gotten stuck using the same model (random forest) before, but remember that no one model is always the best.", "Fortunately, with Scikit-Learn, it\u2019s easy to evaluate many machine learning models using the same syntax. While we won\u2019t do hyperparameter tuning for each one, we can compare the models with the default hyperparameters in order to select the most promising model for optimization.", "In the notebook, we try out six models spanning the range of complexity from simple \u2014 Gaussian Naive Bayes \u2014 to complex \u2014 Random Forest and Gradient Boosting Machine. Although Scikit-Learn does have a GBM implementation, it\u2019s fairly slow and a better option is to use one of the dedicated libraries such as XGBoost or LightGBM. For this notebook, I used Light GBM and choose the hyperparameters based on what have worked well in the past.", "To compare models, we calculate the cross validation performance on the training data over 5 or 10 folds. We want to use the training data because the testing data is only meant to be used once as an estimate of the performance of our final model on new data. The following plot shows the model comparison. The height of the bar is the average Macro F1 score over the folds recorded by the model and the black bar is the standard deviation:", "(To see an explanation of the names, refer to the notebook. RF stands for Random Forest and GBM is Gradient Boosting Machine with SEL representing the feature set after feature selection). While this isn\u2019t entirely a level comparison \u2014 I did not use the default hyperparameters for the Gradient Boosting Machine \u2014 the general results hold: the GBM is the best model by a large margin. This reflects the findings of most other data scientists.", "Notice that we cross-validated the data before and after feature selection to see its effect on performance. Machine learning is still largely an empirical field, and the only way to know if a method is effective is to try it out and then measure performance. It\u2019s important to test out different choices for the steps in the pipeline \u2014 such as the correlation threshold for feature selection \u2014 to determine if they help. Keep in mind that we also want to avoid placing too much weight on cross-validation results, because even with many folds, we can still be overfitting to the training data. Finally, even though the GBM was best for this dataset, that will not always be the case!", "Based on these results, we can choose the gradient boosting machine as our model (remember this is a decision we can go back and revise!). Once we decide on a model, the next step is to get the most out of it, a process known as model hyperparameter optimization.", "Recognizing that not everyone has time for a 30-minute article (even on data science) in one sitting, I\u2019ve broken this up into two parts. The second part covers model optimization, interpretation, and an experimental section.", "By this point, we can see how all the different parts of machine learning come together to form a solution: we first had to understand the problem, then we dug into the data, cleaning it as necessary, then we made features for a machine learning model, and finally we evaluated several different models.", "We\u2019ve covered many techniques and have a decent model (although the F1 score is relatively low, it places in the top 50 models submitted to the competition). Nonetheless, we still have a few steps left: through optimization, we can improve our model, and then we have to interpret our results because no analysis is complete until we\u2019ve communicated our work.", "As a next step, see part two, check out the notebook (also on GitHub), or get started solving the problem for yourself.", "As always, I welcome feedback, constructive criticism, and hearing about your data science projects. I can be reached on Twitter @koehrsen_will.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1977dd701dbc&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----1977dd701dbc---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/dconnolly/chromecast-backgrounds", "anchor_text": "Source"}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "the sequel"}, {"url": "https://www.kdnuggets.com/2015/07/guide-data-science-good.html", "anchor_text": "Data Science for Good"}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "Jupyter Notebook both on Kaggle"}, {"url": "https://github.com/WillKoehrsen/data-science-for-good/blob/master/costa-rican-poverty/A%20Complete%20Walkthrough.ipynb", "anchor_text": "on GitHub"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction", "anchor_text": "Costa Rican Household Poverty Level Prediction"}, {"url": "https://www.iadb.org/en", "anchor_text": "Inter-American Development Bank"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction", "anchor_text": "on Kaggle"}, {"url": "https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420", "anchor_text": "machine learning problem is"}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "available in the notebooks"}, {"url": "https://www.kaggle.com/willkoehrsen/featuretools-for-good", "anchor_text": "experimenting with new techniques"}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "part two"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/kernels", "anchor_text": "work shared by other data scientists"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/discussion", "anchor_text": "discussion platform"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data", "anchor_text": "data definitions"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html", "anchor_text": "Macro F1 Score"}, {"url": "https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin", "anchor_text": "\u201cMacro\u201d is one of the averaging options for multi-class problems."}, {"url": "https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428", "anchor_text": "appropriate measure"}, {"url": "https://en.wikipedia.org/wiki/Exploratory_data_analysis", "anchor_text": "Exploratory Data Analysis (EDA)"}, {"url": "https://towardsdatascience.com/data-cleaning-101-948d22a92e4", "anchor_text": "Data cleaning"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "Scikit-Learn random forest classifier"}, {"url": "https://stats.stackexchange.com/a/250742/157316", "anchor_text": "stratified sampling"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/basics.html#basics-dtypes", "anchor_text": "different column data types"}, {"url": "https://en.wikipedia.org/wiki/Statistical_data_type", "anchor_text": "different statistical types of data"}, {"url": "https://towardsdatascience.com/why-automated-feature-engineering-will-change-the-way-you-do-machine-learning-5c15bf188b96", "anchor_text": "automated feature engineering"}, {"url": "https://www.kaggle.com/willkoehrsen/featuretools-for-good", "anchor_text": "another notebook with automated feature engineering"}, {"url": "https://www.featuretools.com/", "anchor_text": "automated feature engineering (with Featuretools)"}, {"url": "https://en.wikipedia.org/wiki/Collinearity#Usage_in_statistics_and_econometrics", "anchor_text": "called collinear"}, {"url": "https://medium.com/mindorks/what-is-feature-engineering-for-machine-learning-d8ba3158d97a", "anchor_text": "calculate features one at a time using domain knowledge"}, {"url": "https://github.com/Featuretools/Automated-Manual-Comparison", "anchor_text": "manual vs automated feature engineering"}, {"url": "https://github.com/Featuretools/featuretools", "anchor_text": "Featuretools"}, {"url": "https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219", "anchor_text": "Here\u2019s an article"}, {"url": "http://scikit-learn.org/stable/modules/feature_selection.html", "anchor_text": "feature selection"}, {"url": "https://stats.stackexchange.com/a/100272/157316", "anchor_text": "highly correlated with one another are called collinear."}, {"url": "https://machinelearningmastery.com/feature-selection-machine-learning-python/", "anchor_text": "methods for feature selection"}, {"url": "http://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection.html", "anchor_text": "univariate"}, {"url": "https://github.com/danielhomola/mifs", "anchor_text": "multivariate"}, {"url": "http://scikit-learn.org/stable/modules/feature_selection.html#tree-based-feature-selection", "anchor_text": "model-based feature importances for feature selection"}, {"url": "https://towardsdatascience.com/visualizing-data-with-pair-plots-in-python-f228cf529166", "anchor_text": "pairsplot"}, {"url": "https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf", "anchor_text": "highly readable paper"}, {"url": "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/", "anchor_text": "Gradient Tree Boosting"}, {"url": "https://www.quora.com/What-does-the-No-Free-Lunch-theorem-mean-for-machine-learning-In-what-ways-do-popular-ML-algorithms-overcome-the-limitations-set-by-this-theorem", "anchor_text": "one model is always the best"}, {"url": "http://scikit-learn.org/", "anchor_text": "Scikit-Learn"}, {"url": "http://scikit-learn.org/stable/supervised_learning.html", "anchor_text": "many machine learning models"}, {"url": "https://blog.easysol.net/wp-content/uploads/2017/03/ME-Explained-600x486.png", "anchor_text": "range of complexity"}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api", "anchor_text": "Light GBM"}, {"url": "https://www.openml.org/a/estimation-procedures/1", "anchor_text": "cross validation"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization", "anchor_text": "model hyperparameter optimization"}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "second part"}, {"url": "http://shop.oreilly.com/product/0636920052289.do", "anchor_text": "machine learning"}, {"url": "https://towardsdatascience.com/the-most-important-part-of-a-data-science-project-is-writing-a-blog-post-50715f37833a", "anchor_text": "we\u2019ve communicated our work"}, {"url": "https://medium.com/@williamkoehrsen/a-data-science-for-good-machine-learning-project-walk-through-in-python-part-two-2773bd52daf0", "anchor_text": "see part two"}, {"url": "https://www.kaggle.com/willkoehrsen/a-complete-introduction-and-walkthrough", "anchor_text": "the notebook"}, {"url": "https://github.com/WillKoehrsen/data-science-for-good/blob/master/costa-rican-poverty/A%20Complete%20Walkthrough.ipynb", "anchor_text": "GitHub"}, {"url": "https://www.kaggle.com/c/costa-rican-household-poverty-prediction", "anchor_text": "solving the problem for yourself"}, {"url": "http://twitter.com/koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1977dd701dbc---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----1977dd701dbc---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/education?source=post_page-----1977dd701dbc---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/python?source=post_page-----1977dd701dbc---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1977dd701dbc---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----1977dd701dbc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----1977dd701dbc---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1977dd701dbc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1977dd701dbc---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1977dd701dbc--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1977dd701dbc--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1977dd701dbc--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-data-science-for-good-machine-learning-project-walk-through-in-python-part-one-1977dd701dbc&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}