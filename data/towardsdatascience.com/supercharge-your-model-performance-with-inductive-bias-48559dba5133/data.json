{"url": "https://towardsdatascience.com/supercharge-your-model-performance-with-inductive-bias-48559dba5133", "time": 1683015108.152898, "path": "towardsdatascience.com/supercharge-your-model-performance-with-inductive-bias-48559dba5133/", "webpage": {"metadata": {"title": "Supercharge your model performance with inductive bias | by Sebastian Dick | Towards Data Science", "h1": "Supercharge your model performance with inductive bias", "description": "From the beautifully regular shape of a snowflake, and the self-similar (fractal) structure of romanesco, to the hexagonal pattern of honeycombs. Nature seems to seek out symmetry. In fact the very\u2026"}, "outgoing_paragraph_urls": [{"url": "https://aip.scitation.org/doi/full/10.1063/1.4966192", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://twitter.com/semodi92", "anchor_text": "twitter", "paragraph_index": 52}, {"url": "https://www.linkedin.com/in/sebastianmdick/", "anchor_text": "LinkedIn", "paragraph_index": 52}], "all_paragraphs": ["From the beautifully regular shape of a snowflake, and the self-similar (fractal) structure of romanesco, to the hexagonal pattern of honeycombs.", "Nature seems to seek out symmetry. In fact the very laws of our existence exhibit a plethora of them: Physicists speak of translation (\u201cmove through\u201d) symmetry in both time and space. What they mean is that forces such as gravity work the same way they did millions of years ago and that they don\u2019t vary between Sydney and New York.", "Another one of their favorites, rotation symmetry, simply states that an object\u2019s properties don\u2019t change as you look at it from different angles.", "The list of symmetries goes on and on, and some of them are easier to grasp than others (Lorentz symmetry, stating that the speed of light is the same for co-moving observers in inertial frames, might already escape less physically versed minds).", "Even though some of these symmetries are obvious to humans, most machine learning models are surprisingly oblivious to their existence. Let me give an example from my own work:", "Roughly speaking, the goal of my research is to use ML to predict properties of molecules from structural information only. This means, that I am given a list of atoms together with their coordinates.", "For a water molecule, it would look like this:", "The atom\u2019s coordinates are conveniently summarized in a matrix with rows corresponding to atoms and columns corresponding to the x,y and z positions respectively. I would like to predict how much energy is needed to break up the molecule into its constituent atoms (the atomization energy). I could do so by training a neural network F that uses the raw coordinates as features and outputs the energy:", "Let\u2019s say I successfully train this neural network on a large and diverse dataset of molecules and I want to find the atomization energy of the following water molecule:", "You might have noticed that it is simply a rotated version of our original molecule. It should therefore have the same atomization energy. Do we have any guarantee that the neural network will respect this rotation symmetry? Unfortunately, no.", "Even worse, if we simply exchange the two Hydrogen (H) atoms", "the network might again give a completely different answer. The ordering of identical atoms has no physical meaning and is only an artifact stemming from the functional form of the neural network. Ideally we would want the neural network output to respect this permutation symmetry (permutation: \u201cexchange the order of things\u201d), but how?", "In the early days of machine learning in chemistry and physics, it became clear very quickly that models need to observe these symmetries in order to be sufficiently accurate. Therefore, a lot of effort was dedicated to figuring out how to incorporate symmetries in ML algorithms. Nowadays, this is often achieved through a combination of clever feature engineering and neural network design. A comprehensive survey of these methods can be found here [1]. Something that all these methods have in common, is that they in some shape or form introduce inductive bias to the learning algorithm.", "The inductive bias [\u2026] of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered [2]", "In the case of rotation symmetry, this inductive bias could be phrased as the assumption: \u201cAny information that is not invariant under rotations can and should be ignored\u201d", "Whether you were aware of it or not, if you have used any machine learning model before, you have encountered inductive bias:", "While all of these standard algorithms have some built-in bias, I would like to demonstrate with an example how introducing additional assumptions can substantially improve your model\u2019s accuracy.", "Your client is interested in buying and selling real estate and tasks you with creating a machine learning model that accurately predicts the fair market value of a building. To your luck, training data is readily available but somewhat limited (~1000 samples). To complicate matters, any information about the building is given on a per unit level, except for prices, which are only available for the entire building.", "To simplify our analysis let\u2019s assume that every building contains exactly ten units, and we have the following information about every unit:", "Our data comes in tabular form with 1000 rows (the number of samples) and 51 columns ( 5 features x 10 units + total price). Let us write the model input (the first 50 columns) as a design matrix \ud835\udc4b and the dependent variable as the vector \ud835\udc66", "A good starting point for any regression task is always linear regression, which aims to choose weights w so that the model predictions", "To analyze how well this model performs for our data, we can look at the generalization error (the error on the test-set) with respect to the training set size. Plotting our results, we get what is commonly known as the learning curve:", "Notice, that we have included a baseline model that simply predicts the mean price of the training set. We can infer several things from this plot. We see that for extremely small training set the baseline model is more accurate than LR, however as we go to larger dataset sizes (note the log-log scale!) we can improve upon the baseline. But a root-mean-square error (RMSE) of 1 million USD is not very impressive, nor is it useful for our fictional customer. We would like to do better \u2026", "Let\u2019s think about symmetries: Should it matter in which order units are listed in the design matrix \ud835\udc4b? The answer is: no. We could exchange columns 1\u20135 with columns 6\u201310 and still expect the building to have the same net-worth. However this is not accurately captured by our Vanilla LR, as there is no guarantee that the weights 1\u22125 are the same as 6\u221210. The goal is clear: we need our model to be invariant under permutation of units.", "Let us consider a slightly different version of our design matrix, which we will call \ud835\udc4b\u0303 . It has the dimensions 10000 x 5 and is obtained from the original \ud835\udc4b by considering every unit (i.e. every set of five columns) as a separate data point. Then, linear regression becomes", "where \ud835\udc67 is the price for every unit. Unfortunately we don\u2019t have access to \ud835\udc67 (a latent variable), but we can make the assumption that there is a relationship between \ud835\udc67 and \ud835\udc66. As we want to stay within the realm of linear models, we write", "which, in plain words, just means that the price of a building is the sum of the prices for every unit inside that building. We can write the above sum for all buildings \ud835\udc56 by introducing an appropriate matrix \ud835\udc3f so that", "which allows us to solve the linear regression model", "Effectively, \ud835\udc3f\ud835\udc4b\u0303 means that we sum up features for all units inside one building, so e.g. instead of the area per unit we only look at the total area per building. So for linear regression, imposing permutation symmetry is really a trivial task. The advantages of using this more abstract notation will become clear as we move to kernel methods.", "Using the permutation invariant version of LR we get the following learning curve:", "While we see substantial improvement for smaller datasets, the model does not seem to learn much from the data, resulting in a flat curve. This indicates that a linear model is not expressive enough to capture all the details contained in the training data. Clearly the assumption of linearity is too strong, as for a \u2018healthy\u2019 model we would expect the learning curve to be linear on a log-log scale (indicating the expected power-law behavior) [3].", "Let\u2019s consider a more complex model. We have several options: Neural Networks are certainly popular but are notoriously unreliable for such small datasets. Valid options are tree-based as well as k-means methods. My personal favorite are kernel based methods, in particular Gaussian Process Regression (GPR).", "We are looking for a function \ud835\udc66=\ud835\udc53(\ud835\udc65) that maps the input, or independent variables \ud835\udc65 to a dependent variable \ud835\udc66 (the price). In GPR, we take a Bayesian approach to finding this function, by first specifying a prior over all possible \ud835\udc53 and then conditioning on the observed datapoints (\ud835\udc4b,\ud835\udc66). This prior is defined through a Gaussian Process with covariance matrix \ud835\udc58.", "A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution [4].", "For any value of \ud835\udc65, the output \ud835\udc53(\ud835\udc65) follows a normal distribution. Further, the outputs at two distant points \ud835\udc65 and \ud835\udc65\u2032 are joint normal with a covariance defined through \ud835\udc58(\ud835\udc65,\ud835\udc65\u2032). In practice, this means we can determine the shape of the fitting function \ud835\udc53 by choosing a suitable covariance function (also called kernel) \ud835\udc58(\ud835\udc65,\ud835\udc65\u2032).", "A very popular choice for the kernel is the squared exponential covariance (sometimes also called radial basis function)", "Picking this kernel, we essentially put a smoothness condition on \ud835\udc53 as points that are close together (meaning |\ud835\udc65\u2212\ud835\udc65\u2032| is small) will be highly correlated.", "Let\u2019s return to our example. Once we condition our Gaussian Process on training data we can make predictions on the test set.", "Using our design matrix \ud835\udc4b and the target values \ud835\udc66, the model predictions are given as", "This looks very similar to linear regression, except that \ud835\udc4b has been replaced by \ud835\udc58(\ud835\udc4b,\ud835\udc4b). Because the equation is linear in the parameters \ud835\udf14, it is still straightforward to solve:", "Notice, that we have included a parameter \ud835\udf0e multiplied by the identity matrix I. This parameter is used to model noise present in the data and at the same time help avoid numerical issues in the matrix inversion.", "Plugging our training data into these equations, we get the following learning curve:", "Hm, that didn\u2019t work so well. While we have given the model much more expressive power compared to LR, its test error is still comparable to the latter. Our model seems to be overfitting the data.", "Let\u2019s add back permutation invariance. As before, we want to solve the auxiliary problem", "with \ud835\udc66=\ud835\udc3f\ud835\udc67. The matrix L remains the same as in the Linear Regression problem.", "After juggling around with some terms, we get the following equation:", "we recover the original form for GPR", "but this time with a permutation invariant kernel \ud835\udc58\u0303 . Plugging in our data, we get the following learning curve:", "Hooray! By using permutational invariance we have been able to decrease our best test error from roughly $700,000 to $30,000. Moreover, the scaling is linear (on a log-log scale), which indicates that we can further improve the model\u2019s accuracy by collecting more data, if need be.", "I hope I have been able to convince you that it\u2019s sometimes worth to pause and think twice before fitting your data with a machine learning model. By simply recognizing symmetries in our data, we have been able to improve our model accuracy by a factor of more than 20!", "As a little bonus, if you understood everything in this post, you\u2019re actually one step closer to becoming a computational chemist.", "If you replace \u201cbuilding\u201d by \u201cmolecule\u201d and \u201cunit\u201d by \u201catom\u201d we have essentially built a model that is routinely used by researchers in chemistry and material science [5]. The model can predict a molecule\u2019s property such as its energy by expressing it as combination of atomic contributions. Just as in our example, these atomic contributions are unknown \u2014 we only know the entire molecule\u2019s energy \u2014 but we can still phrase the problem in terms of latent variables z, thereby making it permutation invariant.", "If you enjoyed this article please feel free to follow me here, on twitter or connect on LinkedIn.", "[3] C. Cortes, L. D. Jackel, S. A. Solla, V. Vapnik, and J. S. Denker, Learning Curves: Asymptotic Values and Rate of Convergence, Advances in Neural Information Processing Systems (Curran Associates, Inc., 1994),pp. 327\u2013334", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Physics PhD student using machine learning to study molecules and materials. Always striving to learn more about data science and statistics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F48559dba5133&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----48559dba5133--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sebastiandick42?source=post_page-----48559dba5133--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sebastiandick42?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Sebastian Dick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6e6f577a7bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&user=Sebastian+Dick&userId=a6e6f577a7bc&source=post_page-a6e6f577a7bc----48559dba5133---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://libreshot.com/about-libreshot/", "anchor_text": "Martin Vorel"}, {"url": "https://towardsdatascience.com/tagged/hands-on-tutorials", "anchor_text": "Hands-on Tutorials"}, {"url": "https://creativecommons.org/licenses/by-sa/3.0", "anchor_text": "https://creativecommons.org/licenses/by-sa/3.0"}, {"url": "https://aip.scitation.org/doi/full/10.1063/1.4966192", "anchor_text": "here"}, {"url": "https://twitter.com/semodi92", "anchor_text": "twitter"}, {"url": "https://www.linkedin.com/in/sebastianmdick/", "anchor_text": "LinkedIn"}, {"url": "https://en.wikipedia.org/wiki/Inductive_bias", "anchor_text": "https://en.wikipedia.org/wiki/Inductive_bias"}, {"url": "https://medium.com/tag/data-science?source=post_page-----48559dba5133---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----48559dba5133---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gaussian-process?source=post_page-----48559dba5133---------------gaussian_process-----------------", "anchor_text": "Gaussian Process"}, {"url": "https://medium.com/tag/hands-on-tutorials?source=post_page-----48559dba5133---------------hands_on_tutorials-----------------", "anchor_text": "Hands On Tutorials"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----48559dba5133---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&user=Sebastian+Dick&userId=a6e6f577a7bc&source=-----48559dba5133---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&user=Sebastian+Dick&userId=a6e6f577a7bc&source=-----48559dba5133---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----48559dba5133--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F48559dba5133&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----48559dba5133---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----48559dba5133--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----48559dba5133--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----48559dba5133--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----48559dba5133--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----48559dba5133--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sebastiandick42?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sebastiandick42?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sebastian Dick"}, {"url": "https://medium.com/@sebastiandick42/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "112 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa6e6f577a7bc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&user=Sebastian+Dick&userId=a6e6f577a7bc&source=post_page-a6e6f577a7bc--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F173d000ac8a4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupercharge-your-model-performance-with-inductive-bias-48559dba5133&newsletterV3=a6e6f577a7bc&newsletterV3Id=173d000ac8a4&user=Sebastian+Dick&userId=a6e6f577a7bc&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}