{"url": "https://towardsdatascience.com/multi-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e", "time": 1683006169.772326, "path": "towardsdatascience.com/multi-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e/", "webpage": {"metadata": {"title": "Nash Equilibria and FFQ Learning | Towards Data Science", "h1": "Introduction to Nash Equilibria: Friend or Foe Q-Learning", "description": "Outlines multi-agent reinforcement learning algorithm, Nash equilibria, and Friend or Foe Q Learning"}, "outgoing_paragraph_urls": [{"url": "http://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf", "anchor_text": "Nash Q-Learning for General-Sum Stochastic Games", "paragraph_index": 24}], "all_paragraphs": ["For whatever reason, humans innately possess the ability to collaborate. It\u2019s become so commonplace that its nuances slip right under our noses. How do we just know how to coordinate when moving a heavy couch? How do we reason splitting up in a grocery store to minimize time? How are we able to observe others\u2019 actions and understand how to best respond?", "Here\u2019s an interpretation: we reach a balance. An equilibrium. Each person takes actions that not only best complements the others\u2019 but altogether achieves the task at hand most efficiently. This application of equilibria comes up pretty often in game theory and extends to multi-agent RL (MARL). In this article, we explore two algorithms, Nash Q-Learning and Friend or Foe Q-Learning, both of which attempt to find multi-agent policies fulfilling this idea of \u201cbalance.\u201d We assume basic knowledge of single-agent formulations and Q-learning. For more background on MARL and basic principles specifically pertaining to it, check out my previous article:", "Multi-agent learning environments are typically represented by Stochastic Games. Each agent aims to find a policy that maximizes their own expected discounted reward. Together, the overall goal is to find a joint policy that gathers the most reward for each agent. This joint reward is defined below in the form of a value function:", "This goal applies to both competitive and collaborative situations. Agents can find policies that best counter or complement others. We call this optimal policy the Nash Equilibrium. More formally, it is a policy such that has this property:", "At first, it seems like we\u2019re beating a dead horse. The best policy gathers the most reward, so what?", "Underneath all the fancy greek letters and notation, the Nash Equilibrium tells us a bit more. It says that each agent\u2019s policy in Nash Equilibrium is the best response to the other agents\u2019 optimal policies. No agent is incentivized to change their policy because any tweak gives less reward. In other words, all of the agents are at a standstill. Landlocked. Kind of trapped in a sense.", "To give an example, imagine a competitive game between two small robots: C3PO and Wall-E. During each round, they each choose a number one through ten, and whoever selects the higher number wins. As expected, both pick the number ten every time as neither robot wants to risk losing. If C3PO were to choose any other number, he would risk losing against Wall-E\u2019s optimal policy of always choosing ten and vice versa. In other words, the two are at an equilibrium.", "As a result, we define a term called the Nash Q-Value:", "Very similar to its single-agent counterpart, the Nash Q-Value represents an agent\u2019s expected future cumulative reward when, after choosing a specific joint action, all agents follow a joint Nash Equilibrium policy. It can be viewed as a state-action pair\u2019s \u201cbest-case scenario\u201d reward gain. This definition lends itself to the multi-agent version of Q-learning: Nash Q-Learning.", "In single-agent Q-learning, we update Q values using the temporal difference (TD) with the equation:", "where gamma represents the discount factor and alpha is the learning rate. Here\u2019s the slightly boring part: Nash Q-learning reduces to Q-learning. More explicitly, the update equation for Nash Q-Learning is:", "where we compute the Nash Q-value by maximizing over the action space. Switch around a few terms and you arrive at a nearly identical update equation, the only difference being a joint action space. So, what was the point of all this?", "The point: we can\u2019t always assume that single-agent logic applies to multi-agent settings. In other words, algorithms that apply to MDPs (single-agent RL) aren\u2019t always translatable to Stochastic Games (multi-agent RL). Nash Q-Learning just happens to be a somewhat special case. Hu and Wellman [1], among many other things in the paper, prove that Nash Q-Learning always converged. The fact that its update equation looks identical to Q-learning is intuitive but not always assumable beforehand.", "The entire Nash Q-learning algorithm is analogous to single-agent Q-learning and is shown below:", "Q-values have a natural interpretation. They represent a state-action pair\u2019s expected cumulative discounted reward, but how does that motivate our update equation? Let\u2019s take a look at it again.", "This is a weighted sum where alpha is the learning rate, a value in the interval (0,1). The first term is our Q value at a previous time step multiplied by some fraction. Let\u2019s dissect the second term, our learning rate multiplied by a sum of two other terms.", "The first part of the second term is a reward we get after executing an action while the other is a discounted, Nash Q-value maximized over actions for the next state. Remember, this Nash Q-value equals the expected, best-case, cumulative reward after a given state. Putting the two parts together, we have a sum of expected rewards where we assume the best-case scenario at the next state. It\u2019s another definition for the current Q value, just an updated version! More formally, this sum represents a closer estimate of the actual Q value. Zooming out and looking at the entire update equation, we can interpret it as \u201cnudging\u201d the current Q-value closer to this better estimate.", "In Friend or Foe Q-Learning (FFQ), Littman [2] uses this interpretation to create an intuitive algorithm for multi-agent systems. FFQ starts by letting each agent maintain their own Q function, contrary to Nash Q-learning. We assume that agents are aware of others\u2019 action spaces but don\u2019t know what actions others will choose at any given moment.", "Also, Littman generalizes to systems where there is a combination of collaborators and adversaries, hence the name Friend or Foe. Let\u2019s use C3PO (agent 1) and Wall-E (agent 2) as examples again, and view things from C3PO\u2019s point of view. If the two are working together, C3PO\u2019s Nash-Q value would be replaced with:", "as expected, as both agents attempt to maximize their \u201cstream\u201d of rewards, similar to the Q update function from before. However, if the two robots are competing against each other, C3PO\u2019s Nash-Q would be replaced with:", "Notice how we first minimize over Wall-E\u2019s actions before we maximize over C3PO\u2019s actions. In other words, while Wall-E attempts to minimize C3PO\u2019s stream of rewards, C3PO learns how to best respond to Wall-E\u2019s counteraction, updating his Q-value accordingly. This is an example of when the idea of Nash Equilibria shows its colors: agents learning the optimal way to complement each other.", "More generally, this can be adapted to larger systems of agents. The Nash-Q value becomes as follows:", "where X represents the set of all collaborators while Y represents the set of all adversaries. Similar to Nash Q-learning, Littman[2] showed that Friend or Foe Q-learning also converged. By replacing these Nash Q-values appropriately, a distributed system of agents can learn a policy that reaches equilibrium, if one exists.", "These algorithms are what some consider the \u201cfirst steps\u201d into the multi-agent learning scene. While the ideas are relatively simple, the results are powerful. Next time, we\u2019ll look into more novel, modern-day approaches to solving complex, multi-agent environments! For a more in-depth look at Nash Q-learning and Friend or Foe Q-learning, feel free to take a look at the two papers listed below.", "[1] J. Hu and M. Wellman, Nash Q-Learning for General-Sum Stochastic Games (2003), Journal of Machine Learning Research 4", "From the classic to state-of-the-art, here are related articles discussing both multi-agent and single-agent reinforcement learning:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Part-time writer \u00b7 Full-time learner \u00b7 PhD Student @ University of Michigan"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4a0b9aae3a1e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab----4a0b9aae3a1e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@heftiba?utm_source=medium&utm_medium=referral", "anchor_text": "Toa Heftiba"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/swlh/the-gist-multi-agent-reinforcement-learning-767b367b395f", "anchor_text": "Multi-Agent Reinforcement Learning: The GistAs if one robot learning everything wasn\u2019t enough alreadymedium.com"}, {"url": "https://unsplash.com/@introspectivedsgn?utm_source=medium&utm_medium=referral", "anchor_text": "Erik Mclean"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@neonbrand?utm_source=medium&utm_medium=referral", "anchor_text": "NeONBRAND"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@jonflobrant?utm_source=medium&utm_medium=referral", "anchor_text": "Jon Flobrant"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@rihok?utm_source=medium&utm_medium=referral", "anchor_text": "Riho Kroll"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@hudsonhintze?utm_source=medium&utm_medium=referral", "anchor_text": "Hudson Hintze"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@ptrikutam?utm_source=medium&utm_medium=referral", "anchor_text": "Pavan Trikutam"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://www.jmlr.org/papers/volume4/hu03a/hu03a.pdf", "anchor_text": "Nash Q-Learning for General-Sum Stochastic Games"}, {"url": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.589.8571&rep=rep1&type=pdf", "anchor_text": "Friend-or-Foe Q-learning in General-Sum Games"}, {"url": "https://towardsdatascience.com/stigmergic-reinforcement-learning-why-it-tells-us-1-1-3-4d50dfa9cb19", "anchor_text": "Unpacking Stigmergic Independent Reinforcement Learning and Why It MattersBecause 1+1=3towardsdatascience.com"}, {"url": "https://towardsdatascience.com/hierarchical-reinforcement-learning-feudal-networks-44e2657526d7", "anchor_text": "Hierarchical Reinforcement Learning: FeUdal NetworksLetting computers see the bigger picturetowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4a0b9aae3a1e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4a0b9aae3a1e---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-agent-systems?source=post_page-----4a0b9aae3a1e---------------multi_agent_systems-----------------", "anchor_text": "Multi Agent Systems"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4a0b9aae3a1e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/beginners-guide?source=post_page-----4a0b9aae3a1e---------------beginners_guide-----------------", "anchor_text": "Beginners Guide"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----4a0b9aae3a1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=-----4a0b9aae3a1e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4a0b9aae3a1e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4a0b9aae3a1e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4a0b9aae3a1e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@austinnguyen517?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Austin Nguyen"}, {"url": "https://medium.com/@austinnguyen517/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "208 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6a1b6c5da9ab&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=post_page-6a1b6c5da9ab--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2cb997cef03c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulti-agent-rl-nash-equilibria-and-friend-or-foe-q-learning-4a0b9aae3a1e&newsletterV3=6a1b6c5da9ab&newsletterV3Id=2cb997cef03c&user=Austin+Nguyen&userId=6a1b6c5da9ab&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}