{"url": "https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d", "time": 1682993752.813667, "path": "towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d/", "webpage": {"metadata": {"title": "Custom Loss Functions for Gradient Boosting | by Prince Grover | Towards Data Science", "h1": "Custom Loss Functions for Gradient Boosting", "description": "Gradient boosting is widely used in industry and has won many Kaggle competitions. The internet already has many good explanations of gradient boosting (we\u2019ve even shared some selected links in the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/@souravdey", "anchor_text": "Sourav Dey", "paragraph_index": 0}, {"url": "https://github.com/Microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions", "anchor_text": "competitions", "paragraph_index": 1}, {"url": "https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "anchor_text": "this post", "paragraph_index": 1}, {"url": "https://cortexintel.com/", "anchor_text": "Cortex Building Intelligence", "paragraph_index": 3}, {"url": "https://www.manifold.ai/blog/data-science-at-cortex", "anchor_text": "this post", "paragraph_index": 3}, {"url": "https://github.com/scikit-learn/scikit-learn/issues/3071", "anchor_text": "here", "paragraph_index": 7}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "this", "paragraph_index": 7}, {"url": "https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst/#metric-parameters", "anchor_text": "available in library", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9", "anchor_text": "nested cross validation", "paragraph_index": 9}, {"url": "https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb", "anchor_text": "Here", "paragraph_index": 13}, {"url": "https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb", "anchor_text": "notebook", "paragraph_index": 14}, {"url": "http://www.manifold.ai/lean-ai", "anchor_text": "lean, iterative approach", "paragraph_index": 24}, {"url": "http://explained.ai/gradient-boosting/index.html", "anchor_text": "How to explain gradient boosting", "paragraph_index": 25}, {"url": "http://parrt.cs.usfca.edu/", "anchor_text": "Terence Parr", "paragraph_index": 25}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "Gradient boosting from scratch", "paragraph_index": 25}, {"url": "https://github.com/Microsoft/LightGBM/issues/695", "anchor_text": "This", "paragraph_index": 25}, {"url": "https://www.analyticsvidhya.com/blog/author/aarshay/", "anchor_text": "Aarshay Jain", "paragraph_index": 25}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/", "anchor_text": "blog", "paragraph_index": 25}, {"url": "https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db", "anchor_text": "CatBoost vs. Light GBM vs. XGBoost", "paragraph_index": 25}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/", "anchor_text": "Which algorithm takes the crown: Light GBM vs XGBOOST?", "paragraph_index": 25}, {"url": "http://www.manifold.ai/", "anchor_text": "Manifold", "paragraph_index": 26}, {"url": "http://manifold.ai/careers", "anchor_text": "hiring data scientists", "paragraph_index": 27}, {"url": "http://Manifold.ai", "anchor_text": "Manifold.ai", "paragraph_index": 29}], "all_paragraphs": ["Authors: Prince Grover and Sourav Dey", "Gradient boosting is widely used in industry and has won many Kaggle competitions. The internet already has many good explanations of gradient boosting (we\u2019ve even shared some selected links in the references), but we\u2019ve noticed a lack of information about custom loss functions: the why, when, and how. This post is our attempt to summarize the importance of custom loss functions in many real-world problems \u2014 and how to implement them with the LightGBM gradient boosting package.  Machine learning algorithms are trained to minimize a loss function on the training data. There are a number of commonly used loss functions that are readily available in common ML libraries. If you want to learn more about some of these, read this post, which Prince wrote while doing his Masters in Data Science. Out in the real world, these \u201coff-the-shelf\u201d loss functions are often not well-tuned to the business problem we are trying to solve. Enter custom loss functions.", "One example where a custom loss function is handy is the asymmetric risk of airport punctuality. The problem is to decide when to leave the house so you get to the airport at the right time. We don\u2019t want to leave too early and wait for hours at the gate. At the same time, we don\u2019t want to miss our flight. The losses on either side are very different: if we get to the gate early, it\u2019s really not so bad; if we arrive too late and miss the flight, it really sucks. If we use machine learning to decide when to leave the house, we might want to take care of this risk asymmetry directly in our model, by using a custom loss function that penalizes late errors much more than early errors.", "Another common example occurs in classification problems. For example, for disease detection, we may consider false negatives to be much worse than false positives, as giving medication to a healthy person is usually less harmful than failing to treat an ill person. In such cases, we might want to optimize the F-beta score where beta depends upon magnitude of weight we want to give to false positives. This is sometimes called a Neyman-Pearson criterion.   At Manifold, we recently ran into a problem that required a custom loss function. One of our clients, Cortex Building Intelligence, provides an app that helps engineers operate a buildings heating, ventilation, and air conditioning (HVAC) systems more precisely. Most commercial buildings have a \u201clease obligation\u201d to condition the buildings indoor temperature in a \u201ccomfortable\u201d temperature range during working hours on operating days, e.g., between 70 and 74 degrees during the hours of 9am to 6pm. At the same time, HVAC is the single biggest operational cost of a building. The key to efficient HVAC operation is to turn off the system when it is not needed, like at night, and turn it on again early in the morning to meet the \u201clease obligation\u201d. To that end, Manifold helped Cortex build a predictive model to recommend the exact time when HVAC systems should be turned on in a building.  The penalty of incorrect prediction is not symmetric, though. If our predicted start time is earlier than the actual required start time, then the building will come to a comfortable temperature too early and some energy will be wasted. But if the predicted time is later than the actual required start time, then the building will come to a comfortable temperature too late and the tenants will not be happy \u2014 no one wants to work, shop, or learn in a freezing/boiling building. So being late is much worse than being early, because we don\u2019t want tenants (who pay $$$ in rent) to be unhappy. We encoded this business knowledge in our model by creating a custom asymmetric Huber loss function that has a higher error when the residual is positive vs. negative. More details about this problem can be found in this post.", "Takeaway: Find a loss function that closely matches your business objectives. Often, these loss functions don\u2019t have a default implementation in the popular machine learning libraries. That\u2019s ok: it\u2019s not hard to define your own loss function and use it to crush your problem.", "Before moving further, let\u2019s be clear in our definitions. Many terms are used in the ML literature to refer to different things. We will choose one set of definitions that we think is the most clear:", "In many cases, customizing these losses can be really effective in building a better model. This is particularly simple for gradient boosting, as we show below.", "The training loss is optimized during training. It is hard to customize for certain algorithms, like the random forest (see here), but relatively easy for others, like gradient boosting and neural nets. Because some variant of gradient descent is usually the optimization method, the training loss typically needs to be a function that has a convex gradient (first derivative) and hessian (second derivative). It is also preferably continuous, finite, and non-zero. The last one is important because sections where the function is zero can freeze gradient descent.   In the context of gradient boosting, the training loss is the function that is optimized using gradient descent, e.g., the \u201cgradient\u201d part of gradient boosting models. Specifically, the gradient of the training loss is used to change the target variables for each successive tree. (If you\u2019re interested in more details, see this post.) Note that even though the training loss defines the \u201cgradient\u201d, each tree is still grown using a greedy split algorithm that is not tied to this custom loss function.  Defining a custom training loss usually requires us to do some calculus to find the gradient and hessian. As we\u2019ll see next, often it is easier to first change the validation loss, as it doesn\u2019t require as much overhead.", "The validation loss is used to tune hyper-parameters. It is often easier to customize, as it doesn\u2019t have as many functional requirements like the training loss does. The validation loss can be non-convex, non-differentiable, and discontinuous. For this reason, it is often an easier place to start with customization.   For example, in LightGBM, an important hyperparameter is number of boosting rounds. The validation loss can be used to find the optimum number of boosting rounds. This validation loss in LightGBM is called eval_metric. We can either use one of the validation losses available in library or define our own custom function. Since it is so easy, you should definitely customize if it is important to your business problem.   Concretely, instead of directly optimizing num boosting rounds, we usually use the early_stopping_rounds variable. It stops boosting when the validation loss starts increasing for the given number of early stopping rounds. Effectively, it prevents overfitting by monitoring the validation loss on the out of sample validation set. As shown in the figure below, setting stopping rounds higher leads to the model running for more boosting rounds.", "Bear in mind that the validation strategy is extremely important as well. The train/validation split above is one of many possible validation strategies. It may not be right for your problem. Others include k-fold cross validation and nested cross validation, which we used on our HVAC start time modelling problem.", "If appropriate for the business problem, we want to use a custom function for both our training and validation loss. In some situations, because of the functional form of the custom loss, it may not be possible to use it as the training loss. In that case, it may make sense to just update the validation loss and use a default training loss like the MSE. You will still get benefit, because the hyper parameters will be tuned using the desired custom loss.", "Let\u2019s examine what this looks like in practice and do some experiments on simulated data. First, let\u2019s assume that overestimates are much worse than underestimates. In addition, lets assume that squared loss is a good model for our error in either direction. To encode that, we defined a custom MSE function that gives 10 times more penalty to positive residuals than negative residuals. The plot below illustrates how our custom loss function looks vs. the standard MSE loss function.", "The asymmetric MSE, as defined, is nice because it has an easy to compute gradient and hessian, which are plotted below. Note that the hessian is constant at two different values, 2 on the left and 20 on the right, though that is hard to see on the plot below.", "LightGBM offers an straightforward way to implement custom training and validation losses. Other gradient boosting packages, including XGBoost and Catboost, also offer this option. Here is a Jupyter notebook that shows how to implement a custom training and validation loss function. The details are in the notebook, but at a high level, the implementations are slightly different:", "The Jupyter notebook also does an in-depth comparison of a default Random Forest, default LightGBM with MSE, and LightGBM with custom training and validation loss functions. We work with the Friedman 1 synthetic dataset, with 8,000 training observations, 2,000 validation observations, and 5,000 test observations. The validation set is used to find the best set of hyperparameters that optimize our validation loss. The scores reported below are evaluated on the test observations to assess the generalizability of our models.  We have done a sequence of experiments summarized in the table below. Note that the most important score we care about is asymmetric MSE as it specifically defines our problem of asymmetric penalty.", "Lets look at some comparisons in detail.", "Random Forest \u2192 LightGBMUsing default settings, LightGBM performs better than Random Forest on this dataset. With more trees and better combination of hyperparameters, Random Forest may also give good results, but that\u2019s not the point here.", "LightGBM \u2192 LightGBM with customized training lossThis shows that we can make our model optimize what we care about. The default LightGBM is optimizing MSE, hence it gives lower MSE loss (0.24 vs. 0.33). The LightGBM with custom training loss is optimizing asymmetric MSE and hence it performs better for asymmetric MSE (1.31 vs. 0.81).", "LightGBM \u2192 LightGBM with tuned early stopping rounds using MSEBoth the LightGBM models are optimizing MSE. We see a big improvement in default MSE scores with just a small tweak of using early stopping rounds (MSE: 0.24 vs 0.14). So, rather than limiting the number of boosting rounds to default (i.e., 100), we should let the model decide optimal number of boosting rounds using the early stopping hyper parameter. Hyper parameter optimization matters!", "LightGBM with tuned early stopping rounds using MSE \u2192 LightGBM with tuned early stopping using custom MSEThe scores from both of these models are very close with no material difference. This is because the the validation loss is only used to decide when to stop boosting. The gradient is optimizing default MSE in both the cases. Each subsequent tree produces same output for both of the models. The only difference is that the model with custom validation loss stops at 742 boosting iterations while the other runs for a few more.", "LightGBM with tuned early stopping using custom MSE \u2192 LightGBM trained on custom loss and tuned early stopping with MSEOnly customizing the training loss without changing the validation loss hurts the model performance. The model with only custom training loss boosts for more rounds (1848) than other cases. If we observe carefully, this model has really low training loss (0.013) and is highly overfitting on the training set. Each gradient boosting iteration makes a new tree using training errors as target variables, but the boosting stops only when loss on validation data start increasing. The validation loss usually starts increasing when the model starts overfitting, which is the signal to stop building more trees. In this case, since validation and training loss are not aligned with each other, the model doesn\u2019t seem to \u201cget the message\u201d which leads to overfitting. This configuration was just included for completeness and is not something one should use in practice.", "LightGBM with tuned early stopping rounds with MSE \u2192 LightGBM trained on custom training loss and tuned early stopping rounds with customized validation loss The final model uses both custom training and validation losses. It gives the best asymmetric MSE score with relatively small number of boosting iterations. The losses are aligned with what we care about!", "Let\u2019s take a closer look at the residual histograms for some more detail.", "Note that with LightGBM (even with default hyperparameters), the prediction performance improves as compared to the Random Forest model. The final model with custom validation loss appears to make more predictions on the right side of histogram, i.e. the actual values are greater than the predicted values. This is expected because of the asymmetric custom loss function. This right sided shift of the residuals can be better visualized using a kernel density plot of the residuals.", "All models have error, but many business problems do not treat underestimates and overestimates equally. Sometimes, we intentionally want our model to bias our errors in a certain direction, depending on which errors are more costly. Hence, we should not restrict ourselves with \u201coff-the-shelf\u201d symmetric loss functions from common ML libraries.   LightGBM offers a simple interface to incorporate custom training and validation loss functions. When appropriate, we should utilize this functionality to make better predictions. At the same time, you should not immediately jump to using custom loss functions. It\u2019s always best to take a lean, iterative approach and first start with a simple baseline model like a Random Forest. In the next iteration, you can move more complex models like LightGBM and do hyperparameter optimizaiton. Only once these baselines are stabilized, it makes sense to move onto customizing validation and training losses.   Hopefully that was useful! Happy customizing!", "If you are unclear about how general gradient boosting works, I recommend reading How to explain gradient boosting by Terence Parr, and Gradient boosting from scratch by Prince.   There are plenty of articles out there on how to tune hyper-parameters in different GBM frameworks. If you want to use one of these packages, you can spend some time on understanding which range of hyperparameters to search for. This LightGBM GitHub issue gives a rough idea about the range of values to use. Aarshay Jain has written a nice blog about tuning XGBoost and sklearn gradient boosting. I think there is room for a good blogpost about tuning LightGBM.  To get some intuition about which gradient boosting package is right for your situation, read Alvira Swalin\u2019s CatBoost vs. Light GBM vs. XGBoost, and Pranjan Khandelwal\u2019s Which algorithm takes the crown: Light GBM vs XGBOOST?.", "Manifold is a full-service AI consulting company offering a complete range of AI engineering services, including machine learning, data science, data engineering, devops, cloud, and edge. We have a proven ability to design, build, deploy, and manage complex data applications. Manifold is trusted by CTOs, CIOs, and GMs at Global 500 and high-growth companies. Our work spans industries such as consumer electronics, industrials, wireless, online commerce, digital health, and more. Manifold\u2019s experienced engineers have a track record of innovation at organizations like Google, Qualcomm, MIT, and successful venture-backed startups. Our Advisory Board includes leading researchers in deep learning at Stanford and Harvard, and we have an extended network of experts that we draw on across industry and engineering sub-specialties.", "Wanna work on stuff like this? Reach out at sdey@manifold.com! We\u2019re hiring data scientists.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Applied Scientist at Amazon. Previously - Machine Learning Engineer at Manifold.ai, USF-MSDS and IIT-Roorkee Alumnus (Twitter: @groverpr4)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff79c1b40466d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pgrover3?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgrover3?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Prince Grover"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2f052b054cee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&user=Prince+Grover&userId=2f052b054cee&source=post_page-2f052b054cee----f79c1b40466d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/@souravdey", "anchor_text": "Sourav Dey"}, {"url": "https://github.com/Microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions", "anchor_text": "competitions"}, {"url": "https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0", "anchor_text": "this post"}, {"url": "https://cortexintel.com/", "anchor_text": "Cortex Building Intelligence"}, {"url": "https://www.manifold.ai/blog/data-science-at-cortex", "anchor_text": "this post"}, {"url": "https://github.com/scikit-learn/scikit-learn/issues/3071", "anchor_text": "here"}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "this"}, {"url": "https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.rst/#metric-parameters", "anchor_text": "available in library"}, {"url": "https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9", "anchor_text": "nested cross validation"}, {"url": "https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb", "anchor_text": "Here"}, {"url": "https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb", "anchor_text": "notebook"}, {"url": "http://www.manifold.ai/lean-ai", "anchor_text": "lean, iterative approach"}, {"url": "http://explained.ai/gradient-boosting/index.html", "anchor_text": "How to explain gradient boosting"}, {"url": "http://parrt.cs.usfca.edu/", "anchor_text": "Terence Parr"}, {"url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d", "anchor_text": "Gradient boosting from scratch"}, {"url": "https://github.com/Microsoft/LightGBM/issues/695", "anchor_text": "This"}, {"url": "https://www.analyticsvidhya.com/blog/author/aarshay/", "anchor_text": "Aarshay Jain"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/", "anchor_text": "blog"}, {"url": "https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db", "anchor_text": "CatBoost vs. Light GBM vs. XGBoost"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/", "anchor_text": "Which algorithm takes the crown: Light GBM vs XGBOOST?"}, {"url": "http://www.manifold.ai/", "anchor_text": "Manifold"}, {"url": "http://manifold.ai/careers", "anchor_text": "hiring data scientists"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f79c1b40466d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----f79c1b40466d---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f79c1b40466d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f79c1b40466d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----f79c1b40466d---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&user=Prince+Grover&userId=2f052b054cee&source=-----f79c1b40466d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&user=Prince+Grover&userId=2f052b054cee&source=-----f79c1b40466d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff79c1b40466d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f79c1b40466d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f79c1b40466d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f79c1b40466d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f79c1b40466d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgrover3?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pgrover3?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prince Grover"}, {"url": "https://medium.com/@pgrover3/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3.5K Followers"}, {"url": "http://Manifold.ai", "anchor_text": "Manifold.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2f052b054cee&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&user=Prince+Grover&userId=2f052b054cee&source=post_page-2f052b054cee--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F53a4075eb9b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcustom-loss-functions-for-gradient-boosting-f79c1b40466d&newsletterV3=2f052b054cee&newsletterV3Id=53a4075eb9b7&user=Prince+Grover&userId=2f052b054cee&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}