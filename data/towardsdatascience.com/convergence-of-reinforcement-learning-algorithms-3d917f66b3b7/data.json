{"url": "https://towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7", "time": 1683005578.598733, "path": "towardsdatascience.com/convergence-of-reinforcement-learning-algorithms-3d917f66b3b7/", "webpage": {"metadata": {"title": "Convergence of Reinforcement Learning Algorithms | by Nathan Lambert | Towards Data Science", "h1": "Convergence of Reinforcement Learning Algorithms", "description": "Deep reinforcement learning convergence is tricky. Here we study how Q learning (and value / policy iteration) converges numerically."}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Stochastic_matrix", "anchor_text": "stochastic matrix", "paragraph_index": 11}, {"url": "https://math.stackexchange.com/questions/40320/proof-that-the-largest-eigenvalue-of-a-stochastic-matrix-is-1", "anchor_text": "guaranteed", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Geometric_series#Proof_of_convergence", "anchor_text": "Proof for geometric series convergence", "paragraph_index": 24}, {"url": "https://arxiv.org/pdf/1812.11103.pdf", "anchor_text": "run", "paragraph_index": 33}, {"url": "https://bair.berkeley.edu/blog/2018/11/30/visual-rl/", "anchor_text": "fold towels", "paragraph_index": 33}, {"url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16669/16677", "anchor_text": "play", "paragraph_index": 33}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com", "paragraph_index": 40}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com", "paragraph_index": 40}], "all_paragraphs": ["Deep reinforcement learning algorithms may be the most difficult algorithms in recent machine learning developments to put numerical bounds on their performance (among those that function). The reasoning is twofold:", "Here, I will walk you through a heuristic we can use to describe how RL algorithms can converge, and explain how to generalize it to more scenarios.", "Some related articles that can be read before, or after this:", "This article addresses the question of how do iterative methods like value iteration, q-learning, and advanced methods converge when training? Here we will briefly review Markov Decision Processes (see links 1 and 3 above for more detail), explain how we can view Bellman Updates as an eigenvector equation (more in link 2), and show the math for how the values converge to optimal quantities.", "Markov Decision Processes (MDPs) are the stochastic model underpinning reinforcement learning (RL). If you\u2019re familiar, you can skip this section.", "There are two important characteristic utilities of a MDP \u2014 values of a state, and q-values of a chance node. The * in any MDP or RL value denotes an optimal quantity. The utilities are 1) a value of a state and 2) a Q-value of a state, action pair.", "What we try to do is estimate these values, q-values, and utilities so that our agents can plan actions that maximize reward.", "In the first section of new content I will recall the RL concepts I am using, and highlight the mathematical transformations needed to get a system of equations that evolves in discrete steps and has a convergence bound.", "RL is the paradigm where we are trying to \u201csolve\u201d and MDP, but we don\u2019t know the underlying environment. The simple RL solutions are sampling-based variants of fundamental MDP-solving algorithms (Value and Policy Iteration). Recall Q-value Iteration, which is the Bellman Update I will focus on:", "Looking at how accurate Value Iteration or Policy Iteration distills to comparing a value vector after each assignment (\u2190) in the above equation, which is one round of the recursive update. The convergence of these methods yields a measure proportional to how reinforcement learning algorithms will converge because reinforcement learning algorithms are sampling-based versions of Value and Policy Iteration, with a few more moving parts.", "Recall: Q-learning is the same update rule as Q-value Iteration, but the transition function is replaced by the action of sampling and the reward function is replaced with the actual sample, r, received from the environment.", "We need to formulate our Bellman Updates as a linear operator, B,(a matrix is a subset of linear operators) and see if we can get it to behave as a stochastic matrix. A stochastic matrix is guaranteed to have an eigenvector paired with the eigenvalue 1.", "This says, we can study the iterative updates in RL like we would the evolution of an eigenspace.", "For now, we need to make a couple of notational changes to transition from formulas on Q-values in a matrix to formulas that are acting on Utilities in a vector (Utilities generalize to values and Q-values, anyways because it is defined as the discounted sum of rewards).", "These changes are crucial for formulating the problem as eigenvectors.", "Start with the base equation for Q-value iteration below, how can we generalize this to a linear system?", "In this case, we need to change the right-hand side so it is in the form below:", "This leaves us with a final form (merging equations from this section, and the end of the eigenvalue section). Can we use this as the linear operator that we need? Consider if we are using the optimal policy (without loss of generality), then the tricky maximum over actions drops out, leaving us with:", "In this section, I will derive a relationship that guarantees a minimum error of epsilon after N steps and show what it means.", "We saw above that we can formulate the utility update rule in a way that is very close to an eigenvector, but we were off by a constant vector r representing the underlying reward surface of an MDP. What happens if we take the difference between two utility vectors? The constant term drops out.", "We now have our system that we can study like a dynamical system of eigenvectors, but we are working in the space of the difference between utilities, with a matrix B \u2014 the sum of weighted transition probabilities. Because the utility vectors are defined by each other, we can magically rearrange this (substitute the recursive Bellman Equation, take the norm).", "Studying the difference between any utility estimate is ingenious, because it shows a) how an estimate differs from the true or b) how the data from only the recursive update evolves (not the little vector r).", "Any convergence proof will be looking for a relationship between the error bound, \u03b5, and the number of steps, N,(iterations). This relationship will give us the chance to bound the performance with an analytical equation.", "In other words, we are looking for a bound for epsilon that this a function of N.", "To start, we know (by definition of an MDP) that the reward at each step, r, is bounded in the interval [-Rmax, +Rmax]. Then, looking at the definition of utility (discounted sum of reward) as a geometric series, we can bound the difference from any vector to the true vector. (Proof for geometric series convergence).", "The bound comes from the worst-case estimate \u2014 where the true reward at every step is +Rmax, but we initialize our estimate to -Rmax. Alas, we have an initial bound on the error of our utilities! Recall U0 is what we initialize the utility vector to, and then the index will increase each time we run a Bellman Update.", "To the Bellman Updates \u2014 how does this bound at initialization evolve with each step? Above, we see that the error is reduced by the discount factor at each step (from how the sum in the recursive update is always prepended with a gamma). This evolves below into a series of decreasing errors with each iteration.", "All that is left is declaring the bound, epsilon, in relation to the number of steps, N.", "On the right-hand side of the equation above, we have a bound on the accuracy of our utility estimate.", "Logically \u2014 for any epsilon, we know that the error will be less than epsilon in N steps.", "We can also change back and forth between epsilon and N with a mathematical trick \u2014 so if we know how accurate we want the estimates to be, we can solve for how long to let our algorithm run!", "The bound we have found is the bound on the cumulative value error across the state-space (solid line below). What the astute reader will wonder is, how does the policy error compare? Conceptually, this means, \u2018in how many states would the current policy differ from the optimal?\u2019 Turns out, with some normalization of the values (so they are numerically similar in magnitude) the policy error converges faster and gets to zero more rapidly (no asymptote!).", "This represents the advantage in some situations to use Policy Iteration over Value Iteration. Does it carry to more algorithms?", "Bounding deep RL algorithms is what everyone wants. We have seen impressive results in recent years where robots can run, fold towels, and play games. It would be fantastic if we have bounds on performance.", "What we can do, is bound how our representation of the world will converge. We have shown that the utility function will converge. There are two lasting challenges:", "We can study algorithm convergence, but the majority of engineering problems limiting adoption of deep RL for real world tasks are reward engineering and safe learning.", "That is where I leave you \u2014 a call to action to help us engineer better systems, so we can show off more of the underlying mathematics governing it.", "Here is my summary of what people are employing in Deep RL Research.", "More? Subscribe to my newsletter on robotics, artificial intelligence, and society!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Trying to think freely and create equitable & impactful automation @ UCBerkeley EECS. Subscribe directly at robotic.substack.com. More at natolambert.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3d917f66b3b7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://natolambert.medium.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d----3d917f66b3b7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@agk42?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Alex Knight"}, {"url": "https://www.pexels.com/photo/high-angle-photo-of-robot-2599244/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://towardsdatascience.com/what-is-a-markov-decision-process-anyways-bdab65fd310c", "anchor_text": "What is a Markov Decision Process anyways?"}, {"url": "https://towardsdatascience.com/the-hidden-linear-algebra-of-reinforcement-learning-406efdf066a", "anchor_text": "The hidden linear algebra of reinforcement learning."}, {"url": "https://towardsdatascience.com/fundamental-iterative-methods-of-reinforcement-learning-df8ff078652a", "anchor_text": "Fundamentals iterative methods of reinforcement learning."}, {"url": "https://inst.eecs.berkeley.edu/~cs188/sp20/assets/lecture/lec10.pdf", "anchor_text": "lecture"}, {"url": "https://inst.eecs.berkeley.edu/~cs188/sp20/", "anchor_text": "cs188 at UC Berkeley"}, {"url": "https://en.wikipedia.org/wiki/Stochastic_matrix", "anchor_text": "stochastic matrix"}, {"url": "https://math.stackexchange.com/questions/40320/proof-that-the-largest-eigenvalue-of-a-stochastic-matrix-is-1", "anchor_text": "guaranteed"}, {"url": "https://en.wikipedia.org/wiki/Lambda#Lower-case_letter_%CE%BB", "anchor_text": "\u03bb"}, {"url": "https://en.wikipedia.org/wiki/Generalized_eigenvector", "anchor_text": "generalized eigenvector"}, {"url": "https://en.wikipedia.org/wiki/Geometric_series#Proof_of_convergence", "anchor_text": "Proof for geometric series convergence"}, {"url": "https://en.wikipedia.org/wiki/Geometric_series", "anchor_text": "geometric series"}, {"url": "http://aima.cs.berkeley.edu/", "anchor_text": "Source"}, {"url": "https://arxiv.org/pdf/1812.11103.pdf", "anchor_text": "run"}, {"url": "https://bair.berkeley.edu/blog/2018/11/30/visual-rl/", "anchor_text": "fold towels"}, {"url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/16669/16677", "anchor_text": "play"}, {"url": "https://towardsdatascience.com/getting-just-the-gist-of-deep-rl-algorithms-dbffbfdf0dec", "anchor_text": "Gists of Recent Deep RL AlgorithmsA resource for getting the gist of RL algorithms without needing to surf through piles of documentation or equations.towardsdatascience.com"}, {"url": "https://robotic.substack.com/", "anchor_text": "Democratizing AutomationA blog about robots & artificial intelligence, making them beneficial for everyone, and the coming automation wave\u2026robotic.substack.com"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----3d917f66b3b7---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----3d917f66b3b7---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3d917f66b3b7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----3d917f66b3b7---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----3d917f66b3b7---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&user=Nathan+Lambert&userId=890b1765e6d&source=-----3d917f66b3b7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&user=Nathan+Lambert&userId=890b1765e6d&source=-----3d917f66b3b7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3d917f66b3b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3d917f66b3b7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3d917f66b3b7--------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://natolambert.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathan Lambert"}, {"url": "https://natolambert.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "653 Followers"}, {"url": "http://robotic.substack.com", "anchor_text": "robotic.substack.com"}, {"url": "http://natolambert.com", "anchor_text": "natolambert.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F890b1765e6d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&user=Nathan+Lambert&userId=890b1765e6d&source=post_page-890b1765e6d--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F15278b7ad062&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fconvergence-of-reinforcement-learning-algorithms-3d917f66b3b7&newsletterV3=890b1765e6d&newsletterV3Id=15278b7ad062&user=Nathan+Lambert&userId=890b1765e6d&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}