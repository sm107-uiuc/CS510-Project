{"url": "https://towardsdatascience.com/ml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b", "time": 1683013807.5735219, "path": "towardsdatascience.com/ml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b/", "webpage": {"metadata": {"title": "ML from Scratch: K-Nearest Neighbors Classifier | by Aman Sharma | Towards Data Science", "h1": "ML from Scratch: K-Nearest Neighbors Classifier", "description": "When it comes to solving classification problems via machine learning, there\u2019s a wide variety of algorithm choices available for almost any data type or niche problem that one might be dealing with\u2026"}, "outgoing_paragraph_urls": [{"url": "https://archive.ics.uci.edu/ml/datasets/wine+quality#:~:text=UCI%20Machine%20Learning%20Repository%3A%20Wine%20Quality%20Data%20Set&text=Abstract%3A%20Two%20datasets%20are%20included,%2C%20%5BWeb%20Link%5D).", "anchor_text": "UCI Red Wine Dataset", "paragraph_index": 17}, {"url": "https://datamahadev.com/machine-learning-classification-algorithms-step-by-step-comparison/", "anchor_text": "In another article of mine", "paragraph_index": 71}, {"url": "https://github.com/amansharma2910/MachineLearning-Scratch_WineClassificationKNN", "anchor_text": "Link to the project GitHub files", "paragraph_index": 73}], "all_paragraphs": ["When it comes to solving classification problems via machine learning, there\u2019s a wide variety of algorithm choices available for almost any data type or niche problem that one might be dealing with. These algorithmic choices can be broadly categorized into two groups, which are as follows.", "Today, in this article, we are going to study one such non-parametric classification algorithm in detail\u2014 the K-Nearest Neighbors (KNN) algorithm.", "This is going to be a project-based guide, where in the first part, we will be understanding the basics of the KNN algorithm. This will then be followed by a project where we will be implementing a KNN model from scratch using basic PyData libraries like NumPy and Pandas, while understanding the mathematical foundations of the algorithm.", "So buckle up, and let\u2019s get started!", "To begin with, the KNN algorithm is one of the classic supervised machine learning algorithms that is capable of both binary and multi-class classification. Non-parametric by nature, KNN can also be used as a regression algorithm. However, for the scope of this article, we will only focus on the classification aspect of KNN.", "\u2192 Used for both regression and classification", "\u2192 Support for both binary and multi-class classification", "Before we move any further, let us first break down the definition and understand a few of the terms that we came across.", "With that done, we have a rough idea regarding what KNN is. But now, a very important question arises.", "As we read earlier, KNN is a non-parametric algorithm. Therefore, training a KNN classifier doesn\u2019t require going through the more traditional approach of iterating over the training data for multiple epochs in order to optimize a set of parameters.", "Rather, the actual training process in the case of KNN is quite the opposite. Training a KNN model involves simply fitting (or saving) all the training data instances into the computer memory at the same time, which technically requires a single training cycle.", "After this is done, then during the inference stage, where the model has to predict the target class for a completely new data point, the model simply compares this new data with the existing training data instances. Then finally, on the basis of this comparison, the model assigns this new data point to its target class.", "But now another question arises. What exactly is this comparison that we are talking about, and how does it occur? Well quite honestly, the answer to this question is hidden in the name of the algorithm itself \u2014 K-Nearest Neighbors.", "To understand this better, let us dive deeper into how the inference process works.", "And that\u2019s how the KNN classification algorithm works.", "Regarding the calculation of the distances between the data points, we will be using the Euclidean distance formula. We will be understanding this distance calculation in the next section where we will be code our own KNN-based machine learning model from scratch.", "So now onto the fun, practical part! We will begin by having a quick glance at the problem statement that we are addressing via our project.", "For this project, we will be working on the famous UCI Red Wine Dataset. The aim of this project is to create a machine learning solution that can predict the quality of a red wine sample.", "This is a multi-class classification problem. The target variable, i.e., the \u2018quality\u2019 of the wine, accepts a discrete integer value ranging from 0\u201310, where a quality score of 10 denotes a wine of the highest quality standards.", "Now that we have understood the problem, let us begin with the project by importing all the necessary project dependencies, which includes the necessary PyData modules and the dataset.", "In the first step, let us import all the necessary Python modules.", "Now that we have imported the dataset, let us try to understand what each of the columns in our data denotes.", "The following is a brief description of all the individual columns within our dataset.", "As we discussed earlier, the \u2018quality\u2019 column is the target variable for this project. The rest of the columns in the dataset represent the feature variables that will be used for training the model.", "Now that we know what the different columns in our dataset represent, let us move on to the next section where we will be doing some pre-processing and exploration of our data.", "Data wrangling (or preprocessing) involves analyzing the data to see if it needs any sort of cleaning or scaling so that it can be prepared for training the model.", "As the first step of data preprocessing, we will check if there are any null values within our data that need to be dealt with.", "As we can see, there are no null values within our dataset. This is a good thing since we won\u2019t have to deal with any missing data. Now, let us have a look at the statistical analysis of the data.", "One prominent observation from the above given statistical analysis is that there\u2019s a visible inconsistency in the range of values across different columns within our dataset. To be more clear, the values in some columns are of the order 1e-1 while in a few others, the values can go as high as of the order 1e+2. Because of this inconsistency, there are chances that a feature weight bias might arise at the time of training the model. What is basically means is that some features might end up affecting the final prediction more than the others. Therefore, in order to prevent this weight imbalance, we will have to scale our data.", "For this scaling, we will be standardizing our data. Standardization typically means rescaling data in a way such that each feature column has a mean of 0 and a standard deviation of 1 (unit variance).", "The following is the mathematical formula for standard scaling.", "Now that we know the mathematical formula, let us go ahead and implement this from scratch in Python.", "Step-1: Separating the feature matrix and the target array.", "Step-3: Performing standardization on the feature set.", "With this, we are done with standardized our data. This will most probably take care of weight bias.", "Now for the last part of our data wrangling and EDA section, we will have a look at the distribution of values across the target column of our dataset.", "Some of the observations from the above-given graph are-", "Now that we are done exploring our data, let us move on to the final part of our project where we will be code our multi-class classifier KNN model using NumPy.", "As the first step of our modeling process, we will first split our dataset into training and test sets. This is done because training and evaluating your model on the same data is considered a bad practice.", "Let\u2019s see how to implement the code to split the dataset using Python.", "Step-2: Running the splitting function on our standardized dataset.", "Now that we have created our training and validation sets, we will finally see how to implement the model.", "On the basis of what we have learned till now, here are the steps involved in creating a KNN model.", "Step-1: Training the model- As we read earlier, in the case of KNN, this simply means saving the training dataset within the memory. We have already done that when we created our training and validation data splits.", "Step-2: Calculating the distance- A part of the inference process in the KNN algorithm, the process of calculating the distance is an iterative process where we calculate the Euclidean distance of a data point (basically, a data instance/row) in the test data from every single data point within the training data.", "Now, let us understand how the Euclidean distance formula works so that we can implement it for our model.", "Therefore, the Euclidean distance between the data points A and B is calculated using the following formula-", "Let\u2019s now implement this distance calculation step in Python.", "Before we move on to the next step, let us test our distance function.", "As we can see, our distance function successfully calculated the distance of the first point in our test data from all the training data points. Now we can move on to the next step.", "Step-3: Selecting k-nearest neighbors and making the prediction- This is the final step of the inference stage, where the algorithm selects the k-closest training data points to the test data point based on the distances calculated in step 2. Then, we consider the target labels of these k-nearest neighboring points. The label with the highest occurring frequency is assigned as the target class to the test data point.", "Let us see how to implement this in Python.", "Step-3.1: Defining the KNN Classification function.", "Step-3.2: Running inference on our test dataset.", "With this, we have completed the modeling and inference process. As a final step, we will evaluate our models\u2019 performance. For this, we will be using a simple accuracy function that calculates the number of correct predictions made by our model.", "Let\u2019s have a look at how to implement the accuracy function in Python.", "Step-2: Checking the accuracy of our model.", "Step-3: Comparing with the accuracy of a KNN classifier built using the Scikit-Learn library.", "An interesting observation here! Though our model didn\u2019t perform very well (with only 57% correct prediction), it has the exact same accuracy as a Scikit-Learn KNN model. This means the model that we defined from scratch was least able to replicate the performance of a pre-defined model, which is an achievement in itself!", "However, I believe we can further improve the model\u2019s performance to some extent. Therefore, as the last part of our project, we will find the best value of the hyperparameter \u2018k\u2019 for which our model gives the highest accuracy.", "Before we actually go on to finding the best k-value, first, let us understand the importance of the k-value in the K-Nearest Neighbors algorithm.", "The following graph roughly represents the relation between the k-value and stability of a KNN classifier model.", "Now, let us finally evaluate the model for a range of different k-values. The one with the highest accuracy will be chosen as the final k-value for our model.", "As we can see, k-value 1 has the highest accuracy. But as we discussed earlier, for k=1, the model will be very sensitive to outliers. Hence, we will go with k=8 which has the second-highest accuracy. Let us observe the results with k=8.", "As you can see, we got a performance boost here! Just by tweaking the hyperparameter \u2018k\u2019, our model\u2019s accuracy bumped up by almost 3 percent.", "With this, we come to the end of our project.", "To finish things off, let us have a quick rundown of all that we learned today, as well as some of the key takeaways from this lesson.", "In this article, we had an in-depth analysis of the K-Nearest Neighbors classification algorithm. We understood how the algorithm uses Euclidean distance between the data instances as a criterion of comparison, on the basis of which it predicts the target class for a particular data instance.", "In the second part of this guide, we went through a step-by-step process of creating a KNN classification model from scratch, primarily using Python and NumPy.", "Though our model was not able to give a stellar performance, at least we were able to match the performance of a predefined Scikit-Learn mode. Now, while we were able to increase the model\u2019s accuracy up to 60% via hyperparameter optimization, the performance was still not apt.", "This has a lot to do with how the data was structured. As we observed earlier, there was a huge sampling bias in the data. This certainly affected our model\u2019s performance. Another reason for the poor performance can be that probably, the data had a large number of outliers. All this brings us to a very important part that I left for the very end, where we will have a look at the advantages and disadvantages of the KNN classification algorithm.", "With this, we finally come to an end of today\u2019s learning session. In another article of mine, I have directly pitted a bunch of machine learning algorithms against each other. There, you can check out how KNN fares against other classification algorithms like logistic regression, decision tree classifiers, random forest ensembles, etc.", "By the way, this was the fourth article in my ML from Scratch series, where I cover different machine learning algorithms and their mathematical foundations in detail. If you are interested in learning more, the other articles in this series are-", "Link to the project GitHub files.", "If you liked the article and would love to keep seeing more articles in the ML from Scratch series, make sure you hit that follow button.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Programming computers to do the work for me and using data to solve problems are my passion. My machine and I are learning. Connect on Twitter @amansharma2910"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3fc51438346b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://amansharma2910.medium.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": ""}, {"url": "https://amansharma2910.medium.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Aman Sharma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e03a4d7261&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&user=Aman+Sharma&userId=84e03a4d7261&source=post_page-84e03a4d7261----3fc51438346b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@qwitka?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Maksym Kaharlytskyi"}, {"url": "https://unsplash.com/s/photos/wine?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://datamahadev.com/classification-algorithms-explained-in-30-minutes/", "anchor_text": "traditional machine learning algorithms"}, {"url": "https://datamahadev.com/artificial-neural-networks-detailed-explanation/", "anchor_text": "artificial neural nets"}, {"url": "https://archive.ics.uci.edu/ml/datasets/wine+quality#:~:text=UCI%20Machine%20Learning%20Repository%3A%20Wine%20Quality%20Data%20Set&text=Abstract%3A%20Two%20datasets%20are%20included,%2C%20%5BWeb%20Link%5D).", "anchor_text": "UCI Red Wine Dataset"}, {"url": "https://datamahadev.com/sampling-bias-in-data-science/", "anchor_text": "this article"}, {"url": "https://datamahadev.com/machine-learning-classification-algorithms-step-by-step-comparison/", "anchor_text": "In another article of mine"}, {"url": "https://towardsdatascience.com/ml-from-scratch-multinomial-logistic-regression-6dda9cbacf9d", "anchor_text": "ML from Scratch-Multinomial Logistic RegressionYour Complete Guide to Multinomial Logistic Regression, a.k.a. Softmax Regressiontowardsdatascience.com"}, {"url": "https://towardsdatascience.com/polynomial-regression-using-pytorch-from-scratch-500b7887b0ed", "anchor_text": "Polynomial Regression Using PyTorch (From Scratch)When it comes to predictive analysis, regression models prove out to be one of the most cost-efficient methods. While\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/linear-regression-model-with-numpy-7d270feaca63", "anchor_text": "Linear Regression Model with NumPyIn this project, we will see how to create a machine learning model that uses Multiple Linear Regression algorithm.towardsdatascience.com"}, {"url": "https://github.com/amansharma2910/MachineLearning-Scratch_WineClassificationKNN", "anchor_text": "Link to the project GitHub files"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3fc51438346b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----3fc51438346b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----3fc51438346b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/classification?source=post_page-----3fc51438346b---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----3fc51438346b---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&user=Aman+Sharma&userId=84e03a4d7261&source=-----3fc51438346b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&user=Aman+Sharma&userId=84e03a4d7261&source=-----3fc51438346b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3fc51438346b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3fc51438346b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3fc51438346b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3fc51438346b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3fc51438346b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3fc51438346b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3fc51438346b--------------------------------", "anchor_text": ""}, {"url": "https://amansharma2910.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://amansharma2910.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Aman Sharma"}, {"url": "https://amansharma2910.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "94 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F84e03a4d7261&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&user=Aman+Sharma&userId=84e03a4d7261&source=post_page-84e03a4d7261--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F186d48c6a7d2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-from-scratch-k-nearest-neighbors-classifier-3fc51438346b&newsletterV3=84e03a4d7261&newsletterV3Id=186d48c6a7d2&user=Aman+Sharma&userId=84e03a4d7261&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}