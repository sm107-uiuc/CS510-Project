{"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955", "time": 1683000951.422473, "path": "towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955/", "webpage": {"metadata": {"title": "The Basics: KNN for classification and regression | by Max Miller | Towards Data Science", "h1": "The Basics: KNN for classification and regression", "description": "Data science or applied statistics courses typically start with linear models, but in its way, K-nearest neighbors is probably the simplest widely used model conceptually. KNN models are really just\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Data science or applied statistics courses typically start with linear models, but in its way, K-nearest neighbors is probably the simplest widely used model conceptually. KNN models are really just technical implementations of a common intuition, that things that share similar features tend to be, well, similar. This is hardly a deep insight, yet these practical implementations can be extremely powerful, and, crucially for someone approaching an unknown dataset, can handle non-linearities without any complicated data-engineering or model set up.", "As an illustrative example, let\u2019s consider the simplest case of using a KNN model as a classifier. Let\u2019s say you have data points that fall into one of three classes. A two dimensional example may look like this:", "You can probably see pretty clearly that the different classes are grouped together \u2014 the upper left hand corner of the graphs seems to belong to the orange class, the right-hand/middle section to the blue class. If you were given the coordinates of new point somewhere on the graph and asked which class it was likely to belong to, most of the time the answer would be pretty clear. Any point in the upper left corner of the graph is likely to be orange, etc.", "The task becomes a little less certain in between the classes, however, where we need to settle on a decision boundary. Consider the new point added in red below:", "Should this new point be classified as orange or blue? The point seems to fall in between the two clusters. Your first intuition might be to choose the cluster that is closer to the new point. This would be the \u2018nearest neighbor\u2019 approach, and despite being conceptually simple, it frequently yields pretty sensible predictions. Which previously identified point is the new point nearest to? It might not be obvious just eyeballing the graph what the answer is, but it is easy for the computer to run through the points and give us an answer:", "It looks like the nearest point is in the blue category, so our new point probably is too. That is the nearest neighbor method.", "At this point you may be wondering what the \u2018k\u2019 in k-nearest-neighbors is for. K is the number of nearby points that the model will look at when evaluating a new point. In our simplest nearest neighbor example, this value for k was simply 1 \u2014 we looked at the nearest neighbor and that was it. You could, however, have chosen to look at the nearest 2 or 3 points. Why is this important and why would someone set k at a higher number? For one thing, the boundaries between classes might bump up next to each other in ways that make it less obvious that the nearest point will give us the right classification. Consider the blue and green regions in our example. In fact, let\u2019s zoom in on them:", "Notice that while the overall regions seem distinct enough, their boundaries seem to be a little intertwined with each other. This is a common feature of data sets with a bit of noise. When this is the case, it becomes harder to classify things in the boundary areas. Consider this new point:", "On the one hand, visually it definitely looks like the closest previously identified point is blue, which our computer can easily confirm for us:", "On the other hand, that nearest blue point seems like a bit of an outlier itself, far away from the center of the blue region and sort of surrounded by green points. And this new point is even on the outside of that blue point! What if we looked at the three nearest points to the new red point?", "Or even the nearest five points to the new point?", "Now it seems that our new point is in a green neighborhood! It happened to have a nearby blue point, but the preponderance or nearby points are green. In this case, maybe it would make sense to set a higher value for k, to look at a handful of nearby points and have them vote somehow on the prediction for the new point.", "The issue being illustrated is over-fitting. When k is set to one, the boundary between which regions are identified by the algorithm as blue and green is bumpy, it snakes back forth with each individual point. The red point looks like it might be in the blue region:", "Bringing k up to 5, however, smooths out the decision boundary as the different nearby points vote. The red point now seems firmly in the green neighborhood:", "The trade off with higher values of k is the loss of granularity in the decision boundary. Setting k very high will tend to give you smooth boundaries, but the real world boundaries you\u2019re trying to model might not be perfectly smooth.", "Practically speaking, we can use the same sort of nearest neighbors approach for regressions, where we want an individual value rather than a classification. Consider the following regression below:", "The data was randomly generated, but was generated to be linear, so a linear regression model would naturally fit this data well. I want to point out, though, that you can approximate the results of the linear method in a conceptually simpler way with a K-nearest neighbors approach. Our \u2018regression\u2019 in this case won\u2019t be a single formula like an OLS model would give us, but rather a best predicted output value for any given input. Consider the value of -.75 on the x-axis, which I\u2019ve marked with a vertical line:", "Without solving any equations we can come to a reasonable approximation of what the output should be just by considering the nearby points:", "It makes sense that the predicted value should be near these points, not much lower or higher. Perhaps a good prediction would be the average of these points:", "You can imagine doing this for all the possible input values and coming up with predictions everywhere:", "Connecting all these predictions with a line gives us our regression:", "In this case, the results aren\u2019t a clean line, but they do trace the upward slope of the data reasonably well. This may not seem terribly impressive, but one benefit of the simplicity of this implementation is that it handles non-linearity well. Consider this new collection of points:", "These points were generated by simply squaring the values from the previous example, but suppose you came across a data set like this in the wild. It\u2019s obviously not linear in nature and while an OLS style model can easily handle this sort of data, it requires the use of non-linear or interaction terms, meaning the data scientist has to make some decisions about what sort of data engineering to perform. The KNN approach requires no further decisions \u2014 the same code I used on the linear example can be re-used entirely on the new data to yield a workable set of predictions:", "As with the classifier examples, setting a higher value k helps us to avoid overfit, though you may start to lose predictive power on the margin, particularly around the edges of your data set. Consider the first example dataset with predictions made with k set to one, that is a nearest neighbor approach:", "Our predictions jump erratically around as the model jumps from one point in the dataset to the next. By contrast, setting k at ten, so that ten total points are averaged together for prediction yields a much smoother ride:", "Generally that looks better, but you can see something of a problem at the edges of the data. Because our model is taking so many points into account for any given prediction, when we get closer to one of the edges of our sample, our predictions start to get worse. We can address this issue somewhat by weighting our predictions to the nearer points, though this comes with its own tradeoffs.", "When setting up a KNN model there are only a handful of parameters that need to be chosen/can be tweaked to improve performance.", "K: the number of neighbors: As discussed, increasing K will tend to smooth out decision boundaries, avoiding overfit at the cost of some resolution. There is no single value of k that will work for every single dataset. For classification models, especially if there are only two classes, an odd number is usually chosen for k. This is so the algorithm never runs into a \u2018tie\u2019: e.g. it looks at the nearest four points and finds that two of them are in the blue category and two are in the red category.", "Distance metric: There are, as it turns out, different ways to measure how \u2018close\u2019 two points are to each other, and the differences between these methods can become significant in higher dimensions. Most commonly used is Euclidean distance, the standard sort you may have learned in middle school using the pythagorean theorem. Another metric is so-called \u2018Manhattan distance\u2019, which measures the distance taken in each cardinal direction, rather than along the diagonal (as if you were walking from one street intersection in Manhattan to another and had to follow the street grid rather than being able to take the shortest \u2018as the crow flies\u2019 route). More generally, these are actually both forms of what is called \u2018Minkowski distance\u2019, whose formula is:", "When p is set to 1, this formula is the same as Manhattan distance, and when set to two, Euclidean distance.", "Weights: One way to solve both the issue of a possible \u2019tie\u2019 when the algorithm votes on a class and the issue where our regression predictions got worse towards the edges of the dataset is by introducing weighting. With weights, the near points will count more than the points further away. The algorithm will still look at all k nearest neighbors, but the closer neighbors will have more of a vote than those further away. This isn\u2019t a perfect solution and brings up the possibility of overfitting again. Consider our regression example, this time with weights:", "Our predictions go right to the edge of the data set now, but you can see that our predictions now swing much closer to the individual points. The weighted method works reasonably well when you\u2019re between points, but as you get closer and closer to any particular point, that point\u2019s value has more and more of an influence on the algorithm\u2019s prediction. If you get close enough to a point, it\u2019s almost like setting k to one, since that point has so much influence.", "Scaling/normalizing: One last, but crucially important, point is that KNN models can be thrown off if different feature variables have very different scales. Consider a model that tries to predict, say, the sales price of a house on the market based on features like the number of bedrooms and the total square footage of the house, etc. There is more variance in the number of square feet in a house than there is in the number of bedrooms. Typically, houses only have a small handful of bedrooms, and not even the biggest mansion is going to have tens or hundreds of bedrooms. Square feet, on the other hand, are relatively small, so houses may range from close to, say, 1,000 sq feat on the small side to tens of thousands of sq feet on the large side.", "Consider the comparison between a 2,000 sq foot house with 2 bedrooms and a 2,010 sq foot house with two bedrooms \u2014 10 sq. feet hardly makes a difference. By contrast, a 2,000 sq foot house with three bedrooms is very different, and represents a very different and possibly more cramped layout. A naive computer wouldn\u2019t have the context to understand that, however. It would say that the 3 bedroom is only \u2018one\u2019 unit away from the 2 bedroom, while the 2,010 sq footer is \u2018ten\u2019 away from the 2,000 sq footer. To avoid this, feature data should be scaled before implementing a KNN model.", "KNN models are easy to implement and handle non-linearities well. Fitting the model also tends to be quick: the computer doesn\u2019t have to calculate any particular parameters or values, after all. The trade off here is that while the model is quick to set up, it\u2019s slower to predict, since in order to predict an outcome for a new value, it will need to search through all the points in its training set to find the nearest ones. For large datasets, KNN can therefore be a relatively slow method compared to other regressions that may take longer to fit but then make their predictions with relatively straightforward computations.", "One other issue with a KNN model is that it lacks interpretability. An OLS linear regression will have clearly interpretable coefficients that can themselves give some indication of the \u2018effect size\u2019 of a given feature (although, some caution must taken when assigning causality). Asking which features have the largest effect doesn\u2019t really make sense for a KNN model, however. Partly because of this, KNN models also can\u2019t really be used for feature selection, in the way that a linear regression with an added cost function term, like ridge or lasso, can be, or the way that a decision tree implicitly chooses which features seem most valuable.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc1e8a6c955&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----c1e8a6c955---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c1e8a6c955---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c1e8a6c955---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----c1e8a6c955---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/tag/knn?source=post_page-----c1e8a6c955---------------knn-----------------", "anchor_text": "Knn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=-----c1e8a6c955---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=-----c1e8a6c955---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c1e8a6c955---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c1e8a6c955--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c1e8a6c955--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c1e8a6c955--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "409 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}