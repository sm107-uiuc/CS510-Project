{"url": "https://towardsdatascience.com/audio-to-guitar-tab-with-deep-learning-d76e12717f81", "time": 1682996766.364336, "path": "towardsdatascience.com/audio-to-guitar-tab-with-deep-learning-d76e12717f81/", "webpage": {"metadata": {"title": "Automated Guitar Transcription with Deep Learning | by Darren Tio | Towards Data Science", "h1": "Automated Guitar Transcription with Deep Learning", "description": "This post outlines the implementation of automatic guitar transcription from audio files using Python, TensorFlow, and Keras as well as details the surface level methods performed. For training, the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://zenodo.org/record/1422265#.XQvsmohKi01", "anchor_text": "GuitarSet", "paragraph_index": 0}, {"url": "http://nemisig2019.nemisig.org/images/kimSlides.pdf", "anchor_text": "here", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "anchor_text": "Convolutional Neural Networks", "paragraph_index": 1}, {"url": "https://librosa.github.io/librosa/", "anchor_text": "libROSA", "paragraph_index": 7}, {"url": "https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing", "anchor_text": "Constant-Q transform toolbox for music processing", "paragraph_index": 27}], "all_paragraphs": ["This post outlines the implementation of automatic guitar transcription from audio files using Python, TensorFlow, and Keras as well as details the surface level methods performed. For training, the GuitarSet data set is employed for its large quantity of isolated guitar recordings with corresponding tabs. Please note that much of the direction in this project was provided by a research poster from NEMISIG 2019 found here.", "If you are familiar with Convolutional Neural Networks (CNNs), then you might have heard about their potential for image processing and analysis used for Computer Vision. It is this functionality of CNNs that we want to harness to output the guitar tab; therefore, it is first necessary to transform the input audio files into spectrogram images using the Constant-Q transform.", "In order to understand the benefits of using the Constant-Q transform over the Fourier transform to select frequencies and create our input images, we must examine how musical notes are defined:", "In the NOTE column, the musical note is identified by the letter on the left and the number on the right represents the current octave. Below is the plot of the frequencies (Hz) for the first six octaves of the musical note C.", "We can see that each of the sequential octaves is twice the frequency of the previous octave. Since an octave spans twelve notes, we know that the frequency must double every twelve notes, which can be represented by the following formula [1]:", "By plotting this relationship, we can see that the graph below displays an exponential curve:", "Due to this exponential nature, the Constant-Q transform is better suited for fitting musical data than the Fourier transform, as its output is amplitude versus the log frequency. Also, the Constant-Q transform\u2019s accuracy is analogous to the logarithmic scale and mimics the human ear, having a higher frequency resolution at the lower frequencies and a lower resolution at the higher frequencies [1].", "The Constant-Q transform can easily be applied to audio files in python using the libROSA library.", "By passing through each audio file in the GuitarSet data set from a specified start time (start) through a duration (dur) and saving the output as an image, we can create the input images necessary to train the CNN. For this project, dur was set to 0.2 seconds and start was set to increase from zero to the length of each audio file by the set duration, which can yield the following:", "Note that when being used as input images to the CNN, the color scheme was first converted to gray-scale.", "For each Constant-Q transform image, there must be a solution so that the network can adjust its guesses. Luckily, the GuitarSet data set contains all the notes played as MIDI values, time each note begins in the recording, and duration of the note for each audio file. Note: The following code snippets were placed in a function such that they could be used for every 0.2 seconds of audio.", "First, the unique notes (retrieved as MIDI notes) being played during the 0.2 seconds of audio loaded must be extracted from the jams files.", "There can only be six possible notes being played at one time (maximum of one notes on each string); therefore, code will often be repeated six times.", "First, a matrix (6, 18) of MIDI values which represents the six strings and 18 frets of a guitar is created under variable Fret:", "All possible locations of the unique notes retrieved on the guitar were then determined using Fret, where the matrix below shows a possible solution:", "All possible solutions for the combination of frets and strings must be determined. The idea of \u2018finger economy\u2019 is created \u2014 the lowest note of the chord, the root note, is compared to the rest of the notes in the chord where the number of frets (disregarding the string) each note is from the root note is summed to create a \u2018finger economy\u2019 number. The solution with the lowest \u2018finger economy\u2019 number is chosen as the correct chord shape.", "Although this method does not always match the correct version of the chord being played in the recording, it does not negatively affect performance of the CNN as a C major chord played in the open position does not differ than C major played on the 8th fret.", "The final solution is subsequently chosen using a combination of the strings and frets arrays in the final solution:", "Additionally, in the first column for each row, if there exists a note (1 in the row), a zero is appended and vice versa if a note does not exist. This is done so the softmax function can still choose a category for strings without a note being played.", "The previous code snippets return data such that the output is similar to a one-hot encoding of categories, the following matrix format was returned for each 0.2 seconds of audio:", "The above matrix is the guitar tab solution for one random 0.2 second selection from the GuitarSet data set. Each matrix shape is (6, 19) where the six rows correspond to each guitar string (eBGDAE from top to bottom). The first column identifies whether that the string is not being played, the second column identifies if the open string is being played, and the third through nineteenth columns identify the specific fret that is being played starting from the first fret. When training, this matrix is broken up into six separate arrays to train each head of the model.", "The Keras functional API was used to create the following multi-task classification model with a 90/10 split in training and test data. The model features six tasks (eBGDAE strings) to determine if the string is not played, open, or a note is being played. For each of these six outputs, a softmax activation along with a categorical cross-entropy loss function are applied. Dropout layers are added to reduce overfitting.", "The model was run for 30 epochs and the final accuracy for each string was recorded.", "The entirety of the GuitarSet data set audio files was not used for this model, however, a sufficient number of input files were used totaling 40828 training images and 4537 test samples.", "The accuracy of each string was determined to be:", "Which resulted in an average accuracy of 84.23%.", "This model is not yet ready to begin creating full length guitar tablature as a couple of issues still linger. The current model does not yet take into account the duration a note is held and will continue to repeat the tab for the duration specified in the code. Also, since chords can have different variations each containing the same notes, the model does not recognize when to use a specific voicing \u2014 which may prove inconvenient \u2014 but is not a significant problem. However, the model\u2019s ability to correctly tab audio snippets is a fantastic development.", "[1] C. Sch\u00f6rkhuber and Anssi Klapuri, Constant-Q transform toolbox for music processing (2010), 7th Sound and Music Computing Conference.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd76e12717f81&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d76e12717f81--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@darrentio?source=post_page-----d76e12717f81--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@darrentio?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Darren Tio"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9ddbd8662c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&user=Darren+Tio&userId=a9ddbd8662c9&source=post_page-a9ddbd8662c9----d76e12717f81---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@dylu?utm_source=medium&utm_medium=referral", "anchor_text": "Jacek Dylag"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://zenodo.org/record/1422265#.XQvsmohKi01", "anchor_text": "GuitarSet"}, {"url": "http://nemisig2019.nemisig.org/images/kimSlides.pdf", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53", "anchor_text": "Convolutional Neural Networks"}, {"url": "https://www.soundonsound.com/forum/viewtopic.php?f=16&t=64005", "anchor_text": "Source"}, {"url": "https://librosa.github.io/librosa/", "anchor_text": "libROSA"}, {"url": "https://www.researchgate.net/publication/228523955_Constant-Q_transform_toolbox_for_music_processing", "anchor_text": "Constant-Q transform toolbox for music processing"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d76e12717f81---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----d76e12717f81---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/python?source=post_page-----d76e12717f81---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/guitar?source=post_page-----d76e12717f81---------------guitar-----------------", "anchor_text": "Guitar"}, {"url": "https://medium.com/tag/keras?source=post_page-----d76e12717f81---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&user=Darren+Tio&userId=a9ddbd8662c9&source=-----d76e12717f81---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&user=Darren+Tio&userId=a9ddbd8662c9&source=-----d76e12717f81---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd76e12717f81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d76e12717f81---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d76e12717f81--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d76e12717f81--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d76e12717f81--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d76e12717f81--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d76e12717f81--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@darrentio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@darrentio?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Darren Tio"}, {"url": "https://medium.com/@darrentio/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9ddbd8662c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&user=Darren+Tio&userId=a9ddbd8662c9&source=post_page-a9ddbd8662c9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F916d4d3584f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faudio-to-guitar-tab-with-deep-learning-d76e12717f81&newsletterV3=a9ddbd8662c9&newsletterV3Id=916d4d3584f4&user=Darren+Tio&userId=a9ddbd8662c9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}