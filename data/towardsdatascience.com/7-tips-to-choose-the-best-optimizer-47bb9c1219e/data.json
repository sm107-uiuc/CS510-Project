{"url": "https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e", "time": 1683011555.420969, "path": "towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e/", "webpage": {"metadata": {"title": "7 tips to choose the best optimizer | by Davide Giordano | Towards Data Science", "h1": "7 tips to choose the best optimizer", "description": "In machine learning when we need to compute the distance between a predicted value and an actual value, we use the so-called loss function. Contrary to what many believe, the loss function is not the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://ruder.io/optimizing-gradient-descent/index.html", "anchor_text": "website", "paragraph_index": 7}], "all_paragraphs": ["In machine learning when we need to compute the distance between a predicted value and an actual value, we use the so-called loss function.", "Contrary to what many believe, the loss function is not the same thing as the cost function. While the loss function computes the distance of a single prediction from its actual value, the cost function is usually more general. Indeed, the cost function can be, for example, the sum of loss functions over the training set plus some regularization.", "Another term often incorrectly used as a synonym of the first two is represented by the objective function, that is the most general term for any function optimized during training.", "Once clarified the right terminology, we can give the definition of optimizer.", "Optimizers in machine learning are used to tune the parameters of a neural network in order to minimize the cost function.", "The choice of the optimizer is, therefore, an important aspect that can make the difference between a good training and bad training.", "Actually, there are many optimizers and so the choice is not straightforward. In this article, I\u2019ll briefly describe the most used ones and then I\u2019ll give you some guidelines that helped me in different projects and that I hope will help you when it comes to choosing the most suitable optimizer for your task.", "For further information about how the specific optimizers work, you can give a look to this website.", "As just said, there are many optimizers. Each of them has advantages and disadvantages often related to the specific task.", "I like to divide optimizers into two families: gradient descent optimizers and adaptive optimizers. This division is exclusively based on an operational aspect which forces you to manually tune the learning rate in the case of Gradient Descent algorithms while it is automatically adapted in adaptive algorithms \u2014 that\u2019s why we have this name.", "There are three types of gradient descent optimizers, which differ in how much data we use to compute the gradient of the objective function.", "Also known as vanilla gradient descent, it\u2019s the most basic algorithm among the three. It computes the gradients of the objective function J with respect to the parameters \u03b8 for the entire training set.", "As we use the entire dataset to perform just one step, batch gradient descent can be very slow. Moreover, it is not suitable for datasets that don\u2019t fit in memory.", "It is an improved version of batch gradient descent. Instead of computing the gradients over the entire dataset, it performs a parameter update for each example in the dataset.", "So the formula now depends also on the values of the input x and output y.", "The problem of SGD is that the updates are frequent and with a high variance, so the objective function heavily fluctuates during training.", "This fluctuation can be an advantage with respect to batch gradient descent because it allows the function to jump to better local minima, but at the same time it can represent a disadvantage with respect to the convergence in a specific local minima.", "A solution to this problem is to slowly decrease the learning rate value in order to make the updates smaller and smaller, so avoiding high oscillations.", "The intuition behind this algorithm is to exploit the advantages of both gradient descent\u2019s methods that we have seen so far.", "It basically computes the gradients on small batches of data in order to reduce the variance of the updates.", "This family of optimizers has been introduced to solve the issues of the gradient descent\u2019s algorithms. Their most important feature is that they don\u2019t require a tuning of the learning rate value. Actually some libraries \u2014 i.e. Keras \u2014 still let you the possibility to manually tune it for more advanced trials.", "It adapts the learning rate to the parameters performing small updates for frequently occurring features and large updates for the rarest ones.", "In this way, the network is able to capture information belonging to features that are not frequent, putting them in evidence and giving them the right weight.", "The problem of Adagrad is that it adjusts the learning rate for each parameter according to all the past gradients. So, the possibility of having a very small learning rate after a high number of steps \u2014 resulting from the accumulation of all the past gradients \u2014 is relevant.", "If the learning rate is too much small, we simply can\u2019t update weights and the consequence is that the network doesn\u2019t learn anymore.", "It improves the previous algorithm by introducing a history window which sets a fixed number of past gradients to take in consideration during the training.", "In this way, we don\u2019t have the problem of the vanishing learning rate.", "It is very similar to Adadelta. The only difference is in the way they manage the past gradients.", "It adds to the advantages of Adadelta and RMSprop, the storing of an exponentially decaying average of past gradients similar to momentum.", "Adam is the best choice in general. Anyway, many recent papers state that SGD can bring to better results if combined with a good learning rate annealing schedule which aims to manage its value during the training.", "My suggestion is to first try Adam in any case, because it is more likely to return good results without an advanced fine tuning.", "Then, if Adam achieves good results, it could be a good idea to switch on SGD to see what happens.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Artificial intelligence and robotics student. IT engineer. Sports lover."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F47bb9c1219e&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@davidegiordano?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidegiordano?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Davide Giordano"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F25b62fff1eb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&user=Davide+Giordano&userId=25b62fff1eb0&source=post_page-25b62fff1eb0----47bb9c1219e---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@markuswinkler?utm_source=medium&utm_medium=referral", "anchor_text": "Markus Winkler"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://ruder.io/optimizing-gradient-descent/index.html", "anchor_text": "website"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----47bb9c1219e---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----47bb9c1219e---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----47bb9c1219e---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----47bb9c1219e---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----47bb9c1219e---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&user=Davide+Giordano&userId=25b62fff1eb0&source=-----47bb9c1219e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&user=Davide+Giordano&userId=25b62fff1eb0&source=-----47bb9c1219e---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F47bb9c1219e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----47bb9c1219e---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----47bb9c1219e--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----47bb9c1219e--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----47bb9c1219e--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidegiordano?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@davidegiordano?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Davide Giordano"}, {"url": "https://medium.com/@davidegiordano/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "61 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F25b62fff1eb0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&user=Davide+Giordano&userId=25b62fff1eb0&source=post_page-25b62fff1eb0--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F36224c5986cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F7-tips-to-choose-the-best-optimizer-47bb9c1219e&newsletterV3=25b62fff1eb0&newsletterV3Id=36224c5986cc&user=Davide+Giordano&userId=25b62fff1eb0&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}