{"url": "https://towardsdatascience.com/policy-gradient-methods-104c783251e0", "time": 1683013663.573836, "path": "towardsdatascience.com/policy-gradient-methods-104c783251e0/", "webpage": {"metadata": {"title": "Policy-Gradient Methods. REINFORCE algorithm | by Jordi TORRES.AI | Towards Data Science", "h1": "Policy-Gradient Methods", "description": "This is a new post devoted to Policy-Gradient Methods, in the \u201cDeep Reinforcement Learning Explained\u201d series. Policy-Gradient methods are a subclass of Policy-Based methods that estimate an optimal\u2026"}, "outgoing_paragraph_urls": [{"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "previous post", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "Post 6", "paragraph_index": 5}, {"url": "http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf", "anchor_text": "original paper", "paragraph_index": 7}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "previous Post", "paragraph_index": 18}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "Post 13 of this series", "paragraph_index": 23}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole environment", "paragraph_index": 36}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 37}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 37}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction", "paragraph_index": 62}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "A2C, A3C, DDPG, TD3, SAC, PPO", "paragraph_index": 65}, {"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "anchor_text": "next post", "paragraph_index": 65}, {"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "anchor_text": "Reinforcement Learning frameworks", "paragraph_index": 65}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 66}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 66}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 67}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 68}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 71}], "all_paragraphs": ["This is a new post devoted to Policy-Gradient Methods, in the \u201cDeep Reinforcement Learning Explained\u201d series. Policy-Gradient methods are a subclass of Policy-Based methods that estimate an optimal policy\u2019s weights through gradient ascent.", "Intuitively, gradient ascent begins with an initial guess for the value of policy\u2019s weights that maximizes the expected return, then, the algorithm evaluates the gradient at that point that indicates the direction of the steepest increase of the function of expected return, and so we can make a small step in that direction. We hope that we end up at a new value of policy\u2019s weights for which the value of the expected return function is a little bit larger. The algorithm then repeats this process of evaluating the gradient and taking steps until it considers that it is eventually reached the maximum expected return.", "Although we have coded a deterministic policy in the previous post, Policy-based methods can learn either stochastic or deterministic policies. With a stochastic policy, our neural network\u2019s output is an action vector that represents a probability distribution (rather than returning a single deterministic action).", "The policy we will follow in the new method presented in this Post is selecting an action from this probability distribution. This means that if our Agent ends up in the same state twice, we may not end up taking the same action every time. Such representation of actions as probabilities has many advantages, for instance the advantage of smooth representation: if we change our network weights a bit, the output of the neural network will change, but probably just a little bit.", "In the case of a deterministic policy, with a discrete numbers output, even a small adjustment of the weights can lead to a jump to a different action. However, if the output is a probability distribution, a small change of weights will usually lead to a small change in output distribution. This is a very important property due gradient optimization methods are all about tweaking the parameters of a model a bit to improve the results.", "But how can be changed network\u2019s parameters to improve the policy? If you remember from Post 6, we solved a very similar problem using the Cross-Entropy method: our network took observations as inputs and returned the probability distribution of the actions. In fact, the cross-entropy method is, somehow, a preliminary version of the methods that we will introduce in this Post.", "The key idea underlying policy gradients is reinforcing good actions: to push up the probabilities of actions that lead to higher return, and push down the probabilities of actions that lead to a lower return, until you arrive at the optimal policy. The policy gradient method will iteratively amend the policy network weights (with smooth updates) to make state-action pairs that resulted in positive return more likely, and make state-action pairs that resulted in negative return less likely.", "To introduce this idea we will start with a vanilla version (the basic version) of the policy gradient method called REINFORCE algorithm ( original paper). This algorithm is the fundamental policy gradient algorithm on which nearly all the advanced policy gradient algorithms are based.", "Let\u2019s look at a more mathematical definition of the algorithm since it will be good for us in order to understand the most advanced algorithms in following Posts.", "The first thing we need to define is a trajectory, just a state-action-rewards sequence (but we ignore the reward). A trajectory is a little bit more flexible than an episode because there are no restrictions on its length; it can correspond to a full episode or just a part of an episode. We denote the length with a capital H, where H stands for Horizon, and we represent a trajectory with \u03c4:", "The method REINFORCE is built upon trajectories instead of episodes because maximizing expected return over trajectories (instead of episodes) lets the method search for optimal policies for both episodic and continuing tasks.", "Although for the vast majority of episodic tasks, where a reward is only delivered at the end of the episode, it only makes sense just to use the full episode as a trajectory; otherwise, we don\u2019t have enough reward information to meaningfully estimate the expected return.", "We denote the return for a trajectory \u03c4 with R(\u03c4), and it is calculated as the sum reward from that trajectory \u03c4:", "The parameter Gk is called the total return, or future return, at time step k for the transition k", "It is the return we expect to collect from time step k until the end of the trajectory, and it can be approximated by adding the rewards from some state in the episode until the end of the episode using gamma \u03b3:", "Remember that the goal of this algorithm is to find the weights \u03b8 of the neural network that maximize the expected return that we denote by U(\u03b8) and can be defined as:", "To see how it corresponds to the expected return, note that we have expressed the return R(\u03c4) as a function of the trajectory \u03c4. Then, we calculate the weighted average, where the weights are given by P(\u03c4;\u03b8), the probability of each possible trajectory, of all possible values that the return R(\u03c4) can take. Note that probability depends on the weights \u03b8 in the neural network because \u03b8 defines the policy used to select the actions in the trajectory, which also plays a role in determining the states that the agent observes.", "As we already introduced, one way to determine the value of \u03b8 that maximizes U(\u03b8) function is through gradient ascent.", "Equivalent to Hill Climbing algorithm presented in the previous Post, intuitively we can visualize that the gradient ascent draws up a strategy to reach the highest point of a hill, U(\u03b8), just iteratively taking small steps in the direction of the gradient:", "Mathematically, our update step for gradient ascent can be expressed as:", "where \u03b1 is the step size that is generally allowed to decay over time (equivalent to the learning rate decay in deep learning). Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that \u03b8 converges to the value that maximizes U(\u03b8).", "Gradient ascent is closely related to gradient descent, where the differences are that gradient descent is designed to find the minimum of a function (steps in the direction of the negative gradient), whereas gradient ascent will find the maximum (steps in the direction of the gradient). We will use this approach in our code in PyTorch.", "To apply this method, we will need to be able to calculate the gradient \u2207\u200bU(\u03b8); however, we won\u2019t be able to calculate the exact value of the gradient since that is computationally too expensive because, to calculate the gradient exactly, we\u2019ll have to consider every possible trajectory, becoming an intractable problem in most cases.", "Instead of doing this, the method samples trajectories using the policy and then use those trajectories only to estimate the gradient. This sampling is equivalent to the approach of Monte Carlo presented in Post 13 of this series, and for this reason, method REINFORCE is also known as Monte Carlo Policy Gradients.", "In summary, the pseudocode that describes in more detail the behavior of this method can be written as:", "Let\u2019s look a bit more closely at the equation of step 3 in the pseudocode to understand it. We begin by making some simplifying assumptions, for example, assuming that corresponds to a full episode.", "Remember that R(\u03c4) is just the cumulative rewards from the trajectory \u03c4 (the only one trajectory) at each time step. Assume that the reward signal at time step t and the sample play we are working with gives the Agent a reward of positive one (Gt=+1) if we won the game and a reward of negative one (Gt=-1) if we lost. In the other hand, the term", "looks at the probability that the Agent selects action at from state st in time step t. Remember that \u03c0 with the subscript \u03b8 refers to the policy which is parameterized by \u03b8. Then, the full expression takes the gradient of the log of that probability is", "This will tell us how we should change the weights of the policy \u03b8 if we want to increase the log probability of selecting action at from state st. Specifically, suppose we nudge the policy weights by taking a small step in the direction of this gradient. In that case, it will increase the log probability of selecting the action from that state, and if we step in the opposite direction will decrease the log probability.", "The following equation will do all of these updates all at once for each state-action pair, at and st, at each time step t in the trajectory:", "To see this behavior, assume that the Agent won the episode. Then, Gt is just a positive one (+1), and what the sum does is add up all the gradient directions we should step in to increase the log probability of selecting each state-action pair. That\u2019s equivalent to just taking H+1 simultaneous steps where each step corresponds to a state-action pair in the trajectory.", "In the opposite, if the Agent lost, Gt becomes a negative one, which ensures that instead of stepping in the direction of the steepest increase of the log probabilities, the method steps in the direction of the steepest decrease.", "The proof of how to derive the equation that approximates the gradient can be safely skipped, what interests us much more is the meaning of the expression.", "In Gradient methods where we can formulate some probability \ud835\udc5d which should be maximized, we would actually optimize the log probability log\ud835\udc5d instead of the probability p for some parameters \ud835\udf03.", "The reason is that generally, work better to optimize log\ud835\udc5d(\ud835\udc65) than \ud835\udc5d(\ud835\udc65) due to the gradient of log\ud835\udc5d(\ud835\udc65) is generally more well-scaled. Remember that probabilities are bounded by 0 and 1 by definition, so the range of values that the optimizer can operate over is limited and small.", "In this case, sometimes probabilities may be extremely tiny or very close to 1, and this runs into numerical issues when optimizing on a computer with limited numerical precision. If we instead use a surrogate objective, namely log p (natural logarithm), we have an objective that has a larger \u201cdynamic range\u201d than raw probability space, since the log of probability space ranges from (-\u221e,0), and this makes the log probability easier to compute.", "Now, we will explore an implementation of the REINFORCE to solve OpenAI Gym\u2019s Cartpole environment.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "First, we will import all necessary packages with the following lines of code:", "And also the OpenAI Gym\u2019s Cartpole Environment:", "We will build a neural network that serves as a policy network. The policy network will accept a state vectors as inputs, and it will produce a (discrete) probability distribution over the possible actions.", "The model is only two linear layers, with a ReLU activation function for the first layer, and the Softmax function for the last layer. By default, the initialization is with random weights).", "With the result of the neural network, the Agent samples from the probability distribution to take an action that will be executed in the Environment.", "The second line of this code samples an action from the probability distribuion produced by the policy network obtained in the firt line. Then in the last line of this code the Agent takes the action.", "The training loop trains the policy network by updating the parameters \u03b8 to following the pseudocode steps describes in the previous section.", "First we define the optimizer and initialize some variables:", "where is learning_rate is the step size \u03b1 , Horizon is the H and gammais \u03b3 in the previous pseudocode. Using these variables, the main loop with the number of iterations is defined by MAX_TRAJECTORIESis coded as:", "With score list we will keep track of the trajectory length over training time. We keep track of the actions and states in the list transactions for the transactions of the current trajectory.", "Following we compute the expected return for each transaction (code snippet from the previous listing):", "The listbatch_Gvals is used to compute the expected return for each transaction as it is indicated in the previous pseudocode. The list expected_return stores the expected returns for all the transactions of the current trajectory. Finally this code normalizes the rewards to be within in the [0,1] interval to improve numerical stability.", "The loss function requires an array of action probabilities, prob_batch, for the actions that were taken and the discounted rewards:", "For this purpose we recomputes the action probabilities for all the states in the trajectory and subsets the action-probabilities associated with the actions that were actually taken with the following two lines of code:", "An important detail is the minus sign in the loss function of this code:", "Why we introduced a - in the log_prob? In general, we prefer to set things up so that we are minimizing an objective function instead of maximizing, since it plays nicely with PyTorch\u2019s built-in optimizers (using stochastic gradient descend) . We should instead tell PyTorch to minimize 1-\u03c0 . This loss function approaches 0 as \u03c0 nears 1, so we are encouraging the gradients to maximize \u03c0 for the action we took.", "Also, let\u2019s remember that we use a surrogate objective, namely \u2013log \u03c0 (where log is the natural logarithm), because we have an objective that has a larger dynamic range than raw probability space (bounded by 0 and 1 by definition), since the log of probability space ranges from (\u2013\u221e,0), and this makes the log probability easier to compute.", "Finally, mention that we included in the code the following lines of code to control the progres of the training loop:", "We can visualise the results of this code running the following code:", "You should be able to obtain a plot with a nicely increasing trend of the trajectory duration.", "We also can render how the Agent applies the policy with the following code:", "Now that we know the two families of methods, what are the main differences between them?", "Policy methods will be the more natural choice in some situations, and in other situations, value methods will be a better option. In any case, and as we will see from the next post, both families of methods can be combined to achieve hybrid methods that take advantage of each of them\u2019 properties.", "In this post, we have explained in detail the REINFORCE algorithm, and we have coded it. As a stochastic gradient method, REINFORCE works well in simple problems, and has good theorical convergence properties.", "As R. Sutton and G. Barton indicates in the textbook Reinforcement Learning: An Introduction, by construction, the expected update over a trajectory is in the same direction as the performance gradient. This assures an improvement in expected performance for sufficiently small, and convergence to a local optimum under standard stochastic approximation conditions for decreasing . However, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning.", "But because we are using full Monte-Carlo return for calculating the gradient, the method may be of high variance and it is a problem for learning.", "Also, there are some limitations associated with REINFORCE algorithm:", "In summary, REINFORCE works well for a small problem like CartPole, but for a more complicated, for instance, Pong Environment, it will be painfully slow. Can REINFORCE be improved? Yes, there are many training algorithms that the research community created: A2C, A3C, DDPG, TD3, SAC, PPO, among others. However, programming these algorithms requires a more complex mathematical treatment, and its programming becomes more convoluted than that of REINFORCE. For this reason, in the next post, we will introduce Reinforcement Learning frameworks that simplify the use of these advanced methods, and above, all distributed algorithms.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F104c783251e0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----104c783251e0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 19"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/10-m%C3%A9todos-policy-based-reinforce-2f13c11b290f", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/10-m%C3%A9todos-policy-based-reinforce-2f13c11b290f", "anchor_text": "10. M\u00e9todos policy-based: REINFORCEAcceso abierto al cap\u00edtulo 10 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "previous post"}, {"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "Post 6"}, {"url": "http://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf", "anchor_text": "original paper"}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "previous Post"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "Post 13 of this series"}, {"url": "https://towardsdatascience.com/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4", "anchor_text": "Cartpole environment"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_19_REINFORCE_Algorithm.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://docs.ray.io/en/latest/rllib-toc.html#algorithms", "anchor_text": "A2C, A3C, DDPG, TD3, SAC, PPO"}, {"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "anchor_text": "next post"}, {"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "anchor_text": "Reinforcement Learning frameworks"}, {"url": "https://towardsdatascience.com/reinforcement-learning-frameworks-e349de4f645a", "anchor_text": "the next"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----104c783251e0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----104c783251e0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----104c783251e0---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----104c783251e0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----104c783251e0---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----104c783251e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----104c783251e0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F104c783251e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----104c783251e0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----104c783251e0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----104c783251e0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----104c783251e0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----104c783251e0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----104c783251e0--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpolicy-gradient-methods-104c783251e0&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}