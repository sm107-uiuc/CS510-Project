{"url": "https://towardsdatascience.com/self-paced-learning-for-machine-learning-f1c489316c61", "time": 1683003942.031581, "path": "towardsdatascience.com/self-paced-learning-for-machine-learning-f1c489316c61/", "webpage": {"metadata": {"title": "Self-Paced Learning for Machine Learning | by Phillip Wenig | Towards Data Science", "h1": "Self-Paced Learning for Machine Learning", "description": "You torment your neural net by cramming data into it ruthlessly! Usually, when training a machine learning model with Stochastic Gradient Descent (SGD), the training data set gets shuffled. In that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/wenig/spl", "anchor_text": "GitHub repository", "paragraph_index": 8}], "all_paragraphs": ["You torment your neural net by cramming data into it ruthlessly! Usually, when training a machine learning model with Stochastic Gradient Descent (SGD), the training data set gets shuffled. In that way, we make sure that the model sees different data points in no particular order and can learn the task evenly without getting stuck in local optima. However, already in 2009, Bengio et al. showed that a certain sorting is beneficial. They called their approach Curriculum Learning and showed that a machine learning model reaches higher accuracy if its training data is in a specific order. More precisely, easier examples in the beginning and harder ones towards the end. For their experiments, they used an already trained model and let that model decide what data points are easy or hard. A new model then trained on the correctly ordered data set and converged to a higher accuracy than models that were trained on random orders or the opposite curriculum.", "While working on my current project, I came across a technique called Self-Paced Learning (SPL). It\u2019s not a new idea and the respective paper was published about 10 years ago. Anyhow, this technique is quite interesting and still important as it helps Stochastic Gradient Descent (SGD) to converge faster and even at higher accuracy. It skips certain data points that are considered to be yet too hard. It is based on Curriculum learning but sorts the data while training. There is no need for an additional pre-trained model which would decide over the ordering. Hence the name Self-Paced Learning.", "The term of Self-Paced Learning originated from a learning technique used by humans. It allows you to define your speed to suit your learning patterns. SPL can be seen as studying or training a special skill, for instance, maths. When we started learning about maths, we started with counting, then went on to addition, subtraction, and so on. We didn\u2019t hear about matrix multiplication or derivatives until a certain age. In the same way, SPL for machine learning starts with very easy examples and, once learned, continues with harder ones benefitting from the already learned \u201cbasics\u201d. I imagine SPL as a kind of narrowing down a task over time. Consider a simple classification task in a 2-dimensional space and a model that needs to split two point clouds at the right spot. The easier samples are far away from the intersecting area. The harder samples are close to the intersecting area. The initial state of the model is a line somewhere in this space. If we start with only easy data points, the model gets gradients that tell it to go in a certain direction. If we started with only hard points, our model would know it is wrong and would get a direction to go to but might be far off again to the other side. Narrowing down the points to the right area helps the model avoid overshooting and converge more smoothly, as seen in the animation I created below.", "The trick is very simple. It uses a threshold, that we call lambda. It exists to be compared to the loss values of the data points in the training set. Usually, lambda starts very low at a number close to 0. With each epoch, lambda gets multiplied by a fixed factor greater than 1. The model which is being trained has to calculate the loss values of its training points to perform SGD. Generally, these loss values are getting smaller with further training iterations because the model is getting better at the training tasks and makes fewer mistakes. The threshold lambda is now determining whether a data point is considered easy or hard. Whenever the loss of a data point is below lambda, it is an easy data point. If it is above, it\u2019s considered hard. During training, the back-propagation step is only performed on easy data points and hard ones are skipped. Hence, the model increases the difficulty of the training instances during training whenever it has progressed enough. Certainly, in the beginning, the model may consider no data point to be easy and won\u2019t train at all. Therefore, the authors of SPL introduced a warm-up phase in which no skipping is allowed and only a small subset of the training set is used.", "The SPL paper introduces a two-step learning approach. Here, the loss function is trained twice keeping some variables fixed for each step. The loss functions need to be minimized given the model weights \u201cw\u201d and the variables \u201cv\u201d. In the loss function, we see several terms. The first term \u201cr(w)\u201d is a usual regularization term that helps the model avoid overfitting. It\u2019s also part of other loss functions not related to SPL. The second term is a sum over the data point loss \u201cf(x, y, w)\u201d of our model multiplied by a variable \u201cv\u201d. This variable \u201cv\u201d will decide later on whether the current data point \u201c(x, y)\u201d is easy enough to train on. The third term is a sum over all \u201cv\u201ds multiplied by the threshold \u201clambda\u201d which we already mentioned in earlier sections. The variables \u201cv\u201d are integers and can only take the values \u201c0\u201d or \u201c1\u201d. In the first learning step, the variables \u201cw\u201d are fixed and only the variables \u201cv\u201d are changed according to the optimization. If you look at the loss function carefully, we see that \u201clambda\u201d, indeed, acts as a threshold. If \u201cf(x, y, w)\u201d is smaller than lambda and \u201cv\u201d is one, we would subtract something from the regularization term. Hence, with \u201cv=0\u201d, we wouldn\u2019t subtract anything, which is bigger than subtracting something. If \u201cf(x, y, w)\u201d is bigger than lambda and \u201cv=1\u201d, \u201cf(x, y, w)-lambda\u201d would be positive and we would add something. Hence, with \u201cv=0\u201d, we wouldn\u2019t add anything, which is smaller than adding something. In summary, whenever \u201cf(x, y, w)\u201d is smaller than \u201clambda\u201d the first step optimizes \u201cL\u201d by setting \u201cv\u201d to \u201c1\u201d, otherwise to \u201c0\u201d. The second step fixes the before calculated \u201cv\u201d and optimizes \u201cw\u201d. If \u201cv\u201d was \u201c1\u201d, the usual model update is performed, for instance, back-propagation. If \u201cv\u201d was \u201c0\u201d, the gradients for \u201cf(x, y, w)\u201d will be also 0 and no update is performed (except for the regularization term but you can ignore this for now for a better understanding). Setting the threshold to a very low number, in the beginning, would result in nothing as all \u201cv\u201ds would be \u201c0\u201d because no data point loss would be below the threshold. Therefore, the authors of SPL suggested to have a warm-up phase without SPL for a certain number of iterations and start with SPL afterward.", "In the following code example, I am showing how to implement SPL with PyTorch on a dummy data set. In particular, we will be implementing the training for the animation that I have shown at the beginning of this blog post. At first, we will define a very easy model taking in 2 features and outputting two numbers which define the probability for each class. The output tells us what class the model thinks it\u2019s seeing. To transform the output in probabilities, we are using a softmax function. In the code, I am using a log_softmax function, though. This is due to the loss function I am using later on. In the end, the model trains in the same way.", "The loss function code can be seen in the next section. Here, we calculate the loss of each point, which is the NLL loss. If the loss is smaller than the threshold, we multiply the loss with one, otherwise with zero. Hence, zero multiplied losses don\u2019t have any effect on the training.", "The training function, in the end, looks as usual. We load a data loader, initialize the model and optimizer, and start iterating over the data set multiple epochs. For simplicity, I left out the plotting function for the animation.", "The whole project can be found on my GitHub repository. Feel free to play around with it. Also, the plotting function can be found there.", "Since the SPL approach is somehow sorting the data points by hardness based on loss, I had the idea to use it for anomaly detection. An anomaly is a data point that isn\u2019t similar to any other data point in the data set, is far off, and might be the result of a false input or a systematic error. If an anomaly occurs in the data set, its loss should be higher than the loss of normal points because machine learning models cannot generalize to errors if they are very rarely presented. The SPL approach should move the threshold over the anomaly in the very end. That way, we can easily classify them as anomalies by observing the order of \u201cactivation\u201d of data points, i.e. considering them as easy.", "For this experiment, I used the aforementioned code and didn\u2019t run a fixed number of epochs. Instead, I ran the training as long as there are more than 5 data points above the threshold and, hence, considered hard. Once, I have 5 or less, I stop the training and plot them as red points. As you can see in the animation below, the algorithm found an anomaly in the left lower part of the blue cloud. I added this anomaly by taking the furthest point from the orange centroid and changing its class to \u201corange\u201d.", "Certainly, this example isn\u2019t hard but it illustrates the problem anomaly detection is facing. If there are more dimensions than 2 or 3, the task gets more complex and obvious anomalies as in our example aren\u2019t found that easy.", "Why isn\u2019t everyone using SPL? It takes some time to find the right starting and growing factor values for the threshold because this isn\u2019t anything generic and varies based on models, loss functions, and data sets. For the examples I used in this post, I had to give it multiple tries to finally find the right configurations. Imagine you have a very large data set and also a very large model. It is basically infeasible to check the whole progress multiple times before you start the actual training. However, there are multiple other techniques for curriculum learning that fit different training setups. Despite those points, the current one has been a very intuitive idea that is easy to grasp and fun to work with. It\u2019s basically just another set of hyper-parameters you need to optimize ;-)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Data Scientist and PhD student drinking coffee and philosophizing about AI, algorithms, concepts, etc. and their effects on our world and society."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff1c489316c61&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1c489316c61--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pw33392?source=post_page-----f1c489316c61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pw33392?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Phillip Wenig"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9dbc010b40c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&user=Phillip+Wenig&userId=9dbc010b40c6&source=post_page-9dbc010b40c6----f1c489316c61---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@scw1217?utm_source=medium&utm_medium=referral", "anchor_text": "Suzanne D. Williams"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/wenig/spl", "anchor_text": "GitHub repository"}, {"url": "https://github.com/wenig/spl", "anchor_text": "GitHub repository with examples"}, {"url": "https://dl.acm.org/doi/abs/10.1145/1553374.1553380?casa_token=ep1KmLpw4FIAAAAA:_-S1hEHrOehjJkn164s4y1RAkDyNUmL5PG1YvpdwtunO06HQ7L53kGLc9TgIVvPBTOouvTY3zMsW", "anchor_text": "Curriculum Learning"}, {"url": "https://papers.nips.cc/paper/3923-self-paced-learning-for-latent-variable-models", "anchor_text": "Self-Paced Learning For Latent Variable Models"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----f1c489316c61---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f1c489316c61---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----f1c489316c61---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----f1c489316c61---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/anomaly-detection?source=post_page-----f1c489316c61---------------anomaly_detection-----------------", "anchor_text": "Anomaly Detection"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&user=Phillip+Wenig&userId=9dbc010b40c6&source=-----f1c489316c61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&user=Phillip+Wenig&userId=9dbc010b40c6&source=-----f1c489316c61---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff1c489316c61&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f1c489316c61---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f1c489316c61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f1c489316c61--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f1c489316c61--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f1c489316c61--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f1c489316c61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pw33392?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pw33392?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Phillip Wenig"}, {"url": "https://medium.com/@pw33392/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "306 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9dbc010b40c6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&user=Phillip+Wenig&userId=9dbc010b40c6&source=post_page-9dbc010b40c6--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc0c7c976a038&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fself-paced-learning-for-machine-learning-f1c489316c61&newsletterV3=9dbc010b40c6&newsletterV3Id=c0c7c976a038&user=Phillip+Wenig&userId=9dbc010b40c6&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}