{"url": "https://towardsdatascience.com/why-batchnorm-works-518bb004bc58", "time": 1683004838.122197, "path": "towardsdatascience.com/why-batchnorm-works-518bb004bc58/", "webpage": {"metadata": {"title": "The real reason why BatchNorm works | Medium | Towards Data Science", "h1": "The real reason why BatchNorm works", "description": "It makes the landscape of the corresponding optimization problem significantly more smooth."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Normalization techniques are some of the great tools we have while analyzing any form of data, a simple operation of adjusting the mean and variance of a distribution results in the catastrophic success of various normalizing techniques in deep neural networks, one of them being the famous batch normalization Ioffe et al.\u00b9.", "Currently, the most widely accepted explanation of BatchNorm\u2019s success, as well as its original motivation, relates to the so-called internal covariate shift (ICS). Informally, ICS refers to the change in the distribution of layer inputs caused by updates to the preceding layers. It is conjectured that such continual change negatively impacts training. The goal of BatchNorm was to reduce ICS and thus remedy this effect.", "In Ilyas et al.\u00b2, the authors present a view that there does not seem to be any link between the performance gain of BatchNorm and the reduction of internal covariate shift. Or that this link is tenuous, at best. In fact, they found that in a certain sense BatchNorm might not even be reducing internal covariate shift, and prove that, under natural conditions, the Lipschitzness of both the loss and the gradients (basically smoothness of the loss surface and its gradients) are improved in models with BatchNorm, hence demonstrating that BatchNorm impacts network training in a fundamental way: it makes the landscape of the corresponding optimization problem significantly more smooth.", "Here, we\u2019ll analyze the second-order properties of the error surface for models where the error space is quadratic in weights, then re-parametrizing the surface in order to get an analysis in terms of eigenspectrum of the covariance matrix of the input, and thence using the results to infer why batch norm might be improving the rate of convergence and why does it make the network invariant to initialization.", "For various learning algorithms that search the space of weights {W} for optimal values W* for which the error function E(W) is minimal, based on gradient descent, their properties are controlled by second-order properties of E(W) surface.", "For this analysis, I\u2019ll focus on a single layer of a neural network with X(Nx1) as an input vector to this layer and W(1xN) be the weights of this layer and Y(1x1) be the output of this layer, for convenience considering mean squared error:", "p is the size of the dataset", "Now clearly error surface is quadratic in weights, therefore rewriting it:", "where Xui is ith component of uth input vector(Nx1), therefore R is clearly the NxN input covariance matrix.", "Q is an N-dim vector, defined as:", "Now, the gradient of E(W) w.r.t. W is J(E) = RW-Q while the Hessian matrix of second derivatives is clearly H = R", "The solution space W* which minimizes E(W) obviously contains entries for which gradient is zero, therefore this solution space is the subspace of solutions of linear equation:", "Now, this RW=Q is a common type of problem in linear algebra(Ax=b), many problems in engineering applications can be solved by formulating in this format and then analyzing the eigenspectrum of A.", "As the solution space W* was a subspace of solutions of RW=Q, now if all columns of R are independent, therefore R has full rank, clearly, this subspace collapses to a single point(unique solution).", "Now all this is good, but why should we do this?", "The eigenvectors of R define the principal axes of E(W) when loss surface is quadratic in weights (this can be studied from \u201cActive Control of Noise and Vibration (Colin Hansen, Scott Snyder)\u201d section-6.5.2)\u2075and we can compute the second derivative of a surface in the direction of any unit vector u by:", "Choosing u to be an eigenvector of H(=R), we can get the second derivative in the direction of principal axes of E(W), which, as known from diagonalization of a matrix, will be the eigenvalues of H(=R).", "We already have our gradient expressed in terms of R and our Hessian = R, and as we can define principal axes of E(W) in terms of R, we will do 2 transformations on the space containing our error surface:", "Based on this, our equation of E(W) gets reparametrized as :", "where D is just the diagonal matrix containing eigenvalues of R on the diagonal (also called diagonalization of a matrix) and E_o is just E(W*).", "Remember V is still a vector, but each component of V represents the corresponding principal axis of the error surface.", "\u03bb_j is jth diagonal element of D, also the eigenvalue of jth eigenvector of R(=H)", "Clearly the eigenvalues of the input covariance matrix give the second derivative of error surface w.r.t. its principal axes.", "Putting these two results in a matrix gives the Hessian as the diagonal eigenmatrix D, (H = D), hence", "V(k) is the value of V at kth timestep.", "V being Nx1 vector, this results in N decoupled equations (as each component in V is orthonormal), and as V=U(W-W*), for the optimal solution V decays to zero, therefore each component evolves along a principal direction as", "Vj(0) is the jth component of V at initialization", "Now for this Vj to converge to zero over multiple timesteps(k>0), a term with positive power must be less than 1 in magnitude, so", "As this is just a formulation of exponential decay, Vj decays to zero in the characteristic time:", "Now, if all eigenvalues are equal,\u03bb_j = \u03bb, for all 1<=j<=N, as H=D, convergence is reached in a single step with:", "But this highly symmetric case rarely occurs. Geometrically, this occurs if the cross-sections of error surface E(W) are hyper-spheres in N-dim space {W}(hyper-sphere formed by encircling principle components, with these eigenvalues being equal, they get stretched or squished equally, thence a hyper-sphere), but this is very rare. The cross-section of E(W) is elliptical with different eigenvalues along different principle directions.", "Based on the above equations, \u03b7 must be chosen between (0,1/\u03bb_mx) and \u03bb_mx being the largest eigenvalue of the input covariance matrix, the slowest time constant is", "to decay along the principle direction of smallest non-zero curvature (basically eigenvector with smallest non-zero eigenvalue, i.e. \u03bb_mn).", "This clearly shows, that for error surface quadratic in weights, learning dynamics are controlled by the eigenvalue distribution of Hessian.", "Now that we know this, why stop here, let\u2019s examine the eigenspectrum here.", "It is assumed, that the input components, i.e., {Xi} are independent for all 1<=i<=N and each component is drawn from mean m and variance v( for convenience, considering all components of an input having numerically same mean and variance). We have p as the data size and N as vector dimension, say \u03b1 = p/N (ratio quantifying the training set), the eigenspectrum is yielded as (the complete derivation of the eigenspectrum can be found in the combined works of I. Kanter, Yann Lecun and A. Solla 1991)", "So, the eigenvector U_N = (1\u2026.1) thus corresponds to the eigenvalue", "and all other N-1 eigenvalues are equal to v,(how? satisfying the fact that trace(R) = sum of eigenvalues = N(m\u00b2 + v) ), moreover the continuous part of spectrum collapses onto a delta function at \u03bb- = \u03bb+= v as p->inf and only one value(=\u03bb_N) is greater than \u03bb+.", "The largest part of \u03bb_N is eliminated if m=0, that is, centered inputs with no bias, reduces \u03bb_N significantly, and as \u03bb_N was the largest(\u03bb_max), reducing its value affects t_mx significantly based on previous equations, clearly showing that using centered inputs improves learning time.", "So it\u2019s clear that biased inputs produce large eigenvalues and are responsible for slow convergence, to remove this, either the inputs can be centered,or, another way to deal with this based on our previous equations, is to use individual learning rates inversely proportional to N(number of inputs to the neuron under observation).", "All this proves the following clearly:", "Basically these small eigenvalues matter a lot to the loss, the path to optimum consists of these directions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Working on understanding loss landscapes in DL | Causal inference and biologically plausible learning agents | GitHub - llStringll"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F518bb004bc58&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----518bb004bc58--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@harshveer_singh?source=post_page-----518bb004bc58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshveer_singh?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Harshveer Singh"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea6ad2102628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&user=Harshveer+Singh&userId=ea6ad2102628&source=post_page-ea6ad2102628----518bb004bc58---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/photos/schrecksee-bergsee-allg%C3%A4u-2534484/", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1502.03167.pdf", "anchor_text": "https://arxiv.org/pdf/1502.03167.pdf"}, {"url": "https://arxiv.org/pdf/1805.11604.pdf", "anchor_text": "https://arxiv.org/pdf/1805.11604.pdf"}, {"url": "http://yann.lecun.com/exdb/publis/pdf/lecun-kanter-solla-91.pdf", "anchor_text": "http://yann.lecun.com/exdb/publis/pdf/lecun-kanter-solla-91.pdf"}, {"url": "https://arxiv.org/pdf/1901.10159.pdf", "anchor_text": "https://arxiv.org/pdf/1901.10159.pdf"}, {"url": "https://books.google.co.in/books?id=COfqBgAAQBAJ&pg=PA403&lpg=PA403&dq=The+eigenvectors+of+the+input+correlation+matrix+define+the+principal+axes+of+the+error+surface&source=bl&ots=jKSC91Oc_1&sig=ACfU3U2XD6E2ar4WqziymyQYxSGzWLHMDQ&hl=en&sa=X&ved=2ahUKEwjk4Kfb_MflAhVl6nMBHZPFDBkQ6AEwAnoECAYQAQ#v=onepage&q&f=false", "anchor_text": "https://books.google.co.in/books?id=COfqBgAAQBAJ&pg=PA403&lpg=PA403&dq=The+eigenvectors+of+the+input+correlation+matrix+define+the+principal+axes+of+the+error+surface&source=bl&ots=jKSC91Oc_1&sig=ACfU3U2XD6E2ar4WqziymyQYxSGzWLHMDQ&hl=en&sa=X&ved=2ahUKEwjk4Kfb_MflAhVl6nMBHZPFDBkQ6AEwAnoECAYQAQ#v=onepage&q&f=false"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----518bb004bc58---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----518bb004bc58---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----518bb004bc58---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----518bb004bc58---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----518bb004bc58---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&user=Harshveer+Singh&userId=ea6ad2102628&source=-----518bb004bc58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&user=Harshveer+Singh&userId=ea6ad2102628&source=-----518bb004bc58---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F518bb004bc58&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----518bb004bc58---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----518bb004bc58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----518bb004bc58--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----518bb004bc58--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----518bb004bc58--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----518bb004bc58--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshveer_singh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@harshveer_singh?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Harshveer Singh"}, {"url": "https://medium.com/@harshveer_singh/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "14 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea6ad2102628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&user=Harshveer+Singh&userId=ea6ad2102628&source=post_page-ea6ad2102628--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4e9b9786772c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-batchnorm-works-518bb004bc58&newsletterV3=ea6ad2102628&newsletterV3Id=4e9b9786772c&user=Harshveer+Singh&userId=ea6ad2102628&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}