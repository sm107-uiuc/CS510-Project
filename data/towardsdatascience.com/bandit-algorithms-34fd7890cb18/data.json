{"url": "https://towardsdatascience.com/bandit-algorithms-34fd7890cb18", "time": 1683015207.4540641, "path": "towardsdatascience.com/bandit-algorithms-34fd7890cb18/", "webpage": {"metadata": {"title": "Bandit Algorithms. Multi-Armed Bandits: Part 3 | by Steve Roberts | Towards Data Science", "h1": "Bandit Algorithms", "description": "This is the third instalment, in a six part series, on Multi-Armed Bandits. So far we\u2019ve covered the Mathematical Framework and Terminology used in Multi-Armed Bandits and The Bandit Framework, where\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697", "anchor_text": "Mathematical Framework and Terminology", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "The Bandit Framework", "paragraph_index": 2}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits", "paragraph_index": 6}, {"url": "http://link to Part 2", "anchor_text": "Part 2", "paragraph_index": 19}], "all_paragraphs": ["Baby Robot is lost in the mall. Using Reinforcement Learning we want to help him find his way back to his mum. However, before he can even begin looking for her, he needs to recharge, from a set of power sockets that each give a slightly different amount of charge.", "Using the strategies from the multi-armed bandit problem we need to find the best socket, in the shortest amount of time, to allow Baby Robot to get charged up and on his way.", "This is the third instalment, in a six part series, on Multi-Armed Bandits. So far we\u2019ve covered the Mathematical Framework and Terminology used in Multi-Armed Bandits and The Bandit Framework, where we described the problem that we\u2019re trying to solve and gave details of the basic code blocks that we\u2019ll be using to define and test the problem environment.", "Now that we\u2019ve got the terminology, notation and test framework in place, all that\u2019s missing is an actual algorithm that can solve the bandit problem, choosing between actions that give different levels of reward, in search of the best action. Over the next few parts we\u2019ll look at several such algorithms, of increasing complexity.", "To get us started we\u2019ll first take a look at some of the most basic approaches to solving the Bandit Problem and examine the following algorithms:", "Additionally, to let us evaluate the different approaches to solving the Bandit Problem, we\u2019ll describe the concept of Regret, in which you compare the performance of your algorithm to that of the theoretically best algorithm and then regret that your approach didn\u2019t perform a bit better!", "All code for the bandit algorithms and testing framework can be found on github: Multi_Armed_Bandits", "Baby Robot has entered a charging room containing 5 different power sockets. Each of these sockets returns a slightly different amount of charge. We want to get Baby Robot charged up in the minimum amount of time, so we need to locate the best socket and then use it until charging is complete.", "This is identical to the Multi-Armed Bandit problem except that, instead of looking for a slot machine that gives the best payout, we\u2019re looking for a power socket that gives the most charge.", "Every time we plug into a socket we get a reward, in the form of an amount of charge, and every reward we get lets us calculate a more accurate estimate of a socket\u2019s true output. If we then just choose the socket with the highest estimate hopefully this will be the best available socket.", "When selecting the action with the highest value, the action chosen at time step \u2018t\u2019, can be expressed by the formula:", "Where \u201cargmax\u201d specifies choosing the action \u2018a\u2019 for which Q\u209c(a) is maximised . Remember that \u2018Q\u209c(a)\u2019 is the estimated value of action \u2018a\u2019 at time step \u2018t\u2019, so we\u2019re choosing the action with the highest currently estimated value.", "When taking this approach, choosing the action with the highest estimated value, you are said to be \u201cchoosing greedily\u201d and the actions with the maximum values, of which there may be more than one, are know as the \u201cgreedy actions\u201d.", "All of this sounds like a reasonable way to select a good action, but there is one major flaw: the Greedy Algorithm wants to choose the action with the best estimate, yet provides no way to form these estimates. It purely exploits the information available, but does none of the exploration required to generate this information.", "Consequently, the greedy algorithm is initially selecting from actions that have not yet been tried and therefore have no estimate of their true value. Additionally, in the off chance that a good action is selected, there\u2019s no guarantee that this is the best action and, since the greedy algorithm will now have locked on to the chosen action, no other actions will ever be tested to confirm if the current one is the best or not. Therefore actions that could provide a higher long term return will be missed.", "One very simple way to modify the Greedy Algorithm, to make it explore the set of available actions in search of the optimal action, is to set the initial action estimates to very high values.", "In the power socket problem, if the action estimates are initialised to zero, as soon as one action is taken, assuming the socket returns even the smallest amount of charge, that action\u2019s mean reward will become greater than zero. As a result, in the eyes of the greedy algorithm, this action would then be better than all others and would therefore be the selected action forevermore.", "If instead the action estimates are initialised to a value that is higher than any of the possible socket outputs, when a socket is first tried, its mean reward decreases. It then has a value estimate lower than any of the others and is therefore not selected at the next time step. This results in every socket being tried in the first \u2018k\u2019 time steps (were \u2018k\u2019 is the number of sockets).", "If we implement this algorithm we can see exactly what\u2019s going on:", "So, in the code above, we derive a new OptimisticPowerSocket from the standard PowerSocket class that we defined in Part 2. The only difference is that now \u2018Q\u2019, the estimate of the socket\u2019s reward value, is initialised to a supplied initial estimate value and \u2019n\u2019, the number of times this socket has been tried, is set to 1 to take account of the initialisation.", "If we initialise our socket estimates to a value slightly higher than the maximum possible reward value then this should encourage exploration in the early time steps, without taking too long to discover which socket produces the highest reward.", "Looking back at the Violin Plot of the reward distribution for each socket the highest output is approximately 16 seconds of charge. Therefore we\u2019ve chosen a value of 20, which is slightly higher than the maximum possible reward, and initialised all socket estimates to this, as shown in the table below.", "Table 1 shows the reward estimates for each of the 5 sockets taken over 20 time steps. The yellow highlighted cells show the socket that has been selected at each time step. The points to note from this are:", "The same results are shown in the graph below (although here they\u2019re shown over the first 30 time-steps). It can be seen how the estimated rewards, for each of the sockets, starts at the initial value of 20 and then decreases over time. Once the estimates of the non-optimal sockets have fallen below the true value of the optimal socket (shown as the black line of the graph) then they will not be tried again. After this has happened for all but the optimal socket (socket 4) only it will be selected and only its estimate will converge towards the true value. The reward estimates for the non-optimal actions will never converge to their true reward.", "From the graph above it can be seen that the Optimistic Greedy algorithm works pretty well. It doesn\u2019t take too long for the sub-optimal actions to be discarded and for the focus to turn to the optimal action, which is the action we want to take if we\u2019re to get the maximum amount of charge in the shortest amount of time.", "However, there are a couple of problems with our experiment:", "Take a look at the plots below, showing the mean total reward and the socket selection percentages, obtained over 30 time steps for a range of initial values.", "In the plots above, there are a few points of note:", "The Optimistic-Greedy algorithm is a simple way to encourage exploration during the early stages of testing. Initially all actions will be tried and, if a suitable initialisation value is used, the algorithm will quickly discard non-optimal actions and focus on the best actions. However, as shown above, a poorly chosen initial value can result in sub-optimal total return and, without some prior knowledge of the range of possible rewards, it can be hard to select the best initial values.", "Another major drawback is the fact that exploration is confined to the initial time steps, restricting its use to stationary problems, in which the rewards for each action never change. It is not suitable for non-stationary situations, where the action rewards can change with time, due to the lack of ongoing exploration.", "As we\u2019ve seen, a pure Greedy strategy has a very high risk of selecting a sub-optimal socket and then sticking with this selection. As a result, the best socket will never be found.", "A simple way to overcome this problem is by introducing an element of exploration. This is exactly what Epsilon-Greedy does:", "In this way exploration is added to the standard Greedy algorithm. Over time every action will be sampled repeatedly to give an increasingly accurate estimate of its true reward value.", "The code to implement the Epsilon-Greedy strategy is shown below. Note that this changes the behaviour of the socket tester class, modifying how it chooses between the available sockets, rather than a change to the actual sockets.", "At each time step \u2018select_socket\u2019 is called. If the random value \u2018p\u2019 is less than Epsilon then a random action will be chosen, otherwise the socket with the highest current estimated reward will be selected.", "Looking at the graphs below, it can be seen how the value of \u03b5 affects exploration and exploitation on the socket problem:", "The Epsilon-Greedy strategy is an easy way to add exploration to the basic Greedy algorithm. Due to the random sampling of actions, the estimated reward values of all actions will converge on their true values. This can be seen in the graph of Final Socket Estimates shown above. With \u03b5 values higher than about 0.2, the estimated socket values all match their true values. And this is also the downside of the Epsilon-Greedy algorithm: non-optimal actions continue to be chosen, and their reward estimates refined, long after they have been identified as being non-optimal. Consequently, exploitation of the optimal action isn\u2019t maximised and the total overall reward is less than it could be.", "In the socket selection problem the maximum possible reward would be obtained if the best socket was chosen at every time step. Every time a non-optimal socket is selected the total possible reward that can be obtained reduces further from this theoretical maximum. As a result you regret choosing this socket and wish instead that you\u2019d chosen the best one. As the term regret implies, you may have no way to know in advance that you\u2019re making a non-optimal choice, only in hindsight do you realise your mistake.", "Although it may not be possible to tell in advance if you\u2019re choosing the best action, you can calculate how your selection policy performs when compared against the optimal policy, in which the best action is selected at every time step. The difference in the total reward obtained by these two policies represents the regret.", "The optimal action is given by:", "The optimal action is the one which maximises the expected (mean) reward when it is chosen at every time step.", "The regret \u2018L\u2019 is calculated by taking the difference between the reward obtained by the implemented policy and the reward that would have been obtained if instead the optimal policy had been followed, over a total of \u2018T\u2019 time steps:", "So, for all \u2018T\u2019 time steps, the optimal policy gets the expected reward obtained when taking the optimal action at every time step. From this we subtract the sum of the expected rewards returned from the actions chosen by our policy, where the chosen action, and therefore its expected reward, can vary at every time step.", "By minimising the value of the regret we can maximise the sum of the rewards.", "Epsilon-Greedy implements exploration by choosing any of the possible actions, at random, with a probability \u2018\u03b5\u2019. Therefore, over the course of \u2018T\u2019 time steps, \u2018\u03b5T\u2019 of the actions will have been chosen randomly.", "Additionally, in a general form of the socket selection problem, there are \u2018k\u2019 different sockets to choose from and only one of these will give the maximum possible reward. The remaining \u2018k-1\u2019 sockets will give a sub-optimal reward. As a result, there are \u2018\u03b5T(k-1)/k\u2019 rounds in which a sub-optimal socket is selected.", "If, during the rounds when the sub-optimal action is chosen, we get lucky and instead choose the second best socket, where the reward differs by a fixed amount \u2018\u0394\u2019 (delta) from the best socket, then this will give us the lowest possible regret for Epsilon-Greedy. This is given by:", "For all other non-optimal sockets the difference between the obtained reward and the optimal reward will be higher than \u2018\u0394\u2019, hence the greater than or equal sign, since in this case the regret will be larger.", "Since \u2018\u03b5\u2019, \u2018\u0394\u2019 and \u2018k\u2019 are all constants, the resultant lowest bounds regret for Epsilon-Greedy is linear with time (\u2018T\u2019 will just be multiplied by a constant value). Meaning that the regret will continue to increase long after the optimal action has been found.", "This can be seen in the graphs of Epsilon Greedy Regret shown below. After the first few time steps the total accumulated reward obtained by the Epsilon Greedy method increases linearly. However, due to the random exploration that still occurs, this rate of increase is slightly less than the rate at which the optimal action would accumulate its reward, resulting in the linear increase in the regret over time. The slightly higher rate of increase in regret that occurs during the early stages of the run is due to the best action not yet having been located and so a sub-optimal socket is more likely to be selected during these time steps.", "In this part of our look at Multi-Armed Bandits we finally got to investigate some of the algorithms that can be used to tackle the bandit problem.", "The pure Greedy algorithm, unless it has some initial knowledge of the possible action rewards, doesn\u2019t fare much better than a simple random selection method. However, with some slight modifications, such as Optimistic-Greedy\u2019s use of large initial values or Epsilon Greedy\u2019s approach of introducing random exploration, the selection performance can be greatly improved.", "However, even with these improvements, the total reward that is returned is still much less than optimal. Optimistic-Greedy\u2019s performance is very dependent on the values selected for its initial rewards and Epsilon Greedy continues to explore the set of all actions, long after it has gained sufficient knowledge to know which of these actions are bad actions to take. As a result, the Epsilon Greedy algorithm was shown to have linear regret, where the difference between the returned reward and the optimal reward continued to increase linearly with time.", "In the remaining parts of this series we\u2019ll take a look at a couple of more sophisticated approaches to solving the Bandit Problem, namely the Upper Confidence Bound algorithm and Thompson Sampling, both of which reduce the level of regret, resulting in a higher level of return. Using these we\u2019ll get Baby Robot charged in super quick time!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Ph.D., \"The evolution of artificial neural networks\""], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F34fd7890cb18&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652----34fd7890cb18---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/baby-robot-guide", "anchor_text": "A Baby Robot\u2019s Guide To Reinforcement Learning"}, {"url": "https://unsplash.com/@elpepe?utm_source=medium&utm_medium=referral", "anchor_text": "el pepe"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697", "anchor_text": "Mathematical Framework and Terminology"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "The Bandit Framework"}, {"url": "https://github.com/WhatIThinkAbout/BabyRobot/tree/master/Multi_Armed_Bandits", "anchor_text": "Multi_Armed_Bandits"}, {"url": "http://link to Part 2", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/multi-armed-bandits-part-2-5834cb7aba4b", "anchor_text": "The Bandit Framework"}, {"url": "https://towardsdatascience.com/the-upper-confidence-bound-ucb-bandit-algorithm-c05c2bf4c13f", "anchor_text": "UCB Bandit Algorithm"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----34fd7890cb18---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/multi-armed-bandit?source=post_page-----34fd7890cb18---------------multi_armed_bandit-----------------", "anchor_text": "Multi Armed Bandit"}, {"url": "https://medium.com/tag/baby-robot-guide?source=post_page-----34fd7890cb18---------------baby_robot_guide-----------------", "anchor_text": "Baby Robot Guide"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&user=Steve+Roberts&userId=6b6735266652&source=-----34fd7890cb18---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&user=Steve+Roberts&userId=6b6735266652&source=-----34fd7890cb18---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F34fd7890cb18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----34fd7890cb18---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----34fd7890cb18--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----34fd7890cb18--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----34fd7890cb18--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "593 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b6735266652&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&user=Steve+Roberts&userId=6b6735266652&source=post_page-6b6735266652--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1d5f26d16450&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbandit-algorithms-34fd7890cb18&newsletterV3=6b6735266652&newsletterV3Id=1d5f26d16450&user=Steve+Roberts&userId=6b6735266652&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}