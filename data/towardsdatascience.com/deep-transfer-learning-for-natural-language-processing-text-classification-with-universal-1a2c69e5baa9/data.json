{"url": "https://towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9", "time": 1682994068.559121, "path": "towardsdatascience.com/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9/", "webpage": {"metadata": {"title": "Deep Transfer Learning for Natural Language Processing \u2014 Text Classification with Universal Embeddings | by Dipanjan (DJ) Sarkar | Towards Data Science", "h1": "Deep Transfer Learning for Natural Language Processing \u2014 Text Classification with Universal Embeddings", "description": "Transfer learning is an exciting concept where we try to leverage prior knowledge from one domain and task into a different domain and task. The inspiration comes from us \u2014 humans, ourselves \u2014 where\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "\u2018Feature Engineering for Text Data\u2019", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "one of my previous articles", "paragraph_index": 5}, {"url": "http://www.scholarpedia.org/article/Neural_net_language_models", "anchor_text": "Neural Network based language models", "paragraph_index": 6}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "an amazing article", "paragraph_index": 8}, {"url": "https://medium.com/huggingface", "anchor_text": "HuggingFace", "paragraph_index": 8}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "the article on universal embedding trends", "paragraph_index": 9}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis", "anchor_text": "distributional hypothesis", "paragraph_index": 12}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis", "anchor_text": "distributional semantics", "paragraph_index": 12}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "my article on word embeddings", "paragraph_index": 12}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 13}, {"url": "https://allennlp.org", "anchor_text": "Allen Institute for Artificial Intelligence", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "in my article", "paragraph_index": 16}, {"url": "https://openreview.net/forum?id=SyK00v5xx", "anchor_text": "\u2018A Simple but Tough-to-Beat Baseline for Sentence Embeddings\u2019", "paragraph_index": 17}, {"url": "https://arxiv.org/abs/1405.4053", "anchor_text": "\u2018Distributed Representations of Sentences and Documents\u2019", "paragraph_index": 18}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "\u2018A Neural Probabilistic Language Model\u2019", "paragraph_index": 20}, {"url": "https://tfhub.dev/google/nnlm-en-dim128/1", "anchor_text": "nnlm-en-dim128", "paragraph_index": 21}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "\u2018Skip-Thought Vectors\u2019", "paragraph_index": 22}, {"url": "https://openreview.net/forum?id=rJvJXZb0W", "anchor_text": "\u2018An efficient framework for learning sentence representations\u2019", "paragraph_index": 24}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "\u2018Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\u2019", "paragraph_index": 26}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "\u2018Universal Sentence Encoder\u2019", "paragraph_index": 30}, {"url": "https://tfhub.dev/", "anchor_text": "TF-Hub", "paragraph_index": 30}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "universal-sentence-encoder", "paragraph_index": 30}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Large Movie Review Dataset", "paragraph_index": 31}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository", "paragraph_index": 31}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository", "paragraph_index": 33}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_deep_transfer_learning_nlp_classification/Deep%20Transfer%20Learning%20for%20NLP%20-%20Text%20Classification%20with%20Universal%20Embeddings.ipynb", "anchor_text": "Jupyter Notebook", "paragraph_index": 33}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "download it", "paragraph_index": 33}, {"url": "https://www.paperspace.com", "anchor_text": "Paperspace", "paragraph_index": 33}, {"url": "https://www.leadtek.com/eng/products/workstation_graphics(2)/NVIDIA_Quadro_P4000_(10775)/detail", "anchor_text": "NVIDIA Quadro P4000", "paragraph_index": 34}, {"url": "http://ruder.io/about/", "anchor_text": "Sebastian Ruder", "paragraph_index": 63}, {"url": "https://thegradient.pub/nlp-imagenet/", "anchor_text": "a very interesting article", "paragraph_index": 63}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository", "paragraph_index": 65}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_deep_transfer_learning_nlp_classification/Deep%20Transfer%20Learning%20for%20NLP%20-%20Text%20Classification%20with%20Universal%20Embeddings.ipynb", "anchor_text": "Jupyter Notebook", "paragraph_index": 65}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS", "paragraph_index": 67}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn", "paragraph_index": 67}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba", "paragraph_index": 68}], "all_paragraphs": ["Transfer learning is an exciting concept where we try to leverage prior knowledge from one domain and task into a different domain and task. The inspiration comes from us \u2014 humans, ourselves \u2014 where in, we have an inherent ability to not learn everything from scratch. We transfer and leverage our knowledge from what we have learnt in the past for tackling a wide variety of tasks. With computer vision, we have excellent big datasets available to us, like Imagenet, on which, we get a suite of world-class, state-of-the-art pre-trained model to leverage transfer learning. But what about Natural Language Processing? Therein lies the challenge, considering text data is so diverse, noisy and unstructured. We\u2019ve had some recent successes with word embeddings including methods like Word2Vec, GloVe and FastText, all of which I have covered in my article \u2018Feature Engineering for Text Data\u2019.", "In this article, we will be showcasing several state-of-the-art generic sentence embedding encoders, which tend to give surprisingly good performance, especially on small amounts of data for transfer learning tasks as compared to word embedding models. We will be covering the following models:", "We will try to cover essential concepts and also showcase some hands-on examples leveraging Python and Tensorflow, in a text classification problem focused on sentiment analysis!", "What is this sudden craze behind embeddings? I\u2019m sure many of you might be hearing it everywhere. Let\u2019s clear up the basics first and cut through the hype.", "An embedding is a fixed-length vector typically used to encode and represent an entity (document, sentence, word, graph!)", "I\u2019ve talked about the need for embeddings in the context of text data and NLP in one of my previous articles. But I will briefly reiterate this here for the sake of convenience. With regard to speech or image recognition systems, we already get information in the form of rich dense feature vectors embedded in high-dimensional datasets like audio spectrograms and image pixel intensities. However, when it comes to raw text data, especially count-based models like Bag of Words, we are dealing with individual words, which may have their own identifiers, and do not capture the semantic relationship among words. This leads to huge sparse word vectors for textual data and thus, if we do not have enough data, we may end up getting poor models or even overfitting the data due to the curse of dimensionality.", "Predictive methods like Neural Network based language models try to predict words from its neighboring words looking at word sequences in the corpus and in the process, it learns distributed representations, giving us dense word embeddings.", "Now you might be thinking, big deal, we get a bunch of vectors from text. What now? Well, this craze for embeddings is that, if we have a good numeric representation of text data which captures even the context and semantics, we can use it for a wide variety of downstream real-world tasks like sentiment analysis, text classification, clustering, summarization, translation and so on. The fact of the matter is, machine learning or deep learning models run on numbers, and embeddings are the key to encoding text data that will be used by these models.", "A big trend here has been finding out so-called \u2018Universal Embeddings\u2019 which are basically pre-trained embeddings obtained from training deep learning models on a huge corpus. This enables us to use these pre-trained (generic) embeddings in a wide variety of tasks including, scenarios with constraints like lack of adequate data. This is a perfect example of transfer learning, leveraging prior knowledge from pre-trained embeddings to solve a completely new task! The following figure showcases some recent trends in Universal Word & Sentence Embeddings, thanks to an amazing article from the folks over at HuggingFace!", "Definitely, some interesting trends in the above figure including, Google\u2019s Universal Sentence Encoder, which we will be exploring in detail in this article! I definitely recommend readers to check out the article on universal embedding trends from HuggingFace.", "Now, let\u2019s take a brief look at trends and developments in word and sentence embedding models before diving deeper into Universal Sentence Encoder.", "The word embedding models are perhaps some of the older and more mature models which have been developed starting with Word2Vec in 2013. The three most common models leveraging deep learning (unsupervised approaches) models based on embedding word vectors in a continuous vector space based on semantic and contextual similarity are:", "These models are based on the principle of distributional hypothesis in the field of distributional semantics, which tells us that words which occur and are used in the same context, are semantically similar to one another and have similar meanings (\u2018a word is characterized by the company it keeps\u2019). Do refer to my article on word embeddings which cover these three methods in detail, if you are interested in the gory details!", "Another interesting model in this area which has been developed recently, is ELMo. This has been developed by the Allen Institute for Artificial Intelligence. ELMo is a take on the famous muppet character of the same name from the famed show, \u2018Sesame Street\u2019, but actually is an acronym which stands for \u2018Embeddings from Language Models\u2019.", "Basically, ELMo gives us word embeddings which are learnt from a deep bidirectional language model (biLM), which is typically pre-trained on a large text corpus, enabling transfer learning for these embeddings to be used across different NLP tasks. Allen AI tells us that ELMo representations are contextual, deep and character-based which uses morphological clues to form representations even for OOV (out-of-vocabulary) tokens.", "The concept of sentence embeddings is not a very new concept, because back when word embeddings were built, one of the easiest ways to build a baseline sentence embedding model was by averaging.", "A baseline sentence embedding model can be built by just averaging out the individual word embeddings for every sentence\\document (kind of similar to bag of words, where we lose that inherent context and sequence of words in the sentence). We do cover this in detail in my article. The following figure shows a way of implementing this.", "Of course, there are more sophisticated approaches like encoding sentences in a linear weighted combination of their word embeddings and then removing some of the common principal components. Do check out, \u2018A Simple but Tough-to-Beat Baseline for Sentence Embeddings\u2019.", "Doc2Vec is also a very popular approach proposed by Mikolov et. al. in their paper \u2018Distributed Representations of Sentences and Documents\u2019. Herein, they propose the Paragraph Vector, an unsupervised algorithm that learns fixed-length feature embeddings from variable-length pieces of texts, such as sentences, paragraphs, and documents.", "Based on the above depiction, the model represents each document by a dense vector which is trained to predict words in the document. The only difference being the paragraph or document ID, used along with the regular word tokens to build out the embeddings. Such a design enables this model to overcome the weaknesses of bag-of-words models.", "Neural-Net Language Models (NNLM) is a very early idea based on a neural probabilistic language model proposed by Bengio et al. in their paper, \u2018A Neural Probabilistic Language Model\u2019 in 2003, they talk about learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously a distributed representation for each word along with the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence.", "Google has built a universal sentence embedding model, nnlm-en-dim128 which is a token-based text embedding-trained model that uses a three-hidden-layer feed-forward Neural-Net Language Model on the English Google News 200B corpus. This model maps any body of text into 128-dimensional embeddings. We will be using this in our hands-on demonstration shortly!", "Skip-Thought Vectors were also one of the first models in the domain of unsupervised learning-based generic sentence encoders. In their proposed paper, \u2018Skip-Thought Vectors\u2019, using the continuity of text from books, they have trained an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are mapped to similar vector representations.", "This is just like the Skip-gram model, but for sentences, where we try to predict the surrounding sentences of a given source sentence.", "Quick Thought Vectors is a more recent unupervised approach towards learning sentence emebddings. Details are mentioned in the paper \u2018An efficient framework for learning sentence representations\u2019. Interestingly, they reformulate the problem of predicting the context in which a sentence appears as a classification problem by replacing the decoder with a classfier in the regular encoder-decoder architecture.", "Thus, given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their embedding representations. Given an input sentence, it is first encoded by using some function. But instead of generating the target sentence, the model chooses the correct target sentence from a set of candidate sentences. Viewing generation as choosing a sentence from all possible sentences, this can be seen as a discriminative approximation to the generation problem.", "InferSent is interestingly a supervised learning approach to learning universal sentence embeddings using natural language inference data. This is hardcore supervised transfer learning, where just like we get pre-trained models trained on the ImageNet dataset for computer vision, they have universal sentence representations trained using supervised data from the Stanford Natural Language Inference datasets. Details are mentioned in their paper, \u2018Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\u2019. The dataset used by this model is the SNLI dataset that comprises 570k human-generated English sentence pairs, manually labeled with one of the three categories: entailment, contradiction and neutral. It captures natural language inference useful for understanding sentence semantics.", "Based on the architecture depicted in the above figure, we can see that it uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v. Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v :", "The resulting vector is then fed into a 3-class classifier consisting of multiple fully connected layers culminating in a softmax layer.", "Universal Sentence Encoder from Google is one of the latest and best universal sentence embedding models which was published in early 2018! The Universal Sentence Encoder encodes any body of text into 512-dimensional embeddings that can be used for a wide variety of NLP tasks including text classification, semantic similarity and clustering. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks which require modeling the meaning of sequences of words rather than just individual words.", "Their key finding is that, transfer learning using sentence embeddings tends to outperform word embedding level transfer. Do check out their paper, \u2018Universal Sentence Encoder\u2019 for further details. Essentially, they have two versions of their model available in TF-Hub as universal-sentence-encoder. Version 1 makes use of the transformer-network based sentence encoding model and Version 2 makes use of a Deep Averaging Network (DAN) where input embeddings for words and bi-grams are first averaged together and then passed through a feed-forward deep neural network (DNN) to produce sentence embeddings. We will be using Version 2 in our hands-on demonstration shortly.", "It\u2019s time for putting some of these universal sentence encoders into action with a hands-on demonstration! Like the article mentions, the premise of our demonstration today will focus on a very popular NLP task, text classification \u2014 in the context of sentiment analysis. We will be working with the benchmark IMDB Large Movie Review Dataset. Feel free to download it here or you can even download it from my GitHub repository.", "This dataset comprises a total of 50,000 movie reviews, where 25K have positive sentiment and 25K have negative sentiment. We will be training our models on a total of 30,000 reviews as our training dataset, validate on 5,000 reviews and use 15,000 reviews as our test dataset. The main objective is to correctly predict the sentiment of each review as either positive or negative.", "Now that we have our main objective cleared up, let\u2019s put universal sentence encoders into action! The entire tutorial is available in my GitHub repository as a Jupyter Notebook. Feel free to download it and play around with it. I recommend using a GPU-based instance for playing around with this. I love using Paperspace where you can spin up notebooks in the cloud without needing to worry about configuring instances manually.", "My setup was an 8 CPU, 30 GB, 250 GB SSD and an NVIDIA Quadro P4000 which is usually cheaper than most AWS GPU instances (I love AWS though!).", "Note: This tutorial is built using TensorFlow entirely given that they provide an easy access to the sentence encoders. However I\u2019m not a big fan of their old APIs and I\u2019m looking for someone to assist me on re-implementing the code using the tf.keras APIs instead of tf.estimator. Do reach out to me if you are interested in contributing and we can even feature your work on the same! (contact links in my profile and in the footer)", "We start by installing tensorflow-hub which enables us to use these sentence encoders easily.", "Let\u2019s now load up our essential dependencies for this tutorial!", "The following commands help you check if tensorflow will be using a GPU (if you have one set up already!)", "We can now load up out dataset and view it using pandas. I provide a compressed version of the dataset in my repository which you can use as follows.", "We encode the sentiment column as 1s and 0s just to make things easier for us during model development (label encoding).", "We will now create train, validation and test datasets before we start modeling. We will use 30,000 reviews for train, 5,000 for validation and 15,000 for test. You can use a train-test splitting function also like train_test_split() from scikit-learn. I was just lazy and subsetted the dataset using simple list slicing.", "There is some basic text wrangling and pre-processing we need to do to remove some noise from our text like contractions, unnecessary special characters, HTML tags and so on. The following code helps us build a simple, yet effective text wrangling system. Do install the following libraries in case you don\u2019t have them.", "The following functions help us build our text wrangling system.", "Let\u2019s now pre-process our datasets using the function we implemented above.", "Since we will be implementing our models in tensorflow using the tf.estimator API, we need to define some functions to build data and feature engineering pipelines to enable data flowing into our models during training. The following functions will help us. We leverage the numpy_input_fn() which helps in feeding a dict of numpy arrays into the model.", "We are now ready to build our models!", "We need to first define the sentence embedding feature which leverages the universal sentence encoder before building the model. We can do that using the following code.", "Like we discussed, we use the Universal Sentence Encoder Version 2 and it works on the sentence attribute in our input dictionary which will be a numpy array of our reviews. We will build a simple feed-forward DNN now with two hidden layers. Just a standard model, nothing too sophisticated since we want to see how well these embeddings perform even on a simple model. Here we are leveraging transfer learning in the form of pre-trained embeddings. We are not fine-tuning here by keeping the embedding weights fixed by setting trainable=False.", "We had set our batch_size to 256 and we will be flowing in data in batches of 256 records for 1500 steps which translates to roughly 12\u201313 epochs.", "Let\u2019s train our model now on our training dataset and evaluate on both train and validation datasets at steps of 100.", "I have highlighted the metrics of interest in the output logs above and as you can see, we get an overall accuracy of close to 87% on our validation dataset and an AUC of 94% which is quite good on such a simple model!", "Let\u2019s now evaluate our model and check the overall performance on the train and test datasets.", "We get an overall accuracy of close to 87% on the test data giving us consistent results based on what we observed on our validation dataset earlier! Thus, this should give you an idea of how easy it is to leverage pre-trained universal sentence embeddings and not worry about the hassle of feature engineering or complex modeling.", "Let\u2019s now try building different deep learning classifiers based on different sentence embeddings. We will try the following:", "We will also cover the two most prominent methodologies for transfer learning here.", "The following generic function can plug and play different universal sentence encoders from tensorflow-hub!", "We can now train our models using the above-defined approaches.", "I\u2019ve depicted the evaluation metrics of importance in the above outputs, and you can see we definitely get some good results with our models. The following table summarizes these comparative results in a nice way.", "Looks like Google\u2019s Universal Sentence Encoder with fine-tuning gave us the best results on the test data. Let\u2019s load up this saved model and run an evaluation on the test data.", "One of the best ways to evaluate our model performance is to visualize the model predictions in the form of a confusion matrix.", "We can also print out the model\u2019s classification report using scikit-learn to show the other important metrics which can be derived from the confusion matrix including precision, recall and f1-score.", "We obtain an overall model accuracy and f1-score of 90% on the test data which is really good! Go ahead and try this out and maybe get an even better score and let me know about it!", "Universal Sentence Embeddings are definitely a huge step forward in enabling transfer learning for diverse NLP tasks. In fact, we have seen models like ELMo, Universal Sentence Encoder, ULMFiT have indeed made headlines by showcasing that pre-trained models can be used to achieve state-of-the-art results on NLP tasks. Famed Research Scientist and Blogger Sebastian Ruder, mentioned the same in his recent tweet based on a very interesting article which he wrote recently.", "I\u2019m definitely excited about what the future holds for generalizing NLP even further, and enabling us to solve complex tasks with ease!", "The code used for hands-on demonstrations in this article is available in my GitHub repository as a Jupyter Notebook which you can play around with!", "Help Needed: Like I mentioned, I\u2019m looking for someone to help me convert this code to use the newer tf.keras APIs instead of tf.estimator. Interested? Reach out to me!", "Have feedback for me? Or interested in working with me on research, data science, artificial intelligence or even publishing an article on TDS? You can reach out to me on LinkedIn.", "Thanks to Durba for editing this article.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1a2c69e5baa9&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://djsarkar.medium.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682----1a2c69e5baa9---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "\u2018Feature Engineering for Text Data\u2019"}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "Understanding Feature Engineering (Part 4) \u2014 A hands-on intuitive approach to Deep Learning\u2026Newer, advanced strategies for taming unstructured, textual datatowardsdatascience.com"}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "one of my previous articles"}, {"url": "http://www.scholarpedia.org/article/Neural_net_language_models", "anchor_text": "Neural Network based language models"}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "an amazing article"}, {"url": "https://medium.com/huggingface", "anchor_text": "HuggingFace"}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a"}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "the article on universal embedding trends"}, {"url": "https://huggingface.co/", "anchor_text": "HuggingFace"}, {"url": "https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a", "anchor_text": "\ud83d\udcdaThe Current Best of Universal Word Embeddings and Sentence EmbeddingsWord & Sentence Embeddings have evolved really fast over the last few months - A brief primer on what happenedmedium.com"}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis", "anchor_text": "distributional hypothesis"}, {"url": "https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_Hypothesis", "anchor_text": "distributional semantics"}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "my article on word embeddings"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://allennlp.org", "anchor_text": "Allen Institute for Artificial Intelligence"}, {"url": "https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa", "anchor_text": "in my article"}, {"url": "https://openreview.net/forum?id=SyK00v5xx", "anchor_text": "\u2018A Simple but Tough-to-Beat Baseline for Sentence Embeddings\u2019"}, {"url": "https://arxiv.org/abs/1405.4053", "anchor_text": "\u2018Distributed Representations of Sentences and Documents\u2019"}, {"url": "https://arxiv.org/abs/1405.4053", "anchor_text": "https://arxiv.org/abs/1405.4053"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "\u2018A Neural Probabilistic Language Model\u2019"}, {"url": "https://tfhub.dev/google/nnlm-en-dim128/1", "anchor_text": "nnlm-en-dim128"}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "\u2018Skip-Thought Vectors\u2019"}, {"url": "https://arxiv.org/abs/1506.06726", "anchor_text": "https://arxiv.org/abs/1506.06726"}, {"url": "https://openreview.net/forum?id=rJvJXZb0W", "anchor_text": "\u2018An efficient framework for learning sentence representations\u2019"}, {"url": "https://openreview.net/forum?id=rJvJXZb0W", "anchor_text": "https://openreview.net/forum?id=rJvJXZb0W"}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "\u2018Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\u2019"}, {"url": "https://arxiv.org/abs/1705.02364", "anchor_text": "https://arxiv.org/abs/1705.02364"}, {"url": "https://arxiv.org/abs/1803.11175", "anchor_text": "\u2018Universal Sentence Encoder\u2019"}, {"url": "https://tfhub.dev/", "anchor_text": "TF-Hub"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "universal-sentence-encoder"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "IMDB Large Movie Review Dataset"}, {"url": "http://ai.stanford.edu/~amaas/data/sentiment/", "anchor_text": "here"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_deep_transfer_learning_nlp_classification/Deep%20Transfer%20Learning%20for%20NLP%20-%20Text%20Classification%20with%20Universal%20Embeddings.ipynb", "anchor_text": "Jupyter Notebook"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "download it"}, {"url": "https://www.paperspace.com", "anchor_text": "Paperspace"}, {"url": "https://www.paperspace.com", "anchor_text": "PaperspaceCloud Machine Learning, AI, and effortless GPU infrastructurewww.paperspace.com"}, {"url": "https://www.leadtek.com/eng/products/workstation_graphics(2)/NVIDIA_Quadro_P4000_(10775)/detail", "anchor_text": "NVIDIA Quadro P4000"}, {"url": "https://tfhub.dev/google/nnlm-en-dim128/1", "anchor_text": "https://tfhub.dev/google/nnlm-en-dim128/1"}, {"url": "https://tfhub.dev/google/nnlm-en-dim128/1", "anchor_text": "https://tfhub.dev/google/nnlm-en-dim128/1"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "https://tfhub.dev/google/universal-sentence-encoder/2"}, {"url": "https://tfhub.dev/google/universal-sentence-encoder/2", "anchor_text": "https://tfhub.dev/google/universal-sentence-encoder/2"}, {"url": "http://ruder.io/about/", "anchor_text": "Sebastian Ruder"}, {"url": "https://thegradient.pub/nlp-imagenet/", "anchor_text": "a very interesting article"}, {"url": "https://github.com/dipanjanS/data_science_for_all/tree/master/tds_deep_transfer_learning_nlp_classification", "anchor_text": "my GitHub repository"}, {"url": "http://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_deep_transfer_learning_nlp_classification/Deep%20Transfer%20Learning%20for%20NLP%20-%20Text%20Classification%20with%20Universal%20Embeddings.ipynb", "anchor_text": "Jupyter Notebook"}, {"url": "https://towardsdatascience.com/", "anchor_text": "TDS"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "LinkedIn"}, {"url": "https://www.linkedin.com/in/dipanzan/", "anchor_text": "Dipanjan Sarkar - AI Consultant &amp; Data Science Mentor - Springboard | LinkedInView Dipanjan Sarkar's profile on LinkedIn, the world's largest professional community. Dipanjan has 2 jobs listed on\u2026www.linkedin.co"}, {"url": "https://www.linkedin.com/in/durba-dutta-bhaumik-44532ab1/", "anchor_text": "Durba"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1a2c69e5baa9---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----1a2c69e5baa9---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1a2c69e5baa9---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----1a2c69e5baa9---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----1a2c69e5baa9---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----1a2c69e5baa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=-----1a2c69e5baa9---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1a2c69e5baa9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1a2c69e5baa9---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1a2c69e5baa9--------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://djsarkar.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dipanjan (DJ) Sarkar"}, {"url": "https://djsarkar.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "10.4K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6278d12b0682&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=post_page-6278d12b0682--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa34c887aa0f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9&newsletterV3=6278d12b0682&newsletterV3Id=a34c887aa0f4&user=Dipanjan+%28DJ%29+Sarkar&userId=6278d12b0682&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}