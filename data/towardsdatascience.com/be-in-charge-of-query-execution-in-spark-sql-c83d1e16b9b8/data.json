{"url": "https://towardsdatascience.com/be-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8", "time": 1683008827.9193358, "path": "towardsdatascience.com/be-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8/", "webpage": {"metadata": {"title": "Be in charge of Query Execution in Spark SQL | by David Vrba | Towards Data Science", "h1": "Be in charge of Query Execution in Spark SQL", "description": "Querying data in Spark has become a luxury since Spark 2.x because of SQL and declarative DataFrame API. Using just few lines of high level code allows to express quite complex logic and carry out\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Querying data in Spark has become a luxury since Spark 2.x because of SQL and declarative DataFrame API. Using just few lines of high level code allows to express quite complex logic and carry out complicated transformations. The big benefit of the API is that users don\u2019t need to think about the execution and can let the optimizer figure out the most efficient way to execute the query. And efficient query execution is often a requirement not only because the resources may become costly, but also it makes the work of the end user more comfortable by reducing the time he/she has to wait for the result of the computation.", "The Spark SQL optimizer is indeed quite mature, especially now with the upcoming version 3.0 which will introduce some new internal optimizations such as dynamic partition pruning and adaptive query execution. The optimizer is internally working with a query plan and is usually able to simplify it and optimize by various rules. For example it can change the order of some transformations or drop them completely if they are not necessary for the final output. Despite all the clever optimizations there are however still situations in which a human brain can do better. In this article we will take a look at one of these cases and see how, using a simple trick, we can lead Spark towards a more efficient execution.", "The code is tested in the current version of Spark which is 2.4.5 (written in June 2020) and it is checked against Spark 3.0.0-preview2 to see possible changes in the upcoming Spark 3.0.", "Let me now first introduce a simple example for which we will try to achieve efficient execution. Imagine that we have data in json format with the following structure:", "Each record is like a transaction so the user_id column may contain lots of duplicated values (possibly including nulls) and besides these three columns there can be many other fields describing the transaction. Now our query will be based on a union of two similar aggregations where each of these aggregations differs by some conditions. In the first aggregation we want to take users for which the sum of the price is less than 50 and in the second aggregation we take users for which the sum of the price is more than 100. Moreover in the second aggregation we want to consider only records where user_id is not null. This model example is just a simplified version of a more complex situation that can occur in practice and for the sake of simplicity we will use it throughout the article. Here is a basic way how to express such a query using DataFrame API of PySpark (very similarly we could write it also using the Scala API):", "The key to achieve a good performance for your query is the ability to understand and interpret the query plan. The plan itself can be displayed by calling explain function on the Spark DataFrame or if the query is already running (or has finished) we can also go to the Spark UI and find the plan in the SQL tab.", "The SQL tab has lists of completed and running queries on the cluster so by selecting our query we will see the graphical representation of the physical plan (here i removed the metrics information to make the plot smaller):", "The plan has a tree structure where each node represents some operator that caries some information about the execution. We can see that in our example there are two branches with the root at the bottom and the leafs at the top where the execution starts. The leafs Scan json represent reading the data from the source, then there is a pair of HashAggregate operators which are responsible for the aggregation, and in between them there is Exchange which represents the shuffle. The Filter operators carry the information about the filtering conditions.", "The plan has a typical shape for union operations, there is a new branch for each DataFrame in the union, and since in our example both DataFrames are based on the same datasource, it means that the datasource will be scanned twice. Now we can see that there is a space for improvement. Having the datasource scanned only once can lead to a nice optimization, especially in situations where I/O is expensive.", "Conceptually, what we want to achieve here, is reusing some computation \u2014 scanning the data and computing the aggregation, because these are the operations that are the same in both DataFrames and in principle it should be sufficient to compute them only once.", "One typical approach how to reuse a computation in Spark is using caching. There is a function cache that can be called on a DataFrame:", "It is a lazy transformation which means that the data will be put to the caching layer after we call some action. Caching is a very common technique used in Spark however it has its limitations, especially if the cached data is large and the resources on the cluster are limited. Also one needs to be aware that storing the data in the caching layer (memory or disk) will bring some additional overhead and the operation itself is not for free. Calling cache on the whole DataFrame df is not optimal also from the reason that it will try to put all the columns to memory which may be unnecessary. More careful way is to select a superset of all columns that will be used in the following queries and then call the cache function after this select.", "Apart from caching, there is another technique which is not that well described in the literature and this technique is based on reusing the Exchange. The Exchange operator represents shuffle which is a physical data movement on the cluster. This happens when the data must be reorganized (repartitioned) which is usually required for aggregations, joins and some other transformations. The important thing about shuffle is that when the data is repartitioned, Spark will always save it on the disk as the shuffle write (this is an internal behavior and it is not under control of the end user). And because it is saved on the disk, it can be reused later on if it is required. Spark will indeed reuse the data if it finds an opportunity for that. This happens each time Spark detects that the same branch from the leaf node up to an Exchange is repeating somewhere in the plan. If there is such a situation it means that these repeated branches represent identical computation and thus it is sufficient to compute it only once and then reuse it. We can recognize from the plan whether Spark found such a case, because those branches would be merged together like this:", "In our example, Spark didn\u2019t reuse the Exchange, but with a simple trick, we can push him to do so. The reason why the Exchange is not reused in our query is the Filter in the right branch that corresponds to the filtering condition user_id is not null. The filter is indeed the only difference in our two DataFrames that are in the union, so if we can eliminate this difference and make the two branches the same, Spark will take care of the rest and will reuse the Exchange.", "How can we make the branches the same? Well, if the only difference is the filter, we can certainly switch the order of transformations and call the filter after the aggregation because that will make no impact on the correctness of the result that will be produced. However there is a catch! If we move the filter like this:", "and check the final query plan, we will see that the plan has not changed at all! The explanation is simple \u2014 the filter was moved back by the optimizer.", "Conceptually it is good to understand that there are two major types of the query plan \u2014 logical plan and physical plan. And the logical plan undergoes an optimization phase before it is turned into the physical plan which is the final plan that will be executed. When we change some transformations, it is reflected in the logical plan, but then we loose control over the next steps. The optimizer will apply a set of optimization rules which are mostly based on some heuristics. The rule related to our example is called PushDownPredicate and this rule makes sure that the filters are applied as soon as possible and are pushed closer to the source. It is based on the idea that it is more efficient to first filter the data and then do the computation on the reduced dataset. This rule is indeed very useful in most of the situations, however in this very case it is fighting against us.", "To achieve custom position of the Filter in the plan, we have to limit the optimizer. This is possible since Spark 2.4 because there is a configuration setting which allows us to list all the optimization rules that we want to exclude from the optimizer:", "After setting this configuration and running the query again, we will see that now the filter stays positioned as we need. The two branches become really the same and Spark will now reuse the Exchange! The dataset will now be scanned only once and the same goes for computing the aggregation.", "In Spark 3.0 the situation is changed a little bit, the optimization rule now has a different name \u2014 PushDownPredicates and there is an additional rule that is also responsible for pushing a filter PushPredicateThroughNonJoin, so we actually need to exclude both of them to achieve the desired goal.", "We can see that through this technique Spark developers gave us the power to control the optimizer. But with power comes also responsibility. Let\u2019s list a couple of points that is good to keep in mind when using this technique:", "We have seen that being able to achieve the optimal performance may require understanding the query plan. Spark optimizer does a very good job by optimizing our query using a set of heuristic rules. There are, however, situations in which these rules miss the most optimal configuration. Sometimes rewriting the query is good enough, but sometimes it is not, because by rewriting the query we will achieve a different logical plan but we do not have a direct control over the physical plan that will be executed. Since Spark 2.4 we can use a configuration setting excludedRules that allows us to limit the optimizer and thus navigate Spark to a more custom physical plan.", "In many cases relying on the optimizer will lead to a solid plan with a quite efficient execution, however, there are cases mostly in performance critical workloads, where it might be worth to check the final plan and see whether we can improve it by taking the control over the optimizer.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Senior ML Engineer at Sociabakers and Apache Spark trainer and consultant. I lecture Spark trainings, workshops and give public talks related to Spark."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc83d1e16b9b8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@vrba.dave?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33----c83d1e16b9b8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/spark-sql?source=post_page-----c83d1e16b9b8---------------spark_sql-----------------", "anchor_text": "Spark Sql"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----c83d1e16b9b8---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/apache-spark?source=post_page-----c83d1e16b9b8---------------apache_spark-----------------", "anchor_text": "Apache Spark"}, {"url": "https://medium.com/tag/query-optimization?source=post_page-----c83d1e16b9b8---------------query_optimization-----------------", "anchor_text": "Query Optimization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&user=David+Vrba&userId=b7f216c64e33&source=-----c83d1e16b9b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&user=David+Vrba&userId=b7f216c64e33&source=-----c83d1e16b9b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc83d1e16b9b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c83d1e16b9b8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c83d1e16b9b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@vrba.dave?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "David Vrba"}, {"url": "https://medium.com/@vrba.dave/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb7f216c64e33&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&user=David+Vrba&userId=b7f216c64e33&source=post_page-b7f216c64e33--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F83cdb92c0d8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbe-in-charge-of-query-execution-in-spark-sql-c83d1e16b9b8&newsletterV3=b7f216c64e33&newsletterV3Id=83cdb92c0d8c&user=David+Vrba&userId=b7f216c64e33&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}