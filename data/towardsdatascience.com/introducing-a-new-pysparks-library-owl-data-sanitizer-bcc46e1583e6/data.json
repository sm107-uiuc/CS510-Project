{"url": "https://towardsdatascience.com/introducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6", "time": 1683006863.931155, "path": "towardsdatascience.com/introducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6/", "webpage": {"metadata": {"title": "Introducing a new pySpark\u2019s library: owl-data-sanitizer | by Ronald \u00c1ngel | Towards Data Science", "h1": "Introducing a new pySpark\u2019s library: owl-data-sanitizer", "description": "Data quality is a crucial component for companies building great products, no matters how robust your tech stack is or how talented is your data team, when your data is inconsistent or erroneous you\u2026"}, "outgoing_paragraph_urls": [{"url": "https://hbr.org/2020/02/data-driven-decisions-start-with-these-4-questions", "anchor_text": "studies", "paragraph_index": 0}, {"url": "https://github.com/awslabs/deequ", "anchor_text": "deequ", "paragraph_index": 7}, {"url": "https://greatexpectations.io/", "anchor_text": "great expectations.", "paragraph_index": 7}, {"url": "https://github.com/ronald-smith-angel/owl-data-sanitizer", "anchor_text": "owl-data-sanitizer", "paragraph_index": 25}], "all_paragraphs": ["Data quality is a crucial component for companies building great products, no matters how robust your tech stack is or how talented is your data team, when your data is inconsistent or erroneous you will end up with a bad product. Therefore, studies have found that companies lose an average of $15M per year due to bad data.", "3Cs: Data quality could be define as a combination of:", "correctness: how correct is your data in terms of inconsistencies in content and structure.", "completeness: are you missing records on the result?", "comparison: may I compare my entire data with the source?", "Thus, we started asking questions to data teams regarding the process they follow to guarantee the cleanliness on their datasets, finding some revelations:", "Therefore, we found a problem: We have to make data quality control an integral part of all data-driven squads by making it centralized, democratic, and smarter.", "In order to start this process, we analyzed how to help developers with this time-consuming validation task. In our case, most of the pipelines written in a diverse team of data engineers, data scientists, and full-stack developers use pySpark as the ETL framework, then we identified two amazing libraries to perform data quality asserts, deequ and great expectations.", "As a result, both libraries allow datasets validation based on columns operations easier and faster. We were still missing something though, democratization: data engineers will have to be in process writing quality rules that data testers and business people do not understand, the drawbacks for our needs a listed below:", "Data rules in our case are written by business people or data tester, then they deliver a file with rules definition and this should be translated to deequ or great expectations asserts. Thus, extra development work is needed. Then, the following question came up:", "Generate SQL from a UI drawn by business people is possible,", "why if we write the rules using SQL and avoid the process in the middle.", "NOTE: Regarding pySpark support, deequ only works with the scala version.", "Both, deequ and great expectations deliver results per column. Hence, in the reports, you will find errors in column validations and some examples of errors.", "rule: column name is in [a, b].", "These results are very useful. However, when working with big data (where values not matching maybe millions) once you see that an assert failed, this report is not enough to discover the issue. Consequently, you have to go to the source data and execute row operations. Yes \u2026, a) get some Ids where this column has these wrong values, b) then join with source dataset, and c) see other columns in a dataset to detect where else records are inconsistent. Then the next question came up:", "What if the reports show results per column and rows as a matrix?", "This way you have error computed per column, but also the entire rows that you'd use easily to debug and fix errors. Besides you can easily get the following reports: number of errors per column, examples of errors per column, number of rows with errors, example IDs with errors.", "Both frameworks validate deduplication computing the number of unique values in a column. However, in big data, this is mostly a row operation that implies seeing how many times a combination of multiple columns (ID and NAME, for example), is within the same dataset. The following question came up :", "What if duplicate validation receives a set of columns?", "Sometimes duplicates are discovered using fuzzy matching, then:", "What and sometimes a fuzzy operator is needed?", "We did not find a way to validate tree structure, for example, a column PARENT_ID and validate this as a valid ID within a dataset.", "Finally, a way to apply checksum comparisons:", "We did not find an easy way to do this on existing frameworks.", "For those that share these same challenges, we decided to write a small library based on pySpark for data quality validation democratization owl-data-sanitizer.", "Let\u2019s follow an example of a family dataset.", "The only input that you need to validate is the following .yaml file:", "In this file, we follow the 3Cs concept and add extra configuration variables.", "Source table: Section with the source table metadata and paths to deliver the results. Currently working with:", "Besides the deduplication columns as we explained previously are added here.", "Correctness validations: The correctness validations are written using simple SQL, you can validate now everything you can write in SQL and the framework does the magic work of computing them using spark automatically.", "Parent-children validations: As explained before, we need to validate our tree structures.", "Completeness validations: To validate how complete your data is you can use two options.", "a) Validate against the count of a source dataset, it means a simple SQL rule.", "b) In a partitioned or micro batched dataset, you can validate against a threshold (example max 10%), how the data counts are growing with respect to the previous partition or batch.", "Compare related tables: As explained before, data is compared for the sake of reconciliation or validation.", "As results of running a spark job using the config the following reports are delivered:", "a) correctness report per column/row matrix-based as was explained before.", "b) completeness report showing how complete the data is.", "When this is run using the file system, HTML reports are also shown. For our example, these reports are produced:", "This report shows per column how many errors we have, how many rows with error (for 8 out of 9 rows, at least 1 validation failing), and always keep the ID for debugging.", "Finally comparing this with a sample diff dataset, show IDs where the data differs. So, they will be IDs 6 and 7 that were not present in the second dataset:", "Installing the library is as easy as:", "you can write a basic example, following the structure of this small unit test where you have control of output spark dataframes:", "or simply, deliver the results using the config file:", "Using the library a validation was performed reading from a big Hive table and delivering the results to Hive, indexing them later on to a superset dashboard.", "Having owl-data-sanitizer as the base, there is still a component missing, including the business people on the rules creation and reports notifications. Then, we are adding a frontend component, where dragging and dropping data quality rules would be possible for data teams, including testers and business people.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Developer focused on Big Data and Distributed Systems. @Amsterdam"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbcc46e1583e6&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ronaldangel?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ronaldangel?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Ronald \u00c1ngel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d16c75e5c24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&user=Ronald+%C3%81ngel&userId=8d16c75e5c24&source=post_page-8d16c75e5c24----bcc46e1583e6---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tsteitle?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Todd Steitle"}, {"url": "https://unsplash.com/s/photos/owl?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://hbr.org/2020/02/data-driven-decisions-start-with-these-4-questions", "anchor_text": "studies"}, {"url": "https://github.com/awslabs/deequ", "anchor_text": "deequ"}, {"url": "https://greatexpectations.io/", "anchor_text": "great expectations."}, {"url": "https://github.com/ronald-smith-angel/owl-data-sanitizer", "anchor_text": "owl-data-sanitizer"}, {"url": "https://github.com/ronald-smith-angel/owl-data-sanitizer", "anchor_text": "ronald-smith-angel/owl-data-sanitizerA pyspark lib to validate data quality. Contribute to ronald-smith-angel/owl-data-sanitizer development by creating an\u2026github.com"}, {"url": "https://medium.com/tag/data-quality?source=post_page-----bcc46e1583e6---------------data_quality-----------------", "anchor_text": "Data Quality"}, {"url": "https://medium.com/tag/spark?source=post_page-----bcc46e1583e6---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/pyspark?source=post_page-----bcc46e1583e6---------------pyspark-----------------", "anchor_text": "Pyspark"}, {"url": "https://medium.com/tag/open-source?source=post_page-----bcc46e1583e6---------------open_source-----------------", "anchor_text": "Open Source"}, {"url": "https://medium.com/tag/democratization?source=post_page-----bcc46e1583e6---------------democratization-----------------", "anchor_text": "Democratization"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&user=Ronald+%C3%81ngel&userId=8d16c75e5c24&source=-----bcc46e1583e6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&user=Ronald+%C3%81ngel&userId=8d16c75e5c24&source=-----bcc46e1583e6---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbcc46e1583e6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bcc46e1583e6---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bcc46e1583e6--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ronaldangel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ronaldangel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ronald \u00c1ngel"}, {"url": "https://medium.com/@ronaldangel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "322 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8d16c75e5c24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&user=Ronald+%C3%81ngel&userId=8d16c75e5c24&source=post_page-8d16c75e5c24--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd7d6272f6f51&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-new-pysparks-library-owl-data-sanitizer-bcc46e1583e6&newsletterV3=8d16c75e5c24&newsletterV3Id=d7d6272f6f51&user=Ronald+%C3%81ngel&userId=8d16c75e5c24&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}