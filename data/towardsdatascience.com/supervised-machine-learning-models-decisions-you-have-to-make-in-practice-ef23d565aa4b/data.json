{"url": "https://towardsdatascience.com/supervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b", "time": 1683007396.468399, "path": "towardsdatascience.com/supervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b/", "webpage": {"metadata": {"title": "Supervised Machine Learning Models: Decisions you have to make in practice | by Jeswanth Yadagani | Towards Data Science", "h1": "Supervised Machine Learning Models: Decisions you have to make in practice", "description": "Intuition about various models like Linear Regression, SVM, Tree-based models, and also ways to play with bias-variance trade-off is discussed."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Although I provide some intuition behind the machine learning models, the focus of this article is not on explaining the math behind machine learning models but on the practical use of them i.e. I will try to answer questions like what to use, how to tune, how to evaluate. This article discusses the tasks, challenges, assumptions, pros-cons, and considerations that a data scientist needs to look at while trying to fit a model to a given data.", "All the models suffer from this trade-off. Understanding this is trivial to tweak the performance of machine learning models. Say we have a dataset and we have fit an ML model.", "A good model should be consistent and be able to handle any changes in the training dataset without much change in itself i.e. it should have already generalized for unseen data as well (that is what we aim for in the first place). The amount by which a model varies due to change in the training dataset is interpreted as the variance of the model.", "Bias can be interpreted as the inclination of a model towards fewer predictor variables. Having a very high bias makes a model irresponsive to changes in the data (lower variance) and results in a poor approximation of the situation. On the other hand, having a lower bias makes the model tap all unnecessary information and allows higher variance.", "To conclude, it is a trade-off that a data scientist has to smartly chose considering the need and use of Model and data.", "Let us start easy and say that we have a line equation with slope m and y-intercept c. Given this line, we can calculate the Y value for any particular X value. Simple LR aims to estimate the slope and intercept of a line (called as a regression line) which in a way best fits all the data points i.e. minimizes the residual sum of squares (known as least-squares approach).", "It may be the case that we will have a zero slope (m). This in a way communicates the independence of Y on the X value. We detect this behavior using the Null Hypothesis test which determines if m is sufficiently far from zero. In practice we use p-value which in a way tells us the probability of data being independent on the predictor variable. We get this probability from the t-distribution of the null hypothesis assumption. To conclude, we just have to look at p-value and if it is less than 0.05 (in general), we can reject the null hypothesis.", "Once we reject the Null Hypothesis, we would like to know how better our model fits the data. For this we use R-square measure (we can also use RSE, but RSE is dependent on units of Y and can\u2019t be generalized for any data). R-square close to 1 implies that our LR model explains variability in Y and is a good fit. If R-square is close to 0, then it could be because of two reasons. (refer to explainable variance section at below)", "What if we have more than one predictor variable? We can extend simple LR from a line to a multi-dimensional hyperplane to fit the data. For our imagination, assume we have two predictor variables. Now we fit a plane in the 3D space where the response is a function of these two predictor variables with some intercept. This can further be extended into higher dimensions.", "We test the Null Hypothesis using f-statistic. Although we have p-values for individual predictors, why should we look at overall F-statistic? When there are a large number of predictors(say 100), then just by chance, we are guaranteed to observe at least one predictor with a p-value below 0.05 even in the absence of a true association between the predictor and response. However, f-statistic doesn\u2019t suffer from this as it adjusts for the number of predictors.", "Once we reject the Null Hypothesis, we may have to select only important predictors to reduce computation needs. This is done using subset selection approaches.", "We make use of R-square to evaluate the model fit. However, a point to be noted \u2014 R-square improves as we include more predictors. It is on us to decide the trade-off between computation needs and model fit. Also aiming for higher R-square might lead to over-fitting issues.", "It is considered as Linear Regression because of its linearity in the coefficients of predictors", "Observing Multiple LR, we see two main assumptions. Firstly, the effect on response Y due to changes in a particular predictor variable is independent of any other predictor variables. (Additive Assumption). Secondly, the change in response Y due to unit change in a predictor variable is constant. (Linear Assumption).", "However, it might not be the case with our data always. We might have to consider higher degree predictor variables to properly fit the data (Note: this is where over-fitting and bias-variance concepts kick in. Also, it is worthwhile to learn about synergy effect i.e. interaction effect across predictor variables)", "What is the degree of predictor variable that we have to choose? This is the question that we as data scientists try to solve and it is on us to choose this degree and interaction terms (known as feature engineering). There is no direct answer or solution to tackle this. However, a trial and error approach can be adopted using Residual plots.", "Residual plots in which residues (true response \u2014 estimated response) are plotted against estimated response values will reveal some insights about the assumed model. If the residual plot shows no pattern, then the assumed model fits data well. Else, there is some problem with the assumed model and we may have to adjust the degree of the polynomial.", "Once, we have finalized our model, we evaluate the fit using R-square. We have to note that having a higher degree predictor variable might overfit the data (look at bias-variance trade-off).", "By now, we must have understood that it is very important to assume a proper model to fit data well in a linear regression setting. KNN Regression removes this necessity and is more flexible. Thus if prediction accuracy is our goal, given enough data, KNN outperforms Linear Regression. But, there is a catch here. \u201cGiven enough data\u201d leads us to curse of dimensionality. KNN Regression is data-hungry and in higher dimensions, practically, there will never be enough data and thus KNN performs worse than LR Regression. Also, LR is more interpretable as it estimates the effect of each predictor on the response. On the other hand, KNN is not interpretable as it just follows data and doesn\u2019t care about predictors.", "Parametric methods tend to outperform Non-Parametric methods when there is a small number of observations per predictor", "It is used in a classification setting of two classes in particular (could be tweaked for more than two but not efficient and thus not discussed here). Rather than modeling response Y, it leverages logistic function to model the probability of Y belonging to a particular category. This analysis can be utilized to see how a change in a particular predictor variable affects the odds of the response.", "Estimates of logistic regression coefficients are found by maximizing likelihood function. z-statistic which is similar to t-statistic is used to perform the null hypothesis test. Larger z-statistic indicates evidence against the null hypothesis. Evaluation of the logistic regression model is done using the classification error rate.", "When classes are well separated, logistic regression fails to produce promising results on unseen data points i.e. it is highly unstable even for small variation in data. Also, in higher dimensions, where data is suffered from \u201ccurse of dimensionality\u201d, logistic regression fails to perform well due to a very high unstable nature as a result of lack of enough data.", "It is an ideal classifier that utilizes Bayes theorem to find out the probability of a data point belonging to each class. Posterior probability or probability of class Y for a data point X is proportional to the product of the prior probability of the class Y and class conditional probability of X belonging to Y.", "It fits data with the least possible error. But to our bad luck, we don\u2019t know the exact class conditional and prior probabilities of the data and we try to estimate these under few assumptions in LDA and QDA below.", "LDA can be used for a multi-class (>2) classification setting. First, we model the distribution of predictors in each class separately. Then we utilize Bayes Theorem to find out the probability of the data point belonging to a particular class.", "Prior probabilities of each class can be found using the proportion of data points belong to the respective class. However, for class conditional probabilities, we have to assume the distribution from which data points are drawn.", "We assume that data point X is drawn from a multivariate Gaussian distribution with a class-specific mean vector and common covariance matrix. Now, our problem is reduced to estimating the mean vector for each class and common covariance. The discriminant equation is linear in X.", "This is similar to LDA, except for the assumption about common covariance of predictor variables across all classes. Here, each class will have its covariance matrix. This leads the discriminant equation to be quadratic in X.", "When decision boundaries are linear, LDA and logistic regression approaches tend to perform better. For moderately non-linear boundaries, QDA may perform better. For more complicated decision boundaries, KNN with well-chosen smoothing parameter performs better with enough data.", "In general, we minimize RSS (known as least-squares approach) to estimate predictor coefficients in a regression setting. However, sometimes these estimates might suffer from high variance as a result of flexibility in the fit.", "To tackle this issue, we penalize the predictor coefficient estimates by shrinking (regularization) few of them towards zero (in case of the ridge) or by making exactly zero (in case of the lasso). The intuition behind ridge and lasso regression models is to compromise on a slight rise in the bias of the model for a larger decrease in variance.", "The amount of shrinkage is dependent on a parameter (lambda) which has to be decided by us as a data scientist. A different set of predictor coefficient estimates are produced for each lambda value that we select i.e. for each lambda, we get a different model that fits the data.", "From the ridge or lasso regression equations, we can observe that the predictor coefficients are dependent on the scale of the predictor variables. Thus, before applying ridge or lasso regression on data, it is very important to normalize each predictor in the data to have a standard deviation of 1.", "We compare each model using R-square value on training data or Mean Square Error (MSE) on testing data. The cross-Validation approach can be used to decide on the value of lambda. It is important to note that in a high dimensional setting, training R-square is highly variable due to the lack of enough data and should not be used to evaluate model performance.", "If the response is a function of many predictors with roughly similar coefficient magnitudes, then ridge regression performs better. If a small number of predictors dominate the response, then lasso regression performs better.", "Tree-based methods split the predictor space into boxes (leaves in the tree) and make the same prediction for all the data points that fall into that box. In the regression setting, this prediction is generally mean of all training observations in this box and for classification setting, the majority vote is considered to predict response to a new data point.", "While making a cut in the tree, we decide on the predictor variable and its cut-point in a way to have the lowest possible RSS (regression tree) or classification error (classification tree). For classification setting, metrics like the Gini index and cross-entropy could also be used as they are more sensitive towards the purity of the leaf (or box).", "A question to answer as a data scientist \u2014 When should I stop splitting the tree or how many boxes should I make?", "Cost Complexity Pruning or weakest link pruning: Intuitively, we grow a large tree and then prune it to select a tree that leads to the lowest error. Instead of considering all possible sub-trees, we use a tuning parameter alpha to prune the tree to the desired level.", "For each alpha value, we will have a corresponding tree. As alpha increases, the tree gets pruned more resulting in fewer splits. Having lower splits leads to lower variance of the model at the cost of a little rise in bias. We can make use of cross-validation to decide on the value of alpha and thus obtain the best possible fit for the data.", "Bagging (Bootstrapped Aggregation): We leverage the bootstrap method to generate multiple datasets and grow a deep bootstrapped tree for each bootstrapped dataset. Although each bootstrapped tree has a very high variance, averaging all of the grown trees will reduce the variance. We make use of out-of-bag test error to evaluate the performance of the tree.", "We use Bagging when prediction accuracy is our goal. However, bagging compromises the interpretability of the model. On the other hand, the cost complexity pruning method generates an interpretable tree.", "Random Forests: This is an interesting and smart tweak to Bagging to decorrelate the generated trees. In Bagging, while building a decision tree, we allow all predictor variables to participate in the split. This process might suffer from generating highly correlated trees if there are few dominant predictors.", "In Random Forest, at each split, we allow a random sample of m(approx sqrt(no.of predictors)) predictors to participate and thus allow all predictors to participate and in turn produce decorrelated trees. The advantage of this method comes visible due to a larger reduction in variance as a result of averaging uncorrelated trees.", "Before jumping into the Support vector classifier, we have to understand Max-Margin Classifier. Max-Margin Classifier generates a hyperplane that separates two classes such that it is perpendicular to the closest data points each belonging to a different class. These data points which guide the Max-Margin Classifier are called \u201cSupport Vectors\u201d. It is interesting to find that the classifier only depends on these support vectors and is independent of all other data points.", "The generalization of the Max-Margin Classifier to the non-separable data set is the Support Vector Classifier. As we can\u2019t find the hard margin that perfectly classifies the dataset, we have to allow a few data points to be wrongly classified. Question \u2014 How many data points should we allow to be wrongly classified?", "We introduce a tuning parameter or cost \u2018C\u2019 which allows wrong classification of the points by the model. It can be chosen using cross-validation. C controls the bias-variance trade-off of the Support vector classifier. Data points that lie on the wrong side of the margin of their class and points that lie on the margin are Support vectors in this classifier as only these account for the formation of support vector classifier.", "A smaller value of C accounts for a hard fit of data and is highly unstable i.e. classifier changes even with a small change in support vectors. This leads to high variance but low bias. On the other hand, higher values of C allow wider margins and produce flexible classifiers which has a higher bias but lower variance.", "We can fit non-linear decision boundaries by introducing higher degree features as well as interaction terms in the predictor space. The concept of Kernel and support vector machines arise as advancements to this concept with complex initializations, but the central idea remains the same.", "Total Sum of Squares: It is the total amount of variance that is inherent in the response of data.", "Residual sum of Squares: RSS measures the amount of variance in the response of the data that is not explained by the model that we used to fit.", "R-square: Proportion of variance in the response of the data that can be explained using the model that we used to fit. Having R-square close to 1 indicates a good model. The unexplained variance could be because of two reasons: a. The considered model doesn\u2019t accurately represent the data (line can\u2019t fit a quadratic form-data) b. Inherent error in the data (due to noise that can\u2019t be modeled).", "When R-square is being measured on training data, trying to achieve R-square closer to 1 by increasing model complexity would result in overfitting of data as it taps the inherent noise that is present in the data. Thus, it is always recommended to report what dataset we use along with the evaluation metric.", "Reference: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani. An Introduction to Statistical Learning: with Applications in R. New York: Springer, 2013.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fef23d565aa4b&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jeswanthyadagani?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jeswanthyadagani?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Jeswanth Yadagani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F756157ee60e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&user=Jeswanth+Yadagani&userId=756157ee60e2&source=post_page-756157ee60e2----ef23d565aa4b---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://monkeylearn.com/word-cloud/", "anchor_text": "monkeylearn"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ef23d565aa4b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----ef23d565aa4b---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistical-analysis?source=post_page-----ef23d565aa4b---------------statistical_analysis-----------------", "anchor_text": "Statistical Analysis"}, {"url": "https://medium.com/tag/statistical-learning?source=post_page-----ef23d565aa4b---------------statistical_learning-----------------", "anchor_text": "Statistical Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----ef23d565aa4b---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&user=Jeswanth+Yadagani&userId=756157ee60e2&source=-----ef23d565aa4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&user=Jeswanth+Yadagani&userId=756157ee60e2&source=-----ef23d565aa4b---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fef23d565aa4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ef23d565aa4b---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ef23d565aa4b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jeswanthyadagani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jeswanthyadagani?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jeswanth Yadagani"}, {"url": "https://medium.com/@jeswanthyadagani/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F756157ee60e2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&user=Jeswanth+Yadagani&userId=756157ee60e2&source=post_page-756157ee60e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F756157ee60e2%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-machine-learning-models-decisions-you-have-to-make-in-practice-ef23d565aa4b&user=Jeswanth+Yadagani&userId=756157ee60e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}