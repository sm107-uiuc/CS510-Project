{"url": "https://towardsdatascience.com/backpropagation-intuition-and-derivation-97851c87eece", "time": 1683017833.683805, "path": "towardsdatascience.com/backpropagation-intuition-and-derivation-97851c87eece/", "webpage": {"metadata": {"title": "Backpropagation: Intuition and Explanation | by Max Reynolds | Towards Data Science", "h1": "Backpropagation: Intuition and Explanation", "description": "Backpropagation is a popular algorithm used to train neural networks. In this article, we will go over the motivation for backpropagation and then derive an equation for how to update a weight in the\u2026"}, "outgoing_paragraph_urls": [{"url": "http://maxwellreynolds.com", "anchor_text": "maxwellreynolds.com", "paragraph_index": 30}], "all_paragraphs": ["Backpropagation is a popular algorithm used to train neural networks. In this article, we will go over the motivation for backpropagation and then derive an equation for how to update a weight in the network.", "A fully-connected feed-forward neural network is a common method for learning non-linear feature effects. It consists of an input layer corresponding to the input features, one or more \u201chidden\u201d layers, and an output layer corresponding to model predictions.", "In each layer, a weighted sum of the previous layer\u2019s values is calculated, then an \u201cactivation function\u201d is applied to obtain the value for the new node. This activation function is a non-linear function such as a sigmoid function.", "Note: without this activation function, the output would just be a linear combination of the inputs (no matter how many hidden units there are).", "Each connection from one node to the next requires a weight for its summation. The goal of backpropagation is to learn the weights, maximizing the accuracy for the predicted output of the network.", "To maximize the network\u2019s accuracy, we need to minimize its error by changing the weights. We can imagine the weights affecting the error with a simple graph:", "We want to change the weights until we get to the minimum error (where the slope is 0).", "The idea of gradient descent is that when the slope is negative, we want to proportionally increase the weight\u2019s value. When the slope is positive (the right side of the graph), we want to proportionally decrease the weight value, slowly bringing the error to its minimum.", "This leaves us with the equation:", "To determine how much we need to adjust a weight, we need to determine the effect that changing that weight will have on the error (a.k.a. the partial derivative of the error function with respect to that weight). The error is calculated from the network\u2019s output, so effects on the error are most easily calculated for weights towards the end of the network. We can then use the \u201cchain rule\u201d to propagate error gradients backwards through the network.", "The chain rule is essential for deriving backpropagation.", "In short, we can calculate the derivative of one term (z) with respect to another (x) using known derivatives involving the intermediate (y) if z is a function of y and y is a function of x.", "Here we\u2019ll derive the update equation for any weight in the network. We start with the previous equation for a specific weight w_i,j:", "It is helpful to refer to the above diagram for the derivation.", "We begin with the following equation to update weight w_i,j:", "We know the previous w_i,j and the current learning rate a. Therefore, we need to solve for", "Using the chain rule we can expand:", "We expand the \u2202E/\u2202z again using the chain rule. For \u2202z/\u2202w, recall that z_j is the sum of all weights and activations from the previous layer into neuron j. It\u2019s derivative with respect to weight w_i,j is therefore just A_i(n-1)", "Again, here is the diagram we are referring to.", "We can solve \u2202A/\u2202z based on the derivative of the activation function. This solution is for the sigmoid activation function. ReLu, TanH, etc. will be different.", "Next we can write \u2202E/\u2202A as the sum of effects on all of neuron j \u2019s outgoing neurons k in layer n+1.", "As we saw in an earlier step, the derivative of the summation function z with respect to its input A is just the corresponding weight from neuron j to k.", "All of these elements are known. w_j,k(n+1) is simply the outgoing weight from neuron j to every following neuron k in the next layer. A_j(n) is the output of the activation function in neuron j. A_i(n-1) is the output of the activation function in neuron i. \u2202E/\u2202z_k(n+1) is less obvious. Considering we are solving weight gradients in a backwards manner (i.e. layer n+2, n+1, n, n-1,\u2026), this error signal is in fact already known. But how do we get a first (last layer) error signal?", "Let\u2019s go back to step 1", "If we are examining the last unit in the network, \u2202E/\u2202z_j is simply the slope of our error function.", "The error signal (green-boxed value) is then propagated backwards through the network as \u2202E/\u2202z_k(n+1) in each layer n. Hence, why backpropagation flows in a backwards direction.", "We have now solved the weight error gradients in output neurons and all other neurons, and can model how to update all of the weights in the network.", "Here is the full derivation from above explanation:", "In this article we looked at how weights in a neural network are learned. We examined online learning, or adjusting weights with a single example at a time. Batch learning is more complex, and backpropagation also has other variations for networks with different architectures and activation functions. Although the derivation looks a bit heavy, understanding it reveals how neural networks can learn such complex functions somewhat efficiently.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Researching ML and neuroimaging | maxwellreynolds.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F97851c87eece&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://maxwellreynolds.medium.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": ""}, {"url": "https://maxwellreynolds.medium.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Max Reynolds"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98ac14124ac1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&user=Max+Reynolds&userId=98ac14124ac1&source=post_page-98ac14124ac1----97851c87eece---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@hharritt?utm_source=medium&utm_medium=referral", "anchor_text": "Hunter Harritt"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "http://aima.cs.berkeley.edu/", "anchor_text": "Artificial Intelligence: A Modern Approach"}, {"url": "https://medium.com/tag/data-science?source=post_page-----97851c87eece---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----97851c87eece---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----97851c87eece---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/programming?source=post_page-----97851c87eece---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/algorithms?source=post_page-----97851c87eece---------------algorithms-----------------", "anchor_text": "Algorithms"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&user=Max+Reynolds&userId=98ac14124ac1&source=-----97851c87eece---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&user=Max+Reynolds&userId=98ac14124ac1&source=-----97851c87eece---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F97851c87eece&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----97851c87eece---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----97851c87eece--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----97851c87eece--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----97851c87eece--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----97851c87eece--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----97851c87eece--------------------------------", "anchor_text": ""}, {"url": "https://maxwellreynolds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://maxwellreynolds.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Max Reynolds"}, {"url": "https://maxwellreynolds.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "341 Followers"}, {"url": "http://maxwellreynolds.com", "anchor_text": "maxwellreynolds.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98ac14124ac1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&user=Max+Reynolds&userId=98ac14124ac1&source=post_page-98ac14124ac1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5dfc048cdc87&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbackpropagation-intuition-and-derivation-97851c87eece&newsletterV3=98ac14124ac1&newsletterV3Id=5dfc048cdc87&user=Max+Reynolds&userId=98ac14124ac1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}