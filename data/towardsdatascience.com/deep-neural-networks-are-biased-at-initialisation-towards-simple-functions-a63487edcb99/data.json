{"url": "https://towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99", "time": 1683018381.212246, "path": "towardsdatascience.com/deep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99/", "webpage": {"metadata": {"title": "Deep Neural Networks are biased, at initialisation, towards simple functions | by Chris Mingard | Towards Data Science", "h1": "Deep Neural Networks are biased, at initialisation, towards simple functions", "description": "This is a follow-up post to Neural networks are fundamentally Bayesian, which presented a candidate theory of generalisation from [1], based on ideas in [2,3]. The reading order is not too important\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian", "paragraph_index": 28}], "all_paragraphs": ["This is a follow-up post to Neural networks are fundamentally Bayesian, which presented a candidate theory of generalisation from [1], based on ideas in [2,3]. The reading order is not too important \u2014 this post predominately summarises material in [2.3], which study why neural networks are biased at initialisation towards simple functions (with low Kolmogorov complexity). This work motivated results in [1], which explained how this bias at initialisation translates into good generalisation in the real world, and thus why neural networks enjoy the success they do! Thus, this post is best thought of as a prequel.", "We begin by asking the following question:", "What functions do neural networks express before being trained?", "More specifically, what is the probability that a neural network will express a function f after being randomly initialised? We will call this quantity P(f),\u00b9 and take the random initialisation to be i.i.d. Gaussian (although [2] suggests that P(f) is not too sensitive to the type of initialisation).", "In general, changing one parameter in a neural network by a small amount will affect the raw output of the network\u2014 so in general, there is a 1\u20131 correspondence between the parameters in a neural network and the function it expresses\u00b2. However, for many problems, we are not interested in the raw output of the network. This is best explained with an example problem.", "\u00b9Note P(f) is denoted P\u1d66 ( f ) in Neural networks are fundamentally Bayesian.\u00b2If we take the domain to be the entire vector space over which the network is defined.", "Consider the problem of correctly classifying images of the handwritten digits 0\u20139 (the MNIST dataset). We are clearly not interested in the raw output of the network \u2014 we only care about its final (discrete) decision. Let us imagine, for simplicity, that we want to only classify the images as even or odd numbers. This can be done with a neural network with a single output neuron, and thresholding at zero \u2014 a positive output implies an even number, a negative output implies an odd number.", "So, if we consider a subset of m images in MNIST, which we call S, then the network N models a function", "where 1 corresponds to even and 0 to odd. This is because ultimately we (mostly) care about the post-thresholded output of the network \u2014 not the raw outputs \u2014 and in which case, small changes to parameters in N may not change the function expressed (i.e. the classification will not change). The notation {0, 1}\u1d50 denotes that m images are being mapped either 0 or 1.", "We can then ask, what is P(f) for these functions?", "See Figures 1 and 2 below for two visualisations of P(f), for the system discussed above. P(f) was calculated by sampling from 10\u2077 different random initialisations of a 2-hidden layer fully connected network, using 100 images from MNIST.", "It is clear from Figures 1 and 2 that there is a huge range in P(f). Results in [2] suggest that the range for the above problem is over 30 orders of magnitude. But why does this matter?", "It matters because, for a set of 100 images, there are 2\u00b9\u2070\u2070 \u2248 10\u00b3\u2070 different possible functions (i.e. possible ways of classifying each image as even or odd). Without information about N, we might assume that each function is equally likely, meaning P(f) \u2248 10\u207b\u00b3\u2070 (this would be the case where images are classified by unbiased coin flip).", "Given that P(f) can be as large as 0.05, neural networks are clearly not unbiased at initialisation. Instead, there is a strong bias towards certain types of functions at initialisation \u2014 before any training has taken place.", "There is a theorem originally due to Levin, and repurposed for input-output maps [4] which when used in the context of neural networks [3] states the following: For the map from the parameters of a neural network N to the function expressed by N, the following result holds:", "where K(f) is the Kolmogorov complexity of the function f and the O(1) terms are independent of f but dependent on N. There are some further conditions that need to be satisfied for the bound to hold for neural networks, but empirical evidence [3] plus a few theoretical results [2] indicate that it does, and is non-vacuous.", "In essence, this says that complex functions will have low P(f), and simple functions can have large P(f), if the bound is tight. However, Kolmogorov complexity is uncomputable \u2014 so proving this for general architectures and datasets would be, at best, non-trivial. Instead, a very clever experiment in [3] allows us to empirically test this upper bound. The results of this experiment are shown in Figure 3, where a fully connected network models functions of the form:", "chosen because a suitable complexity measure exists \u2014 see [3] for details.", "Evidently, P(f) is exponentially larger for simple f. There are functions that lie below the bound, but it is argued in [7] that (very informally) there is a limit on the number of functions that can lie beyond a certain distance from the bound.", "We call this a simplicity bias \u2014 because P(f) is higher for simple functions.", "There is substantial further evidence from [1,2,3] that this simplicity bias is a general property of neural networks of different architectures and datasets\u00b3. For example, similar experiments were performed on MNIST and Cifar10 [1,3] where the CSR complexity measure was used to approximate K(f). There is also an analytic result that perceptrons acting on the boolean hypercube are biased towards low-entropy (and thus simple) functions, in [2].", "Also note that functions with large P(f) have greater \u2018volumes\u2019 in parameter-space (see [1,2] for details). This is intuitively obvious \u2014 if you are more likely to randomly sample some function, it must have more associated parameters, and thus a greater volume in parameter-space.", "\u00b3Bear in mind that the function is defined relative to a dataset, as it specifies its domain and co-domain.", "It is thought [5] that real-world data has an underlying simple description. For example, when we read handwritten digits we do not worry too much about precise details \u2014 if it\u2019s a single oval-like shape, it\u2019s probably a zero. We don\u2019t need to take the exact pixel value of every pixel into account.", "If real-world data that we are interested in learning really does have a simple underlying description, then simple functions will generalise better than complex functions.", "Consider a supervised learning problem \u2014 a training set S from MNIST, containing m examples. Then, an ideal learning agent would be able to calculate the Kolmogorov complexity of all functions\u2074 from the images of the digits (i.e. from the pixel values) to the classifications of the digits. It would then throw out all functions that did not correctly predict all m examples in S. Finally, of these functions, it would choose the one with the lowest Kolmogorov complexity.", "In other words, an ideal learning agent would choose the simplest function that fits the training data.", "\u2074Defined relative to some UTM (see [3] for details).", "At this point, if you have read Neural networks are fundamentally Bayesian, you can stop reading, as you already know the answer! If you haven\u2019t, then please check it out, as it:", "Thus, neural networks generalise well because P(f) is much larger for simple functions, which generalise better.", "This has been a very brief summary of the main results in [2,3]. There are also further experiments which demonstrate that P(f) is not sensitive to the choice of initialisation, and how the observed simplicity bias is found in real-world datasets (e.g. cifar10 and MNIST), using a complexity measure called CSR.", "One final point concerns the strength of the inductive bias \u2014 obviously we only have an upper bound \u2014 it does not guarantee that the probability really does vary exponentially with complexity. If the bias were too weak, then we would not get good generalisation with high probability.", "The PAC-Bayes bound provides a probabilistic bound on generalisation. Applications of this bound in [1,6] show that, for cutting-edge architectures on real-world datasets, the simplicity bias in P(f) is sufficient to guarantee good generalisation. This will be the subject of a future post!", "Finally, if you think I have missed anything or said anything inaccurate, please let me know. Also note that this my interpretation of work done with a number of co-authors, and while I believe it to accurately approximate their views, it may not always be a perfect representation!", "[4] Kamaludin Dingle, Chico Q Camargo, and Ard A Louis. Input\u2013output maps are strongly biased towards simple outputs (2018). Nature communications, 9(1):761.", "[5] J\u00fcrgen Schmidhuber. Discovering problem solutions with low kolmogorov complexity and high generalization capability (1994). MACHINE LEARNING: PROCEEDINGS OF THE TWELFTH INTERNATIONAL CONFERENCE.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "DPhil student at Oxford University in Machine Learning."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fa63487edcb99&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chrismingard.medium.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Chris Mingard"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10----a63487edcb99---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian"}, {"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian"}, {"url": "https://towardsdatascience.com/neural-networks-are-fundamentally-bayesian-bee9a172fad8", "anchor_text": "Neural networks are fundamentally Bayesian"}, {"url": "https://arxiv.org/abs/2006.15191", "anchor_text": "https://arxiv.org/abs/2006.15191"}, {"url": "https://arxiv.org/abs/1909.11522", "anchor_text": "https://arxiv.org/abs/1909.11522"}, {"url": "https://arxiv.org/abs/1805.08522", "anchor_text": "https://arxiv.org/abs/1805.08522"}, {"url": "https://arxiv.org/abs/2012.04115", "anchor_text": "https://arxiv.org/abs/2012.04115"}, {"url": "https://doi.org/10.1038/s41598-020-61135-7", "anchor_text": "https://doi.org/10.1038/s41598-020-61135-7"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----a63487edcb99---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/inductive-bias?source=post_page-----a63487edcb99---------------inductive_bias-----------------", "anchor_text": "Inductive Bias"}, {"url": "https://medium.com/tag/bayesian-machine-learning?source=post_page-----a63487edcb99---------------bayesian_machine_learning-----------------", "anchor_text": "Bayesian Machine Learning"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----a63487edcb99---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&user=Chris+Mingard&userId=3befb9679b10&source=-----a63487edcb99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&user=Chris+Mingard&userId=3befb9679b10&source=-----a63487edcb99---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fa63487edcb99&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----a63487edcb99---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----a63487edcb99--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a63487edcb99--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a63487edcb99--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a63487edcb99--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a63487edcb99--------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chrismingard.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chris Mingard"}, {"url": "https://chrismingard.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "85 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3befb9679b10&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&user=Chris+Mingard&userId=3befb9679b10&source=post_page-3befb9679b10--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5e3e3335a2ce&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-neural-networks-are-biased-at-initialisation-towards-simple-functions-a63487edcb99&newsletterV3=3befb9679b10&newsletterV3Id=5e3e3335a2ce&user=Chris+Mingard&userId=3befb9679b10&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}