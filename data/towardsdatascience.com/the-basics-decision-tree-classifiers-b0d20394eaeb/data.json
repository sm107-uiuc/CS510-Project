{"url": "https://towardsdatascience.com/the-basics-decision-tree-classifiers-b0d20394eaeb", "time": 1683001005.471524, "path": "towardsdatascience.com/the-basics-decision-tree-classifiers-b0d20394eaeb/", "webpage": {"metadata": {"title": "The Basics: Decision Tree Classifiers | by Max Miller | Towards Data Science", "h1": "The Basics: Decision Tree Classifiers", "description": "Decision trees are a conceptually simple and explicable style of model, though the technical implementations do involve a bit more calculation that is worth understanding. Still, the intuition behind\u2026"}, "outgoing_paragraph_urls": [{"url": "http://norvig.com/mayzner.html", "anchor_text": "discussed here", "paragraph_index": 17}], "all_paragraphs": ["Decision trees are a conceptually simple and explicable style of model, though the technical implementations do involve a bit more calculation that is worth understanding. Still, the intuition behind a decision tree should be easy to understand. Indeed, decision trees are in a way quite similar to how people actually make choices in the real world.", "When confronted with a choice to make, real people might think in a series of cascading decisions. If I need to choose what to wear in the morning, I don\u2019t pick an outfit randomly out of the closet, I first breakdown my options. For starters, I probably already have heavy winter clothes and lighter summer clothes separated. I then might check the weather, further narrowing down the range of outfits \u2014 if it\u2019s raining I might want to wear my boots, say. Finally, I may consider what plans I have today and if I need to dress up for any reason. You can think of my decision process as starting with all the options in my closet, and then progressively narrowing down my choices until I\u2019m picking from a much smaller set of options.", "Perhaps we can teach the computer to make decisions or predictions in a similar way. One aspect is that the decision making process is rules based, which makes it easy to implement on a computer, assuming we know what the rules are. The task therefore is finding what the rules are.", "Consider a simple classification problem. Here are points scattered among three categories:", "If presented with a new point somewhere on the grid, how would you predict which category it belonged to? You might start by noticing, say, that all the points in the upper portion of the grid are orange. So, let\u2019s start by simply drawing a horizontal line at the bottom of this \u2018orange territory\u2019. If the new point we\u2019re trying to predict falls above this line, we\u2019ll guess it belongs to the orange category:", "What happens if the point falls below that line? Well, now we notice that the blue points tend to be on the right side and the green points on the left, so let\u2019s divide this lower territory into blue and green halves:", "If the new point is on the left of this line, we\u2019ll guess it\u2019s blue, and if it\u2019s on the right, we\u2019ll guess green. It\u2019s common to see decision trees also visualized as flow charts. This flow chart describes the decision process for this example data set:", "We first ask if the new point has an y-value of more or less than 6.138. If less, we move down to the left and then follow up by asking if the x-value is more or less than -2.337.", "Notice that as we segmented the graph into smaller pieces, the new segments were more homogenous than the prior overall graph had been. We started with one square graph which had equal numbers of the three categories. Then we split the graph in two, a top section that is almost entirely homogenous (all orange) and a bottom section that is now split between only two colors, instead of three. In the next round we split that bottom section in two to get two nearly homogenous sections. Regardless of how complicated our dataset might be, with more features or categories, the goal in a decision tree is to find ways to segment the data so that the subsegments are as \u2018pure\u2019 as possible.", "One of the strengths of the decision tree is that it handles non-linearities well. Consider a slightly more complicated example:", "This graph only has two categories, but they\u2019re not so neatly segmented. We can\u2019t say something like \u201cthe points on the top are more likely to be orange\u201d, since the orange and blue are a little mixed together. Still we can take a similar strategy as we used before, and segment the the data set piece by piece until we have relatively homogenous subsets. A decision algorithm would make the following splits. First:", "And then, after two more steps:", "The algorithm has successfully identified the various centers of these groupings.", "Here\u2019s a more concrete/less abstract setting where these sorts of non-linearities are well identified by decision tree algorithms. One commonly used dataset for data scientists in training has information on the passengers of the Titanic including whether or not they died when the ship sank. If you were making a model that predicted whether a given passenger died, you might notice that a higher fraction of men died than did women. In a logistic regression model, you would notice that the coefficient on the variable is_man is negative. There is an exception, which is that child passengers that were identified as male had higher survival rates. To capture that non-linearity in the logistic model you would have to do some feature engineering, interacting your gender and age variables somehow. The decision tree algorithm can discover those sorts of relationships implicitly, however, by skillfully choosing which features to use to determine its splits.", "Up until now I\u2019ve simply been saying that the decision tree algorithm is deciding where to split up the dataset without saying how. To understand a little bit about how the algorithm works, why it works and why it doesn\u2019t always give perfect results, it\u2019s worth taking a quick digression to talk about information theory.", "Information theory was basically created by one person, Claude Shannon, particularly with the 1948 publication of his paper \u201cA Mathematical Theory of Communication\u201d. Shannon was a researcher at Bell Labs and part of the motivation for his work and many of the early applications of information theory had to do with the sort of communication problems Bell Telephone was dealing with: how to send decodable information over frequently noisy telephone wires. An important aspect of this was determining what exactly is information and how you could measure it. Shannon formalized intuitions about information into a concrete formula, turning the whole question into essentially an applied statistics problem.", "The key insights are that the amount of information contained within a string of characters, whether those are the letters of written English or binary digits, is related to the length of the string, how many available characters there are and the probability of each character\u2019s use. That the amount information ought to be proportional to how long the string is probably pretty intuitive \u2014 you can say more in a paragraph than you can in a single sentence after all \u2014 but the other two points deserve a bit of unpacking.", "First, that the number of available characters should be related to the amount of information contained within each character. Consider how many characters it takes to write a single word. The average number of letters in an English word is just shy of 5. (That stat and many other interesting English language stats are discussed here.) The English alphabet only has 26 letters, after all, and there are a great many different possible words, so naturally expressing any given word should require using a combination of different letters. Written Chinese, on the other hand, has thousands of available characters. Most common words can be expressed as either single characters or two-character compounds. When you have so many characters, any given character can be much more specific, representing a whole semantic unit. The reverse is also true; rendering a word in binary, with only two available characters, 0 and 1, will require many more characters than rendering it in English.", "But, not every character is created equal. Predictable patterns of characters don\u2019t really carry new information; a book with one sentence that\u2019s just been repeated over and over again doesn\u2019t really tell you anything new once you get past the first sentence and get what\u2019s going on. Predictable patterns create informational redundancies.", "Witten English appears to be full of redundancies. Consider text writing practice that shortens a word like \u2018are\u2019 or \u2018you\u2019 to the single letters \u2018r\u2019 or \u2019u\u2019. A frequently cited example is an old ad that used to be found in the New York subway advertising secretarial jobs which read: \u2018f u cn rd ths, u cn bcm a sec & gt a gd jb w hi pa\u2019. Even with more than half of the letters removed, most people can properly parse this sentence: \u2018if you can read this\u2026\u2019 So it seems that not every letter carries as much information as others. Exactly how much information a given letter carries, as it turns out, is related to how likely you are to see it in that position.", "Letters in English are used with different frequencies, but, even more importantly, they also have different frequencies relative to each other and to their position in a word. A common example for English is that the letter \u2018q\u2019 is almost always followed by the letter \u2018u\u2019 (exceptions tend to be loanwords from other languages). Another might be that \u2018e\u2019 is the most common letter, but doesn\u2019t often come as the first letter of a word. The letter \u2018i\u2019 is relatively common, but only rarely comes as the last letter of a word. Certain letters are frequently seen next to each (\u2018th\u2019, \u2018he\u2019, etc.), others are not (how many words contain \u2018kd\u2019 or \u2018bp\u2019?). As a result, there are predictable patterns.", "Shannon illustrated this point by trying guess what letter would come next in a sentence revealed one letter at a time. Shannon would pull a book off of the shelf and randomly choose a sentence and his wife would try to guess each letter in turn. Imagine trying to do this yourself. I have in mind a sentence, what do you think the first letter is? Well, it could be anything! You\u2019ll have to guess, and there\u2019s a high probability you\u2019ll get it wrong. But, you don\u2019t have to guess any random letter out of all 26, because some letters are more common in English than others. Sure it could be a \u2018z\u2019 or a \u2018q\u2019 or a \u2018j\u2019, but those letters don\u2019t actually appear that often, so you probably shouldn\u2019t guess one of them. The letter \u2018e\u2019 is very common, but as we already mentioned, it doesn\u2019t start words very often, so again probably safer not to guess that.", "Knowing that this is the first letter of a sentence not just the first letter of a word also helps. Depending on context, certain words are common at the beginning of sentences: \u2018the\u2019, \u2018then\u2019, \u2018I\u2019, \u2018a\u2019, \u2019to\u2019, \u2018that\u2019, etc. You still have to guess, but there\u2019s a much smaller pool of letters to guess from. You guess one of these letters that you think is most likely. In this case, let\u2019s say that I confirm that the first letter of the sentence is a \u2019t\u2019. Now guessing the second letter is much easier than the first. For starters, while before there were letters we thought were uncommon, now there are letters we can rule out pretty much entirely. Is there a single word in English that begins with the letters \u2019tg\u2019? Or \u2019tk\u2019? Once you find out that the next letter is \u2018h\u2019, guessing the third letter becomes even easier. For starters, you can pretty much rule out all of the other consonants, meaning that at worst, your guess will be one in five rather than one in 26. By the time you get to the third letter, you might even be able to guess all the remaining letters in one go. How many words start with the letters \u2018tho\u2019 and are likely to begin a sentence? Maybe it\u2019s \u2018though\u2019, or possibly \u2018thousands\u2019, but there can\u2019t be more than a dozen likely options.", "You can think about the amount of information each letter carries as being related to the number of reasonable options that could have come in that slot. The first letter in the word carried a lot of information in this example \u2014 it could have been nearly any letter \u2014 while the second letter carried a little less information and the later letters in the word carried nearly no information. Indeed, texters in a hurry can shorten the word \u2018though\u2019 to \u2018tho\u2019 without the meaning being lost in any way, though it may bother orthographical purists. The last three letters of the word \u2018though\u2019 are essentially redundant.", "Shannon formalized all of this into a measurement he called entropy, which essentially describes the amount of information that can be said to be carried by any given message. The formal expression for the entropy of one character, called H, is a sum over all the different possible characters:", "Where pi is the probability of that character appearing. As this probability goes to zero, the value of the expression inside the sum goes to zero with it \u2014 it doesn\u2019t do you much good to have the letter \u2018q\u2019 as an option if it never actually shows up. Crucially, the value of the expression also goes to zero as the probability goes to one (log of one is zero): if you know exactly what the next character is going to be ahead of time, you haven\u2019t learned any new information when that character is revealed.", "As a result, the way to maximize the overall entropy is for each possible character to appear with equal probability, at which point there are no patterns in the text and the characters look essentially random. There are practical reasons why human language doesn\u2019t look like that. In particular, the redundancy in human languages helps to avoid errors \u2014 you can still make out what a word is supposed to be even if there is a spelling mistake or some letters have been left out \u2014 but the cost of this robustness is adding extraneous letters, making the text correspondingly longer.", "This is the formula that the decision tree algorithm uses. It looks at the overall collection of data points and calculates a value for this entropy. In the first example we had three different categories with an equal number of each category \u2014 maximum possible entropy.", "The algorithm then considers each segmentation it can make and calculates an entropy value for each subsection individually. It then searches looks for the split it can make that results in the lowest possible combined entropy. Remember that the first split it made in this example essentially split the orange top half of the graph from the bottom section with the blue and greens:", "In this new configuration, the entropy of the top section is close to zero, since it contains basically only orange points. The entropy of the bottom section is also reduced since even though it\u2019s still evenly split between colors, there are only two color options instead of three. Practical implementations of the decision tree may use a different particular metric \u2014 GINI impurity is a similar but slightly different measure of how homogenous a group is \u2014 but the functional goal is the same.", "When setting up a decision tree model, there are a number of parameters that can be tweaked, particularly to avoid overfit.", "Minimum samples per leaf/minimum samples per split: both of these values are present to avoid extreme over fit. One is the minimum number of data points that the algorithm will accept in one of the final nodes of the tree and the other is the number of points that need to be in a subgroup before the algorithm decides to make another split. Without having a minimum number of samples per leaf, the algorithm might simply continue to make smaller and smaller cuts, until there were groups with only one sample in them. These would look homogenous and low entropy the algorithm, but at the risk of major overfit. Consider one of our earlier examples:", "Some of these regions are still not homogenous! What if we let the algorithm keep going?", "Now the strange outliers, the one blue in mostly orange territory, say, have been given their own little carve outs, super thin regions which may contain only one or two points. The model is now overfit.", "Maximum depth: similarly, you can avoid overfit by setting a maximum depth to the tree, allowing the algorithm to only make so many splits. How deep you want to allow your model to go probably depends on the nature of the data \u2014 how many features you have, how big the sample is, etc.", "Decision trees are easy to set up; they can be improved with feature engineering, but they don\u2019t necessarily require it. They also don\u2019t really require scaling or normalizing features. They handle non-linearities well and, unlike KNN models, they are interpretable. A user can look at the thresholds of the decision tree to find out how a given prediction was arrived at. They also perform their own feature selection implicitly. A feature that is not particularly useful on its own will probably not be utilized because the algorithm will find that splitting the sample along that feature does not work.", "Unfortunately decision trees are also brittle. The algorithm that most implementations use to design the tree tries to make the best possible split at any given point \u2014 a so called \u2018greedy algorithm\u2019 \u2014 which doesn\u2019t necessarily yield the best possible results, though it usually works well enough. An unfortunate consequence of this is that a small change to the training data can cause the algorithm to find a totally different tree, perhaps one of the early splits will be made in a different place thereby changing all the decisions that follow. While decision trees are generally interpretable, a deep tree trained on data with many features will be complex and therefore not really so easy to explain as, say, the coefficients in a linear regression.", "One last point to make is that the nature of the decisions being made by the tree means that certain shapes of decision boundaries will be easier or harder for the tree to map. When groups of similar datapoints are sufficiently separated from each other or the boundaries between groups are aligned with the features, a decision tree will be able to track them reasonably well. When many points are near a boundary that is not aligned with the features, it becomes harder for the tree. Consider the following example:", "You can probably tell visually that the boundary between the two regions is effectively linear. If you could take both the x and y coordinates into account, you could accurately separate the blue and orange regions with essentially only one split, like this:", "A decision tree algorithm, however, struggles to define this decision boundary that isn\u2019t aligned with one feature. For this example, you\u2019d need to let the tree get four or five layers deep at least, and you would end up with a much messier boundary:", "Assuming you have a good view into the data, minor feature engineering would allow you to overcome this, but these sorts of boundaries might be better modeled with other tools.", "Data scientist with a particular passion for limericks, policy and renewable energy."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb0d20394eaeb&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Max Miller"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----b0d20394eaeb---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0d20394eaeb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=-----b0d20394eaeb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0d20394eaeb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=-----b0d20394eaeb---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "http://norvig.com/mayzner.html", "anchor_text": "discussed here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b0d20394eaeb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science-ground-up?source=post_page-----b0d20394eaeb---------------data_science_ground_up-----------------", "anchor_text": "Data Science Ground Up"}, {"url": "https://medium.com/tag/decision-tree?source=post_page-----b0d20394eaeb---------------decision_tree-----------------", "anchor_text": "Decision Tree"}, {"url": "https://medium.com/tag/information-theory?source=post_page-----b0d20394eaeb---------------information_theory-----------------", "anchor_text": "Information Theory"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----b0d20394eaeb---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0d20394eaeb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=-----b0d20394eaeb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb0d20394eaeb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=-----b0d20394eaeb---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb0d20394eaeb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----b0d20394eaeb---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=-----b0d20394eaeb---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Written by Max Miller"}, {"url": "https://medium.com/@max.samuel.miller/followers?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "409 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fdfd5ba1a8332&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&user=Max+Miller&userId=dfd5ba1a8332&source=post_page-dfd5ba1a8332----b0d20394eaeb---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F930bd413e257&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-decision-tree-classifiers-b0d20394eaeb&newsletterV3=dfd5ba1a8332&newsletterV3Id=930bd413e257&user=Max+Miller&userId=dfd5ba1a8332&source=-----b0d20394eaeb---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Max Miller"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "The Basics: KNN for classification and regressionBuilding an intuition for how KNN models work"}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "\u00b711 min read\u00b7Oct 18, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&user=Max+Miller&userId=dfd5ba1a8332&source=-----c1e8a6c955----0-----------------clap_footer----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-knn-for-classification-and-regression-c1e8a6c955?source=author_recirc-----b0d20394eaeb----0---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc1e8a6c955&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-knn-for-classification-and-regression-c1e8a6c955&source=-----b0d20394eaeb----0-----------------bookmark_preview----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----b0d20394eaeb----1---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----b0d20394eaeb----1-----------------bookmark_preview----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----b0d20394eaeb----2---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b0d20394eaeb----2-----------------bookmark_preview----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Max Miller"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "The Basics: Logistic Regression and RegularizationExtensions to the linear model"}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": "\u00b79 min read\u00b7Nov 4, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&user=Max+Miller&userId=dfd5ba1a8332&source=-----828b0d2d206c----3-----------------clap_footer----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-basics-logistic-regression-and-regularization-828b0d2d206c?source=author_recirc-----b0d20394eaeb----3---------------------aae075f7_f9a3_428a_aa20_6cbb15d85456-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F828b0d2d206c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-basics-logistic-regression-and-regularization-828b0d2d206c&source=-----b0d20394eaeb----3-----------------bookmark_preview----aae075f7_f9a3_428a_aa20_6cbb15d85456-------", "anchor_text": ""}, {"url": "https://medium.com/@max.samuel.miller?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "See all from Max Miller"}, {"url": "https://towardsdatascience.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----b0d20394eaeb----0-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----b0d20394eaeb----1-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----0-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----b0d20394eaeb----0---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----b0d20394eaeb----0-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://erdogant.medium.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Erdogan Taskesen"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "From Data to Clusters; When is Your Clustering Good Enough?Sensible clusters and hidden gems can be found using clustering approaches but you need the right cluster evaluation method!"}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b717 min read\u00b75 days ago"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&user=Erdogan+Taskesen&userId=4e636e2ef813&source=-----5895440a978a----1-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a?source=read_next_recirc-----b0d20394eaeb----1---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5895440a978a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffrom-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a&source=-----b0d20394eaeb----1-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----2-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----b0d20394eaeb----2---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----b0d20394eaeb----2-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://piero-paialunga.medium.com/?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://piero-paialunga.medium.com/?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Piero Paialunga"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "Naive Bayes Classifier from Scratch, with PythonFrom theory to practice with Bayes Theorem"}, {"url": "https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": "\u00b710 min read\u00b7Jan 4"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F942708211470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnaive-bayes-classifier-from-scratch-with-python-942708211470&user=Piero+Paialunga&userId=254e653181d2&source=-----942708211470----3-----------------clap_footer----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470?source=read_next_recirc-----b0d20394eaeb----3---------------------85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F942708211470&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnaive-bayes-classifier-from-scratch-with-python-942708211470&source=-----b0d20394eaeb----3-----------------bookmark_preview----85cc4a2a_9a30_4f3d_b5ba_e8ed2b0edf89-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----b0d20394eaeb--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}