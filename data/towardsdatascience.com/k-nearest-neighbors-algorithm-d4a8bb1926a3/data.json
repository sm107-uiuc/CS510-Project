{"url": "https://towardsdatascience.com/k-nearest-neighbors-algorithm-d4a8bb1926a3", "time": 1683017515.115934, "path": "towardsdatascience.com/k-nearest-neighbors-algorithm-d4a8bb1926a3/", "webpage": {"metadata": {"title": "K-Nearest Neighbors Algorithm. A Breakdown of Classification With KNN | by Jason Wong | Towards Data Science", "h1": "K-Nearest Neighbors Algorithm", "description": "K-Nearest Neighbors (KNN) is a classification and regression algorithm which uses nearby points to generate predictions. It takes a point, finds the K-nearest points, and predicts a label for that\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/neighbors.html", "anchor_text": "KNN from Scikit-Learn", "paragraph_index": 1}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Titanic dataset from Kaggle", "paragraph_index": 1}, {"url": "https://www.cut-the-knot.org/pythagoras/DistanceFormula.shtml", "anchor_text": "Euclidean distance", "paragraph_index": 3}, {"url": "https://www.cut-the-knot.org/pythagoras/index.shtml", "anchor_text": "Pythagorean theorem", "paragraph_index": 3}, {"url": "https://docs.python.org/3/library/math.html", "anchor_text": "Python\u2019s built-in Math module", "paragraph_index": 5}, {"url": "https://www.sciencedirect.com/topics/mathematics/manhattan-distance#:~:text=The%20Manhattan%20distance%20between%20two,the%20%E2%80%9Ctaxi%20cab%E2%80%9D%20metric.", "anchor_text": "Manhattan distance", "paragraph_index": 6}, {"url": "https://rittikghosh.com/Minkowski_distance.html", "anchor_text": "Minkowski distance", "paragraph_index": 9}, {"url": "https://www.kaggle.com/c/titanic", "anchor_text": "The Titanic dataset", "paragraph_index": 12}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html", "anchor_text": "KNeighborsClassifier from Sklearn", "paragraph_index": 14}, {"url": "https://scikit-learn.org/stable/modules/neighbors.html#classification", "anchor_text": "user guide", "paragraph_index": 14}], "all_paragraphs": ["K-Nearest Neighbors (KNN) is a classification and regression algorithm which uses nearby points to generate predictions. It takes a point, finds the K-nearest points, and predicts a label for that point, K being user defined, e.g., 1,2,6. For classification, the algorithm uses the most frequent class of the neighbors. For regression, the algorithm uses the average of k data points to predict a continuous value. KNN is a supervised learning algorithm that is in a sense, lazy when it comes training (more on this later).", "This post will serve as a high-level overview of what\u2019s happening under the hood of KNN when performing classification. I\u2019ll start by going over the different distance metrics and move into code examples with KNN from Scikit-Learn. For the coding examples, I will be using the Titanic dataset from Kaggle.", "KNN, being a distance-based classifier, can use different types of distance metrics in order to calculate similarity. The three I will cover in this post are the Euclidean distance, Manhattan distance, and Minkowski distance.", "Euclidean distance is the most common metric used, and is derived from the Pythagorean theorem. Euclidean distance simply refers to the distance between two points. The formula for calculating Euclidean distance:", "The formula above states, for each dimension, we calculate the length of that side in the triangle by subtracting a point\u2019s value from the other\u2019s. Then, square and add it to the running total. The Euclidean distance is the square root of the running total.", "Take a look at the plot above, to calculate the Euclidean distance we would just subtract X1 from X2, square it, do the same for the Y1 and Y2, add the two, and take the square root. This can be easily coded using the Python\u2019s built-in Math module. The code gist below is calculating the Euclidean distance of 2 points. The distance can be calculated the same way as a 2-dimensional space but with an additional z axis.", "The Manhattan distance is a pretty simple metric we can use to calculate distance. Take a look at the plot below, and imagine the grid is an overview map of city blocks. In order to get from point A to point B in the context of Manhattan distance, we couldn\u2019t just go straight through the all the skyscrapers! We\u2019d have to go n blocks east or west, and n blocks north or south.", "Manhattan distance differs from Euclidean distance when we calculate the difference between two points by using the absolute value of the difference. Back to the walking the blocks of Manhattan to get from point A to point B, if we were to go west first, we\u2019d have a negative value. We still want this move to count so we take the absolute value of the difference. The formula for Manhattan distance:", "The formula above states that the distance between point x and point y equals the sum of the absolute differences of the Y value subtracted from the X value in each dimension. We can also calculate the Manhattan distance using the Math module from Python. This time, instead of using the sqrt() function, we will use the abs() function.", "The Minkowski distance metric is a generalized distance across a normed vector space. A normed vector space, meaning a space where each point within has been run through a function. This distance metric is actually an induction of the Manhattan and Euclidean distances. The formula for Minkowski distance:", "The formula above is similar to the Euclidean and Manhattan distance formulas but is just an exponent as the function. When c is set to 1, the formula is the same as Manhattan distance. When c is set to 2, it\u2019s the same as Euclidean distance.", "When performing classification tasks with KNN, the algorithm will store the training split in memory, and measure the distance from the training points to the new point. Since the model stores the training set in memory, it isn\u2019t really \u201ctraining\u201d when we call fit. All of the real work takes place in the prediction step. It is worth noting that because of this \u201clazy training\u201d, KNN can require quite a bit of memory. Being a non-parametric algorithm, it doesn\u2019t have coefficients.", "Let\u2019s go ahead and put this into code for a better understanding. For the purposes of visualizing KNN, I will only be using the Age, Fare, and Survived features from the titanic dataset. The Titanic dataset is for binary classification, with the target being the Survived feature. The dataset I\u2019m loading in has previously been cleaned. For a description on the features in the dataset, see the data dictionary below.", "The scatterplot above shows the 40 sampled data points (passengers) from the scaled train split in blue and orange. The orange points are passengers that survived and the blue points are passengers that did not. The new data point in green is from the scaled test set. If we take the new data point and have a K-value of 3, the model would look for the 3 nearest points and classify the new point depending on which class label appears more frequently. It looks like the 2 of the nearest points are a 1(survived) and 1 is 0(did not survive), since 2 > 1, the prediction for this data point would be that the passenger survived.", "Now to perform some actual classification with the KNeighborsClassifier from Sklearn. Along with the documentation, sklearn also provides a user guide implementing KNN for classification.", "It\u2019s always a good idea to look over the classifier\u2019s parameter dictionary (see below).", "| Classifier implementing the k-nearest neighbors vote. |  | Read more in the :ref:`User Guide <classification>`. |  | Parameters | \u2014 \u2014 \u2014 \u2014 \u2014  | n_neighbors : int, default=5 | Number of neighbors to use by default for :meth:`kneighbors` queries. |  | weights : {\u2018uniform\u2019, \u2018distance\u2019} or callable, default=\u2019uniform\u2019 | weight function used in prediction. Possible values: |  | \u2014 \u2018uniform\u2019 : uniform weights. All points in each neighborhood | are weighted equally. | \u2014 \u2018distance\u2019 : weight points by the inverse of their distance. | in this case, closer neighbors of a query point will have a | greater influence than neighbors which are further away. | \u2014 [callable] : a user-defined function which accepts an | array of distances, and returns an array of the same shape | containing the weights. |  | algorithm : {\u2018auto\u2019, \u2018ball_tree\u2019, \u2018kd_tree\u2019, \u2018brute\u2019}, default=\u2019auto\u2019 | Algorithm used to compute the nearest neighbors: |  | \u2014 \u2018ball_tree\u2019 will use :class:`BallTree` | \u2014 \u2018kd_tree\u2019 will use :class:`KDTree` | \u2014 \u2018brute\u2019 will use a brute-force search. | \u2014 \u2018auto\u2019 will attempt to decide the most appropriate algorithm | based on the values passed to :meth:`fit` method. |  | Note: fitting on sparse input will override the setting of | this parameter, using brute force. |  | leaf_size : int, default=30 | Leaf size passed to BallTree or KDTree. This can affect the | speed of the construction and query, as well as the memory | required to store the tree. The optimal value depends on the | nature of the problem. |  | p : int, default=2 | Power parameter for the Minkowski metric. When p = 1, this is | equivalent to using manhattan_distance (l1), and euclidean_distance | (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. |  | metric : str or callable, default=\u2019minkowski\u2019 | the distance metric to use for the tree. The default metric is | minkowski, and with p=2 is equivalent to the standard Euclidean | metric. See the documentation of :class:`DistanceMetric` for a | list of available metrics. | If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and | must be square during fit. X may be a :term:`sparse graph`, | in which case only \u201cnonzero\u201d elements may be considered neighbors. |  | metric_params : dict, default=None | Additional keyword arguments for the metric function. |  | n_jobs : int, default=None | The number of parallel jobs to run for neighbors search. | ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context. | ``-1`` means using all processors. See :term:`Glossary <n_jobs>` | for more details. | Doesn\u2019t affect :meth:`fit` method. |  | Attributes | \u2014 \u2014 \u2014 \u2014 \u2014  | classes_ : array of shape (n_classes,) | Class labels known to the classifier |  | effective_metric_ : str or callable | The distance metric used. It will be same as the `metric` parameter | or a synonym of it, e.g. \u2018euclidean\u2019 if the `metric` parameter set to | \u2018minkowski\u2019 and `p` parameter set to 2. |  | effective_metric_params_ : dict | Additional keyword arguments for the metric function. For most metrics | will be same with `metric_params` parameter, but may also contain the | `p` parameter value if the `effective_metric_` attribute is set to | \u2018minkowski\u2019. |  | outputs_2d_ : bool | False when `y`\u2019s shape is (n_samples, ) or (n_samples, 1) during fit | otherwise True.", "Since we didn\u2019t include any categorical features in the examples above, I\u2019ll begin loading in the dataframe again, this time keeping the numerical and categorical features. Then, I\u2019ll scale and encode the features before fitting the classifier.", "Now that we have preprocessed train and test splits, we can move on to fitting a KNN classifier. Below, I will instantiate the KNN classifier with the default parameters. There will be a K-value of 5 (5 neighbors) and a p-value (power parameter) of 2 (Euclidean distance).", "Not too shabby, the KNN classifier with default parameters performed pretty well. The model is performing better on the train set than the test set, indicating our model may be overfit. Let\u2019s try the same classification but with Manhattan distance (power-value of 1).", "Using Euclidean distance didn\u2019t increase the performance by much. Let\u2019s move on to trying different K-values and see how the performance compares.", "Unfortunately, there isn\u2019t a set K-value to get the best performance out of our model. However, we do have the ability to iterate over different K-values and compare the performance. Since KNN makes predictions by voting on frequency, it makes sense to use only odd K-values. Having an odd K-value results in the algorithm not being able to evenly split between two classes. One way to visually see the comparison of K-values is to create a accuracy plot (see below).", "Looking at the plot above, it looks like the best k-value between 11\u201313. Having a k-value of 11 results in:", "We can see that this is the best k-value since the train and test accuracy lines are at the highest point (accuracy) while still being close at 11, indicating the model isn\u2019t overfit or underfit.", "The K Neighbors Classifier is a good model to learn when getting into the field of classification with supervised learning. Even though it doesn\u2019t really \u2018train\u2019, understanding how the different distance metrics are used to perform classification will be helpful when it comes to finding the best k-value. The fact that KNN stores the training set in memory means that as the number of data points and features increase, the computation time will increase as well. Thank you for your time and I hope this post will assist you in your future classification endeavors with KNN!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist with a passion for statistical analysis and machine learning"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd4a8bb1926a3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jwong853.medium.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Jason Wong"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1240e6b56e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&user=Jason+Wong&userId=e1240e6b56e3&source=post_page-e1240e6b56e3----d4a8bb1926a3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@thevisualiza?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Dil"}, {"url": "https://unsplash.com/s/photos/doors?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://scikit-learn.org/stable/modules/neighbors.html", "anchor_text": "KNN from Scikit-Learn"}, {"url": "https://www.kaggle.com/c/titanic/data", "anchor_text": "Titanic dataset from Kaggle"}, {"url": "https://www.shutterstock.com/g/German+Vizulis", "anchor_text": "German Vizulis"}, {"url": "https://www.cut-the-knot.org/pythagoras/DistanceFormula.shtml", "anchor_text": "Euclidean distance"}, {"url": "https://www.cut-the-knot.org/pythagoras/index.shtml", "anchor_text": "Pythagorean theorem"}, {"url": "https://docs.python.org/3/library/math.html", "anchor_text": "Python\u2019s built-in Math module"}, {"url": "https://unsplash.com/@mdisc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Michael Discenza"}, {"url": "https://unsplash.com/s/photos/manhattan?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://www.sciencedirect.com/topics/mathematics/manhattan-distance#:~:text=The%20Manhattan%20distance%20between%20two,the%20%E2%80%9Ctaxi%20cab%E2%80%9D%20metric.", "anchor_text": "Manhattan distance"}, {"url": "https://rittikghosh.com/Minkowski_distance.html", "anchor_text": "Minkowski distance"}, {"url": "https://www.shutterstock.com/g/andrev", "anchor_text": "Andre Valadao"}, {"url": "https://www.kaggle.com/c/titanic", "anchor_text": "The Titanic dataset"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html", "anchor_text": "KNeighborsClassifier from Sklearn"}, {"url": "https://scikit-learn.org/stable/modules/neighbors.html#classification", "anchor_text": "user guide"}, {"url": "https://scikit-learn.org/stable/modules/neighbors.html", "anchor_text": "https://scikit-learn.org/stable/modules/neighbors.html"}, {"url": "http://rosalind.info/glossary/euclidean-distance/", "anchor_text": "http://rosalind.info/glossary/euclidean-distance/"}, {"url": "https://www.cut-the-knot.org/pythagoras/DistanceFormula.shtml", "anchor_text": "https://www.cut-the-knot.org/pythagoras/DistanceFormula.shtml"}, {"url": "https://www.cut-the-knot.org/pythagoras/index.shtml", "anchor_text": "https://www.cut-the-knot.org/pythagoras/index.shtml"}, {"url": "https://www.sciencedirect.com/topics/mathematics/manhattan-distance#:~:text=The", "anchor_text": "https://www.sciencedirect.com/topics/mathematics/manhattan-distance#:~:text=The"}, {"url": "https://commons.wikimedia.org/wiki/File:De_Raum_Zeit_Minkowski_005.jpg", "anchor_text": "https://commons.wikimedia.org/wiki/File:De_Raum_Zeit_Minkowski_005.jpg"}, {"url": "https://medium.com/tag/knn?source=post_page-----d4a8bb1926a3---------------knn-----------------", "anchor_text": "Knn"}, {"url": "https://medium.com/tag/classification?source=post_page-----d4a8bb1926a3---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----d4a8bb1926a3---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/tag/k-nearest-neighbours?source=post_page-----d4a8bb1926a3---------------k_nearest_neighbours-----------------", "anchor_text": "K Nearest Neighbours"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----d4a8bb1926a3---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&user=Jason+Wong&userId=e1240e6b56e3&source=-----d4a8bb1926a3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&user=Jason+Wong&userId=e1240e6b56e3&source=-----d4a8bb1926a3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd4a8bb1926a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d4a8bb1926a3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d4a8bb1926a3--------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jwong853.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jason Wong"}, {"url": "https://jwong853.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "199 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe1240e6b56e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&user=Jason+Wong&userId=e1240e6b56e3&source=post_page-e1240e6b56e3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdec1d5eb8281&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fk-nearest-neighbors-algorithm-d4a8bb1926a3&newsletterV3=e1240e6b56e3&newsletterV3Id=dec1d5eb8281&user=Jason+Wong&userId=e1240e6b56e3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}