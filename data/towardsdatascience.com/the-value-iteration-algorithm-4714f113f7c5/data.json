{"url": "https://towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5", "time": 1683009138.358141, "path": "towardsdatascience.com/the-value-iteration-algorithm-4714f113f7c5/", "webpage": {"metadata": {"title": "The Value Iteration Algorithm. Estimation of Transitions and Rewards\u2026 | by Jordi TORRES.AI | Towards Data Science", "h1": "The Value Iteration Algorithm", "description": "In this post, we will present the Value Iteration method to calculate those V-values and Q-values required by Value-based Agents"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "previous post", "paragraph_index": 0}, {"url": "http://www.incompleteideas.net/book/first/ebook/node44.html", "anchor_text": "to converge to the optimal values", "paragraph_index": 10}, {"url": "https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24", "anchor_text": "the next post", "paragraph_index": 23}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 24}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 24}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 25}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 26}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 29}], "all_paragraphs": ["In the previous post, we presented the Value-based Agents and reviewed the Bellman equation one of the central elements of many Reinforcement Learning algorithms. In this post, we will present the Value Iteration method to calculate those V-values and Q-values required by Value-based Agents.", "In the simple example presented in the previous post, we had no loops in transitions and was clear how to calculate the values of the states: we could start from terminal states, calculate their values, and then proceed to the central state. However, only the presence of a loop in the environment prevents this proposed approach.", "Let\u2019s see how these cases are solved with a simple Environment with two states, state 1 and state 2, that presents the following environment\u2019s states transition diagram:", "We only have two possible transitions: from state 1 we can take only an action that leads us to state 2 with Reward +1 and from state 2 we can take only an action that returns us to the state 1 with a Reward +2. So, our Agent\u2019s life moves in an infinite sequence of states due to the infinite loop between the two states. What is the value of both states?", "Suppose that we have a discount factor \u03b3<1, let\u2019s say 0,9, and remember from the previous post that the optimal value of the state is equal to that of the action that gives us the maximum possible expected immediate reward, plus the discounted long-term reward for the next state:", "In our example, since there is only one action available in each state, our Agent has no other option and therefore we can simplify the previous formula as:", "For instance, if we start from state 1, the sequence of states will be [1,2,1,2,1,2, \u2026], and since every transition from state 1 to state 2 gives us a Reward of +1 and every back transition gives us a Reward of +2 the sequence of Rewards will be [+1,+2,+1,+2,+1,+2, \u2026]. Therefore, the previous formula for state 1 becomes:", "Strictly speaking, it is impossible to calculate the exact value of our state, but with a discount factor \u03b3= 0,9, the contribution of a new action decreases over time. For example, for the sweep i=37 the result of the formula is 14.7307838, for the sweep i=50 the result is 14.7365250 and for the sweep i=100 the result is 14.7368420. That means that we can stop the calculation at some point (e.g. at i=50) and still get a good estimate of the V-value, in this case V(1) = 14.736.", "The preceding example can be used to get the gist of a more general procedure called the Value Iteration algorithm (VI). This allows us to numerically calculate the values of the states of Markov decision processes, with known transition probabilities and rewards.", "The idea behind the Value Iteration algorithm is to merge a truncated policy evaluation step (as shown in the previous example) and a policy improvement into the same algorithm.", "Basically, the Value Iteration algorithm computes the optimal state value function by iteratively improving the estimate of V(s). The algorithm initializes V(s) to arbitrary random values. It repeatedly updates the Q(s, a) and V(s) values until they converge. Value Iteration is guaranteed to converge to the optimal values. The following pseudo-code express this proposed algorithm:", "In practice, this Value Iteration method has several limitations. First of all, the state space should be discrete and small enough to perform multiple iterations over all states. This is not an issue for for our Frozen-Lake Environment but in a general Reinforcement Learning problem, this is not the case. We will address this issue in subsequent posts in this series.", "Another essential practical problem arises from the fact that to update the Bellman equation, the algorithm requires knowing the probability of the transitions and the Reward for every transition of the Environment.", "Remember that in our Frozen-Lake example, we observe the state, decide on an action, and only then do we get the next observation and reward for the transition but we don\u2019t know this information in advance. What can we do to get them?", "Luckily, what we can have is the history of the Agent\u2019s interaction with the Environment. So, the answer to the previous question is to use our Agent\u2019s experience as an estimation for both unknowns. Let\u2019s see below how we can achieve it.", "Estimate Rewards is the easiest part since Rewards could be used as they are. We just need to remember what reward we got on the transition from s to s\u2019 using action a.", "To estimate transitions is also easy, for instance by maintaining counters for every tuple in the Agent\u2019s experience(s, a, s\u2019) and normalize them.", "For instance, we can create a simple table that keeps the counters of the experienced transitions. The key of the table can be a composite \u201cstate\u201d + \u201caction\u201d, (s, a), and the values of each entry there is the information about target states, s\u2019, and a count of times that we have seen each target state, c.", "Let\u2019s look at an example. Imagine that during Agent\u2019s experience, in a given state s0 it has executed an action a several times and it ends up c1 times in state s1 and c2 times in state s2. How many times we have switched to each of these states is stored in our transition table. That is, the entry (s,a) in the table contents {s1: c1, s2: c2}. Perhaps visually you can more easily see the information contained in the table for this example:", "Then, it is easy to use this table to estimate the probabilities of our transitions. The probability that the action will take us from state 0 to state 1 is c1 / (c1 + c2) and that the action will take us from state 0 to state 2 c2 / (c1 + c2).", "For example, imagine that from a state 0 we execute the action 1 ten times, and after 4 times it will lead us to state 1, and after 6 times it will lead us to state 2. For this particular example, the entry with the key (0, 1) in this table contents {1: 4, 2: 6}. And this represents that the probability of transition from state 0 to state 1 is 4/10, that is, 0.4 and that of state 0 to state 2 of 6/10, that is, 0.6.", "With this information estimated from the experience of the Agent, we already have all the necessary information to be able to apply the Value Iteration algorithm.", "Following the practical approach of this series, in the next two posts, you will see the Value Iteration method in practice by solving the Frozen-Lake Environment.", "See you in the next post!.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4714f113f7c5&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----4714f113f7c5---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 09"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "previous post"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/4-programaci%C3%B3n-din%C3%A1mica-924c5abf3bfc", "anchor_text": "4. Programaci\u00f3n din\u00e1micaAcceso abierto al cap\u00edtulo 4 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "http://www.incompleteideas.net/book/first/ebook/node44.html", "anchor_text": "to converge to the optimal values"}, {"url": "https://towardsdatascience.com/value-iteration-for-v-function-d7bcccc1ec24", "anchor_text": "the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4714f113f7c5---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4714f113f7c5---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4714f113f7c5---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----4714f113f7c5---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4714f113f7c5---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----4714f113f7c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----4714f113f7c5---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4714f113f7c5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4714f113f7c5---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4714f113f7c5--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4714f113f7c5--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4714f113f7c5--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-value-iteration-algorithm-4714f113f7c5&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}