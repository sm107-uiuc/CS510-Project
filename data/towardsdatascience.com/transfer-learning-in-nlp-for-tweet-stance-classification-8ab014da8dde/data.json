{"url": "https://towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde", "time": 1682994663.904185, "path": "towardsdatascience.com/transfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde/", "webpage": {"metadata": {"title": "Transfer Learning in NLP for Tweet Stance Classification | by Prashanth Rao | Towards Data Science", "h1": "Transfer Learning in NLP for Tweet Stance Classification", "description": "2018 has been a hugely exciting year in the field of Natural Language Processing (NLP), in particular, for transfer learning \u2014 a technique where instead of training a model from scratch, we use\u2026"}, "outgoing_paragraph_urls": [{"url": "http://ruder.io/", "anchor_text": "Sebastian Ruder", "paragraph_index": 0}, {"url": "http://ruder.io/nlp-imagenet/", "anchor_text": "NLP\u2019s ImageNet moment has arrived", "paragraph_index": 0}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT", "paragraph_index": 1}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI GPT,", "paragraph_index": 1}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT", "paragraph_index": 1}, {"url": "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/", "anchor_text": "Wikipedia articles", "paragraph_index": 2}, {"url": "https://ai.google/research/pubs/pub41880", "anchor_text": "1 billion word benchmark", "paragraph_index": 2}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT", "paragraph_index": 3}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI\u2019s GPT", "paragraph_index": 3}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "this GitHub repo", "paragraph_index": 5}, {"url": "https://medium.com/explorations-in-language-and-learning/transfer-learning-in-nlp-2d09c3dfaeb6", "anchor_text": "this article", "paragraph_index": 7}, {"url": "http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases", "anchor_text": "word2vec", "paragraph_index": 9}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe", "paragraph_index": 9}, {"url": "https://www.sysml.cc/doc/50.pdf", "anchor_text": "powerful pre-trained language models", "paragraph_index": 10}, {"url": "http://ruder.io/nlp-imagenet/", "anchor_text": "In his post", "paragraph_index": 12}, {"url": "http://ruder.io/semi-supervised/", "anchor_text": "semi-supervised learning", "paragraph_index": 13}, {"url": "http://alt.qcri.org/semeval2016/task6/", "anchor_text": "this SemEval 2016 shared task", "paragraph_index": 14}, {"url": "https://arxiv.org/pdf/1605.01655.pdf", "anchor_text": "task creators\u2019 paper.", "paragraph_index": 21}, {"url": "http://www.saifmohammad.com/WebPages/StanceDataset.htm", "anchor_text": "fully interactive visualization provided by the task creators", "paragraph_index": 24}, {"url": "http://www.aclweb.org/anthology/S16-1003", "anchor_text": "use a macro-averaged F-score", "paragraph_index": 27}, {"url": "https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/", "anchor_text": "here", "paragraph_index": 27}, {"url": "http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools", "anchor_text": "perl script", "paragraph_index": 28}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "in this paper", "paragraph_index": 29}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "mentioned in their paper", "paragraph_index": 32}, {"url": "https://github.com/fastai/fastai/tree/master/fastai/text", "anchor_text": "fastai.text", "paragraph_index": 34}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch v1", "paragraph_index": 34}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "original paper", "paragraph_index": 35}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/datasets.py", "anchor_text": "made available publicly", "paragraph_index": 36}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "[ulmfit.ipynb]", "paragraph_index": 37}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "main GitHub repo", "paragraph_index": 37}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "full paper", "paragraph_index": 38}, {"url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/", "anchor_text": "Wikitext 103 dataset by Stephen Merity.", "paragraph_index": 39}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/datasets.py", "anchor_text": "provides an API", "paragraph_index": 39}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "ulmfit.ipynb", "paragraph_index": 45}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "fastai course documentation page", "paragraph_index": 46}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "months of development by Jeremy Howard and the", "paragraph_index": 47}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "fastai", "paragraph_index": 47}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "team", "paragraph_index": 47}, {"url": "https://github.com/fastai/fastai/blob/1c5c007e10a187f4a048a3ee8137016534259768/fastai/text/transform.py#L71", "anchor_text": "[source]", "paragraph_index": 49}, {"url": "https://spacy.io/usage/spacy-101#annotations-token", "anchor_text": "spaCy's underlying tokenizer", "paragraph_index": 49}, {"url": "https://github.com/fastai/fastai/blob/4cd2e454a735e208c60b2ff7f82bbb4b62e85725/fastai/text/transform.py#L19", "anchor_text": "source code here", "paragraph_index": 49}, {"url": "https://github.com/fastai/fastai/blob/050080be574cb1260462bbd03e9600e43e7a54b1/fastai/train.py#L23", "anchor_text": "[source", "paragraph_index": 50}, {"url": "https://github.com/fastai/fastai/blob/4cd2e454a735e208c60b2ff7f82bbb4b62e85725/fastai/text/learner.py#L146", "anchor_text": "source", "paragraph_index": 53}, {"url": "http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip", "anchor_text": "provided by the task creators", "paragraph_index": 59}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "described in their paper", "paragraph_index": 61}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "their paper", "paragraph_index": 62}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer.ipynb", "anchor_text": "[transformer.ipynb]", "paragraph_index": 63}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "main GitHub repo", "paragraph_index": 63}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "GitHub source", "paragraph_index": 66}, {"url": "https://github.com/huggingface/pytorch-openai-transformer-lm", "anchor_text": "this PyTorch port of OpenAI GPT", "paragraph_index": 66}, {"url": "https://github.com/huggingface", "anchor_text": "HuggingFace", "paragraph_index": 66}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "described in their paper", "paragraph_index": 67}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "been proven to be a very powerful language modelling tool", "paragraph_index": 69}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "is explained in this article", "paragraph_index": 69}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "achieve a much longer-range context than LSTM-based approaches", "paragraph_index": 69}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "see the GitHub repo", "paragraph_index": 70}, {"url": "https://github.com/huggingface/pytorch-openai-transformer-lm", "anchor_text": "PyTorch port", "paragraph_index": 71}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer.ipynb", "anchor_text": "transformer.ipynb", "paragraph_index": 77}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer-openai/train_stance.py", "anchor_text": "train_stance.py", "paragraph_index": 77}, {"url": "http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip", "anchor_text": "provided by the task creators", "paragraph_index": 78}, {"url": "http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools", "anchor_text": "provided by the task creators", "paragraph_index": 89}, {"url": "https://www.kaggle.com/kazanova/sentiment140", "anchor_text": "Kaggle Sentiment140 dataset", "paragraph_index": 92}, {"url": "https://github.com/sebastianruder/NLP-progress/blob/master/english/text_classification.md", "anchor_text": "produces state-of-the-art accuracy on various text classification benchmark datasets such as IMDb and AG News", "paragraph_index": 94}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "ULMFiT paper", "paragraph_index": 95}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "excellent transfer learning performance on very limited training samples", "paragraph_index": 101}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "anywhere from 5,000 to 550,000 training samples", "paragraph_index": 102}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2", "paragraph_index": 110}, {"url": "https://www.linkedin.com/in/andrew-wesson-7079728b/", "anchor_text": "Andrew", "paragraph_index": 111}, {"url": "https://www.linkedin.com/in/abhisheksunnak/", "anchor_text": "Abhishek", "paragraph_index": 111}, {"url": "https://www.linkedin.com/in/prrao87/", "anchor_text": "LinkedIn", "paragraph_index": 112}, {"url": "https://twitter.com/tech_optimist", "anchor_text": "Twitter", "paragraph_index": 112}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "the GitHub repo", "paragraph_index": 113}], "all_paragraphs": ["2018 has been a hugely exciting year in the field of Natural Language Processing (NLP), in particular, for transfer learning \u2014 a technique where instead of training a model from scratch, we use models pre-trained on a large dataset and then fine-tune them for specific natural language tasks. Sebastian Ruder provides an excellent account of the past and current state of transfer learning in his post \u201cNLP\u2019s ImageNet moment has arrived\u201d, explaining why this is such a hot field in NLP right now \u2014 his post is a must read.", "In recent times, methods such as ULMFiT, OpenAI GPT, ELMo and Google AI\u2019s BERT have revolutionized the field of transfer learning in NLP by using language modelling during pre-training, which has significantly improved on the state-of-the-art for a variety of tasks in natural language understanding. It can be argued that the use of language modelling (which is not without its limitations) is one of the main reasons computers have shown great improvements in their semantic understanding of language.", "One interesting aspect of the transfer learning methods mentioned above is that they use language models pre-trained on well-formed, massive curated datasets that include full sentences with a clear syntax (such as Wikipedia articles and the 1 billion word benchmark). The natural question that arises is \u2014 how well can such pre-trained language models generalize to natural language tasks from a different distribution, such as Tweets?", "In this post we will discuss and compare two modern transfer learning approaches \u2014 ULMFiT and OpenAI\u2019s GPT (\u201cGenerative Pre-Training\u201d), and show how they can be fine-tuned with relative ease to perform classification tasks from a different distribution \u2014 in this case, classifying the stance of Tweets towards a target topic.", "We will aim to develop a modelling approach that can help answer the following questions:", "All the code and data for the results shown below are available in this GitHub repo.", "These two particular transfer learning methods were chosen for this project because they are very similar in terms of how they use language modelling to perform unsupervised pre-training, followed by a supervised fine-tuning step (they are both semi-supervised methods). But they are also different in that they use different network architectures to achieve generalization. ULMFiT uses a 3-layer bi-LSTM architecture, while the OpenAI approach uses a transformer network.", "A full description of ULMFiT and OpenAI\u2019s GPT is too long a topic for this post, but this article does an excellent job of highlighting the technical details of both model architectures and why they matter, so please do read it!", "Before going into the details of transfer learning for Tweet stance classification, let\u2019s clarify some terminology to understand why transfer learning has so drastically improved state-of-the-art for a variety of natural language tasks in recent times.", "Historically, pre-trained word embedding techniques such as word2vec and GloVe, were heavily used in NLP to initialize the first layer of a neural network before training for a new task. These are shallow representations (a single layer of weights, known as embeddings). Any prior knowledge from the word embeddings are only present in the first layer of the network \u2014 the entire network would still need to be trained from scratch for a new target task.", "To derive meaning from sequences of words (such as those seen in natural language), models that utilize word embeddings would still need tremendous amounts of data to disambiguate large sets of words and \u201clearn\u201d from a completely new and unseen vocabulary. As shown in a benchmark result discussed later in this post, the amount of data required for transferring knowledge through word embeddings can be huge, which can result in very large computational costs. Throughout 2018, the advent of powerful pre-trained language models have shown that is possible to gain a much deeper understanding of language semantics and structure for new tasks, especially for long sequences, using the knowledge gained from pre-training on large text corpora.", "A language model attempts to learn the structure of natural language through hierarchical representations, and thus contains both low-level features (word representations) and high-level features (semantic meaning). A key feature of language modelling is that it is generative, meaning that it aims to predict the next word given a previous sequence of words. It is able to do this because language models are typically trained on very large datasets in an unsupervised manner, and hence the model can \u201clearn\u201d the syntactic features of language in a much deeper way than word embeddings.", "In his post, Sebastian Ruder does a very elegant job of highlighting why language modelling is so powerful for a broad range of NLP tasks. Unlabelled language data is relatively easy to obtain (it is freely available in the form of large text corpora), so by feeding a language model a sufficiently large dataset, it is now possible to perform unsupervised pre-training on billions of words while incorporating a deeper knowledge of language syntax.", "Transfer learning in NLP is now typically done as a multi-step process\u2014 where an entire network is first pre-trained in an unsupervised manner with a language modelling objective. Following this, the model is then fine-tuned on a new task using a supervised approach (with some labelled data), which can then be used for tasks such as text classification. This combination of unsupervised pre-training (using language modelling) followed by supervised fine-tuning is termed as semi-supervised learning, and is the approach used to solve our Tweet stance classification problem in this post.", "The Tweet dataset used in this post comes from this SemEval 2016 shared task, which contains Tweets that pertain to the following five topics :", "The labelled data provided consists of a target topic, a Tweet that pertains to it, and stance of the Tweet towards the target. The data is already split into a training set (containing 2,914 Tweets) and a test set (containing 1,249 Tweets). The stance can be one of three labels: \u201cFAVOUR\u201d, \u201cAGAINST\u201d and \u201cNEITHER\u201d, hence this is a multi-class dataset.", "Stance detection is a subcategory of opinion mining, where the task is to automatically determine whether the author of a piece of text is in favour or against a given target. Consider the following two Tweets:", "We don\u2019t inherit the earth from our parents we borrow it from our children", "Last time I checked, Al Gore is a politician, not a scientist.", "To a human observer, it is reasonably clear that both Tweets are relevant to the topic of climate change, and that each expresses a particular stance towards the topic of climate change. However, to a machine, detecting this stance is a difficult problem on multiple fronts. The informal and unstructured syntax of Tweets combined with the fact that machines lack proper contextual awareness and historical knowledge that humans have (for example, knowing who Al Gore is), makes this a challenging problem for machine learning algorithms. In addition, gathering large amounts of labelled data for Tweet stance in order to train a machine learning algorithm is expensive and tedious. It is becoming more and more necessary to develop deep learning methods that can work with limited amounts of training data and still yield useful insights.", "Stance detection is related to, but not the same as sentiment analysis. In sentiment analysis, we are interested in whether a piece of text is positive, negative, or neutral based on just the content of the language used. Typically, for sentiment analysis, the choice of positive or negative language correlates with the overall sentiment of the text.", "However, the stance of a piece of text is defined with respect to a target topic, and can be independent of whether positive or negative language was used. The target (topic towards which opinion is expressed) may or may not be mentioned directly in the actual text, and any entities mentioned in the text may or may not be the actual target of opinion. Below is an example from the task creators\u2019 paper.", "Tweet: The pregnant are more than walking incubators. They have rights too!", "In the above example, since the topic is phrased as \u201clegalization of abortion\u201d, the Tweet can be interpreted as being in favour of the topic. Had it been phrased as \u201cPro-life movement\u201d, its stance would have been against the topic. It is clear from this example that the language used in the Tweet is only loosely positive in its sentiment; however this sentiment has no bearing on whether it is in favour of, or against the topic.", "The below table shows a breakdown of how many Tweets pertain to each topic in the dataset. The 2,914 Tweets are distributed unequally per topic, and there is significant variation in the number of Tweets belonging to each class for each topic. To explore the distribution and inspect the Tweets in more detail, take a look at the fully interactive visualization provided by the task creators.", "Looking at this distribution, the stance classification task appears quite challenging \u2014 not only is the dataset small (a couple of thousand training samples in total, ranging from a minimum of 395 to a maximum 689 Tweets per topic) \u2014 but there is also a significant class imbalance in the samples. For example, the topic \u201cclimate change is a concern\u201d has a larger percentage of training samples classified as \u201cfavour\u201d, and a very small percentage (less than 4%) classified as \u201cagainst\u201d. On the other hand, the topic \u201catheism\u201d has a much larger fraction of its samples classified as \u201cagainst\u201d.", "Any modelling approach for stance classification must be able to capture this class imbalance, both between and within the target classes.", "To evaluate classification performance, the creators of the task use a macro-averaged F-score, which is the harmonic mean of precision and recall for the two main classes \u201cFAVOUR\u201d and \u201cAGAINST\u201d. An example of how precision and recall are used in sentiment classification is given here. In general, precision is at odds with recall, and hence the F-score provides a good way to gain insights into a classifier\u2019s performance. Although we do not include the third class \u201cNEITHER\u201d in the evaluation, it is implicitly accounted for, since the system has to correctly predict all three classes to avoid being penalized heavily in either of the first two.", "To evaluate the F-score, a perl script (whose usage will be described in later sections) was provided by the task creators\u2014 all we need to do is shape our stance classifier\u2019s prediction output in a way that can be read by the evaluation script. The macro-averaged F-score obtained can then be compared with other models\u2019 results.", "The winning entry for this task in 2016 was from team MITRE, who describe their classification approach in this paper. To detect stance, MITRE used a Recurrent Neural Network (RNN) model organized into 4 layers as shown in the below image. The first layer contained one-hot-encoded tokens (i.e. words from the text) that were projected through a 256-embedding layer called the \u201cprojection layer\u201d. The sequence of outputs were then fed into a \u201crecurrent layer\u201d (containing 128 LSTM units), whose output was then connected to a 128-dimensional layer of Rectified Linear Units (ReLUs) with 90% dropout. The final output layer was a 3-dimensional softmax layer representing each output class: FAVOUR, AGAINST and NONE.", "MITRE also applied transfer learning (via the use of word embeddings) to reuse prior knowledge of Tweet syntax from a pre-trained model. This was accomplished through a multi-step pre-training process as described below:", "The F-scores on unseen test data for each topic and class obtained by MITRE are shown below. Note that for the topic class \u201cClimate change (AGAINST)\u201d, the F-score is zero, meaning that their model did not predict any Tweets from the test set as being against climate change. However, just 3.8% of the training data from the climate change topic was of the class \u201cAGAINST\u201d (which amounts to just 15 training samples!), so it makes sense that the model might be lacking context for this particular class due to the sheer lack of training samples.", "As mentioned in their paper, MITRE obtained a macro F-score (averaged across all topics) of 0.68. This was the best score among all 19 participants who submitted their results to the competition in 2016.", "The approach used by MITRE was a rather elaborate multi-step pre-training procedure using word embeddings that required the use of very large unlabelled datasets (hundreds of millions of samples), significant cleaning of the raw data and separate pre-training steps for each layer in the network. The multitude of hand-crafted steps was mainly because of the limitations of word2vec embeddings (which was the dominant approach to pre-training models in NLP at the time).", "ULMFiT has been entirely implemented in v1 of the fastai library (see fastai.text on their GitHub repo). Version 1 of fastai is built on top of PyTorch v1, so having some knowledge of PyTorch objects is beneficial to get started. In this post, we cover some of the techniques that fastai has developed that make it very convenient to do transfer learning, even for someone with relatively little deep learning experience.", "As described in the original paper, ULMFiT consists of three stages.", "We only perform steps 2 and 3 during Tweet stance classification. Step 1 is an unsupervised pre-training step, and is really computationally expensive \u2014 which is why the models have been made available publicly by fastai so others can benefit from their work. We bank on the pre-trained language model\u2019s ability to capture long-term dependencies in any target text (in English) that we might encounter.", "All the code for this section is available in a Jupyter notebook [ulmfit.ipynb]. For brevity, only the key elements of the approach are discussed in this post \u2014 feel free to look through the full notebook and this project\u2019s main GitHub repo for a deep-dive into the working code for classifying Tweet stance.", "The following novel techniques from the ULMFit paper are what allow it to generalize well even on unseen data from a different distribution. It is recommended to read the full paper for a deeper understanding, but a summary is given below.", "ULMFiT\u2019s pre-trained language model was trained on the Wikitext 103 dataset by Stephen Merity. fast.ai provides an API where this pre-trained model (along with some standard datasets for testing) can be conveniently and easily loaded for any target task before fine-tuning. The main thing to note about Wikitext 103 is that it consists of a pre-processed subset of 103 million tokens extracted from Wikipedia. The dataset retains the original case (it was not lower-cased before training the language model), and all punctuation and numbers are included. The type of text data included in this dataset includes sentences from full Wikipedia articles, so the hope is that the language model is able to capture and retain some longer-term dependencies from relatively complex sentences.", "The original Tweets can contain some arbitrary non-English characters, so we take care of this before loading in the language model. Note that this only removes non-English characters, but does not do any other kind of aggressive pre-processing (like lower-casing or removing entire words or hashtags). We retain the entirety of the information in the raw data and let the language model do the heavy lifting.", "The ULMFiT framework implemented in fast.ai works very well with Pandas DataFrames, so all data is read in using pandas.read_csv and stored as a DataFrame.", "We then make sure that the Tweets only contain ASCII characters by applying a simple cleanup function.", "ULMFiT requires just the stance and the text data (i.e. Tweets) for the language-model finetuning and classification steps, hence we store these in the relevant DataFrame and write the clean data out to a csv file..", "This is the first stage of training, where we use the pre-trained language model weights (Wikitext 103) and fine-tune it with just the provided training data of 2,914 Tweets.", "The published Jupyter notebook (ulmfit.ipynb) describes in detail the pre-processing that is performed under-the-hood by fastai when using ULMFiT. The reader is directed to look at the Jupyter notebook for more code and API details.", "In our case, we specify a minimum word frequency of 1 for our language model fine-tuning step, which tells ULMFiT to only tokenize words in the Tweets that appear more than once with a unique token \u2014 all the words that appear once are given the tag <unk> during tokenization. For a very detailed historical account of how and why tokenization is done this way, this fastai course documentation page contains some very useful information.", "On viewing the tokenized Tweets, we can see that they look markedly different from their original form. The tokenization technique used by fastai.text is quite advanced and obtained after months of development by Jeremy Howard and the fastai team, and thus uses quite a few tricks to capture semantic meaning from the text.", "Note that we are not converting the text to lowercase and removing stopwords (which was a common pre-tokenization approach in NLP until recently) \u2014 this would result in a tremendous loss of information that the model could instead use to gather an understanding of the new task's vocabulary. Instead, a number of added tags are applied to each word as shown above so that minimal information is lost. All punctuation, hashtags and special characters are also retained.", "For example, the xxmaj token [source] indicates that there is capitalization of the word. \"The\" will be tokenized as \"xxmaj the\". Words that are fully capitalized, such as \"I AM SHOUTING\", are tokenized as \"xxup i xxup am xxup shouting\". The method still uses spaCy's underlying tokenizer (including a multi-thread wrapper around spaCy to speed things up), but adds tags in a very smart way . This balances capturing semantic meaning with reducing the number of overall tokens \u2014 so it is both powerful and efficient. For a full list of all the token tags generated by ULMFiT\u2019s fastai implementation, see the source code here.", "Find the optimum learning rate: We define a learner object that uses the tokenized language model data, that is organized into batches for the GPU, and feed it a pre-trained language model as follows.fastai.train [source] provides a convenient utility to search through a range of learning rates to find the optimum one for our dataset. The idea is that our optimization function needs to use a learning rate that is at least an order of magnitude below the point at which the loss starts to diverge.", "Applying discriminative fine-tuning as per the ULMFiT paper, we run the language model fine-tuning step until the validation loss drops to a low value (close to 0.3).", "This fine-tuned encoder layer\u2019s weights are then saved for use during the classification stage.", "This step involves creating a classifier object [source] that can predict a class label once we re-train the model as a classifier. The same network structure is still used for this task \u2014 the output layer is defined in a way that takes into account the number of classes we want to predict in our data.", "Find optimum learning rate for the classifier: Just as before, the lr_find method is run like before to find an optimum learning rate for the classifier.", "Carefully train the classifier: During classification, we first define a classifier learner object, and gradually unfreeze layers while running for one epoch each time as per the ULMFiT paper\u2019s suggestion. This helps us obtain a better classification accuracy than if we were to aggressively train all the layers at once.", "The validation loss for the classifier is much higher than for the language model, which could possibly be linked with the dataset. Since there is significant class imbalance, a significant number of labels during training are expected to be predicted incorrectly. Specific to this Twitter dataset, this problem could be more severe than what we see in the state-of-the-art examples (such as IMDb).", "We read in the test set and store it in a Pandas DataFrame as shown.", "We can then apply our classifier learner\u2019s predict method to predict the stance.", "The perl script provided by the task creators is run on the output and gold reference file (also provided by the creators) to produce a macro F-score that we can then compare with the benchmark result from MITRE.", "The best average macro F-score of 0.65 using ULMFiT across all topics was obtained using the below approach and parameters:", "OpenAI GPT, as described in their paper, is an adaptation of the well-known transformer from Google Brain\u2019s 2017 paper \u201cAttention is All You Need\u201d.", "While the original version from Google Brain used an identical encoder-decoder 6-layer stack, GPT uses a 12-layer decoder-only stack. Each layer has two sub-layers, consisting of a multi-head self-attention mechanism, and a fully connected (position-wise) feed-forward network. A full description of the transformer architecture used by OpenAI for transfer learning is given in their paper.", "All the code for this section is available in a Jupyter notebook [transformer.ipynb]. Just as before, only the key elements of the model are discussed in this post for brevity \u2014 feel free to look through the full notebook and this project\u2019s main GitHub repo for a deep-dive into the working code for classifying the Tweets.", "The following steps are used to train the OpenAI transformer:", "The first step (unsupervised pre-training) is very expensive, and was done by OpenAI (who trained the model for a month on 8 GPUs!) \u2014 thankfully, we can use the downloaded pre-trained model weights and proceed directly to the supervised fine-tuning step.", "NOTE: While the original OpenAI transformer was implemented in TensorFlow [GitHub source] \u2014 for this project we used this PyTorch port of OpenAI GPT, thanks to the amazing work by the folks at HuggingFace. This is so that we can make a more consistent comparison with the fastai implementation of ULMFiT (which is also PyTorch-based), not to mention the ease of maintaining and distributing the code using one single framework.", "To perform out-of-domain target tasks, GPT includes language modelling as an additional objective to the fine-tuning, which helps generalized learning as described in their paper. This auxiliary language modelling objective is specified with a weighting parameter as shown below.", "Where L1, L2 and L3 are the likelihoods for the language modelling objective, task-specific objective and combined objectives respectively.", "The transformer has been proven to be a very powerful language modelling tool, especially in machine translation, thanks to its \u201cself-attention mechanisms\u201d. A very intuitive and visual explanation of how the transformer is suited to capturing longer-range linguistic structure (using masked self-attention) is explained in this article. A key point to note is that OpenAI GPT used a 12-layer decoder-only structure, with 12 attention heads and a 768-dimensional state. Another key point is that each minibatch was capable of sampling a maximum of 512 contiguous tokens, which, according to the OpenAI authors allows the transformer to achieve a much longer-range context than LSTM-based approaches.", "OpenAI designed their transformer to be task-agnostic and be able to generalize to a range of natural language tasks. To accomplish this, they allow the definition of custom \u201ctask-specific heads\u201d as per the above image. The task-specific head acts on top of the base transformer language model, and is defined in the DoubleHeadModel class in model_pytorch.py(see the GitHub repo).", "The PyTorch port of GPT originally written by HuggingFace was for a multiple choice classification problem (ROCStories). For this Tweet stance detection task, we use the guidelines mentioned in the OpenAI paper to write a custom input transform for a classification task-head, such that we pad every text (representing each Tweet, in our case) with a start symbol and tokenize them for input to the encoder layer. This is done as follows.", "Note that in the classification input transform function above, all we need to do is specify the start token to the encoder, and then append the text to this start token. We maintain the same tensor dimensionalities for each of the other variables as the original PyTorch code.", "The dataloader to the training script is defined as follows. Just as in the case of ULMFiT, we clean the data to remove any non-ASCII characters (to avoid issues during the encoding step). The data is then stored as Numpy arrays.", "To feed the data to the classification transform, we split it into training, validation and test sets using scikit-learn\u2019s train_test_split utility.", "Using the above code, we store the training Tweet (trX) alongside its numericalized stance (trY). We feed in the data to the PyTorch dataloader in train_stance.py as Numpy arrays (not DataFrame columns). Below is an example of the input data once it is shaped as required by the transformer. The stance \u201c0\u201d corresponds to \u201cAGAINST\u201d, \u201c1\u201d to \u201cFAVOUR\u201d and 2 to \u201cNEITHER\u201d. Just as in ULMFiT, no information from the raw data is removed \u2014 we rely on the language model objective to identify syntactic relationships between unseen words (once the model fine-tunes).", "In OpenAI GPT, the language model and classifier fine-tuning are both done simultaneously, thanks to its parallelized architecture using multi-headed attention. This makes it very easy to run the training loop and hence a number of experiments were possible.", "The below command was used to run the training loop for 3 epochs, which is what was used for all our experiments. See the Jupyter notebook (transformer.ipynb) and the file train_stance.py for more details on the default arguments and the various experiments run.", "The output of the training script is fed to another script parse_output.py, which shapes the output in a way that can be evaluated by the perl script as provided by the task creators.", "The evaluation perl script is run on the output and gold reference file (also provided by the creators) to produce a macro F-score that we can then compare with the benchmark result from MITRE.", "The best macro F-score of 0.69 obtained from GPT across all topics was obtained using the below approach and parameters:", "Overall, fine-tuning the dropout, the weight of the language modelling objective and changing some of the other default arguments (such as random seed) did little to improve the macro F-score. In general, the transformer was very quick to produce good results and required only some basic customization of the task-head input transforms.", "In this section we compare the F-scores obtained by our two approaches: ULMFiT and GPT with the benchmark result by MITRE.", "When we consider the macro F-scores for stance across all five topics at once, OpenAI GPT is shown to produce the best result.", "What is remarkable about this result is that both ULMFiT and OpenAI GPT utilize pre-trained language models trained on a very different distribution (Wikipedia and Google Books respectively) whereas MITRE used embeddings pre-trained on a massive Twitter dataset (similar distribution).", "It is clear that utilizing language modelling as a training signal during pre-training as well as fine-tuning has its advantages. Even though Tweets are far more informal in their syntax and have shorter sequence lengths than the average sentence from a book corpus, the pre-trained models in either case are able to generalize and make predictions based on some syntactic understanding of the target data.", "The below image compares the F-score (FAVOUR) and F-score (AGAINST) from our two approaches with MITRE\u2019s, this time on a per-topic basis. Looking at these results, it is once more clear that OpenAI GPT clearly outperforms ULMFiT on most topics, for either class, which explains the higher overall F-score.", "The transformer does really well on all topics, regardless of whether it is predicting the \u201cFAVOUR\u201d or \u201cAGAINST\u201d class.", "One very interesting observation is that both ULMFiT and MITRE produce zero predictions for \u201cClimate change is a concern (AGAINST)\u201d whereas GPT is able to make some correct predictions for this case. On inspecting the training data further (shown earlier in this post), this is the same class for which we have very few training samples (just 15 of them!). It is remarkable that with such few training examples, the transformer is able to achieve some context and still assign some predictions to this class. Judging by its good performance in the many other widely varying classes, this doesn\u2019t seem like a fluke!", "We count the total predictions made by both models in each class and cross-tabulate it against the labels in the gold reference set (i.e. the \u201cabsolute truth\u201d) provided by the task creators.", "In these tables, the elements along the main diagonal indicate the correctly predicted labels and the off-diagonal elements show where the classifier went astray. It is clear that both methods predict most classes reasonably well, with ULMFiT making more overall correct predictions in the \u201cAGAINST\u201d class, and the transformer making more correct predictions in the \u201cFAVOUR\u201d and \u201cNEITHER\u201d classes.", "We can look at the training data distribution to see if we can draw additional inferences. In the training data, there are far more Tweets belonging to the \u201cAGAINST\u201d class (image below). The transformer seems to be able to achieve a broader overall context across even the minority classes, even when there are very few training samples per topic!", "ULMFiT produced relatively poor overall average F-scores (of less than 0.55) when we fine-tuned the language model on just the original training set (of 2,914 Tweets). Only when we augmented the language model with 200,000 Tweets from the Kaggle Sentiment140 dataset did ULMFiT produce overall F-scores that were comparable to MITRE\u2019s benchmark results. The language model fine-tuning of the bi-LSTMs in ULMFiT does seem to require more data to generalize in this case than the transformer does.", "Our GPT model did not require an augmentation vocabulary during the fine-tuning step. The fact that it is able to achieve good generalization across even the minority classes seems to show that it achieves a better general understanding of Twitter-syntax in the presence of limited training samples. All this seems very interesting and worthy of further study over more diverse datasets.", "It is well known that ULMFiT produces state-of-the-art accuracy on various text classification benchmark datasets such as IMDb and AG News. The common theme across all these benchmark datasets is that they have really long sequence lengths (some of the reviews/news articles are hundreds of words long), so clearly, the language model used by ULMFiT fine-tunes really well for long sequences. Tweets, on the other hand, have a hard-limit of 140 characters, which are rather small sequences compared to full sentences from a movie review or a news article.", "As described in the ULMFiT paper, the classifier uses \u201cconcatenated pooling\u201d to help identify context in long sequences when a document contains hundreds or thousands of words. To avoid loss of information in case of really long sequences, the hidden state at the last time step is concatenated with both the max-pooled and mean-pooled representation of the hidden states, as shown below.", "In this study, no changes were made to the model architecture for either method\u2014 however, it could be that changing the representation of the hidden state using just one of the two (max or mean) representations could help ULMFiT generalize better to smaller sequences such as this case with Tweets.", "GPT in its original form (with the classification task head) seems to have no trouble generalizing to Tweet syntax even though it was pre-trained on a book corpus that also had possibly long sequences. It could be that the transformer\u2019s self-attention mechanisms and high-dimensional self-attention layers are capable of adapting to varying sequence lengths while learning aspects of Tweet syntax better than the LSTMs with concat pooling in the hidden layer. These are profound concepts and could be studied in greater detail.", "In ULMFiT, language model fine-tuning is a necessary step before training the classifier. Including an augmented Tweet vocabulary during the language model fine-tuning step seemed to provide the model a better understanding of Tweet syntax, which seemed to improve its performance. Varying the learning rates, momentum and dropouts did have a minor, but negligible overall impact on the F-score during fine-tuning.", "In OpenAI GPT, the language model is fine-tuned simultaneously with the classifier using a weighting parameter, through the auxiliary language model objective specified as per the below equation.", "When the language model fine-tuning objective was turned off (i.e. lambda=0) in the case of the transformer, the averaged macro F-score became markedly worse (below 0.6). We can thus reason that the pre-trained model\u2019s weights on their own do not sufficiently capture the syntax of Tweets, and that language model fine-tuning does help our model generalize better to Twitter data. Increasing lambda to a very high value (5) or a very low value (0.1) also did not improve the F-scores in our experiments, so there could be something specific to our Twitter data that makes the lambda value of 0.5 an optimum one. It would be interesting to see whether other values of the LM coefficient (lambda) are favourable when applied on a completely different dataset.", "ULMFiT was able to produce good classification performance on each individual topic (for which we had less than 500 samples to train on). In fact, our best result using ULMFiT was obtained (albeit with some data augmentation) by training five distinct classifiers on a per-topic basis. This makes sense because ULMFiT has been shown to produce excellent transfer learning performance on very limited training samples (as few as 100!). Hence, in cases where we have a very small number of labelled training samples, ULMFiT would be a good option to attempt transfer learning, at least for classification tasks.", "The transformer on the other hand, requires at least a few thousand training samples to generalize well and avoid over-fitting. The OpenAI paper shows good results for a range of tasks which have anywhere from 5,000 to 550,000 training samples. The transformer has 786 dimensions in the self-attention heads and 3,072 dimensional inner states in the feed-forward networks; hence when we have fewer than 1,000 training samples, the model with its high dimensionality seems to memorize the training data and massively over-fit the data (with validation accuracy dropping well below 70%).", "Both language models were pre-trained on different text corpora (Wikitext 103 and Google Books). Also, ULMFiT used 3-layer bi-LSTM architecture whereas OpenAI used a transformer network. This Tweet by Yoav Goldberg on the topic provides some food for thought.", "Based on our Tweet stance classification results, it appears that the transformer seems to have a benefit when it comes to learning case-specific syntax with limited context from a relatively small number of training samples \u2014 this could be more to do with the transformer architecture than the pre-trained language model used. However, until a more consistent language model is available on both architectures, it is relatively hard to tell which of these had a bigger impact on the results.", "In this project, we studied the techniques used in two powerful transfer learning approaches (ULMFiT and OpenAI GPT) for a novel task from a different distribution (stance detection of Tweets). We developed a training and classification pipeline for two separate PyTorch-based frameworks and compared the macro F-scores for stance evaluation with the best results for this task from MITRE in 2016.", "Both methods achieved good performance and good generalization with minimal customization of the model, and fine-tuning was achievable in reasonable time (about an hour overall for either method on a single Tesla P100 GPU). Both methods have also been shown to make transfer learning very easy and achievable with a relatively small learning curve and relatively few additional lines of code.", "While we were able to obtain better F-scores with OpenAI GPT across nearly all topics in this case, it does not imply that a transformer-based model is always a better tool for such classification tasks. This could depend very much on the nature of the data and the language models used, as well as the model architecture. There could be many better ways to fine-tune either model for classification, and better hyper-parameter selection (or model customization) could help further improve the results \u2014 it just wasn\u2019t possible to try them all for this project.", "It appears that the task-agnostic and highly parallelized architecture of the transformer in GPT allows it to easily achieve rapid generalization in just 2\u20133 epochs of training for this Tweet stance task; however, GPT is prone to over-fitting, especially when we have fewer than a thousand training samples. ULMFiT is definitely much better at achieving a good performance with very small datasets (of fewer than 500 training samples); however it seems to require augmenting the vocabulary of the training data during the language model fine-tuning stage (considering that this specific Tweet classification task contained data that was very different from the pre-trained language model).", "Overall, this is a very exciting time to be studying deep learning in NLP, and the advent of powerful transfer learning techniques such as these will hopefully open up deep learning applications to a much wider group of practitioners in the near future!", "EDITED on March 24th, 2019: Since the release of GPT-2 by OpenAI in February 2019, it becomes more important to distinguish between various versions of transformer models, so the previous mentions of \u201cOpenAI Transformer\u201d are now updated to \u201cGPT\u201d, which is the transformer model from their 2018 paper.", "This work was done as part of a final course project in which my colleagues Andrew and Abhishek contributed significantly to idea generation as well as data cleaning and experimentation code. They (and myself included) would welcome any feedback/comments.", "Also, if you liked this article, please connect with me on LinkedIn and Twitter!", "All notebooks and code shown in the GitHub repo were run on a machine that had an NVIDIA GPU, and the entire training process took roughly 1 hour (for ULMFiT) and under an hour (for the transformer) on a P100 GPU\u2014 running the same code on a pure CPU-machine can take orders of magnitude longer!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Engineer by training. Machine Learning practitioner. I like writing about science, technology and computing."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F8ab014da8dde&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tech_optimist?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b----8ab014da8dde---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/amueller/word_cloud", "anchor_text": "wordcloud"}, {"url": "http://ruder.io/", "anchor_text": "Sebastian Ruder"}, {"url": "http://ruder.io/nlp-imagenet/", "anchor_text": "NLP\u2019s ImageNet moment has arrived"}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT"}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI GPT,"}, {"url": "https://allennlp.org/elmo", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT"}, {"url": "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/", "anchor_text": "Wikipedia articles"}, {"url": "https://ai.google/research/pubs/pub41880", "anchor_text": "1 billion word benchmark"}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "ULMFiT"}, {"url": "https://blog.openai.com/language-unsupervised/", "anchor_text": "OpenAI\u2019s GPT"}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "this GitHub repo"}, {"url": "https://medium.com/explorations-in-language-and-learning/transfer-learning-in-nlp-2d09c3dfaeb6", "anchor_text": "this article"}, {"url": "http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases", "anchor_text": "word2vec"}, {"url": "https://nlp.stanford.edu/pubs/glove.pdf", "anchor_text": "GloVe"}, {"url": "https://www.sysml.cc/doc/50.pdf", "anchor_text": "powerful pre-trained language models"}, {"url": "http://ruder.io/nlp-imagenet/", "anchor_text": "In his post"}, {"url": "http://ruder.io/semi-supervised/", "anchor_text": "semi-supervised learning"}, {"url": "http://alt.qcri.org/semeval2016/task6/", "anchor_text": "this SemEval 2016 shared task"}, {"url": "https://arxiv.org/pdf/1605.01655.pdf", "anchor_text": "task creators\u2019 paper."}, {"url": "http://www.saifmohammad.com/WebPages/StanceDataset.htm", "anchor_text": "fully interactive visualization provided by the task creators"}, {"url": "http://www.saifmohammad.com/WebPages/StanceDataset.htm", "anchor_text": "SemEval-2016 Stance Dataset"}, {"url": "http://www.aclweb.org/anthology/S16-1003", "anchor_text": "use a macro-averaged F-score"}, {"url": "https://streamhacker.com/2010/05/17/text-classification-sentiment-analysis-precision-recall/", "anchor_text": "here"}, {"url": "http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools", "anchor_text": "perl script"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "in this paper"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "MITRE\u2019s submission to SemEval 2016"}, {"url": "https://arxiv.org/pdf/1301.3781.pdf", "anchor_text": "word2vec skip-gram method"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "MITRE extracted 218,179,858 English Tweets"}, {"url": "https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf", "anchor_text": "word2phrase"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "MITRE\u2019s paper submitted to SemEval 2016"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "mentioned in their paper"}, {"url": "https://github.com/fastai/fastai/tree/master/fastai/text", "anchor_text": "fastai.text"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch v1"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "original paper"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "ULMFiT paper by Jeremy Howard and Sebastian Ruder"}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/datasets.py", "anchor_text": "made available publicly"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "[ulmfit.ipynb]"}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "main GitHub repo"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "full paper"}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "extensive empirical testing and implementation updates"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "worked well in most cases"}, {"url": "https://arxiv.org/abs/1803.09820", "anchor_text": "report by Leslie Smith"}, {"url": "https://arxiv.org/pdf/1706.05157.pdf", "anchor_text": "given in this paper"}, {"url": "https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/", "anchor_text": "Wikitext 103 dataset by Stephen Merity."}, {"url": "https://github.com/fastai/fastai/blob/master/fastai/datasets.py", "anchor_text": "provides an API"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "ulmfit.ipynb"}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "fastai course documentation page"}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "months of development by Jeremy Howard and the"}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "fastai"}, {"url": "https://github.com/cedrickchee/knowledge/blob/master/courses/fast.ai/deep-learning-part-2/2018-edition/lesson-10-transfer-learning-nlp.md", "anchor_text": "team"}, {"url": "https://github.com/fastai/fastai/blob/1c5c007e10a187f4a048a3ee8137016534259768/fastai/text/transform.py#L71", "anchor_text": "[source]"}, {"url": "https://spacy.io/usage/spacy-101#annotations-token", "anchor_text": "spaCy's underlying tokenizer"}, {"url": "https://github.com/fastai/fastai/blob/4cd2e454a735e208c60b2ff7f82bbb4b62e85725/fastai/text/transform.py#L19", "anchor_text": "source code here"}, {"url": "https://github.com/fastai/fastai/blob/050080be574cb1260462bbd03e9600e43e7a54b1/fastai/train.py#L23", "anchor_text": "[source"}, {"url": "https://github.com/fastai/fastai/blob/4cd2e454a735e208c60b2ff7f82bbb4b62e85725/fastai/text/learner.py#L146", "anchor_text": "source"}, {"url": "http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip", "anchor_text": "provided by the task creators"}, {"url": "https://www.kaggle.com/kazanova/sentiment140", "anchor_text": "Twitter sentiment140 dataset from Kaggle"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "notebook"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/ulmfit.ipynb", "anchor_text": "ulmfit.ipynb"}, {"url": "https://arxiv.org/pdf/1606.03784.pdf", "anchor_text": "explained in their paper"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "described in their paper"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "\u201cAttention is All You Need\u201d paper"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "their paper"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer.ipynb", "anchor_text": "[transformer.ipynb]"}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "main GitHub repo"}, {"url": "https://github.com/openai/finetune-transformer-lm/tree/master/model", "anchor_text": "on the OpenAI GitHub repo"}, {"url": "https://github.com/openai/finetune-transformer-lm", "anchor_text": "GitHub source"}, {"url": "https://github.com/huggingface/pytorch-openai-transformer-lm", "anchor_text": "this PyTorch port of OpenAI GPT"}, {"url": "https://github.com/huggingface", "anchor_text": "HuggingFace"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "described in their paper"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "been proven to be a very powerful language modelling tool"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "is explained in this article"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "achieve a much longer-range context than LSTM-based approaches"}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "see the GitHub repo"}, {"url": "https://github.com/huggingface/pytorch-openai-transformer-lm", "anchor_text": "PyTorch port"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer.ipynb", "anchor_text": "transformer.ipynb"}, {"url": "https://github.com/prrao87/tweet-stance-prediction/blob/master/transformer-openai/train_stance.py", "anchor_text": "train_stance.py"}, {"url": "http://alt.qcri.org/semeval2016/task6/data/uploads/eval_semeval16_task6_v2.zip", "anchor_text": "provided by the task creators"}, {"url": "http://alt.qcri.org/semeval2016/task6/index.php?id=data-and-tools", "anchor_text": "provided by the task creators"}, {"url": "https://www.kaggle.com/kazanova/sentiment140", "anchor_text": "Kaggle Sentiment140 dataset"}, {"url": "https://github.com/sebastianruder/NLP-progress/blob/master/english/text_classification.md", "anchor_text": "produces state-of-the-art accuracy on various text classification benchmark datasets such as IMDb and AG News"}, {"url": "https://arxiv.org/pdf/1801.06146.pdf", "anchor_text": "ULMFiT paper"}, {"url": "http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html", "anchor_text": "excellent transfer learning performance on very limited training samples"}, {"url": "https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf", "anchor_text": "anywhere from 5,000 to 550,000 training samples"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT-2"}, {"url": "https://www.linkedin.com/in/andrew-wesson-7079728b/", "anchor_text": "Andrew"}, {"url": "https://www.linkedin.com/in/abhisheksunnak/", "anchor_text": "Abhishek"}, {"url": "https://www.linkedin.com/in/prrao87/", "anchor_text": "LinkedIn"}, {"url": "https://twitter.com/tech_optimist", "anchor_text": "Twitter"}, {"url": "https://github.com/prrao87/tweet-stance-prediction", "anchor_text": "the GitHub repo"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8ab014da8dde---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8ab014da8dde---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----8ab014da8dde---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----8ab014da8dde---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8ab014da8dde---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----8ab014da8dde---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=-----8ab014da8dde---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F8ab014da8dde&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----8ab014da8dde---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8ab014da8dde--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----8ab014da8dde--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----8ab014da8dde--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tech_optimist?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Prashanth Rao"}, {"url": "https://medium.com/@tech_optimist/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "473 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fefdb4f4a0d1b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=post_page-efdb4f4a0d1b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5aedbeec5972&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransfer-learning-in-nlp-for-tweet-stance-classification-8ab014da8dde&newsletterV3=efdb4f4a0d1b&newsletterV3Id=5aedbeec5972&user=Prashanth+Rao&userId=efdb4f4a0d1b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}