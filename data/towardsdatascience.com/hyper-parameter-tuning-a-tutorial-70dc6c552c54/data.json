{"url": "https://towardsdatascience.com/hyper-parameter-tuning-a-tutorial-70dc6c552c54", "time": 1683012879.087002, "path": "towardsdatascience.com/hyper-parameter-tuning-a-tutorial-70dc6c552c54/", "webpage": {"metadata": {"title": "Hyper Parameter Tuning \u2014 A Tutorial | by Noam (\u516b\u4e91) Rosenberg | Towards Data Science", "h1": "Hyper Parameter Tuning \u2014 A Tutorial", "description": "The idea behind Grid Search is very intuitive. Incrementally move one hyper parameter while keeping the others fixed, and record the results. Basically do that for all the hyper parameters you\u2026"}, "outgoing_paragraph_urls": [{"url": "https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "this", "paragraph_index": 2}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "TPE", "paragraph_index": 4}, {"url": "https://github.com/dataloop-ai/ZazuML", "anchor_text": "Github", "paragraph_index": 10}], "all_paragraphs": ["In this tutorial we\u2019re going to cover 5 approaches for hyper-parameter optimization:", "The idea behind Grid Search is very intuitive. Incrementally move one hyper parameter while keeping the others fixed, and record the results. Basically do that for all the hyper parameters you suspect could be optimized. This is a very expensive and cumbersome way of optimizing and tuning your hyper parameters, and today is considered one of the less efficient ways of conducting hyper parameter search. Let\u2019s see how we can do better.", "Though you might not think it at first, randomly choosing the values of all your hyper parameters is actually a much more efficient approach to hyper-parameter tuning. In this scenario instead of keeping all hyper parameters constant and adjusting one in an iterative fashion, we instead initialize all the hyper parameter values randomly at every trial. This is better because as it turns out, some of the hyper-parameters will be more important for optimization than others, and if we can\u2019t tell the important hyper parameters from the unimportant ones, the next best thing we can do is randomly choose all the hyper parameter values at each trial. That will allow for a higher sampling rate for the important hyper parameters, and thus our optimization will be more efficient. The benefits of Random Search vs. Grid Search are explored in this paper.", "Actually the high level concept is pretty simple, we\u2019re trying to do the same thing with Bayesian optimization that we\u2019re always trying to do in ML, that\u2019s estimate functions, functions that are too complex to be formulated. But now the function we\u2019re trying to approximate is our ML algorithm. In this instance we may be using Deep Learning or some other heavy form of ML, and we can only run a limited number of trials to test out different combinations of hyper parameters. If we could intelligently approximate the results of our ML algorithm before picking out our next hyper-parameter configuration, we could potentially save lot\u2019s of time and money.", "There are several Bayesian optimization functions, but they key idea is to use a bayesian approach to estimating a better configuration of hyper parameters given a previous set of configurations and their result. In the above image we use an algorithm called TPE, the basic concept is to separate our trials into two groups according to their performance, i.e. the group that gets better results and the group that gets worse results. Then we pick the next set of hyper parameters based on its probability of belonging to the good distribution as apposed to the bad distribution.", "In Successive Halving we start training a large number of trials for a small number of epochs, the hyper parameter configuration is random. We then throw out the worst performing trials and only continue training the best performing trials, we do this until a single hyper parameter configuration remains.", "Hyperband is and extension of the Successive Halving algorithm. The problem with Successive Halving is that often we can\u2019t know the right trade-off for number of trials vs. number of epochs. In certain cases some hyper parameter configurations may take longer to converge, so starting off with a lot of trials but a small number of epochs won\u2019t be ideal, in other cases the convergence is quick and the number of trials is the bottleneck.", "That\u2019s where Hyperband comes in. Hyperband is essentially just a grid search over the optimal allocation strategy. So at each individual trial the set of hyper parameters is chosen randomly.", "In the image you can see the hyperband algorithm will run successive halving over 5 resource allocations. s=4 begins it\u2019s first round with 81 trials, offering each one a single epoch and then iteratively throws out 2/3rds of the trials until one is left and is trained for 81 epochs. At s=0 Hyperband algorithm is basically running a random search with 5 trials, each one trained on the max number of epochs.", "ZazuML is an open-source AutoML project me and some friends have been working on. It mixes several search algorithms including the afore mentioned Hyperband and Random Search.", "Please check it out our Github!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F70dc6c552c54&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@noamsrosenberg?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@noamsrosenberg?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Noam (\u516b\u4e91) Rosenberg"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F537eb1c19eaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&user=Noam+%28%E5%85%AB%E4%BA%91%29+Rosenberg&userId=537eb1c19eaa&source=post_page-537eb1c19eaa----70dc6c552c54---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.groundai.com/project/grid-search-random-search-genetic-algorithm-a-big-comparison-for-nas/1", "anchor_text": "Genetic Algorithm paper"}, {"url": "https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "Random Search paper"}, {"url": "https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "this"}, {"url": "https://github.com/fmfn/BayesianOptimization", "anchor_text": "Bayesian Optimization repo"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "TPE paper"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf", "anchor_text": "TPE"}, {"url": "https://arxiv.org/pdf/1603.06560.pdf", "anchor_text": "Hyperband paper"}, {"url": "https://github.com/dataloop-ai/ZazuML", "anchor_text": "Github"}, {"url": "https://medium.com/tag/hyperparameter-tuning?source=post_page-----70dc6c552c54---------------hyperparameter_tuning-----------------", "anchor_text": "Hyperparameter Tuning"}, {"url": "https://medium.com/tag/hyper-parameter-tuning?source=post_page-----70dc6c552c54---------------hyper_parameter_tuning-----------------", "anchor_text": "Hyper Parameter Tuning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----70dc6c552c54---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----70dc6c552c54---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/hyperband?source=post_page-----70dc6c552c54---------------hyperband-----------------", "anchor_text": "Hyperband"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&user=Noam+%28%E5%85%AB%E4%BA%91%29+Rosenberg&userId=537eb1c19eaa&source=-----70dc6c552c54---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&user=Noam+%28%E5%85%AB%E4%BA%91%29+Rosenberg&userId=537eb1c19eaa&source=-----70dc6c552c54---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F70dc6c552c54&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----70dc6c552c54---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----70dc6c552c54--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----70dc6c552c54--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----70dc6c552c54--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@noamsrosenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@noamsrosenberg?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Noam (\u516b\u4e91) Rosenberg"}, {"url": "https://medium.com/@noamsrosenberg/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F537eb1c19eaa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&user=Noam+%28%E5%85%AB%E4%BA%91%29+Rosenberg&userId=537eb1c19eaa&source=post_page-537eb1c19eaa--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F537eb1c19eaa%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhyper-parameter-tuning-a-tutorial-70dc6c552c54&user=Noam+%28%E5%85%AB%E4%BA%91%29+Rosenberg&userId=537eb1c19eaa&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}