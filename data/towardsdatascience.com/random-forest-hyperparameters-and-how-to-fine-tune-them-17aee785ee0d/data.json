{"url": "https://towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d", "time": 1683015205.659252, "path": "towardsdatascience.com/random-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d/", "webpage": {"metadata": {"title": "Random Forest: Hyperparameters and how to fine-tune them | by James Thorn | Towards Data Science", "h1": "Random Forest: Hyperparameters and how to fine-tune them", "description": "Random Forest are an awesome kind of Machine Learning models. They solve many of the problems of individual Decision trees, and are always a candidate to be the most accurate one of the models tried\u2026"}, "outgoing_paragraph_urls": [{"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "RandomForestClassifier", "paragraph_index": 15}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "RandomForestClassifier of Scikit-learn", "paragraph_index": 23}, {"url": "https://howtolearnmachinelearning.com/books/machine-learning-books/", "anchor_text": "How to Learn Machine Learning", "paragraph_index": 33}, {"url": "https://aigents.co/", "anchor_text": "AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers", "paragraph_index": 33}, {"url": "https://z-ai.medium.com/subscribe", "anchor_text": "SUBSCRIBE TO EMAIL LIST", "paragraph_index": 34}], "all_paragraphs": ["Random Forest are an awesome kind of Machine Learning models. They solve many of the problems of individual Decision trees, and are always a candidate to be the most accurate one of the models tried when building a certain application.", "If you don\u2019t know what Decision Trees or Random Forest are do not have an ounce of worry; I got you covered with the following articles. Take a quick look and come back here.", "In this quick article, we will explore some of the nitty-gritty optimisations of Random Forests, along with what each hyper-parameter is, and which ones are worth optimising.", "The most important hyper-parameters of a Random Forest that can be tuned are:", "Alright, now that we know where we should look to optimise and tune our Random Forest, lets see what touching some of these parameters does.", "By building forests with a large number of trees (high number of estimators) we can create a more robust aggregate model with less variance, at the cost of a greater training time. Most times the secret here is to evaluate your data: how much data is available, and how many features does each observation have.", "Because of the randomness of Random Forest, if you have a lot of features and a small number of trees some features with high predictive power could get left out of the forest and not be used whatsoever, or be used very little.", "The same applies for the data: if you have a lot of observations and you are not using the whole dataset to train each tree, if you have a small number of trees, then some observations could be left out.", "As Random Forests rarely overfit, in practice you can use a large number of trees to avoid these problems, and get good results following the guideline that when all other hyper-parameters are fixed, increasing the number of trees generally reduces model error at the cost of a higher training time.", "Don\u2019t be fooled by this statement though, building a forest with 10K trees is a crazy and useless approach: the main takeaway is that as you increase the n\u00ba of trees you will be reducing model variance and generally model error would approximate an optimum value.", "Conclusion: fine tuning the number of trees is unnecessary, simply set the number of trees to a large, computationally feasable number and you\u2019re good to go.", "Decision Trees make locally optimal decisions at each node by computing which feature and which value of that feature best splits the observations up to that point.", "To do this, they use an specific metric (Gini or Entropy for classification) and (MAE or MSE for Regression). For Regression, the general rule is to take MSE if you don\u2019t have many outliers in your data, as it penalises highly those observations that are far away from the mean.", "For classification, the thing is a bit more tricky. We have to calculate a measure of impurity with either Gini or Entropy, which can result in a different split sometimes. Take the following examples of a problem where we have two classes, A and B:", "Depending on which of the two we use our model can change. There is not a real rule of thumb here to know which one to pick. Different decision tree algorithms use different metrics (CART uses Gini, whereas ID3 uses Entropy).", "Having said this, Gini is usually less computationally expensive to calculate as it does not compute a logarithm. The Scikit-Learn implementation of RandomForestClassifier allows us to choose from both, so it might be worth trying both measures and seeing which leads to an smaller error.", "Conclusion: fine tuning the split criteria could lead to different forests, and as there is only two possible values, we recommend trying them both for classification forests.", "Increasing the Depth of individual trees increases the possible number of feature/value combinations that are taken into account. The deeper the tree, the more splits it has and the more information about the data it takes into account.", "In an individual tree this causes overfitting, however in Random Forest, because of the way the ensemble is built, it is harder to overfit, although it is still possible for large depth values.", "This parameter should be set to a reasonable amount depending on the number of features of your tree: don\u2019t build stumps (really shallow trees) nor insanely big trees; set this parameter to a reasonable amount and tune it a little bit if you want, but changes around a reasonable value do not impact the performance of your forest greatly, so you don\u2019t have to include it in a procedure like Grid Search if you don\u2019t want.", "Conclusion: fine tuning the tree depth is unnecessary, pick a reasonable value and carry on with other hyperparameters.", "This is one of the most important hyperparameters to tune in your Random Forest ensemble, so play close attention.", "The best value of this hyperparameter is hard to pick without experimentation, so the best way to obtain it is using a Grid Search with Cross Validation, taking into account the following:", "The most practical approach here is to cross-validate your posible options and keep the model that yields the best results, taking into account the previous considerations. You can try setting the the following values in the grid search space for the RandomForestClassifier of Scikit-learn.", "Conclusion: fine tuning the number of features to consider when splitting at each node is fundamental, therefore it should be considered when using a search approach to find the best hyperparameters for our forest.", "Lastly, we will discuss the importance of the size of the boostrapped dataset. This is what percentage of the training data should be used to train each individual tree.", "Because the observations are sampled with replacement, even if the size of the bootstrapped dataset is the same as the whole training set, both datasets will be different, so many times this parameter is left untouched and each tree is trained with a random set of observations with the same size of the initial training data.", "In Sklearn this is controlled with the max_samples hyperparameter, which by default takes the size of the initial data set.", "In expectation, drawing N samples with replacement from a dataset of size N will select ~2/3 unique samples from the original set, leaving 1/3 behind (what is called the out of bag or OOB data, which can then be used to evaluate the forest).", "Because of these considerations, it doesn\u2019t hurt to use the full size of the training data, so most times the best thing to do is to not touch this hyperparameter.", "In this post we have seen what the most important Hyper parameters of Random Forest are, how to set their values, and which of them are worth fine-tuning.", "Like any ML problem, this is all dependent on your data, resources, and goal, so if you have time, do a sparse grid search first around the recommended values for each hyper-parameter and then a second, more specific search close to the optimal values found in the previous step.", "The best parameter values should always be cross-validated if there is time for it, and at least a couple of combinations should be tried. For further information take a look at the following resources:", "For further resources on Machine Learning and Data Science check out the following repository: How to Learn Machine Learning! For career resources (jobs, events, skill tests) go to AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers.", "Also, you can subscribe to my email list to get the latest update and exclusive content here: SUBSCRIBE TO EMAIL LIST.", "Thank you very much for reading, and have a great day!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F17aee785ee0d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://james-thorn.medium.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "James Thorn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14----17aee785ee0d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/", "anchor_text": "Source"}, {"url": "https://towardsdatascience.com/decision-trees-explained-3ec41632ceb6", "anchor_text": "Decision Trees ExplainedLearn everything about Decision Trees for Machine Learningtowardsdatascience.com"}, {"url": "https://towardsdatascience.com/random-forest-explained-7eae084f3ebe", "anchor_text": "Random Forest ExplainedRandom Forest explained simply: An easy Introduction to training, Classification, and Regressiontowardsdatascience.com"}, {"url": "https://z-ai.medium.com/subscribe", "anchor_text": "Subscribe to my exclusive list!Subscribe to my exclusive list! And get all the fresh articles you love <3! By signing up, you will create a Medium\u2026z-ai.medium.com"}, {"url": "https://towardsdatascience.com/explainable-artificial-intelligence-14944563cc79", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/1705.05654.pdf", "anchor_text": "Source"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "RandomForestClassifier"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html", "anchor_text": "RandomForestClassifier of Scikit-learn"}, {"url": "https://z-ai.medium.com/subscribe", "anchor_text": "Subscribe to my exclusive list!Subscribe to my exclusive list! And get all the fresh articles you love <3! By signing up, you will create a Medium\u2026z-ai.medium.com"}, {"url": "https://scikit-learn.org/stable/modules/ensemble.html#parameters", "anchor_text": "Scikit-Learn adviced on hyperparameter tuning for Random Forest."}, {"url": "https://www.kaggle.com/general/4092", "anchor_text": "Awesome Kaggle discussion around this issue."}, {"url": "https://howtolearnmachinelearning.com/books/machine-learning-books/", "anchor_text": "How to Learn Machine Learning"}, {"url": "https://aigents.co/", "anchor_text": "AIgents.co \u2014 A career community for Data Scientists & Machine Learning Engineers"}, {"url": "https://z-ai.medium.com/subscribe", "anchor_text": "SUBSCRIBE TO EMAIL LIST"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----17aee785ee0d---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----17aee785ee0d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----17aee785ee0d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----17aee785ee0d---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/startup?source=post_page-----17aee785ee0d---------------startup-----------------", "anchor_text": "Startup"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&user=James+Thorn&userId=1fd70d25ff14&source=-----17aee785ee0d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&user=James+Thorn&userId=1fd70d25ff14&source=-----17aee785ee0d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F17aee785ee0d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----17aee785ee0d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----17aee785ee0d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----17aee785ee0d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----17aee785ee0d--------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://james-thorn.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Thorn"}, {"url": "https://james-thorn.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "3K Followers"}, {"url": "https://howtolearnmachinelearning.com/", "anchor_text": "https://howtolearnmachinelearning.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1fd70d25ff14&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&user=James+Thorn&userId=1fd70d25ff14&source=post_page-1fd70d25ff14--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ad84c2cef18&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Frandom-forest-hyperparameters-and-how-to-fine-tune-them-17aee785ee0d&newsletterV3=1fd70d25ff14&newsletterV3Id=5ad84c2cef18&user=James+Thorn&userId=1fd70d25ff14&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}