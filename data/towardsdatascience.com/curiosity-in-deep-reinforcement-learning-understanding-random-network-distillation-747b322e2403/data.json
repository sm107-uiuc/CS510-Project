{"url": "https://towardsdatascience.com/curiosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403", "time": 1682994056.311127, "path": "towardsdatascience.com/curiosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403/", "webpage": {"metadata": {"title": "Curiosity in Deep Reinforcement Learning | by Michael Klear | Towards Data Science", "h1": "Curiosity in Deep Reinforcement Learning", "description": "Learning to play Atari games is a popular benchmark task for deep reinforcement learning (RL) algorithms. These games offer a nice balance between simplicity and complexity: some games (e.g., Pong)\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "Pong", "paragraph_index": 0}, {"url": "http://karpathy.github.io/2016/05/31/rl/", "anchor_text": "vanilla policy gradient", "paragraph_index": 0}, {"url": "https://www.washingtonpost.com/news/innovations/wp/2015/02/25/5-classic-atari-games-that-totally-stump-googles-artificial-intelligence-algorithm/?utm_term=.b7b62f177cce", "anchor_text": "stump even the most advanced algorithms", "paragraph_index": 0}, {"url": "https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/", "anchor_text": "central to many deep RL research papers", "paragraph_index": 1}, {"url": "https://www.wired.com/2011/06/skyrim-e3-hands-on/", "anchor_text": "Skyrim", "paragraph_index": 3}, {"url": "https://openreview.net/pdf?id=rJNwDjAqYX", "anchor_text": "Curiosity-based exploration", "paragraph_index": 7}, {"url": "https://arxiv.org/pdf/1705.05363.pdf", "anchor_text": "one approach", "paragraph_index": 9}, {"url": "https://arxiv.org/pdf/1705.05363.pdf", "anchor_text": "encouraging results in a Super Mario emulator", "paragraph_index": 12}, {"url": "https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html", "anchor_text": "been referred to", "paragraph_index": 13}, {"url": "https://arxiv.org/pdf/1810.12894.pdf", "anchor_text": "Exploration by Random Network Distillation", "paragraph_index": 17}, {"url": "https://openai.com/", "anchor_text": "OpenAI", "paragraph_index": 17}], "all_paragraphs": ["Learning to play Atari games is a popular benchmark task for deep reinforcement learning (RL) algorithms. These games offer a nice balance between simplicity and complexity: some games (e.g., Pong) are simple enough to solve with basic algorithms like vanilla policy gradient, while other games are complex enough to stump even the most advanced algorithms.", "In between the simplest and most complex games is a useful spectrum of tasks that has become central to many deep RL research papers.", "A previously \u201cunsolved\u201d Atari game, Montezuma\u2019s Revenge, was recently solved (sort of) by an algorithm that is able to surpass human performance in terms of points scored. Researchers were able to encourage the agent to explore different rooms in the first level, which is a great way to earn points in this particular game.", "When a human plays adventure games (like Montezuma\u2019s Revenge), there is an intrinsic desire to explore. Game designers build such games to encourage this behavior, often requiring gamers to explore in order to make progress. This is arguably what makes adventure games fun (ask anyone who\u2019s ever enjoyed Skyrim.)", "General deep RL algorithms perform \u201cexploration,\u201d typically, through a stochastic policy: actions are randomly sampled from an action likelihood distribution provided by a neural network. The result of this, especially early on (when the policy hasn\u2019t had time to converge), is an apparent random selection of actions.", "This works in some cases. Pong, for example, can be solved by moving the paddle around randomly and observing outcomes. A few lucky moments where the ball is deflected can get optimization started.", "In a game like Montezuma\u2019s Revenge, this behavior gets us nowhere; imagine the avatar randomly moving left, right, and jumping starting from the beginning of the game. He\u2019ll just end up falling into lava or walking right into an enemy without ever earning points. Without points, or a reward, the algorithm has no signal to optimize on.", "There has been a lot of emphasis on finding better ways to explore. Curiosity-based exploration can be seen as an attempt to emulate the curiosity-driven behavior of a human gamer.", "But how do we create a curious agent?", "There are a number of approaches that can accomplish this, but one approach, which uses next-state prediction, is particularly interesting because of its simplicity and scalability.", "The basic idea is to train an independent predictive model alongside a policy model. This predictive model takes an observation from the current state and the selected action as an input and produces a prediction for the next observation. For well-explored trajectories, we assume this loss will be small (as we are continually training the predictive model via supervised learning). In less-explored trajectories, we assume this loss will be large.", "What we can do, then, is create a new reward function (called \u201cintrinsic reward\u201d) that provides rewards proportional to the loss of the predictive model. Thus, the agent receives a strong reward signal when exploring new trajectories.", "This technique lead to some encouraging results in a Super Mario emulator.", "This technique is not perfect. There\u2019s a known problem: agents are attracted to stochastic or noisy elements in their environment. This is sometimes called the \u201cwhite noise\u201d problem or the \u201cTV problem\u201d; it\u2019s also been referred to as \u201cprocrastination.\u201d", "To demonstrate this effect, imagine an agent who learns to navigate through a maze by observing the pixels that he sees.", "The agent ends up doing well; he\u2019s driven to look for unexplored parts of the maze because of his ability to make good predictions in well-explored areas (or, rather, his inability to make good predictions in unexplored areas.)", "Now put a \u201cTV\u201d on the wall of the maze displaying randomly-selected images in rapid succession. Due to the random source of images, the agent cannot accurately predict what image will come next. The predictive model will produce high loss, which gives high \u201cintrinsic\u201d rewards to the agent. The end result is an agent who prefers to stop and watch TV rather than continue exploring the maze.", "A solution to the noisy TV problem is proposed in Exploration by Random Network Distillation (RND), a very recent paper published by some of the good folks at OpenAI.", "The novel idea here is to apply a similar technique to the next-state prediction method described above, but to remove the dependence on the previous state.", "Rather than predicting the next state, RND takes observations from the next state and tries to make a prediction on the next state. That\u2019s a pretty trivial prediction, right?", "The purpose of the random network part of RND is to take this trivial prediction task and translate it into a hard prediction task.", "This is a clever, albeit counter-intuitive, solution.", "The idea is that we use a randomly-initialized neural network to map the observations to a latent observation vector. The output of this function itself is actually unimportant; what\u2019s important is that we have some unknown, deterministic function (a randomly-initialized neural network) that transforms observations in some way.", "The task of our predictive model, then, is not to predict the next state, but to predict the output of the unknown random model given an observed state. We can train this model using the outputs of the random network as labels.", "When the agent is in a familiar state, the predictive model should make good predictions of the expected output from the random network. When the agent is in an unfamiliar state, the predictive model will make poor predictions about the random network output.", "In this way, we can define an intrinsic reward function that is again proportional to the loss of the predictive model.", "This can be interpreted as a \u201cnovelty detection\u201d scheme, where the computed loss is higher when the observation is new or unfamiliar to the predictive model.", "The authors use MNIST as a proof-of-concept for this idea. In this experiment, they feed MNIST characters from one class through a randomly-initialized neural network. They then train a parallel network to predict the output of the random network given its input. As expected, they see the loss from the parallel network on the target class drop as the number of training examples from the target class increases.", "In this way, when the agent sees a source of random noise, it doesn\u2019t get stuck. It no longer tries to predict the unpredictable next-frame on the screen, but instead just needs to learn how these frames get transformed by the random network.", "Previous next-state prediction curiosity mechanisms failed to solve Montezuma\u2019s revenge because of bad solutions, but RND seems to have overcome these issues.", "Agents driven by curiosity explore rooms, and learn to collect keys which allow them to unlock new rooms.", "Despite this success, the agent only \u201coccasionally\u201d passes the first level. This is because careful management of key usage is required to get through the final door to complete the level. Internal-state models, such as LSTM, will be required to overcome this barrier.", "So, while RND has gotten agents to surpass average human performance in terms of points scored, there is still a ways to go before mastering the game.", "This is a part of a series of posts about experimental deep reinforcement learning algorithms. Check out some previous posts in the series:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Machine learning practitioner and data scientist with a background in astrophysics."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F747b322e2403&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----747b322e2403--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@michael.r.klear?source=post_page-----747b322e2403--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michael.r.klear?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Michael Klear"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F948e1e07d727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&user=Michael+Klear&userId=948e1e07d727&source=post_page-948e1e07d727----747b322e2403---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://atariage.com/forums/blog/729/entry-14810-montezumas-revenge-parker-brothers/", "anchor_text": "Parker Brothers Blog"}, {"url": "https://gym.openai.com/envs/Pong-v0/", "anchor_text": "Pong"}, {"url": "http://karpathy.github.io/2016/05/31/rl/", "anchor_text": "vanilla policy gradient"}, {"url": "https://www.washingtonpost.com/news/innovations/wp/2015/02/25/5-classic-atari-games-that-totally-stump-googles-artificial-intelligence-algorithm/?utm_term=.b7b62f177cce", "anchor_text": "stump even the most advanced algorithms"}, {"url": "https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/", "anchor_text": "central to many deep RL research papers"}, {"url": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/", "anchor_text": "OpenAI Blog"}, {"url": "https://www.wired.com/2011/06/skyrim-e3-hands-on/", "anchor_text": "Skyrim"}, {"url": "https://kotaku.com/5858870/before-you-start-tips-for-playing-skyrim-the-best-way", "anchor_text": "Skyrim"}, {"url": "http://www.atariprotos.com/8bit/software/monty/monty.htm", "anchor_text": "source"}, {"url": "https://openreview.net/pdf?id=rJNwDjAqYX", "anchor_text": "Curiosity-based exploration"}, {"url": "https://arxiv.org/pdf/1705.05363.pdf", "anchor_text": "one approach"}, {"url": "https://arxiv.org/pdf/1705.05363.pdf", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1705.05363.pdf", "anchor_text": "encouraging results in a Super Mario emulator"}, {"url": "https://ai.googleblog.com/2018/10/curiosity-and-procrastination-in.html", "anchor_text": "been referred to"}, {"url": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/", "anchor_text": "source"}, {"url": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1810.12894.pdf", "anchor_text": "Exploration by Random Network Distillation"}, {"url": "https://openai.com/", "anchor_text": "OpenAI"}, {"url": "https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/", "anchor_text": "source"}, {"url": "https://arxiv.org/pdf/1810.12894.pdf", "anchor_text": "research paper"}, {"url": "https://medium.com/@michael.r.klear/understanding-evolved-policy-gradients-cbc2d6b974a1", "anchor_text": "Understanding Evolved Policy Gradients"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----747b322e2403---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----747b322e2403---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----747b322e2403---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----747b322e2403---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/research?source=post_page-----747b322e2403---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&user=Michael+Klear&userId=948e1e07d727&source=-----747b322e2403---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&user=Michael+Klear&userId=948e1e07d727&source=-----747b322e2403---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----747b322e2403--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F747b322e2403&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----747b322e2403---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----747b322e2403--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----747b322e2403--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----747b322e2403--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----747b322e2403--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----747b322e2403--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michael.r.klear?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@michael.r.klear?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Michael Klear"}, {"url": "https://medium.com/@michael.r.klear/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "237 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F948e1e07d727&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&user=Michael+Klear&userId=948e1e07d727&source=post_page-948e1e07d727--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fcd41e2727b94&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcuriosity-in-deep-reinforcement-learning-understanding-random-network-distillation-747b322e2403&newsletterV3=948e1e07d727&newsletterV3Id=cd41e2727b94&user=Michael+Klear&userId=948e1e07d727&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}