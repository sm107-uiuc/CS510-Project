{"url": "https://towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca", "time": 1683003813.7172532, "path": "towardsdatascience.com/ensemble-learning-from-scratch-20672123e6ca/", "webpage": {"metadata": {"title": "Ensemble Learning from Scratch. Introducing Ensemble Learning, a\u2026 | by jiawei hu | Towards Data Science", "h1": "Ensemble Learning from Scratch", "description": "For many beginners in Data Science and applied Machine Learning, finding the best model to fit the data at hand often implies experimenting with as many reasonable options as possible, and keep the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://machinelearningmastery.com/super-learner-ensemble-in-python/", "anchor_text": "this article", "paragraph_index": 27}], "all_paragraphs": ["For many beginners in Data Science and applied Machine Learning, finding the best model to fit the data at hand often implies experimenting with as many reasonable options as possible, and keep the best one while the rest is thrown away. Ensemble Learning provides an alternative by forming an ensemble with all trained models and its performance can be just as good as the best one, if not better!", "A legit representative for the power of ensemble learning is Random Forest and in this article, we will uncover some of the methods that can be used to build an ensemble model and explain intuitively the principles behind it.", "The structure of this article follows Chapter 7 - Ensemble Learning and Random Forest, from Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow, 2nd Edition, by Aur\u00e9lien G\u00e9ron (O\u2019Reilly).", "When I first came across the law of large numbers, this ancient saying in Chinese I was taught in school popped out of my mind. It translates the idea known as the wisdom of the crowd \u2014 the combined wisdom of a big crowd can be better than that of an expert.", "Ensemble learning applies this theorem by combining independent models, known as predictors, to form a strong learner(achieving high accuracy). It even works when all the predictors are weak learners(perform only slightly better than random guessing).", "But why is it like that? Wisdom of the crowd may sound \u2018smart\u2019, but it\u2019s still somewhat counterintuitive. To answer that, the following analogy may help shed some light on the mystery: Say that you tossing a slightly biased coin that has a 51% chance of coming up heads. If you do it 100 times, it is not very likely that there will be exactly 51 heads, and it\u2019s even possible that you end up not having a majority of heads. But as you keep tossing, you\u2019ll observe that the ratio of heads will get closer and closer to 51%. The more trials you perform, the closer the average value will evolve towards the expected value, and the bigger the probability of obtaining a majority of heads will become.", "Ensemble learning is doing the same thing in essence \u2014 a weak learner has only a slight chance of predicting the right class in a classification task, but if we make the same prediction using many independent predictors, the aggregated result will be much more likely to be predicting the correct class.", "The output of an ensemble model is the most frequent prediction when it\u2019s about a classification task. For a regression task, the average is used instead.", "The key element when training an ensemble is that all its classifiers must be perfectly independent, which can be achieved by either using different algorithms or use the same one but trained on different subsets of the training set. Here, we explore both these methods to build ensemble classifiers.", "Training different predictor algorithms to form an ensemble is known as a Voting Classifier and its mechanisms are straightforward \u2014 a set of different algorithms are trained on the same training set and when making predictions, the output of the ensemble is aggregated, taking the most voted class as the ensemble\u2019s prediction.", "It is called Hard Voting when the aggregation is done through each predictor\u2019s output class, and it\u2019s called Soft Voting when it uses output probabilities. Soft Voting generally performs better since it captures more nuances(logically it requires all the predictors to be able to output probabilities).", "On the other hand, instead of training different algorithms to achieve diversity, one alternative is to use the same type of predictors but trained on different randomly sampled subsets of the training set.", "Bagging and Pasting are two different ways of sampling these subsets, they differ in sampling with or without replacement.", "When sampling via bagging(with replacement), an instance is randomly chosen and immediately given back to the training set, so at each selection the source data still has all of its instances and the same instance can be selected again. Pasting is the other way around, the same instance cannot appear more than once in the same subset.", "For both bagging and pasting, the same instance can appear in different subsets, but only bagging allows the same instance to appear in the same subset twice or more. Since all the subsets have the same size, sampling with replacement can achieve more subset diversity and their corresponding predictors will be less correlated. Another advantage of this method is that each predictor has instances that are not in their training subset, and they can be used directly for validation without having to save data for validation.", "Random Forest is a famous member of the ensemble learner family, and it consists of a set of Decision Trees trained on different subsets sampled through bagging(with the same size as the training set). When splitting a given node, the feature used is selected as the most suitable one from a randomly generated features subspace, and not from the space of all available features. Extra Trees, a variety of Random Forest, take a step further \u2014 not only the features are from a randomly chosen subspace, but the thresholds used are also generated randomly.", "Boosting uses the same predictor for an ensemble as before, but instead of sampling different subsets and training predictors in parallel, it\u2019s done sequentially with each predictor trying to correct the mistakes of its predecessor.", "Let\u2019s take a look at AdaBoost and Gradient Boosting.", "When training is done for a given predictor in the ensemble, the algorithm reweightes the instance weights in a way that the misclassified ones can get more \u2018attention\u2019 from the next predictor and hopefully, it\u2019ll be correctly classified. Instance weights define the chance instances are being selected to be part of the subset \u2014 the higher the instance weights of a given instance, the more likely it\u2019ll appear in the subset when sampling.", "During inference time, the prediction of the ensemble is the weighted average of its outputs. The weights used are the predictor weights from step 2. Whether a predictor is performing better, worse, or exactly as random guessing determines its type of contribution to the ensemble\u2019s prediction:", "Gradient Boosting handles training iteratively as AdaBoost, but instead of tweaking instance weights, each predictor corrects its predecessor by fitting to their residual errors.", "But why fitting the residual error? Suppose we are training an ensemble model with N predictors. For every predictor Pn, 1<n<N, we assume it\u2019s imperfect and its performance can be improved by adding an estimator h:", "Now, to find h we start by looking at how a perfect h would be:", "A perfect estimator h will be filling the gaps between target y and the current prediction Pn(x). In essence, it\u2019s trying to minimize a cost function by plugging in an additional predictor.", "The final piece to complete the puzzle is Staking. Instead of using a given function to handle the predictors\u2019 outputs, would it increase accuracy if we train a model to do it for us? The answer is yes. These models are often referred to as a Blenders, and they can find ways to correct and combine the ensemble\u2019s output to achieve enhanced performances.", "The common approach to train a blender goes by first splitting the training data into a training set and a hold-out set. The first is used to train predictors, employing any of the above-mentioned methods. The hold-out set is then used to train the blender. When the predictors are trained, we take their predictions on the hold-out set and use them as the blender\u2019s input. If there are N predictors in the ensemble, then the blender would be trained on an N-dimensional data, plus the original target.", "When done training, the blender is expected to take the ensemble\u2019s output, and \u2018blend\u2019 them in a way that maximizes the accuracy of the overall model.", "You\u2019ve made it to the end! I hope you had a good time reading this article and hopefully have learned one thing or two about Ensemble Learning. It\u2019s a powerful tool and writing about it helped me solidify many of my understandings on the topic. It\u2019s barely scratching the surface of the underlying theories of Ensemble Learning and no actual implementations were presented. I strongly recommend going through this article from Machine Learning Mastery, it walks you through how to build a SuperLearner Ensemble(Stacking) from scratch!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Software Engineer @ Qbeast. I write to clarify and remember."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F20672123e6ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----20672123e6ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@jiaweihu08?source=post_page-----20672123e6ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "jiawei hu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc71c1fba8a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&user=jiawei+hu&userId=c71c1fba8a3&source=post_page-c71c1fba8a3----20672123e6ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.technologyreview.com/s/615102/tree-planting-is-a-great-idea-that-could-become-a-dangerous-climate-distraction/", "anchor_text": "source"}, {"url": "https://i.pinimg.com/originals/ed/d8/87/edd8873400deea9625ee67b37377d077.jpg", "anchor_text": "source"}, {"url": "https://machinelearningmastery.com/super-learner-ensemble-in-python/", "anchor_text": "this article"}, {"url": "https://medium.com/tag/ensemble-learning?source=post_page-----20672123e6ca---------------ensemble_learning-----------------", "anchor_text": "Ensemble Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----20672123e6ca---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/random-forest?source=post_page-----20672123e6ca---------------random_forest-----------------", "anchor_text": "Random Forest"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----20672123e6ca---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&user=jiawei+hu&userId=c71c1fba8a3&source=-----20672123e6ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&user=jiawei+hu&userId=c71c1fba8a3&source=-----20672123e6ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F20672123e6ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----20672123e6ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----20672123e6ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----20672123e6ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----20672123e6ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----20672123e6ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----20672123e6ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jiaweihu08?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "jiawei hu"}, {"url": "https://medium.com/@jiaweihu08/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "59 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc71c1fba8a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&user=jiawei+hu&userId=c71c1fba8a3&source=post_page-c71c1fba8a3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5b5f5226e02b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fensemble-learning-from-scratch-20672123e6ca&newsletterV3=c71c1fba8a3&newsletterV3Id=5b5f5226e02b&user=jiawei+hu&userId=c71c1fba8a3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}