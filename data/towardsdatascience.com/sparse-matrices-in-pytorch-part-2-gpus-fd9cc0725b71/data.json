{"url": "https://towardsdatascience.com/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71", "time": 1682997225.555309, "path": "towardsdatascience.com/sparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71/", "webpage": {"metadata": {"title": "Sparse Matrices in Pytorch. In part 1, I analyzed the execution\u2026 | by Sourya Dey | Towards Data Science", "h1": "Sparse Matrices in Pytorch", "description": "In part 1, I analyzed the execution times for sparse matrix multiplication in Pytorch on a CPU. Here\u2019s a quick recap: One of the key selling points of deep learning frameworks such as Pytorch and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/sparse-matrices-in-pytorch-be8ecaccae6", "anchor_text": "part 1", "paragraph_index": 0}, {"url": "https://aws.amazon.com/ec2/instance-types/p3/", "anchor_text": "p3.2xlarge", "paragraph_index": 1}, {"url": "https://ieeexplore.ieee.org/document/8689061", "anchor_text": "pre-defined sparsity", "paragraph_index": 11}, {"url": "https://github.com/souryadey/speed-tests/tree/master/pytorch_sparse/part2_gpu_awsp3_2xlarge", "anchor_text": "here", "paragraph_index": 12}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/setup-jupyter.html", "anchor_text": "possible", "paragraph_index": 13}, {"url": "https://souryadey.github.io/", "anchor_text": "website", "paragraph_index": 14}, {"url": "https://souryadey.github.io/", "anchor_text": "https://souryadey.github.io/", "paragraph_index": 16}], "all_paragraphs": ["In part 1, I analyzed the execution times for sparse matrix multiplication in Pytorch on a CPU. Here\u2019s a quick recap:", "One of the key selling points of deep learning frameworks such as Pytorch and Keras is their deployability on GPUs, which massively speeds up computation. This comes with its limitations though, Pytorch only supports CUDA-compatible GPUs. CUDA is a computing framework created by Nvidia which leverages parallel processing on compatible GPUs. Not all GPUs are, for example, my Macbook has Intel Iris Pro Graphics which, unfortunately, is not CUDA-compatible. So I frequently use Amazon Web Services (AWS) \u2014 which offers cloud computing resources. For the experiments in this article, I used an AWS p3.2xlarge instance which has 1 Nvidia V100 GPU and is pretty powerful (as an example, training a 11-layer deep CNN on CIFAR-100 is 100 times faster on the p3 as compared to my Macbook).", "I repeated the same 3 experiments from Part 1 on the p3 instance with some minor changes, and compared these results to the CPU results. Let\u2019s dive in!", "As in part 1, all matrices are square. The sparse case is multiplying a diagonal matrix with a dense matrix. This means the density of the sparse matrix is 1/n (n = #rows). The dense case is multiplying 2 dense matrices, however, both dense matrices are randomly generated, unlike Part 1 where one dense matrix was simply the \u2018densified\u2019 version of the sparse matrix. This does not affect the results qualitatively; I only did this because it made the code for timing easier to setup (more on this later). Here are the results:", "Accessing overheads dominate the inset figure showing lower values of n on a magnified y-axis. Barring these, the trends are clear \u2014 a) the GPU is much faster than the CPU, and b) sparse matrices execute faster than dense, due to the extremely low densities in diagonal matrices.", "The sparse matrices now have a fixed density of 12.5%, while the dense cases are the same as before. Here are the results:", "While GPUs again perform better overall, this time it\u2019s the sparse cases which take more time than their dense counterparts. Which brings us to the final experiment \u2014 the impact of density on sparse matrix computation times. As a side note, note that sparse matrices on GPUs seem to have very large overheads which totally dominate for low values of n. Whether this is a limitation of GPUs or is due to the COO format is unknown to me.", "While low-density sparse diagonal matrices are faster to operate on than dense, this result flips for higher density sparse matrices. This experiment varies the density level for a sparse matrix of a given size and measures execution time. The sizes (values of n) I used are a bit different from part 1. Here they are 64, 256, 1024, and 4096. The density is swept through powers of 2. As in part 1, the red dashed line shows computation time for the dense case, while the black dashed lines indicate densities for the previous two experiments \u2014 diagonal matrices and 12.5%.", "The curves for n = 64 and 256 behave a bit funnily near the beginning, showing linear-ish growth for very low density values. The only explanation I can think of this is that the memory where the sparse matrix is stored is accessed differently when the number of non-zero elements is very low. It seems like the accessing overheads don\u2019t set in until an appreciable density is reached, then they dominate for a while (the flat portion), before yielding to computation times (exponential portion).", "Nevertheless, we note once again that there are no practical gains to be had by using sparse matrices instead of dense. For n = 64, 256 and 1024, sparse multiplication always takes more time than dense, while for n = 4096, it only takes about 1.5% density for sparse multiplication to take more time than dense.", "In summary, there are two major takeaways from this article. Firstly, GPUs (at least the modern and powerful ones) are significantly faster than CPUs and should always be used for linear algebra if possible. I say \u2018if possible\u2019 because AWS instances are not free. However, if you are a funded researcher, convincing your supervisor to pay for AWS GPU instances is well worth it if you\u2019ll be doing linear algebra intensive tasks such as deep learning. Secondly, the major finding from part 1 is reinstated on GPUs as well , i.e. 2 dense matrices always multiply faster than a sparse and dense matrix unless the sparse matrix has very low density (< 1.5%).", "Thus, there doesn\u2019t seem to be too much benefit to using sparse matrices in Pytorch. As far as my research on pre-defined sparsity is concerned, densities of 1.5% and below are too low to be of any use. These issues with sparse matrices have been expressed by other Pytorch users, so here\u2019s hoping that the devs come up with more efficient ways of handling sparse matrices.", "The code and images for this article can be found on Github here. If you browse the code, you\u2019ll find that I used the timeit module provided by Python for measuring execution time of code snippets. For the most accurate time measurements, it is good practice to setup all variables in the setup argument to timeit.Timer(). This is as opposed to defining variables beforehand and then giving timeit.Timer() access to the global namespace, which wastes time.", "As an alternative to the timeit module, Jupyter notebooks provide the %timeit magic command, which is what I used in part 1. The results of the 2 approaches are the same (they had better be!), but %timeit is slightly easier to use since it automatically takes care of averaging across many runs. However, running Jupyter notebooks on AWS, while possible, is cumbersome and a lot could depend on the server speed. As a result, I stuck to good old Python scripting for part 2.", "Sourya Dey is pursuing a PhD at the University of Southern California. His research deals with exploring complexity reduction in deep learning. You can read more about him on his website.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Research Engineer, Galois. Loves football, machine learning and arbitrary thinking. Homepage: https://souryadey.github.io/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ffd9cc0725b71&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@souryadey?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souryadey?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Sourya Dey"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb98b8c8fbd76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&user=Sourya+Dey&userId=b98b8c8fbd76&source=post_page-b98b8c8fbd76----fd9cc0725b71---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/predefined-sparsity", "anchor_text": "Predefined sparsity"}, {"url": "https://towardsdatascience.com/sparse-matrices-in-pytorch-be8ecaccae6", "anchor_text": "part 1"}, {"url": "https://aws.amazon.com/ec2/instance-types/p3/", "anchor_text": "p3.2xlarge"}, {"url": "https://ieeexplore.ieee.org/document/8689061", "anchor_text": "pre-defined sparsity"}, {"url": "https://github.com/souryadey/speed-tests/tree/master/pytorch_sparse/part2_gpu_awsp3_2xlarge", "anchor_text": "here"}, {"url": "https://docs.aws.amazon.com/dlami/latest/devguide/setup-jupyter.html", "anchor_text": "possible"}, {"url": "https://souryadey.github.io/", "anchor_text": "website"}, {"url": "https://medium.com/tag/sparse?source=post_page-----fd9cc0725b71---------------sparse-----------------", "anchor_text": "Sparse"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----fd9cc0725b71---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/runtime?source=post_page-----fd9cc0725b71---------------runtime-----------------", "anchor_text": "Runtime"}, {"url": "https://medium.com/tag/gpu?source=post_page-----fd9cc0725b71---------------gpu-----------------", "anchor_text": "Gpu"}, {"url": "https://medium.com/tag/predefined-sparsity?source=post_page-----fd9cc0725b71---------------predefined_sparsity-----------------", "anchor_text": "Predefined Sparsity"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&user=Sourya+Dey&userId=b98b8c8fbd76&source=-----fd9cc0725b71---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&user=Sourya+Dey&userId=b98b8c8fbd76&source=-----fd9cc0725b71---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ffd9cc0725b71&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----fd9cc0725b71---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----fd9cc0725b71--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souryadey?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@souryadey?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sourya Dey"}, {"url": "https://medium.com/@souryadey/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "96 Followers"}, {"url": "https://souryadey.github.io/", "anchor_text": "https://souryadey.github.io/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb98b8c8fbd76&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&user=Sourya+Dey&userId=b98b8c8fbd76&source=post_page-b98b8c8fbd76--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fb98b8c8fbd76%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsparse-matrices-in-pytorch-part-2-gpus-fd9cc0725b71&user=Sourya+Dey&userId=b98b8c8fbd76&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}