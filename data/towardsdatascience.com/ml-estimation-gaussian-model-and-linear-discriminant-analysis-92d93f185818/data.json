{"url": "https://towardsdatascience.com/ml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818", "time": 1683009159.052812, "path": "towardsdatascience.com/ml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818/", "webpage": {"metadata": {"title": "Maximum Likelihood Estimation. Background | by Mengsay Loem | Towards Data Science", "h1": "Maximum Likelihood Estimation", "description": "Maximum likelihood estimation(ML Estimation, MLE) is a powerful parametric estimation method commonly used in statistics fields. The idea in MLE is to estimate the parameter of a model where given\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Maximum likelihood estimation(ML Estimation, MLE) is a powerful parametric estimation method commonly used in statistics fields. The idea in MLE is to estimate the parameter of a model where given data is likely to be obtained. In this section, I will introduce the importance of MLE from the pattern recognition approach. Why do we need MLE? How does it work in a pattern recognition process?", "Pattern recognition is a branch of machine learning. It is a process of recognizing a corresponding category of a given pattern. Here, pattern refers to feature which can be used to define whether or not any spatial or sequential observable data are in the same group. Category refers to the result of pattern recognition, meaning a group of the same or similar pattern. In statistical pattern recognition, statistical features of a given training sample are extracted and used to form a recognition process.", "Pattern Recognition goal is equivalent to determining a discriminator function of multiple categories. To generate a well-performed discriminator function, several criteria, such as maximum a posteriori probability decision rule, minimum discriminator error decision rule, Bayesian decision rule. In this post, we focus on the maximum a posteriori probability decision rule as an example. In the following explanation, we are committing to defining a corresponding category y of a given input data x using maximum a posteriori probability decision rule.", "In order to define a category y of a given input x, it is natural to choose a category in which there is the highest possibility that the input belongs to it. This means to choose a category with the maximum value of posteriori probability p(y|x).", "This kind of decision rule is called maximum a posteriori probability rule. It is the same as setting a decision region as", "From Bayes\u2019 Theory, a posteriori probability can be written in the following form. From this, the maximum a posteriori probability rule is equivalent to maximum a product of conditional probability p(x|y) and the priori probability p(y).", "Therefore, it is necessary to estimate conditional probability p(x|y) and the priori probability p(y) to obtain the posteriori probability p(y|x).", "Since category y is a discrete type random variable, we can simply estimate the priori probability with", "However, it is impossible to estimate a continuous type of random variable input x with this method. For instance, assume input x with a Gaussian distribution. To define the conditional probability of x we need expectation value and standard variation value as parameters. It is why a parametric estimation method such as MLE is required.", "A set of probability density functions form by a finite number of parameters is called a parametric model. We call q(x; theta) a parametric model where theta is the parameter.", "When approximating the probability density function, it would be natural to determine the parameter values so that the training sample we have is most likely to occur. Here, under the parameter theta, we consider the probability that the current training sample x_i (i=1,\u2026,n) will occur. Seeing as a function of the parameter theta, this probability is called likelihood and written as L(theta). With i.i.d(independent and identically distributed) assumption, likelihood is", "In the maximum likelihood estimation method, we find the value of parameter theta which maximizes the likelihood value. The optimal parameter theta with MLE method is written as", "When a parametric model q(x; theta) is derivable by theta, the following equation is true. This equation is called a likelihood equation. It is a necessary condition of the maximum likelihood estimation answer but not a sufficient condition.", "In most cases, it is complicated to solve the likelihood equation. As a solution, a log-likelihood is used. Since a log-function is monotonically increasing, an optimal parameter in a log-likelihood and a likelihood is the same.", "Gaussian model is a parametric model with the Gaussian distribution. A Gaussian model of a d-dimension pattern x is generally given in the following form.", "\\mu and \\Sigma are parameters of the Gaussian Model.\\Sigma is a positive definite symmetric matrix. An important feature of a Gaussian model is that the parameter \\mu and \\Sigma are respectively expectation value and variance-covariance matrix of the probability distribution.", "Now, let\u2019s take Gaussian model as an example. Assume we have n sample data {x_i} (i=1,\u2026,n). We are going to estimate the parameters of Gaussian model using these inputs.", "First, the likelihood and log-likelihood of the model is", "Next, likelihood equation can be written as", "Solving these equations, we finally obtain the estimated parameters", "These are the same to sample mean and sample variance-covariance matrix.", "The following figure shows the result of MLE applied to Gaussian Model using 8000 sample points.", "Back to our problem in defining the corresponding category of a given input data. We are finding this category by choosing the maximum value of the probability such that a given data x belongs to a category y by calculating p(y|x). As discussed in the previous section, our problem is estimating the conditional probability p(x|y).", "Suppose our observed data are represented by Gaussian model. Now, with MLE mentioned just before this, we can estimate the conditional probability of each category y, p(x|y), by", ". \\hat{mu}_y and \\hat{Sigma}_y are estimated expectation value and variance-covariance matrix of patterns belonging to category y.", "Consider this estimated p(x|y) and p(y) mentioned in the previous section, we can calculate the posteriori probability now. To make the calculation simpler, we use log-posteriori probability log p(y|x). n_y is the number of samples in category y, n is the total number of samples. C is the constant which does not relevant to the variable y.", "Finally, to define the corresponding category of patter x, we calculate log p(y|x) for all y in the category set and choose the one with the maximum value.", "To avoid complications, we assume that the variance-covariance matrix of each category is equal, and the common variance-covariance matrix is \\Sigma.", "With this assumption and from the above discussion, the estimate of the common variance-covariance matrix is", "Using this estimate, the log-posteriori probability is now can be written as", "As an example, now suppose the number of categories is 2. In this case, the decision boundary is a set of points whose posteriori probabilities are equal, meaning p(y=1|x)=p(y=2|x). The decision boundary can be written as", "It can say that the decision boundary is a hyperplane of sample x. This kind of approach deciding the decision boundary is called Fisher\u2019s linear discriminant analysis.", "Suppose the sample data x is in 2d space. Here we will do the linear discriminant analysis in real values.", "Training sample data is shown in the following figure where \u2018x\u2019 represents Category1 and \u2018+\u2019 represents Category2. Here, we use n1(=200) and n2(=200) of samples in each category. These training samples are artificially generated with Gaussian distribution with population mean=(2, 2) and (-2,-2), and population variance-covariance matrix [1,0 ; 0,9].", "Same to the previous discussion, we approximate the data with the Gaussian model which the variance-covariance matrixes of both categories are equal.", "Using the maximum likelihood estimation, the estimate of expectation values and variance-covariance matrixes of each category are", "From the result in Linear Discriminant Analysis section, the decision boundary of given data is", "From these results, we can notice that when n1/n2 >1 (n1> n2), the mistake of categorizing pattern with category1 into category2 is fewer than the vice versa. This phenomenon is also found in n1<n2 cases. It means that the decision boundary tends to fit more with the category which has a larger size of training samples. In contrast, the number of errors is in a balanced state when n1=n2.", "Maximum likelihood estimation plays critical roles in generative model-based pattern recognition. As we have discussed in applying ML estimation to the Gaussian model, the estimate of parameters is the same as the sample expectation value and variance-covariance matrix. This is intuitively easy to understand in statistical estimation. However, as a result in the discussion section, the number of samples affects the accuracy of estimation which leads to the effect on the performance of pattern recognition (linear discriminant analysis in this case). Therefore, it is crucial to work with a balanced sample data to avoid overfitting to any category of the recognition process.", "In this post, we discussed only the Gaussian model. However, in real-life data analysis, we need to define a specific model for our data based on its natural features. In the near future, I will introduce \u201cmodel selection in maximum likelihood estimation\u201d.", "[1] Masashi Sugiyama, Statistical Machine Learning \u2014 Generative Model-based Pattern Recognition(2019)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master's Student @ Tokyo Tech | Dept. of Computer Science"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F92d93f185818&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----92d93f185818--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mengsaylms?source=post_page-----92d93f185818--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mengsaylms?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Mengsay Loem"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde1bfda96132&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&user=Mengsay+Loem&userId=de1bfda96132&source=post_page-de1bfda96132----92d93f185818---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/maximum-likelihood?source=post_page-----92d93f185818---------------maximum_likelihood-----------------", "anchor_text": "Maximum Likelihood"}, {"url": "https://medium.com/tag/gaussian-model?source=post_page-----92d93f185818---------------gaussian_model-----------------", "anchor_text": "Gaussian Model"}, {"url": "https://medium.com/tag/linear-discriminant?source=post_page-----92d93f185818---------------linear_discriminant-----------------", "anchor_text": "Linear Discriminant"}, {"url": "https://medium.com/tag/fishers-lda?source=post_page-----92d93f185818---------------fishers_lda-----------------", "anchor_text": "Fishers Lda"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&user=Mengsay+Loem&userId=de1bfda96132&source=-----92d93f185818---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&user=Mengsay+Loem&userId=de1bfda96132&source=-----92d93f185818---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----92d93f185818--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F92d93f185818&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----92d93f185818---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----92d93f185818--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----92d93f185818--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----92d93f185818--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----92d93f185818--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----92d93f185818--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mengsaylms?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mengsaylms?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mengsay Loem"}, {"url": "https://medium.com/@mengsaylms/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "71 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fde1bfda96132&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&user=Mengsay+Loem&userId=de1bfda96132&source=post_page-de1bfda96132--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F48022e86df09&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fml-estimation-gaussian-model-and-linear-discriminant-analysis-92d93f185818&newsletterV3=de1bfda96132&newsletterV3Id=48022e86df09&user=Mengsay+Loem&userId=de1bfda96132&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}