{"url": "https://towardsdatascience.com/stoic-philosophy-built-by-algorithms-9cff7b91dcbd", "time": 1683005345.100627, "path": "towardsdatascience.com/stoic-philosophy-built-by-algorithms-9cff7b91dcbd/", "webpage": {"metadata": {"title": "Stoic Philosophy \u2014 Built by Algorithms | by James Briggs | Towards Data Science", "h1": "Stoic Philosophy \u2014 Built by Algorithms", "description": "How to use TensorFlow in Python with Natural Language Processing (NLP) to generate language in the style of The Meditations by Marcus Aurelius."}, "outgoing_paragraph_urls": [{"url": "http://classics.mit.edu/Antoninus/meditations.html", "anchor_text": "[1]", "paragraph_index": 6}, {"url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks", "paragraph_index": 35}, {"url": "https://github.com/jamescalam/meditations_ai/blob/master/code/meditation.py", "anchor_text": "here", "paragraph_index": 61}, {"url": "https://github.com/jamescalam/meditations_ai/blob/master/works/meditations.md", "anchor_text": "here", "paragraph_index": 61}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "Text generation with an RNN", "paragraph_index": 62}, {"url": "https://www.tensorflow.org/tutorials/text/word_embeddings", "anchor_text": "Word embeddings", "paragraph_index": 63}, {"url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks", "paragraph_index": 64}, {"url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.7389&rep=rep1&type=pdf", "anchor_text": "Recurrent Neural Net Learning and Vanishing Gradient", "paragraph_index": 65}, {"url": "https://arxiv.org/pdf/1412.3555v1.pdf", "anchor_text": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling", "paragraph_index": 66}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs", "paragraph_index": 69}], "all_paragraphs": ["Using Machine Learning to generate language in the style of an ancient text or several ancient texts is one of those common applications that always manages to impress me.", "The Meditations by Marcus Aurelius is a capstone in stoic philosophy \u2014 praised by athletes, entrepreneurs, and leaders across the world.", "Look within. Let neither the peculiar quality of anything nor its value escape thee. \u2014 Book VI", "It is truly incredible that we can read and learn of this ancient discipline from the inner thoughts of someone like Marcus Aurelius\u2019 \u2014 a Roman Emperor, and widely regarded as one of the greatest Stoic philosophers to have ever lived.", "Some days prior to writing this I was reading a few pages of this work, which quickly made my mind return to those captivating excerpts of ML-generated ancient texts.", "Contrary to the seemingly complex nature of a computer generating eloquent passages of text, it is actually a reasonably straightforward process.", "We will begin with an online, text format of all books contained within The Meditations. This corpus is hosted by MIT [1].", "The universe is transformation: life is opinion. \u2014 Book IV", "First we must import and make a few minor changes to the source text of Meditations, such as:", "We import with requests.get and clean the data with the split/replace functions and some minor regex like so:", "Computers work with numbers, not language.", "To translate Meditations into something a computer can understand, we must break our corpus down into smaller input entities. Which can then be converted into indices (and is later converted into one-hot encoding or vectors).", "This is most commonly done at two levels of granularity. Word-level embedding, or character-level embedding.", "In both cases, all individual entities (word or character) are taken to create a vocabulary \u2014 a set of all language entities contained within the corpus. A character-level vocabulary of Meditations looks like:", "From this we can convert each unique character into an index. We do this with the char2idx dictionary, and vise-versa using the idx2char array:", "We can then convert human Meditations into machine Meditations:", "Everything we see is a perspective, not the truth.", "Personally I prefer reading human Meditations. Fortunately, the translation of machine to human Meditations post-training will be taken care by our idx2char array.", "Whatever the rational and political (social) faculty finds to be neither intelligent nor social, it properly judges to be inferior to itself. \u2014 Book VII", "We assign the sequence length per training example with SEQ_LEN. A longer SEQ_LEN seems to improve long-term language structure at the cost of short-term language structure. For example:", "SEQ_LEN = 100 \u2014 \u201cFrom many together with more quiet or more freedom from trouble does no harm to the elements themselves in the soul\u201d", "SEQ_LEN = 300 \u2014 \u201cFrom I mout dyems; but let this prescribed this kind andorgance is a polity which is not life, retarn to thyself, motever, they to care atail for the poor bless society.\u201d", "We can avoid this by increasing the number of training epochs. Resulting in better short and long-term language structure \u2014 however, balancing this against over-fitting can become difficult.", "We convert our indexed data into a Tensorflow Dataset object [2] using:", "This allows us to perform useful built-in operations on our dataset, such as .batch() and .shuffle().", "As we are intending to predict the next character of a sequence, our target values should be our inputs shifted one character forward. For example, if our input is \u201cHello worl\u201d, our target would be \u201cello world\u201d.", "To do this, every sequence must contain SEQ_LEN + 1 characters. We split our dataset into sequences of this length using .batch():", "Now we have our sequences of length SEQ_LEN + 1, we split them into input/output pairs:", "Now we must shuffle our data-set and create training batches. Note that we are using batch again here. Previously we used .batch() to split our 241'199 character long \u2014 indexed corpus \u2014 into many sequences of length SEQ_LEN + 1. This time, we use it to split our sequences into groups of 64 (BATCH_SIZE=64).", "Such as are thy habitual thoughts, such also will be the character of thy mind; for the soul is dyed by the thoughts. \u2014 Book VI", "The model is very straightforward, consisting of an Embedding layer, GRU layer, and Dense layer.", "The tf.keras.layers.Embedding() layer is a trainable array which we use to convert our single indexed character values into dense floating point vectors. This process is called embedding [4]. Each character index maps to a specific vector in this layer.", "The following tf.keras.layers.GRU() layer contains our recurrent neural network (RNN). RNNs are the common choice in NLP applications thanks to the addition of a \u2018time\u2019 dimension to calculations. This means that not only are we predicting based on the contents of our input, but also the order of those contents. Take the following statements:", "[Original]: Everything we see is a perspective, not the truth.", "[Re-ordered]: Everything we see is the truth, not a perspective.", "A method that does not consider sequence would see no difference between these two statements. Clearly, sequence is particularly important in language. See Andrej Kaparthy\u2019s \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d article for more on RNNs.", "Both GRU and LSTM units are adaptions of the original RNN that aim to solve the vanishing gradient problem through the use of information gating [6][7]. Both share similar architectures and performance (in most cases).", "For this model, I noted no clear qualitative difference between either. As GRUs required less training time [8], I continued with that.", "Finally, I found that dropout=0.1 made a huge impact on improving generalization. This randomly masks 10% of the inputs. Resulting in significant reductions of long passages of copied text, whilst still maintaining correct language structure and comprehensible statements.", "The final tf.keras.layers.Dense() layer produces our prediction. The GRU outputs are passed to a densely connected neural network which generates the prediction of one out of 66 characters.", "No longer talk at all about the kind of man that a good man ought to be, but be such. \u2014 Book X", "The model is compiled using the adam optimizer and sparse_categorical_crossentropy loss function.", "During training we save the model weights with:", "checkpoint is then passed to the callbacks argument during model.fit(). At the end of each training epoch, the model weights are saved in the ./training_checkpoint directory.", "After training, we can load our trained model weights from file and use them to rebuild our model. The key difference is that we rebuild the model with a batch size of 1 rather than the 64 used during training. We do this so that we can pass one input (for example \"From\") to produce one output.", "This is all done with just:", "I have often wondered how it is that every man loves himself more than all the rest of men, but yet sets less value on his own opinion of himself than on the opinion of others. \u2014 Book XII", "Once the model is loaded, we can begin generating text easily:", "We get some pretty weird text to start with \u2014 it really likes the letter e. The use of common punctuation is learned pretty quick though:", "The model after very few training epochs figures out words very quickly:", "A few more epochs and we get actual words \u2014 with some errors here and there:", "Eventually the model will produce some comprehensible statements, which is super cool to see:", "Training further, we tend to reproduce mostly almost copies of text from Meditations:", "There are some surprisingly brilliant sections of text produced by training a simple RNN architecture on Meditations. It manages to accurately structure sentences, correctly produce real words, and merge this into something which for the most part is actually readable.", "Although the outcome is really cool, developing this further would require more than just Meditations. In ML, data is king. With this project, sticking with Meditations alone is the biggest limiting factor. Simply put \u2014 we need more data.", "My train of thought is that with the inclusion of more data, we will build a larger vocabulary. From that, we will have a much wider array of words at our disposal, and so word-level embedding seems to be the logical next step. Enabling us to maintain the semantics that can be provided by words, but are lost during character-level embedding.", "For now, I am impressed with how easy it is to build something that occasionally seems genuinely insightful. I look forward to experimenting with this more in the future, with more data, word-vectors, and other tools.", "Let me leave you with a few of my favorite algorithmic Meditations.", "\u201cThou sufferest this justly: but the other is a pestilence of men working to itself the things which procure pleasure, but the good however that every man is worth just so much as the things are worth about which he busies himself.\u201d", "\u201cAll things are the same, familiar in experience, and ephemeral in time, and worthless in the matter. Everything now is just as it was in the time is a point, and the substance is in a flux, and the perception due, and then [C]art not let thy desire find its termination.\u201d", "\u201cAnd remember that the term Rational was intended to signify a dispersion, or a resolution into atoms, or annihilation, it is either extinction or change.\u201d", "The code is here and you can find a 100K long output here.", "[2] Text generation with an RNN, Tensorflow Core Tutorials", "[4] Word embeddings, Tensorflow Core Tutorials", "[5] A. Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks (2015), Andrej Karpathy Blog", "[7] S. Hochreiter, Recurrent Neural Net Learning and Vanishing Gradient (1998), International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6(2):107\u2013116", "[8] J. Chung, C. Gulcehre, K. Cho, Y. Bengio, Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (2014), NIPS 2014 Deep Learning and Representation Learning Workshop", "If you enjoyed this, you may like the next article in the Meditation project \u2014 where I cover using multiple competing RNNs to produce higher quality text, check it out here:", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Freelance ML engineer learning and writing about everything. I post a lot on YT https://www.youtube.com/c/jamesbriggs"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9cff7b91dcbd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://jamescalam.medium.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "James Briggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----9cff7b91dcbd---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://www.towardsdatascience.com/tagged/the-meditations", "anchor_text": "The Meditations Project"}, {"url": "https://commons.wikimedia.org/wiki/File:Roma,_busto_di_marco_aurelio,_170-180_dc_ca.jpg", "anchor_text": "Ancient Roman sculptures in the Art Institute of Chicago"}, {"url": "https://commons.wikimedia.org/wiki/User:Sailko", "anchor_text": "Saiko"}, {"url": "https://creativecommons.org/licenses/by/3.0/", "anchor_text": "CC BY 3.0"}, {"url": "http://classics.mit.edu/Antoninus/meditations.html", "anchor_text": "[1]"}, {"url": "https://unsplash.com/@ibaxez?utm_source=medium&utm_medium=referral", "anchor_text": "Carlos Ib\u00e1\u00f1ez"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://unsplash.com/@dariovero_?utm_source=medium&utm_medium=referral", "anchor_text": "Dario Veronesi"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks"}, {"url": "https://unsplash.com/@nilshuber?utm_source=medium&utm_medium=referral", "anchor_text": "Nils"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/jamescalam/meditations_ai/blob/master/code/meditation.py", "anchor_text": "here"}, {"url": "https://github.com/jamescalam/meditations_ai/blob/master/works/meditations.md", "anchor_text": "here"}, {"url": "http://classics.mit.edu/Antoninus/meditations.html", "anchor_text": "The Meditations"}, {"url": "https://www.tensorflow.org/tutorials/text/text_generation", "anchor_text": "Text generation with an RNN"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices", "anchor_text": "td.data.Dataset"}, {"url": "https://www.tensorflow.org/tutorials/text/word_embeddings", "anchor_text": "Word embeddings"}, {"url": "https://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "The Unreasonable Effectiveness of Recurrent Neural Networks"}, {"url": "https://www.bioinf.jku.at/publications/older/2604.pdf", "anchor_text": "Long Short-Term Memory"}, {"url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.7389&rep=rep1&type=pdf", "anchor_text": "Recurrent Neural Net Learning and Vanishing Gradient"}, {"url": "https://arxiv.org/pdf/1412.3555v1.pdf", "anchor_text": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}, {"url": "https://towardsdatascience.com/recurrent-ensemble-learning-caffdcd94092", "anchor_text": "Supercharged Prediction with Ensemble LearningIncreasing text generation quality with competing neural netstowardsdatascience.com"}, {"url": "https://medium.com/tag/philosophy?source=post_page-----9cff7b91dcbd---------------philosophy-----------------", "anchor_text": "Philosophy"}, {"url": "https://medium.com/tag/stoicism?source=post_page-----9cff7b91dcbd---------------stoicism-----------------", "anchor_text": "Stoicism"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----9cff7b91dcbd---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----9cff7b91dcbd---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/the-meditations?source=post_page-----9cff7b91dcbd---------------the_meditations-----------------", "anchor_text": "The Meditations"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&user=James+Briggs&userId=b9d77a4ca1d1&source=-----9cff7b91dcbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&user=James+Briggs&userId=b9d77a4ca1d1&source=-----9cff7b91dcbd---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F9cff7b91dcbd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----9cff7b91dcbd---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----9cff7b91dcbd--------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://jamescalam.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "James Briggs"}, {"url": "https://jamescalam.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.6K Followers"}, {"url": "https://www.youtube.com/c/jamesbriggs", "anchor_text": "https://www.youtube.com/c/jamesbriggs"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F75e31c56d187&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstoic-philosophy-built-by-algorithms-9cff7b91dcbd&newsletterV3=b9d77a4ca1d1&newsletterV3Id=75e31c56d187&user=James+Briggs&userId=b9d77a4ca1d1&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}