{"url": "https://towardsdatascience.com/principle-component-analysis-pca-intuitions-codes-and-mathematics-d50953d361cb", "time": 1683015518.7414389, "path": "towardsdatascience.com/principle-component-analysis-pca-intuitions-codes-and-mathematics-d50953d361cb/", "webpage": {"metadata": {"title": "Principle Component Analysis (PCA) | by 0 | Medium", "h1": "Principle Component Analysis (PCA)", "description": "Principle Component Analysis (PCA) is one of the essential feature extraction methods in data science. When we handle a complex dataset with many features, it is usually a good idea to reduce the\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "Iris flower dataset", "paragraph_index": 1}], "all_paragraphs": ["Principle Component Analysis (PCA) is one of the essential feature extraction methods in data science. When we handle a complex dataset with many features, it is usually a good idea to reduce the number of features before training the models.", "This article will first introduce the intuitions behind the PCA and then implement it in python using the famous Iris flower dataset with some mathematical proof.", "First, let\u2019s say your boss hands you a dataset with 1000 different features for a machine learning task. The first thoughts that pop into your head are, of course, like \u2026do I need all of them? If I could reduce the number of features, maybe it\u2019s easier to train the model\u2026\u2026.", "You scratch your head and think about reducing the number of features without losing too much information?", "If these 1000 features are not independent of each other, we can use the PCA to extract \u201cthe useful part\u201d of the features.", "Let\u2019s first take a look at a very extreme situation:", "As a data scientist, you first calculate the correlation between each feature and realize that feature No.5 (we call it x5) and feature No.10 (called x10) are almost the same up to some random fluctuations in the last digit (thus, the correlation is almost 1). You can then easily tell that instead of using these two features in training, you can reassign the two features as:", "Because the original feature No.5 and feature No.10 are almost the same, the new feature No.10 is almost useless. It contains mostly random noise! Therefore, we can safely drop it from our dataset.", "The moral of this extreme example is that: if two or many of the features are correlated with each other, maybe it\u2019s a good idea to work with the linear combination of the features instead!", "The intuition behind the PCA is to find a specific linear combination such that the correlations between different features vanish. When the features are independent of each other, usually, you will find some of the features useless, just like the new feature No. 10 in the previous example. Besides, you can also quantify how \u201cimportant\u201d the feature is by merely looking at its \u201cprincipal value,\u201d which we will introduce later.", "Having introduced the intuition behind the PCA, we can start implementing our PCA code.", "We can find the Iris flower data set in the Sklearn package.", "We see that the Petal Length and Petal Width are highly correlated (correlation = 0.96). It indicates that PCA could be a helpful tool for reducing the feature dimension.", "To find the principal component, we first calculate the covariance matrix :", ", where X is the matrix that stores the features (after subtracting the average of each features). And diagonalize it (by finding it\u2019s eigenvalues and eigenvectors). It can be written as", "Each eigenvectors corresponds to a way to do the linear combination of features and its eigenvalue represent how important the feature is (called principal values).", "After the diagonalization, we can transform the features to the new basis,", "The correlation between different Features are zero!", "Note that we computed the correlations between the \u201creal features\u201d like Petal Length and Petal Width in the previous plot. But after the linear transformation, we are considering the correlation between \u201ca*Petal Length + b*Petal Width + \u2026 etc.\u201d and \u201cc*Petal Length + d*Petal Width + \u2026 etc.\u201d, where a, b, c, d are some numbers.", "Although they are completely uncorrelated, which is nice mathematically speaking, we lose insight about the features. Sometimes we need to do more analysis to understand what these new feature means (if possible).", "Feature Reduction : Depending on the magnitude of the eigenvalues, we can decide if we want to keep new features or not. To reduce the number of features, we simply ignore the eigenvector that we want to discard and perform the linear transformation using the remaining eigenvectors.", "For example, if we keep only the Feature 3 and Feature 4, we can plot the class of Iris flower in a 2D plot :", "The above figure shows that Feature 4 (almost) perfectly separate the three categories. Therefore, we can actually train the model using our new feature 3 and feature 4 rather than the original features.", "Sklearn implementation : Instead of implementing diagonalization and linear transformation every time, we can use the PCA function in the Sklearn package. In the following, I use the PCA function and keep 3 features:", "To summarize, PCA is a power tool to \u201crotate\u201d the features so that the features are not correlated to each others. By doing so, we can eliminate the unimportant features in our training dataset."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d50953d361cb--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d50953d361cb--------------------------------", "anchor_text": "0"}, {"url": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "anchor_text": "Iris flower dataset"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d50953d361cb---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----d50953d361cb---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d50953d361cb---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/linear-algebra?source=post_page-----d50953d361cb---------------linear_algebra-----------------", "anchor_text": "Linear Algebra"}, {"url": "https://medium.com/tag/statistics?source=post_page-----d50953d361cb---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://towardsdatascience.com/?source=post_page-----d50953d361cb--------------------------------", "anchor_text": "More from 0"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3964a2896119&operation=register&redirect=https%3A%2F%2Fzpcian309.medium.com%2Fprinciple-component-analysis-pca-intuitions-codes-and-mathematics-d50953d361cb&newsletterV3=f6ec5984411&newsletterV3Id=3964a2896119&user=0&userId=f6ec5984411&source=-----d50953d361cb---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----d50953d361cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d50953d361cb--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d50953d361cb--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d50953d361cb--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d50953d361cb--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d50953d361cb--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d50953d361cb--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "0"}, {"url": "https://towardsdatascience.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "12 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F3964a2896119&operation=register&redirect=https%3A%2F%2Fzpcian309.medium.com%2Fprinciple-component-analysis-pca-intuitions-codes-and-mathematics-d50953d361cb&newsletterV3=f6ec5984411&newsletterV3Id=3964a2896119&user=0&userId=f6ec5984411&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}