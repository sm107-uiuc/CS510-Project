{"url": "https://towardsdatascience.com/differentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf", "time": 1682997435.4468281, "path": "towardsdatascience.com/differentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf/", "webpage": {"metadata": {"title": "Differentiable Programming \u2014 Inverse Graphics AutoEncoder | by Andy Bosyi | Towards Data Science", "h1": "Differentiable Programming \u2014 Inverse Graphics AutoEncoder", "description": "DeepLearning classifier, LSTM, YOLO detector, Variational AutoEncoder, GAN \u2014 are these guys truly architectures in sense meta-programs or just wise implementations of ideas on how to solve particular\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba", "anchor_text": "https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba", "paragraph_index": 1}, {"url": "https://www.linkedin.com/in/andybosyi/", "anchor_text": "Linkedin", "paragraph_index": 31}, {"url": "https://www.facebook.com/profile.php?id=100013517439841", "anchor_text": "Facebook", "paragraph_index": 31}, {"url": "https://www.linkedin.com/in/oleksiy-simkiv/", "anchor_text": "Alex Simkiv", "paragraph_index": 32}, {"url": "https://www.linkedin.com/in/mykola-kozlenko/", "anchor_text": "Mykola Kozlenko", "paragraph_index": 32}, {"url": "https://medium.com/@volodymyrsendetskyi", "anchor_text": "Volodymyr Sendetskyi", "paragraph_index": 32}, {"url": "https://www.linkedin.com/in/nazar-savchenko/", "anchor_text": "Nazar Savchenko", "paragraph_index": 32}, {"url": "http://MindCraft.ai", "anchor_text": "MindCraft.ai", "paragraph_index": 34}], "all_paragraphs": ["DeepLearning classifier, LSTM, YOLO detector, Variational AutoEncoder, GAN \u2014 are these guys truly architectures in sense meta-programs or just wise implementations of ideas on how to solve particular optimization problems? Are machine learning engineers actually developers of decision systems or just operators of GPU-enabled computers with a predefined parameterized optimization program? Differentiable Programming can answer these and some other questions like to explain the model and to stop heat our planet with brute-force end-to-end DNNs. Data Scientists already do a lot of custom programming to clean, filter and normalize data before passing it into a standard neural network block or to stack models for a better result. Now it is time to do custom programming after the standard NN block and this programming has to be differentiable.", "Let\u2019s consider handwritten character recognition using MNIST (EMNIST). The classical approach is to label thousands of samples and pass them through the convolutional or dense neural network. The result will be some model that can predict a letter. This supervised approach requires a lot of data labeled (there are semisupervised methods like Active Learning https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba that can reduce the labeling efforts), and it creates a model that cannot explain data itself and the results as well. It makes sense only for the same distributed data to be recognized in the same way", "If we are brave enough to leave the safe harbor of supervised learning and use AutoEncoder to learn the embedded space using self-regression, we will find us not labeling data, but examining the Z-space for hidden features and insights. And the true result of this process is not a reproduced image, but learned embeddings that can be clustered and used for the recognition, feature explanation, and manipulation. But we still cannot use the bottleneck of the AutoEncoder to connect it to a data transforming pipeline, as the learned features can be a combination of the line thickness and angle. And every time we retrain the model we will need to reconnect to different neurons in the bottleneck z-space.", "What if to ask the neural network block to produce something useful from the beginning, like take a marker and draw the sign? This will be more like human beings are learning. We can interpret the DNN output as a list of points to draw with different marker sizes (pressure) and color intensity or even use the output as a single gesture that has to draw the handwritten sign. In this case, we will interpret the output as a set of velocity records (as first derivatives of positions).", "In order to make these marker commands to produce the output that can be fully differentiable and, accordingly, transparent for backpropagation, we need to use DiffProg approach. Classical programming tends to use too much \u201cdigital\u201d operators that cannot pass gradient through them, like the step function cannot serve to adjust the initial parameters and we need to use sigmoid instead:", "The same way we can use a Gaussian curve to emulate marker points, just need to present it in 2D space:", "Here \ud835\udf0e can be used as an inverse marker size, x and y are coordinates of the marker point and the color intensity can be driven by an additional amplifier coefficient:", "In order to combine multiple marker points together, we can use the max function. And then the generated picture can be passed to the loss function \u2014 the same MSE:", "As an actual result, we will receive a model that can decompose image information in space and time(!), creating a program that will draw the requested sign in a single movement of, let\u2019s say, robotic arm.", "We will not focus on the details of the Neural Network block as the article is about the concept of DiffProg. We just use some DNN with dense layers that act as a statistical estimator. The details of the implementation can be found in the notebooks that are listed in the References section. Let\u2019s focus on the specifics of the TensorFlow implementation.", "After loading MNIST data we create the model. Here we start with some parameters. The varcoef param is needed to translate the size of the marker in pixels to variance in the formula.", "We use input data points as ground true:", "The Neural Network serves to translate 728 MNIST pixels into 10 marker points. Each has vertical and horizontal position and size. We use the sigmoid function for the activation of all the three-point parameters. This is a simplified variant and the results should not surprise us.", "Here the DiffProg part starts \u2014 we need to put Gaussian formula into vectorized form and broadcast over batch size, 10 marker points and two dimensions of the picture. For this purpose we need to prepare GRAD_* as \ud835\udc65, MEAN_* as \ud835\udf07 and VAR2 as 2\ud835\udf0e\u00b2 in the formula:", "As result, we will have the MARKER tensor with shape: [batch size, points, vertical, horizontal].", "The head part is pretty classical, except the fact that we will use max function for joining marker points in one glyph:", "After training for 10 epochs the accuracy metric will be around 97% for both training and test datasets.", "Now we will go from the set of static points to a line that can reproduce the glyph. For this purpose, we will change the code slightly in order to calculate point position as integral of velocity, predicted by the NN. We are increasing the number of points as now not all of them will be visible. We also need a parameter for the maximal size of steps between points.", "Using more deep NN block and introducing color intensity as additional marker point param.", "For velocity of the marker on each step we need now -1:1 output range, thus we will use tanh activation and multiply them on the step_size. To calculate the final point position we will need a cumulative sum of all previous velocity values plus initial point (left middle).", "Markers are calculated now with additional AMP tensor that represents color intensity for each point.", "While training for 50 epochs we will achieve 98% accuracy for train and test datasets. These are reproduced glyphs by one gesture for the training dataset:", "As we already discussed the actual result is the ability to learn to draw symbol glyphs as a human does. These are pictures of gesture tracks with marker size and color. Please note that the color intensity is crucial to draw more complex symbols that require multiple lines.", "If we experiment with the training multiple times, we will observe that the style will be different every time, depending on the random seed.", "Let\u2019s move to more practical results \u2014 how to do symbol classification using this new technique. First, let\u2019s perform clustering for whole 728 pixels of MNIST data over the test data set. This will produce the following results:", "We assign every cluster a dominant sample label and use it for classification errors and accuracy metrics. When we do the same clustering over velocity and color intensity of the learned inverse graphics commands space, we will have:", "As we can see, the number of errors was decreased by more than two times. Accuracy raised over 81%. It means using this unsupervised learning we managed to understand the data better and to classify the 10 handwritten digits much better than a set of not connected points.", "A quick check on EMNIST dataset with the same amount of train/test data \u2014 60K/10K.", "Reproduced glyphs for the training data:", "This was just an example of what is possible if we stop to simply reuse existing vanilla NN blocks and start to do something specific to the problem. End-to-end networks cannot work in an efficient way, as efficiency is an indication of Intelligence that cannot be Artificial. It is just an Intelligence that Data Scientist or future Differential Programming Developer can add to the solution. If we build the model in the same way as the world is built, we need to make everything connected, like a quantum field, and this connection is the ability to take derivatives. Multiple derivatives in one point give information about the whole universe.", "Next steps for this particular model may include:", "If you have personal questions contact me on Linkedin or Facebook, where sometimes I post short news and thoughts about AI.", "I want to thank my colleagues Alex Simkiv, Mykola Kozlenko, Volodymyr Sendetskyi, Viach Bosyi, and Nazar Savchenko for fruitful discussions, cooperation, and helpful tips as well as the entire MindCraft.ai team for their constant support.Andy Bosyi,CEO, MindCraft.aiInformation Technology & Data Science", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Crafting AI engines for data-driven business | Data Science Consulting | Interaction questions andy.bosyi@mindcraft.ai, founder at MindCraft.ai."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe1b0fabe67bf&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@andy.bosyi?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andy.bosyi?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Andy Bosyi"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8bc8d2a62041&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&user=Andy+Bosyi&userId=8bc8d2a62041&source=post_page-8bc8d2a62041----e1b0fabe67bf---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba", "anchor_text": "https://towardsdatascience.com/active-learning-on-mnist-saving-on-labeling-f3971994c7ba"}, {"url": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-MNIST-position.ipynb", "anchor_text": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-MNIST-position.ipynb"}, {"url": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-MNIST-velocity.ipynb", "anchor_text": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-MNIST-velocity.ipynb"}, {"url": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-EMNIST-velocity.ipynb", "anchor_text": "https://github.com/andy-bosyi/articles/blob/master/DiffProg-EMNIST-velocity.ipynb"}, {"url": "https://skymind.ai/wiki/differentiableprogramming", "anchor_text": "https://skymind.ai/wiki/differentiableprogramming"}, {"url": "https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html", "anchor_text": "https://fluxml.ai/2019/02/07/what-is-differentiable-programming.html"}, {"url": "https://www.linkedin.com/in/andybosyi/", "anchor_text": "Linkedin"}, {"url": "https://www.facebook.com/profile.php?id=100013517439841", "anchor_text": "Facebook"}, {"url": "https://www.linkedin.com/in/oleksiy-simkiv/", "anchor_text": "Alex Simkiv"}, {"url": "https://www.linkedin.com/in/mykola-kozlenko/", "anchor_text": "Mykola Kozlenko"}, {"url": "https://medium.com/@volodymyrsendetskyi", "anchor_text": "Volodymyr Sendetskyi"}, {"url": "https://www.linkedin.com/in/nazar-savchenko/", "anchor_text": "Nazar Savchenko"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e1b0fabe67bf---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e1b0fabe67bf---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----e1b0fabe67bf---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&user=Andy+Bosyi&userId=8bc8d2a62041&source=-----e1b0fabe67bf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&user=Andy+Bosyi&userId=8bc8d2a62041&source=-----e1b0fabe67bf---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe1b0fabe67bf&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e1b0fabe67bf---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e1b0fabe67bf--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andy.bosyi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@andy.bosyi?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andy Bosyi"}, {"url": "https://medium.com/@andy.bosyi/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "55 Followers"}, {"url": "http://MindCraft.ai", "anchor_text": "MindCraft.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F8bc8d2a62041&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&user=Andy+Bosyi&userId=8bc8d2a62041&source=post_page-8bc8d2a62041--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F8bc8d2a62041%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdifferentiable-programming-inverse-graphics-autoencoder-e1b0fabe67bf&user=Andy+Bosyi&userId=8bc8d2a62041&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}