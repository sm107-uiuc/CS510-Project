{"url": "https://towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055", "time": 1683000155.961767, "path": "towardsdatascience.com/ai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055/", "webpage": {"metadata": {"title": "AI Learning to land a Rocket | Reinforcement Learning | Towards Data Science | Towards Data Science", "h1": "AI Learning to land a Rocket(Lunar Lander) | Reinforcement Learning", "description": "Reinforcement Learning, Lunar Lander, OpenAI gym, Deep Q-Network(DQN), Artificial Intelligence, AI, Machine Learning"}, "outgoing_paragraph_urls": [{"url": "https://github.com/fakemonk1/Reinforcement-Learning-Lunar_Lander", "anchor_text": "Github Repository", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "anchor_text": "artificial intelligence", "paragraph_index": 6}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "AlphaGo Zero", "paragraph_index": 7}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "AlphaGo", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Branching_factor", "anchor_text": "branching factor", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "alpha-beta pruning", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Tree_traversal", "anchor_text": "tree traversal", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Heuristic", "anchor_text": "heuristic", "paragraph_index": 7}, {"url": "https://github.com/openai/gym", "anchor_text": "OpenAI gym\u2019s GitHub page", "paragraph_index": 8}, {"url": "http://gym.openai.com/docs/", "anchor_text": "OpenAI\u2019s documentation", "paragraph_index": 8}, {"url": "http://arxiv.org/abs/1509.06461", "anchor_text": "Double Q-learning", "paragraph_index": 25}, {"url": "http://arxiv.org/abs/1511.05952", "anchor_text": "Prioritized Experience Replay", "paragraph_index": 25}, {"url": "http://arxiv.org/abs/1511.06581", "anchor_text": "Dueling NetworkArchitecture", "paragraph_index": 25}, {"url": "http://arxiv.org/abs/1509.02971", "anchor_text": "Extension to Continuous Action Space", "paragraph_index": 25}], "all_paragraphs": ["In this article, we will cover a brief introduction to Reinforcement Learning and will solve the \u201cLunar Lander\u201d Environment in OpenAI gym by training a Deep Q-Network(DQN) agent.", "We will see how this AI agent initially does not anything about how to control and land a rocket, but with time it learns from its mistakes and start to improve its performance and in the end learns to fully control the rocket and perfectly lands it.", "Reinforcement Learning is a massive topic and we are not going to cover everything here in detail. Instead, this article aims to get our hands dirty with some practical examples of reinforcement learning and show the implementation of RL in solving real-world use cases.", "We will discuss the rationale behind using the DQN and will cover the Experience Replay and Exploration-Exploitation dilemma encountered while training the Neural Network. In the last, we will discuss the agent\u2019s training and testing performance and the effect of hyper-parameter in the agent\u2019s performance.", "The full code can be found here on Github Repository.", "Reinforcement learning is one of the most discussed, followed, and contemplated topics in artificial intelligence (AI) as it has the potential to transform most businesses.", "At the core of reinforcement learning is the concept that optimal behavior or action is reinforced by a positive reward. Similar to toddlers learning how to walk who adjust actions based on the outcomes they experience such as taking a smaller step if the previous broad step made them fall. Machines and AI agents use reinforcement learning algorithms to determine the ideal behavior based upon feedback from the environment. It\u2019s a form of machine learning and therefore a branch of artificial intelligence.", "An example of reinforcement Learning in Action is AlphaGo Zero which was in the headlines in 2017. AlphaGo is a bot developed by Deepmind that leveraged reinforcement learning and defeated a world champion at the ancient Chinese game of Go. This is the first time artificial intelligence (AI) defeated a professional Go player. Go is considered much more difficult for computers to win than other games such as chess because its much larger branching factor makes it prohibitively difficult to use traditional AI methods such as alpha-beta pruning, tree traversal, and heuristic search.", "We are using the \u2018Lunar Lander\u2019 environment from OpenAI gym. This environment deals with the problem of landing a lander on a landing pad. The steps to set up this environment are mentioned in the OpenAI gym\u2019s GitHub page and on OpenAI\u2019s documentation. Following are the env variables in brief to understand the environment we are working in.", "The deep Q-learning algorithm that includes experience replay and \u03f5-greedy exploration is as follows:", "For running the complete experiment for the \u2018Lunar Landing\u2019 environment, we will first train a benchmark model and then do more experiments to find out the effects of changing the hyperparameters on the model performance.", "Initially, as we can see below the agent is very bad at landing, it\u2019s taking random actions to control the rocket and tries to land it. It fails most of the time and receives negative rewards for crashing the rocket.", "For training the model there is no rule of thumb to find out how many hidden layers you need in a neural network. I have conducted different experiments to try different combinations of node sizes for input and hidden layers. The following benchmark model was finalized based on parameters like training time, number of episodes required for training, and trained model performance.", "Still, this model was sometimes diverging after an average reward of 170 and was taking more than 1000 episodes to diverge. I figured out that this behavior might be attributed to the overtraining of the model and implemented \u2018Early Stopping\u2019. Early Stopping is the practice to stop the neural networks from overtraining. To implement this, I avoided training the model for a specific episode if the average of the last 10 rewards is more than 180.", "Buffer capacity size is chosen of size 500000 to avoid overflow occurring because of large experience tuple. Model is trained for the maximum episode count of 2000 and stopping criteria for the trained model is the average reward of 200 for the last 100 episodes.", "Final benchmark model has the following hyperparameters:", "After around 300 training episodes, it starts learning how to control and land the rocket.", "After 600 the agent is fully trained. It learns to handle the rocket perfectly and lands the rocket perfectly each time.", "The above figure shows the reward values per experience at the time of training. Blue lines denote the reward for each training episode and the orange line shows the rolling mean of the last 100 episodes. The agent keeps learning with the time and the value of the rolling mean increases with the training episodes.", "The average reward in the earlier episodes is mostly negative because the agent has just started learning. Eventually, The agent starts performing relatively better and the average reward starts going up and becoming positive after 300 episodes. After 514 episodes the rolling mean crosses 200 and the training concludes. There are a couple of episodes where the agent has received negative awards at this time, but I believe if the agent is allowed to continue training, these instances will reduce.", "The above figure shows the performance of the trained model for 100 episodes in the Lunar Lander environment. The trained model is performing well in the environment with all the rewards being positive. The average reward for 100 testing episodes is 205.", "The learning rate, set between 0 and 1 and is defined as how much we accept the new value vs the old value. This value then gets added to our previous q-value which essentially moves it in the direction of our latest update. Setting it to 0 means that the Q-values are never updated, hence nothing is learned. Setting a high value such as 0.9 means that learning can occur quickly.", "For validating the effect of the different learning rates on the model performance, I have trained different agents with different learning rates. Learning rates chosen for this experiment are 0.0001, 0.001, 0.01, 0.1. Best performance is observed for the middle value of the learning rate of 0.001. The orange line in figure 3 corresponds to this value and provides the maximum reward. The agent is not able to learn at the higher learning rate and the reward values are diverging.", "The discount factor affects how much weight it gives to future rewards in the value function. A discount factor \ud835\udefe=0 will result in state/action values representing the immediate reward, while a higher discount factor \ud835\udefe=0.9 will result in the values representing the cumulative discounted future reward an agent expects to receive. The below figure shows the variation in the model performance for a different discount factor. The agent has the best performance for the gamma value of 0.99 which is represented by the blue line.", "As discussed above, \u03b5 is the probability where we do not go with the \u201cgreedy\u201d action with the highest Q-value but rather choose a random action. epsilon(\u03b5) decay is the decay rate by which this value decreases after an episode. Figure 5 shows the variations in the rewards values for different values of the epsilon(\u03b5) decay. worst agent performance is observed for epsilon decay of 0.999 and the best performance is for epsilon decay value of 0.9 which is shown in red color in figure 3. This behavior might be because this value of epsilon decay is providing a better balance between Exploration-Exploitation.", "There have been many advancements in the deep Q-learning since its first introduction. In the next article, I will experiment with more advancements like Double Q-learning, Prioritized Experience Replay, Dueling NetworkArchitecture, and Extension to Continuous Action Space.", "P.S. This is my first article on medium. Please let me know your views. Please comment if you find any bug or have any idea on the code/algorithm improvement.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F84d61f97d055&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----84d61f97d055--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@fakemonk?source=post_page-----84d61f97d055--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fakemonk?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Ashish Gupta"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a6eb9ff2574&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&user=Ashish+Gupta&userId=3a6eb9ff2574&source=post_page-3a6eb9ff2574----84d61f97d055---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.nasa.gov/image-feature/intuitive-machines-concept-for-a-commercial-lunar-lander", "anchor_text": "NASA"}, {"url": "https://github.com/fakemonk1/Reinforcement-Learning-Lunar_Lander", "anchor_text": "Github Repository"}, {"url": "https://en.wikipedia.org/wiki/Machine_learning", "anchor_text": "machine learning"}, {"url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "anchor_text": "artificial intelligence"}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "AlphaGo Zero"}, {"url": "https://deepmind.com/blog/alphago-zero-learning-scratch/", "anchor_text": "AlphaGo"}, {"url": "https://en.wikipedia.org/wiki/Branching_factor", "anchor_text": "branching factor"}, {"url": "https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning", "anchor_text": "alpha-beta pruning"}, {"url": "https://en.wikipedia.org/wiki/Tree_traversal", "anchor_text": "tree traversal"}, {"url": "https://en.wikipedia.org/wiki/Heuristic", "anchor_text": "heuristic"}, {"url": "https://github.com/openai/gym", "anchor_text": "OpenAI gym\u2019s GitHub page"}, {"url": "http://gym.openai.com/docs/", "anchor_text": "OpenAI\u2019s documentation"}, {"url": "http://arxiv.org/abs/1509.06461", "anchor_text": "Double Q-learning"}, {"url": "http://arxiv.org/abs/1511.05952", "anchor_text": "Prioritized Experience Replay"}, {"url": "http://arxiv.org/abs/1511.06581", "anchor_text": "Dueling NetworkArchitecture"}, {"url": "http://arxiv.org/abs/1509.02971", "anchor_text": "Extension to Continuous Action Space"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----84d61f97d055---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----84d61f97d055---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----84d61f97d055---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/openai?source=post_page-----84d61f97d055---------------openai-----------------", "anchor_text": "OpenAI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----84d61f97d055---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&user=Ashish+Gupta&userId=3a6eb9ff2574&source=-----84d61f97d055---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&user=Ashish+Gupta&userId=3a6eb9ff2574&source=-----84d61f97d055---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F84d61f97d055&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----84d61f97d055---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----84d61f97d055--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----84d61f97d055--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----84d61f97d055--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----84d61f97d055--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----84d61f97d055--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fakemonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@fakemonk?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashish Gupta"}, {"url": "https://medium.com/@fakemonk/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "89 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a6eb9ff2574&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&user=Ashish+Gupta&userId=3a6eb9ff2574&source=post_page-3a6eb9ff2574--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F3a6eb9ff2574%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fai-learning-to-land-a-rocket-reinforcement-learning-84d61f97d055&user=Ashish+Gupta&userId=3a6eb9ff2574&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}