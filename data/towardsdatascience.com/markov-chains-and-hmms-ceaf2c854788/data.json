{"url": "https://towardsdatascience.com/markov-chains-and-hmms-ceaf2c854788", "time": 1682996069.4555142, "path": "towardsdatascience.com/markov-chains-and-hmms-ceaf2c854788/", "webpage": {"metadata": {"title": "Markov Chains and HMMs. In this article, we\u2019ll focus on Markov\u2026 | by Ma\u00ebl Fabien | Towards Data Science", "h1": "Markov Chains and HMMs", "description": "In this article, we\u2019ll focus on Markov Models, where an when they should be used, and Hidden Markov Models. This article will focus on the theoretical part. In a second article, I\u2019ll present Python\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["In this article, we\u2019ll focus on Markov Models, where an when they should be used, and Hidden Markov Models. This article will focus on the theoretical part. In a second article, I\u2019ll present Python implementations of these subjects.", "Markov Models, and especially Hidden Markov Models (HMM) are used for :", "I publish all my articles and the corresponding code on this repository :", "Don\u2019t hesitate to star the repo :)", "Let\u2019s start by defining what a stochastic model is. It is essentially a discrete time process indexed at times 1,2,\u2026 that takes values, called \u201cstates\u201d, which are observed: q1, q2,\u2026. The states simply correspond to the actual values of the process, usually defined by a finite space: S=1,\u2026Q.", "The process starts at an initial state q1. Then, according to transition probabilities, we move between the states. We can compute the probability of a sequence of states using Bayes Rule :", "In order to characterize the model, we need :", "As you might guess, this is complex to achieve since we need to know a lot of parameters.", "Discrete Time Markov Chain (DTMC) are time and event discrete stochastic process. Markov Chains rely on the Markov Property that there is a limited dependence within the process :", "Let\u2019s illustrate this: Consider a simple maze in which a mouse is trapped. We will denote qt the position of the maze in which the mouse stands after t steps. We will assume that the mouse does not have a memory of the steps it took within the maze. It simply goes to the position randomly, following the probability written next to each move.", "The states here could represent many things, including in NLP. For example, we could have :", "And we would be interested in the probabilities have a verb after a noun for example.", "A Discrete Time Markov chain is said to be homogeneous if its transition probabilities do not depend on the time t :", "We can summarize the process is a transition matrix denoted A=[aij], i \u2208 1\u2026Q, j \u2208 1\u2026Q. A transition matrix is stochastic if :", "In our example, the transition matrix would be :", "Note that if A is stochastic, then A^n is stochastic.", "There are several ways to describe a state. Let pii be the probability of returning to state i after leaving i :", "Therefore, a state is positive recurrent if the average time before return to this same state denoted Tii is finite.", "A DTMC is irreducible if a state j can be reached in a finite number of steps from any other state i. An irreducible DTMC is, in fact, a strongly connected graph.", "A state in a discrete-time Markov chain is periodic if the chain can return to the state only at multiples of some integer larger than 1.", "Otherwise, it is called aperiodic. A state with a self-loop is always aperiodic.", "Let Ti be the time spent in state i before jumping to other states.", "Then, Ti, the sojourn time, follows a geometric distribution :", "The expected average time spent is E(T)=1 / aii", "The probability of going from i to j in m steps is denoted by :", "We can see a22(4) as the probability for the mouse to be in position 2 at time t=4. Therefore, the probability of going from i to j in exactly n steps is given by fij(n) where :", "Let \u03c0i(n) be the probability of being in state i at time n : \u03c0i(n)=P(Xn=i)", "For an irreducible/aperiodic DTMC, the distribution \u03c0(n) converges to a limit vector \u03c0 which is independent of \u03c0(0) and is the unique solution of: \u03c0 = \u03c0P", "\u03c0i is also called stationary probabilities, steady state or equilibrium distribution.", "To simulate the path of the mouse in the maze, we might want to generate sequences.", "When we want to generate a sequence, we start from an initial state q1=1 for example. The general idea is that :", "Suppose we are given the following simple model :", "This corresponds to the following matrix A and initial vector of probabilities \u03c0:", "The generator works as follows, by drawing successively random number of identifying which transition refers to.", "At the first step, we pick a random number and see where it falls in the initial vector of probabilities. This gives us our first state.", "Then, we pick the following number, which corresponds, from the state q1, to the transition probabilities (first row of matrix A). If the value is smaller than 0.3, we stay in q1. Else, we move to q2. And so on\u2026", "The aim of decoding a sequence is to identify the most likely path that lead to the current state. For example, if the mouse is in state 3 and went there in 5 steps, you want to identify the most likely path.", "Markov Chains can be used :", "Hidden Markov Models (HMM) are widely used for :", "I recommend checking the introduction made by Luis Serrano on HMM.", "We will be focusing on Part-of-Speech (PoS) tagging. Part-of-speech tagging is the process by which we are able to tag a given word as being a noun, pronoun, verb, adverb\u2026", "PoS can, for example, be used for Text to Speech conversion or Word sense disambiguation.", "In this specific case, the same word bear has completely different meanings, and the corresponding PoS is therefore different.", "Let\u2019s consider the following scenario. In your office, there are 2 colleagues that talk a lot. You know they either talk about Work or Holidays. Since they look cool, you\u2019d like to join them. But you\u2019re too far to understand the whole conversation, and you only get some words of the sentence", "Before joining the conversation, in order not to sound too weird, you\u2019d like to guess whether he talks about Work or Holidays. For example, here is the kind of sentence your friends might be pronouncing :", "You only hear distinctively the words python or bear, and try to guess the context of the sentence. Since your friends are Python developers, when they talk about work, they talk about Python 80% of the time.", "These probabilities are called the Emission Probabilities.", "You listen to their conversations and keep trying to understand the subject every minute. There is some sort of coherence in the conversation of your friends. Indeed, if one hour they talk about work, there is a lower probability that the next minute they talk about holidays.", "We can define what we call the Hidden Markov Model for this situation :", "The probabilities to change the topic of the conversation or not are called the transition probabilities.", "An HMM \u03bb is a sequence made of a combination of 2 stochastic processes :", "An HMM model is defined by :", "What are the main hypothesis behind HMMs?", "A HMM is a subcase of Bayesian Networks.", "Transition probabilities are based on the observations we have made. We can suppose that after carefully listening, every minute, we manage to understand the topic they were talking about. This does not give us the full information on the topic they are currently talking about though.", "You have 15 observations, taken over the last 15 minutes, W denotes Work and H Holidays.", "We notice that in 2 cases out of 5, the topic Work lead to the topic Holidays, which explains the transition probability in the graph above.", "Well, since we have observations on the topic they were discussing, and we observe the words that were used during the discussion, we can define estimates of the emission probabilities :", "Suppose that you have to grab a coffee, and when you come back, they are still talking. You have no clue what they are talking about! What is at that random moment the probability that they are talking about Work or Holidays?", "We can count from the previous observations: 10 times they were talking about Holidays, 5 times about Work. Therefore, it states that we have 1/3 chance that they talk about Work and 2/3 chance that they talk about Holidays.", "If you hear the word \u201cPython\u201d, the probability that the topic is Work or Holidays is defined by Bayes Theorem!", "Let\u2019s start with 2 observations in a row. Let\u2019s suppose that we hear the words \u201cPython\u201d and \u201cBear\u201d in a row. What are the possible combinations?", "These scenarios can be summarized this way :", "Therefore, the most likely hidden state is Holidays and Holidays. What if you hear more than 2 words? Let\u2019s say 50? It becomes really challenging to compute all the possible paths! This is why the Viterbi Algorithm was introduced, to overcome this issue.", "The main idea behind the Viterbi Algorithm is that when we compute the optimal decoding sequence, we don\u2019t keep all the potential paths, but only the path corresponding to the maximum likelihood.", "Here\u2019s how it works. We start with a sequence of observed events, say Python, Python, Python, Bear, Bear, Python. This sequence corresponds simply to a sequence of observations : P(o1, o2,\u2026, oT \u2223 \u03bbm)", "For the first observation, the probability that the subject is Work given that we observe Python is the probability that it is Work times the probability that it is Python given that it is Work.", "The most likely sequence of states simply corresponds to :", "We can then move on to the next observation. Here\u2019s what will happen :", "For each position, we compute the probability using the fact that the previous topic was either Work or Holidays, and for each case, we only keep the maximum since our aim is to find the maximum likelihood. Therefore, the next step is to estimate the same thing for the Holidays topic and keep the maximum between the 2 paths.", "If you decode the whole sequence, you should get something similar to this (I\u2019ve rounded the values at each step, so you might get slightly different results):", "The most likely sequence when we observe Python, Python, Python, Bear, Bear, Python is therefore Work, Work, Work, Holidays, Holidays, Holidays.", "If you finally go talk to your colleagues after such a long stalking time, you should expect them to be talking about Holidays :)", "The joint probability of the best sequence of potential states ending in state i at time t and corresponding to observations o1,\u2026,oT is denoted by \u03b4T(i). This is one of the potential paths described above.", "By recursion, it can be shown that :", "Where bj denotes a probability of the matrix of observations B and aij denotes a value of the transition matrix for unobserved sequence. Those parameters are estimated from the sequence of observations and states available. The \u03b4 is simply the maximum we take at each step when moving forward.", "I won\u2019t really go into further details here. You should simply remember that there are 2 ways to solve Viterbi, forward (as we have seen) and backward.", "When we only observe partially the sequence and face incomplete data, the EM algorithm is used.", "As we have seen with Markov Chains, we can generate sequences with HMMs. In order to do so, we need to :", "How does the process work? As stated above, this is now a 2 step process, where we first generate the state, then the observation.", "In this article, we covered the basic theory of Markov Chains and HMMs, including terminology, hypothesis, properties, sequence generation, and decoding.", "I hope this introduction to Markov Chains and Hidden Markov Models was helpful. I have listed my sources in the section below.", "In the next article, I\u2019ll try to illustrate these concepts in Python.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fceaf2c854788&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mael4impact?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Ma\u00ebl Fabien"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddcee06de4c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=post_page-ddcee06de4c8----ceaf2c854788---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/towards-data-science/inside-ai/home", "anchor_text": "Inside AI"}, {"url": "https://maelfabien.github.io/machinelearning/HMM_1/#", "anchor_text": "https://maelfabien.github.io/machinelearning/HMM_1/#"}, {"url": "https://github.com/maelfabien/Machine_Learning_Tutorials", "anchor_text": "maelfabien/Machine_Learning_TutorialsThis repo contains exercises, code, tutorials and articles of my personal blog \u2014 maelfabien/Machine_Learning_Tutorialsgithub.com"}, {"url": "https://www.emse.fr/~xie/SJTU/Ch4DMC.ppt", "anchor_text": "https://www.emse.fr/~xie/SJTU/Ch4DMC.ppt"}, {"url": "http://www.math.chalmers.se/Stat/Grundutb/CTH/mve220/1617/redingprojects16-17/IntroMarkovChainsandApplications.pdf", "anchor_text": "http://www.math.chalmers.se/Stat/Grundutb/CTH/mve220/1617/redingprojects16-17/IntroMarkovChainsandApplications.pdf"}, {"url": "https://www.youtube.com/watch?v=kqSzLo9fenk", "anchor_text": "https://www.youtube.com/watch?v=kqSzLo9fenk"}, {"url": "https://maelfabien.github.io/machinelearning/HMM_1/#", "anchor_text": "https://maelfabien.github.io/machinelearning/HMM_1/#"}, {"url": "https://maelfabien.github.io/machinelearning/HMM_2/#", "anchor_text": "https://maelfabien.github.io/machinelearning/HMM_2/#"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----ceaf2c854788---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----ceaf2c854788---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/markov-chains?source=post_page-----ceaf2c854788---------------markov_chains-----------------", "anchor_text": "Markov Chains"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ceaf2c854788---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/inside-ai?source=post_page-----ceaf2c854788---------------inside_ai-----------------", "anchor_text": "Inside Ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=-----ceaf2c854788---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=-----ceaf2c854788---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fceaf2c854788&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----ceaf2c854788---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ceaf2c854788--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ceaf2c854788--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ceaf2c854788--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mael4impact?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ma\u00ebl Fabien"}, {"url": "https://medium.com/@mael4impact/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "749 Followers"}, {"url": "http://biped.ai", "anchor_text": "biped.ai"}, {"url": "https://bento.me/mael4impact", "anchor_text": "https://bento.me/mael4impact"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fddcee06de4c8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=post_page-ddcee06de4c8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc8a99819d6f0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmarkov-chains-and-hmms-ceaf2c854788&newsletterV3=ddcee06de4c8&newsletterV3Id=c8a99819d6f0&user=Ma%C3%ABl+Fabien&userId=ddcee06de4c8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}