{"url": "https://towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f", "time": 1683010781.6512341, "path": "towardsdatascience.com/reviewing-essential-concepts-from-part-1-e28234ee7f4f/", "webpage": {"metadata": {"title": "Reviewing Essential Concepts. Mathematical Notation Updated and a\u2026 | by Jordi TORRES.AI | Towards Data Science", "h1": "Reviewing Essential Concepts", "description": "This post is a preface of new parts of the \u201cDeep Reinforcement Learning Explained\u201d series where we will introduce the implementation of Reinforcement Learning classical methods, as Monte Carlo, SARSA\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd", "anchor_text": "Deep Reinforcement Learning Explained", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2", "paragraph_index": 1}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction", "paragraph_index": 2}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake", "paragraph_index": 5}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI Gym", "paragraph_index": 5}, {"url": "https://gym.openai.com/envs/CartPole-v0", "anchor_text": "Cart-Pole balancing problem", "paragraph_index": 5}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "this document", "paragraph_index": 8}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Post 8", "paragraph_index": 30}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Post 8", "paragraph_index": 35}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "In the following post", "paragraph_index": 36}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "the next post", "paragraph_index": 37}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 38}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 38}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 39}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 40}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 43}], "all_paragraphs": ["This post is a preface of new parts of the \u201cDeep Reinforcement Learning Explained\u201d series where we will introduce the implementation of Reinforcement Learning classical methods, as Monte Carlo, SARSA or Q-learning, among others. In this post we will review and update the mathematical notation introduced in previous posts.", "In post 2 we have seen that we can use a Markov Decision Process (MDP) as a formal definition of the problem that we\u2019d like to solve with Reinforcement Learning. A MDP is defined by 5 parameters <S,A,R,p,\u03b3>, where each one indicates:", "Remember that we tend to use the notation used in the textbook Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto. This book is \u201cthe\u201d classic text with an excellent introduction to Reinforcement Learning fundamentals.", "The main definitions and mathematical symbols that we introduced in previous posts are:", "Before continuing let us briefly add how the discount rate behaves in a continuing task not covered in previous posts.", "In Part 1 of this series, we used an episodic task, the Frozen-Lake Environment, a simple grid-world Environment from OpenAI Gym, a toolkit for developing and comparing RL algorithms. In this section we will introduce a continuing task using another Environment, the Cart-Pole balancing problem:", "As it is shown in the previous figure, a cart is positioned on a frictionless track along the horizontal axis, and a pole is anchored to the top of the cart. The objective is to keep the pole from falling over by moving the cart either left or right, and without falling off the track.", "The system is controlled by applying a force of +1 (left) or -1 (right) to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time-step that the pole remains upright, including the final step of the episode. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.", "The observation space for this Environment at each time point is an array of 4 numbers. At every time step, you can observe its position, velocity, angle, and angular velocity. These are the observable states of this world. You can look up what each of these numbers represents in this document. Notice the minimum (-Inf) and maximum (Inf) values for both Cart Velocity and the Pole Velocity at Tip. Since the entry in the array corresponding to each of these indices can be any real number, that means, the state space is infinite!", "At any state, the cart only has two possible actions: move to the left or move to the right. In other words, the state-space of the Cart-Pole has four dimensions of continuous values and the action-space has one dimension of two discrete values.", "Which discount rates would encourage the Agent to keep the pole balanced in our example of continuing task for as long as possible?", "With any discount rate \u03b3>0, the Agent receives a positive reward for each time-step where the pole has not yet fallen. Thus, the agent will try to keep the pole balanced for as long as possible.", "However, imagine that the reward signal is amended to only give a reward to the Agent at the end of an episode. In other words that is, the reward is 0 for every time step, with the exception of the final time step, when the episode terminates, and then the Agent receives a reward of +1.", "In this case, if the discount rate is \u03b3=1, the agent will always receive a reward of +1 (no matter what actions it chooses during the episode), and so the reward signal will not provide any useful feedback to the agent.", "If the discount rate is \u03b3<1 the Agent will try to terminate the episode as soon as possible (by either dropping the pole quickly or moving off the edge of the track). Thus, in this case, we must redesign the reward signal!", "The solution to this problem, that means a series of actions that need to be learned by the Agent towards the pursuit of a goal, is determined by the Policy. In the next section, we will continue a little further in the formal definition of the solution to this problem.", "The policy is the strategy (e.g. some set of rules) that the Agent employs to determine the next action based on the current state. Typically denoted by \ud835\udf0b(\ud835\udc4e|\ud835\udc60), the Greek letter pi, a policy is a function that determines the next action a to take given a state s.", "The simplest kind of policy is a mapping from the set of environment states S to the set of possible actions A. We call this kind of policy a deterministic policy. But in post 2 we also introduced that the policy \ud835\udf0b(\ud835\udc4e|\ud835\udc60) can be defined as probability and not as concrete action. In other words that are, a stochastic policy that has a probability distribution over actions that an Agent can take at a given state.", "The stochastic policy will allow the Agent to choose actions randomly. More formally, We define a stochastic policy as a mapping that accepts an Environment state S and action A and returns the probability that the agent takes action A while in state S:", "During the learning process, the policy \ud835\udf0b may change as the Agent gains more experience. For example, the Agent may start from a random policy, where the probability of all actions is uniform; meanwhile, the Agent will hopefully learn to optimize its policy toward reaching the optimal policy.", "Now that we know how to specify a policy, what steps can we take to make sure that the Agent\u2019s policy is the best one? We will use the state-value function and action-value function already introduced in Post 2.", "The state-value function, also referred to as the value function, or even the V-function, measures the goodness of each state, it tells us the total return we can expect in the future if we start from that state.", "For each state s, the state-value function tells us the expected discounted return G, if the agent started in that state s, and then use the policy to choose its actions for all time steps. It is important to note that the state value function will always correspond to a particular policy, so if we change the policy, we change the state-value function. For this reason, we typically denote the function with the lowercase v with the corresponding policy \ud835\udf0b in the subscript and defined formally by:", "where \ud835\udd3c[\u00b7] denotes the expected value of a random variable given that the agent follows policy \ud835\udf0b, and t is any time step. As we introduced in Post 8, it is used expectation \ud835\udd3c[.] in this definition because the environment transition function might act in a stochastic way.", "Also in post 2 we extended the definition of state-value function to state-action pairs, defining a value for each state-action pair, which is called the action-value function, also known as Q-function or simply Q. It defines the value of taking action a in state s under a policy \u03c0, as the expected Return G starting from s, taking the action a, and thereafter following policy \u03c0:", "We will use the uppercase or lowercase notation of the value functions interchangeably throughout this series: V(s) or v(s) and Q(s,a) or q(s,a)", "For a general MDP, we have to work in terms of an expectation, since it\u2019s not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in the previous post that the reward r and next state s\u2019 are chosen according to the one-step dynamics of the MDP. In this case, where the r and s\u2032 are drawn from a (conditional) probability distribution p(s\u2032,r\u2223s,a), the Bellman Expectation Equation expresses the value of any state s in terms of the expected immediate reward and the expected value of the next state (satisfying a recursive relationships).", "For the general case, where the Agent\u2019s policy \u03c0 is stochastic, the Agent selects action a with probability \u03c0(a\u2223s) when in state s, and the Bellman Expectation Equation can be expressed as:", "In this case, we multiply the sum of the reward and discounted value of the next state (r+\u03b3v\u03c0\u200b(s\u2032)) by its corresponding probability \u03c0(a\u2223s)p(s\u2032,r\u2223s,a) and sum over all possibilities to yield the expected value.", "We also have the Bellman equation for the action-value function:", "The goal of the Agent is to maximize the total cumulative reward in the long run. The policy, which maximizes the total cumulative reward is called the optimal policy. In Post 8 we introduced the \u201coptimal\u201d value functions.", "A policy \u03c0\u2032 is defined to be better than or equal to a policy \u03c0 if and only if v\u03c0\u2032\u200b(s)\u2265v\u03c0\u200b(s) for all s\u2208S. An optimal policy \u03c0\u2217\u200b satisfies \u03c0\u2217\u200b\u2265\u03c0 for all policies \u03c0. An optimal policy is guaranteed to exist but may not be unique.", "All optimal policies have the same state-value function v\u2217\u200b, called the optimal state-value function. A more formal definition for the optimal state-value functions could be:", "All optimal policies have the same action-value function q\u2217\u200b, called the optimal action-value function.", "This optima action-value are very useful in order to obtain the optimal policy. The Agent estimates it by interacting with the Environment. Once the agent determines the optimal action-value function q\u2217\u200b, it can quickly obtain an optimal policy \u03c0\u2217\u200b by setting:", "As we saw in Post 8 the Bellman equation is used to find the optimal values of the value functions in the algorithms to calculate them. A more formal expression could be:", "We have reached the end of this post!. In the following post, we are going to introduce the Monte Carlo Method, a learning method for estimating value functions and discovering optimal policies.", "See you in the next post!", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fe28234ee7f4f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----e28234ee7f4f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 12"}, {"url": "https://towardsdatascience.com/value-iteration-for-q-function-ac9e508d85bd", "anchor_text": "Deep Reinforcement Learning Explained"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1", "anchor_text": "Spanish version of this publication"}, {"url": "https://medium.com/aprendizaje-por-refuerzo/3-funciones-de-valor-y-la-ecuaci%C3%B3n-de-bellman-7b0ebfac2be1", "anchor_text": "3. Funciones de valor y la ecuaci\u00f3n de BellmanAcceso abierto al cap\u00edtulo 3 del libro Introducci\u00f3n al aprendizaje por refuerzo profundomedium.com"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2"}, {"url": "http://www.incompleteideas.net/book/the-book-2nd.html", "anchor_text": "Reinforcement Learning: An Introduction"}, {"url": "https://gym.openai.com/envs/FrozenLake-v0/", "anchor_text": "Frozen-Lake"}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI Gym"}, {"url": "https://gym.openai.com/envs/CartPole-v0", "anchor_text": "Cart-Pole balancing problem"}, {"url": "https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947", "anchor_text": "Source"}, {"url": "https://github.com/openai/gym/wiki/CartPole-v0", "anchor_text": "this document"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "post 2"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Post 8"}, {"url": "https://towardsdatascience.com/the-bellman-equation-59258a0d3fa7", "anchor_text": "Post 8"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "In the following post"}, {"url": "https://towardsdatascience.com/monte-carlo-methods-9b289f030c2e", "anchor_text": "the next post"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e28234ee7f4f---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----e28234ee7f4f---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e28234ee7f4f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----e28234ee7f4f---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----e28234ee7f4f---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e28234ee7f4f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----e28234ee7f4f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fe28234ee7f4f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----e28234ee7f4f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e28234ee7f4f--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freviewing-essential-concepts-from-part-1-e28234ee7f4f&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}