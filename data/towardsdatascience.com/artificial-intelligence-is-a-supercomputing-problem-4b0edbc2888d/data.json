{"url": "https://towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d", "time": 1683016316.358399, "path": "towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d/", "webpage": {"metadata": {"title": "Artificial Intelligence is a Supercomputing problem | by Jordi TORRES.AI | Towards Data Science", "h1": "Artificial Intelligence is a Supercomputing problem", "description": "[This post will be used in the master course Supercomputers Architecture at UPC Barcelona Tech with the support of the BSC] It\u2019s an exciting time for Artificial Intelligence. We have impressive\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture", "paragraph_index": 0}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 0}, {"url": "https://bsc.es", "anchor_text": "BSC", "paragraph_index": 0}, {"url": "https://bsc.es", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 2}, {"url": "https://www.bsc.es/discover-bsc/organisation/scientific-structure/emerging-technologies-artificial-intelligence/team-people", "anchor_text": "our research group at UPC & BSC", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/2010.00263.pdf", "anchor_text": "iven a video and a linguistic phrase, we show how to generate binary masks for the object to which the phrase refers", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)", "anchor_text": "John McCarthy", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt", "paragraph_index": 4}, {"url": "https://scholar.google.com/citations?hl=en&user=NkzyCvUAAAAJ&view_op=list_works&sortby=pubdate", "anchor_text": "Oriol Vinyals", "paragraph_index": 5}, {"url": "https://twitter.com/OriolVinyalsML/status/1253053130411032576", "anchor_text": "recent tweet", "paragraph_index": 5}, {"url": "https://openai.com/blog/ai-and-efficiency/", "anchor_text": "doubling every 16 months over a period of 7 years", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge", "anchor_text": "ImageNet", "paragraph_index": 7}, {"url": "https://www.youtube.com/watch?v=-fdexQBpRas&t=721s", "anchor_text": "excellent presentation at MIT", "paragraph_index": 8}, {"url": "https://twitter.com/DocXavi/status/1208077661173665794", "anchor_text": "a good friend of our research group", "paragraph_index": 10}, {"url": "https://blog.openai.com/ai-and-compute", "anchor_text": "graphic from OpenAI", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/FLOPS", "anchor_text": "petaflop", "paragraph_index": 15}, {"url": "https://en.wikipedia.org/wiki/Moore%27s_law", "anchor_text": "Moore\u2019s Law", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Gordon_Moore", "anchor_text": "Gordon Moore", "paragraph_index": 18}, {"url": "http://ai.eecs.umich.edu/people/conway/VLSI/BackgroundContext/SMErpt/AppB.pdf", "anchor_text": "Moore, G. Progress in digital integrated electronics. In Proceedings of the International Electronic Devices Meeting (Washington, D.C., Dec.). IEEE, New York, 1975, 1113.", "paragraph_index": 19}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext#R26", "anchor_text": "of processors created by companies like Intel", "paragraph_index": 20}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext", "anchor_text": "The current expectation is that the gap will continue to grow as CMOS technology approaches fundamental limits!", "paragraph_index": 20}, {"url": "https://ieeexplore.ieee.org/document/1050511", "anchor_text": "Dennard Scaling", "paragraph_index": 22}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext", "anchor_text": "\u201cGrowth of computer performance\u201d, created by Hennessy and Patterson", "paragraph_index": 24}, {"url": "https://events.linuxfoundation.org/ray-summit/", "anchor_text": "Ray Summit", "paragraph_index": 27}, {"url": "https://people.eecs.berkeley.edu/~istoica/", "anchor_text": "Ion Stoica", "paragraph_index": 27}, {"url": "https://en.wikipedia.org/wiki/Graphics_processing_unit", "anchor_text": "Nvidia\u2019s GPUs", "paragraph_index": 29}, {"url": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit", "anchor_text": "Google\u2019s TPUs", "paragraph_index": 29}, {"url": "https://www.nvidia.com/en-us/data-center/tensor-cores/", "anchor_text": "Tensor Cores", "paragraph_index": 30}, {"url": "https://blog.openai.com/ai-and-compute/", "anchor_text": "OpenAI graph", "paragraph_index": 32}, {"url": "https://www.bsc.es/user-support/power.php#systemoverview", "anchor_text": "BSC based on 4 GPUs", "paragraph_index": 33}, {"url": "https://www.redbooks.ibm.com/redpapers/pdfs/redp5472.pdf", "anchor_text": "This server, provided by IBM, has two CPUs, power 9, and 4 NVIDIA V100 GPUs", "paragraph_index": 34}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow of Pytorch", "paragraph_index": 39}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html", "anchor_text": "torch.nn.parallel.DistributedDataParallel", "paragraph_index": 39}, {"url": "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy", "anchor_text": "tf.distribute.MirroredStrategy", "paragraph_index": 39}, {"url": "https://developer.nvidia.com/cudnn", "anchor_text": "cuDNN", "paragraph_index": 40}, {"url": "https://www.nvidia.com/en-us/data-center/tensor-cores/", "anchor_text": "Tensor Cores", "paragraph_index": 40}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "our supercomputer in Barcelona", "paragraph_index": 41}, {"url": "https://www.ibm.com/products/power-systems-ac922/details", "anchor_text": "in the case of a server with the Power 9 we are talking about, we can reach a maximum of 6 GPUs", "paragraph_index": 44}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "where 54 servers are linked together with an InfiniBand network on optical fiber", "paragraph_index": 45}, {"url": "https://www.redbooks.ibm.com/redpapers/pdfs/redp5472.pdf", "anchor_text": "two Power 9 processors and four NVIDIA GPUs with 512 GB of main memory", "paragraph_index": 46}, {"url": "https://www.mellanox.com/pdf/whitepapers/IB_Intro_WP_190.pdf", "anchor_text": "InfiniBand", "paragraph_index": 47}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004c b2", "anchor_text": "Horovod", "paragraph_index": 48}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 48}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch", "paragraph_index": 48}, {"url": "https://mxnet.apache.org/versions/1.7.0/", "anchor_text": "MXNet", "paragraph_index": 48}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004c b2", "anchor_text": "Horovod", "paragraph_index": 49}, {"url": "https://en.wikipedia.org/wiki/Message_Passing_Interface", "anchor_text": "Message Passing Interface (MPI)", "paragraph_index": 49}, {"url": "https://developer.nvidia.com/nccl", "anchor_text": "NVIDIA NCCL2 library", "paragraph_index": 49}, {"url": "https://arxiv.org/pdf/2006.16668.pdf", "anchor_text": "recent paper from Google", "paragraph_index": 53}, {"url": "https://en.wikipedia.org/wiki/GPT-3", "anchor_text": "GTP-3", "paragraph_index": 53}, {"url": "https://cloud.google.com/tpu/docs/system-architecture", "anchor_text": "here", "paragraph_index": 56}, {"url": "https://arxiv.org/pdf/2104.04473.pdf", "anchor_text": "Efficient Large-Scale Language Model Training on GPU Clusters", "paragraph_index": 57}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning", "paragraph_index": 58}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "series of posts", "paragraph_index": 59}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "here", "paragraph_index": 59}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "DQN", "paragraph_index": 60}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "Asynchronous Actor-Critic method", "paragraph_index": 60}, {"url": "https://arxiv.org/pdf/1802.01561.pdf", "anchor_text": "IMPALA", "paragraph_index": 61}, {"url": "https://arxiv.org/pdf/1910.06591.pdf", "anchor_text": "SEED method", "paragraph_index": 64}, {"url": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html", "anchor_text": "some drawbacks present in IMPALA method", "paragraph_index": 65}, {"url": "https://grpc.io/", "anchor_text": "gRPC", "paragraph_index": 66}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer", "anchor_text": "Google breaks AI performance records in the Industry-standard benchmark MLPerf", "paragraph_index": 73}, {"url": "https://mlperf.org", "anchor_text": "MLPerf benchmark", "paragraph_index": 73}, {"url": "https://twitter.com/eturner303/status/1266264358771757057", "anchor_text": "the following tweet", "paragraph_index": 78}, {"url": "https://arxiv.org/abs/2011.12692", "anchor_text": "A new paper from Tencent", "paragraph_index": 79}, {"url": "https://www.top500.org", "anchor_text": "top500 list", "paragraph_index": 80}, {"url": "https://sc20.supercomputing.org", "anchor_text": "SC", "paragraph_index": 80}, {"url": "https://www.isc-hpc.com", "anchor_text": "ISC", "paragraph_index": 80}, {"url": "https://www.bsc.es/sites/default/files/public/gallery/2017_bsc_superordenador_marenostrum-4_barcelona-supercomputing-center.jpg", "anchor_text": "chapel of Torre Girona at UPC university campus", "paragraph_index": 82}, {"url": "https://www.bsc.es/discover-bsc/visit-our-supercomputer#virtual-visit-title", "anchor_text": "virtual visit", "paragraph_index": 82}, {"url": "https://www.lavanguardia.com/natural/20201101/4946606386/mundo-digital-nube-contaminante-electricidad-ranking-mundo.html", "anchor_text": "La Vanguardia appeared this article", "paragraph_index": 85}, {"url": "https://www.lavanguardia.com/natural/20201101/4946606386/mundo-digital-nube-contaminante-electricidad-ranking-mundo.html", "anchor_text": "The digital world is the third polluter on the planet", "paragraph_index": 85}, {"url": "https://arxiv.org/pdf/1906.02243.pdf", "anchor_text": "research by the University of Massachusetts", "paragraph_index": 86}, {"url": "https://www.scientific-computing.com/analysis-opinion/true-cost-ai-innov", "anchor_text": "Iceland\u2019s energy is sourced from 100 percent renewable geothermal and hydroelectric power, and its national grid is modern and reliable", "paragraph_index": 87}, {"url": "https://link.springer.com/article/10.1186/s13073-019-0689-8", "anchor_text": "with the help of Artificial Intelligence", "paragraph_index": 91}, {"url": "https://theconversation.com/artificial-intelligence-has-a-gender-bias-problem-just-ask-siri-123937", "anchor_text": "gender biases", "paragraph_index": 92}, {"url": "https://carnegieendowment.org/2019/09/17/global-expansion-of-ai-surveillance-pub-79847", "anchor_text": "monitored citizens at large without their informed consent", "paragraph_index": 92}, {"url": "https://www.aies-conference.com/2021/", "anchor_text": "Among many other bad things", "paragraph_index": 92}, {"url": "https://www.bsc.es/dominguez-bermudez-juan-luis", "anchor_text": "Juan Luis Dom\u00ednguez", "paragraph_index": 96}, {"url": "https://www.bsc.es/jover-alvarez-alvaro", "anchor_text": "Alvaro Jover Alvarez", "paragraph_index": 96}, {"url": "https://www.bsc.es/escobar-castells-miquel", "anchor_text": "Miquel Escobar Castells", "paragraph_index": 96}, {"url": "https://www.bsc.es/garcia-fuentes-raul", "anchor_text": "Raul Garcia Fuentes", "paragraph_index": 96}, {"url": "https://www.bsc.es/discover-bsc/organisation/scientific-structure/emerging-technologies-artificial-intelligence/team-people", "anchor_text": "Our research group at Barcelona Supercomputing Center", "paragraph_index": 97}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 97}, {"url": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html", "anchor_text": "Rich Sutton says recently", "paragraph_index": 98}, {"url": "https://arxiv.org/pdf/2002.03647.pdf", "anchor_text": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills", "paragraph_index": 99}, {"url": "https://icml.cc/", "anchor_text": "37th International Conference on Machine Learning (ICML2020)", "paragraph_index": 99}, {"url": "https://twitter.com/vcampos7", "anchor_text": "@vcampos7", "paragraph_index": 99}, {"url": "https://twitter.com/DocXavi", "anchor_text": "@DocXavi", "paragraph_index": 99}, {"url": "https://twitter.com/alexrtrott", "anchor_text": "@alexrtrott", "paragraph_index": 99}, {"url": "https://twitter.com/CaimingXiong", "anchor_text": "@CaimingXiong", "paragraph_index": 99}, {"url": "https://twitter.com/RichardSocher", "anchor_text": "@RichardSocher", "paragraph_index": 99}, {"url": "https://einstein.ai/", "anchor_text": "Salesforce Research", "paragraph_index": 99}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 100}, {"url": "https://en.wikipedia.org/wiki/Barcelona", "anchor_text": "Barcelona", "paragraph_index": 100}, {"url": "https://en.wikipedia.org/wiki/MareNostrum", "anchor_text": "MareNostrum", "paragraph_index": 100}, {"url": "https://en.wikipedia.org/wiki/TOP500", "anchor_text": "13th in the world", "paragraph_index": 100}, {"url": "https://www.upc.edu/en", "anchor_text": "The Polytechnic University of Catalonia", "paragraph_index": 101}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 103}], "all_paragraphs": ["[This post will be used in the master course Supercomputers Architecture at UPC Barcelona Tech with the support of the BSC]", "The next generation of Artificial Intelligence applications impose new and demanding computing infrastructures. How are the computer systems that support artificial intelligence? How did we get here? Who has access to these systems? What is our responsibility as Artificial Intelligence practitioners?", "It\u2019s an exciting time for Artificial Intelligence. We have impressive scientific data analysis systems at Barcelona Supercomputing Center in genomics, bioinformatics, astronomy, amount many others. Systems that can do amazing things that we didn\u2019t think were possible a few years ago.", "Also, for general purpose applications, we are moving very fast. For example, in Video Analytics, our research group at UPC & BSC obtains valuable video object segmentation results with referring expressions. Given a video and a linguistic phrase, we show how to generate binary masks for the object to which the phrase refers.", "The question is, why now? Artificial Intelligence has been around since the middle of the last century. John McCarthy coined the term Artificial Intelligence in the 1950s, being one of the founding fathers of Artificial Intelligence along with Marvin Minsky. Also, in 1958, Frank Rosenblatt built a prototype neuronal network, which he called the Perceptron. Besides, the key ideas of the Deep Learning neural networks for computer vision were already known in 1989; also, the fundamental algorithms of Deep Learning for time series such as LSTM were already developed in 1997, to give some examples. So, why now this Artificial Intelligence boom?", "Let\u2019s try to find out a trigger for the AI explosion. Oriol Vinyals suggest that Datasets play an important role according to a recent tweet from him:", "Clearly, the availability of big datasets has contributed to the Algorithmic efficiency in Deep Learning, doubling every 16 months over a period of 7 years:", "This means that operations required to train a classifier to AlexNet-level performance on ImageNet have decreased by a factor of 44x between 2012 and 2019.", "In this excellent presentation at MIT, Oriol Vinyals also add in this direction the important contribution big companies and universities with open-source projects like TensorFlow, Pytorch, MXNet, and so on. These DL frameworks allow that we now have access to a vast amount of essentially state-of-the-art components helping researchers to focus on the core algorithmic components we thought perhaps paying too much attention to the details on implementation, which helps accelerate progress in algorithms.", "Big datasets, and open-source DL framework, play an important role to create \u201cbig\u201d algorithms. But the current excitement is due to another crucial component, which was not present before 2012 when AlexNet won ImageNet. Which other things, besides data and algorithms, were now available?", "I don\u2019t want to refuse Oriol Vinyals affirmation; he is the boss in this field!!! and a good friend of our research group! ;-)", "The answer is BIG Computers. \u201cCOMPUTING POWER is a key component of the progress of Artificial Intelligence. Nowadays, Deep Learning or Reinforcement Learning is the result of mix these three components:", "How has computing evolved to meet the needs of artificial intelligence?", "Have a look at this graphic from OpenAI that has become very popular:", "Since 2012, the amount of computation required (or available) to generate artificial intelligence models has increased exponentially (The Y-axis is a logarithmic axis).", "A petaflop/s-day (pfs-day) consists of performing 10 to 15 operations per second for one day, or a total of about 10 to 20 operations.", "Also, during this period, these computing requirements for training the models have grown by more than 300,000x. The amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4x month doubling time.", "Let\u2019s go back a bit and see how computing has evolved. Much of the improvement in computer performance comes from decades of miniaturization of computer components. All of you have heard about Moore\u2019s Law. Right?", "In 1975, Intel founder Gordon Moore predicted the regularity of this miniaturization trend, now called Moore\u2019s law, which, until recently, doubled the number of transistors on computer chips every two years.", "Original paper: Moore, G. Progress in digital integrated electronics. In Proceedings of the International Electronic Devices Meeting (Washington, D.C., Dec.). IEEE, New York, 1975, 1113.", "Although Moore\u2019s Law held for many decades, it began to slow sometime around 2000 and by 2018 showed a roughly 15-fold gap between Moore\u2019s prediction and current capability (of processors created by companies like Intel). The current expectation is that the gap will continue to grow as CMOS technology approaches fundamental limits!", "Sadly, just when we need much faster machines for Deep Learning, Moore\u2019s law began to slow down!", "In reality, other important observations occur in the computer architecture community that accompanying Moore\u2019s Law: Dennard Scaling was a projection made by Robert Dennard, stating that as transistor density increased, power consumption per transistor would drop, so the power per mm2 of silicon would be near constant. Since the computational capability of a mm2 of silicon was increasing with each new generation of technology according Moore\u2019s Law, computers would become more energy efficient. However, Dennard scaling projection began to slow significantly in 2007 and its benefits disappeared around 2010.", "With the end of the Dennard Scaling, increasing the number of cores on a chip allowed the power to also increase at about the same rate. But the energy that goes into a processor must also be removed as heat. Therefore, multi-core processors are limited by heat dissipation power.", "In short, the result of applying all these observations can be summarized in the following graph based on the original one \u201cGrowth of computer performance\u201d, created by Hennessy and Patterson:", "In this graph, we can appreciate that in the 1980s and 90s, when all these laws and observations were alive, and well, we were turning transistors into fasts computers, so doubling performance about every 18 months at the time.", "What good times! What a longing!. Now we have only the same improvement of per two each 20 years approximately. In summary, from a factor of 2 every 18 months to a factor 1.05 every 18 months.", "In a very general way, taking the idea from a talk at Ray Summit by Professor Ion Stoica from Berkeley, we can visually represent the impact of computing performance growth in the previous graph (approximately). As can be seen, taking into account that we are moving in a logarithmic Y-axis, in no case, allow us to respond to the needs of AI algorithms.", "Well, while Moore\u2019s Law may have ended, the demand for increased compute has not. So, a question arises, how without Moore\u2019s Law to get faster machines?", "To address this challenge, computer architects have focused their attention on building domain-specific processors that trade generality for performance. The idea behind it is, \u201cDon\u2019t try to do everything, but just do a few things exceptionally\u201d. Companies have raced to build specialized processors, such as Nvidia\u2019s GPUs and Google\u2019s TPUs:", "What we mean by \u201cdoing a few things exceptionally\u201d? For instance, GPUs contain hundreds of Tensor Cores that operate on a 4x4 matrix, which greatly accelerates the computation of basic operations in Deep Learning, such as the multiplication of the data matrix by the weight matrix and then the sum of the bias.", "But in the end, specialized hardware is not enough. At the same time, accelerators like GPUs and TPUs bring more computational power to the table, they essentially help to prolong Moore\u2019s Law further into the future, not to fundamentally increase the rate of improvement.", "In a very general way, using the same OpenAI graph, we can visually represent the impact of the performance improvements of specialized architectures in relation to CPUs. But, as it can be seen, in no case, allow to respond to the needs of Deep Learning and Reinforcement Learning applications:", "Maybe we can put multiple domain-specific processors to work together?. Let\u2019s see a concrete example that we have at BSC based on 4 GPUs that can work in parallel:", "This server, provided by IBM, has two CPUs, power 9, and 4 NVIDIA V100 GPUs. Now, how can we employ these resources to improve computational speed? In the case of deep learning, we typically have two parallelism approaches that can be taken to accomplish this purpose:", "In the first approach, we have the different layers of the network distributed across different devices; meanwhile, in the second approach, we have the same model in every one of the GPUs, but they are each processing a separate piece of the data, a separate portion of the mini-batch.", "Model parallelism is very useful when we have a large model that might not fit in a single GPU memory.", "Data parallelism, however, is what most practitioners typically use to scale up the training process of a Deep Learning model because they have a data set that is so large that completing a single epoch on a single GPU can take a very long time, maybe, hours, days or even weeks.", "So when it is possible to share the data set to speed up training, we do it, as long as the model can tolerate a larger batch size.", "We can use frameworks as TensorFlow of Pytorch to program a multi-GPU training. To parallelize the training of the model, you only need to wrap the model with torch.nn.parallel.DistributedDataParallelin PyTorch and with tf.distribute.MirroredStrategyin TensorFlow. Very easy!", "TensorFlow and PyTorch require a software stack that includes different software layers installed in the execution environment as Python packages. Also, libraries as cuDNN from NVIDIA help us squeeze all the accelerators\u2019 power, such as using the Tensor Cores I mentioned.", "For instance, when I execute a Deep Learning code in our supercomputer in Barcelona, I need to load all these modules listed here with the module load command:", "A huge world, to which I am partly dedicated, and very important, often transparent for Deep Learning users like you.", "But in the end, with 4 GPUs, it is not possible to meet the needs of the challenges that arise in Deep Learning or Reinforcement Learning:", "And the number of GPUs that we can place in a server is very limited; in the case of a server with the Power 9 we are talking about, we can reach a maximum of 6 GPUs.", "The way companies have to fix this is to put many of these servers together! And this is what we did at the BSC, with this investigational platform, where 54 servers are linked together with an InfiniBand network on optical fiber.", "These servers run Linux as their operating system, and each is composed of two Power 9 processors and four NVIDIA GPUs with 512 GB of main memory, which means that we count with more than two hundred GPUs.", "InfiniBand is an industry-standard to interconnect servers that allows the local memory of one server to be accessed from remote servers speedily.", "In this new scenario, we need an extension of the software stack to deal with multiple distributed GPUs in the neural network training process. There are other options, but in our research group at BSC, we decided to use Horovod, from Uber. Horovod Plugs into TensorFlow, PyTorch, and MXNet.", "Horovod uses Message Passing Interface (MPI) to communicate the processes executed in a distributed fashion. MPI is a programming model ubiquitously present in any supercomputer to communicate processes executed in different servers. It also uses the NVIDIA NCCL2 library to manage the data communication between GPUs in a server.", "To speed up training, Horovod uses data parallelism training model introduced before. That is to say, all workers train on different data, all workers have the same copy of the model, and Neural network gradients are exchanged.", "And the sum of parallelism and distribution strategies has allowed this growing demand for computing that the Artificial Intelligence community required during these years.", "This mixture of hardware and software techniques allows the creation of true monsters of supercomputing. For instance, Google has computing infrastructures with hundreds of TPUs, that can be put together to collaborate in solving challenges that arise in the Deep Learning and Reinforcement Learning community.", "A recent paper from Google presents a model for Multilingual translation quality with 600 Billion parameters. To get an idea of the magnitude of the problem, we can compare it with the famous GTP-3, the third-generation language prediction model created by OpenAI. It \u201conly has\u201d 175 billion parameters.", "In this case, we are talking about computing requirements equivalent to 22 years with 1 TPU. In the paper, the authors measure performance in TPU-years. An interesting metric!. This means that if we only had available one TPU, it would take us 22 years to do the training.", "In this case, Google distributed the training over 2048 TPUs and achieved results in only 4 days.", "Detailed information about the system architecture of these TPU based infrastructures from Google can be found here.", "UPDATE 15/04/2021- Research from Microsoft, NVIDIA, and Stanford University: they study how to scale models of one trillion parameter models. See the paper Efficient Large-Scale Language Model Training on GPU Clusters.", "In the previous section we considered a Deep Learning problem as an example of today\u2019s Artificial Intelligence applications that eagerly and quickly eat computing. But actually cutting edge applications in the Artificial Intelligence arena are based on Deep Reinforcement Learning models that require vast amounts of computing.", "If you want to get introduced to Reinforcement Learning with an introductory series of posts that cover the basic concepts in Reinforcement Learning and Deep Learning to begin in the area of Deep Reinforcement Learning, you can find it here.", "A few years ago, foundational algorithms as DQN were conceived for consuming a few hardware resources; for instance, 1CPU+1GPU was enough. If we follow the timeline, the initial versions of distributed Reinforcement Learning (RL) algorithms required only a few more CPUs; such as the Asynchronous Actor-Critic method (A3C), which works very well with a few CPUs and a single GPU \u201d.", "However, if we take a closer look at Reinforcement Learning algorithms\u2019 most recent evolution, we see that they have increasingly required more computing resources. For instance, two years ago, a large-scale distributed RL named IMPALA was designed to take advantage of hundreds of CPUs.", "The architecture of current Distributed Reinforcement Learning Agents is usually separated into actors and learners. This is the case of IMPALA:", "The actors typically executed on CPUs, iterate between taking steps in the Environment and running inference on the model to predict the next action. After collecting a sufficient amount of observations, the actor will send a trajectory of observations and actions to the learner. Then the Learner optimizes the model and sends the parameters of the model to the actors, and each actor updates the parameters of its inference model. In this algorithm, the learner trains the model on GPUs using input from distributed inference on hundreds of CPU machines.", "A most recent version of a distributed Reinforcement Learning method, the SEED method from DeepMind, allows to use more than two hundred TPUs, amazing, right!! Allowing what we consider a true Massively Scaling Reinforcement Learning.", "Okay, without entering in detail, say that the SEED Reinforcement Learning architecture is designed to solve some drawbacks present in IMPALA method. In this case, neural network inference is made centrally by the learner on TPUs (not in the actor as in IMPALA), enabling accelerated inference and avoiding the data transfer bottleneck by ensuring that the model parameters and state are kept local.", "While the actor sends observations to the learner at every environment step, latency is kept low due to a very efficient network library gRPC (equivalent in functionality to MPI I mentioned before). This makes it possible to achieve up to a million queries per second on a single machine.", "In summary, the learner can be scaled to thousands of cores (e.g., up to 2048 TPUs), and the number of actors can be scaled to thousands of machines to fully utilize the learner, making it possible to train at millions of frames per second. Impressive, right?", "We can conclude that computing is responding to the needs of the Artificial Intelligence community, allowing us to solve the proposed models. My thesis in this publication is that COMPUTING POWER is the real enabler or, if you prefer, a key component of the progress of Artificial Intelligence, when we mix these three components: BIG DATA, BIG ALGORITHMS and, BIG COMPUTERS.", "What drove changes in effective compute over this period? OpenAI split the AI and Compute trend into Moore\u2019s Law and increased spending/parallelization, as well as progress in algorithmic efficiency:", "The authors estimate a 7.5 million times increase in the effective training compute available to the largest AI experiments between 2012 and 2018.", "And it seems that computing will continue responding to the needs of the Deep Learning and Reinforcement Learning community, allowing them to solve the required models.", "Imagine that Google needs more computing power for a new Reinforcement Learning algorithm, then, the only thing that Google will do is aggregate more parallel and distributed servers! And that\u2019s all!", "For example, a few months ago, Google breaks AI performance records in the Industry-standard benchmark MLPerf. MLPerf benchmark models are chosen to be representative of cutting-edge machine learning workloads.", "In this case, the only thing that Google needed to do is aggregate more servers. The resulting system includes 4096 TPUs and hundreds of CPU host machines connected via an ultra-fast interconnect. In total, this system delivers over 430 PFLOPs of peak performance.", "It seems that for now, adding servers allows us to respond to the needs of AI models. Easy, Right? Well, it\u2019s not like that!", "Before finishing my publication, let me assign you some \u201chomework\u201d to do on your own. Do you commit to doing them? I hope so!", "After reading the previous section, an important issue that arises is how much does the computing bill take to solve these challenges? Have you thought about that?", "For instance, according to the following tweet, the estimated cost of training the transformer GPT-3 language model that uses Deep Learning to produce human-like text, which I mentioned earlier, is nearly 12 million dollars on the public cloud.", "UPDATE 29/11/2020: A new paper from Tencent is another demonstration of the power of scale, using for their training a cluster that involves 250,000 CPU cores and 2,000 NVIDIA V100 GPUS.", "Maybe you have heard about the top500 list, the list that records the fastest computers in the world, issued twice a year, in June and November during the two main Supercomputing conferences in the world (SC, ISC).", "In general, there are supercomputers hosted in public institutions, with the Peak performance in Teraflops (10 to 12 operations per second) shown in the following table. For instance, the Top1 has a Peak performance of 500,000 Teraflops.", "Now, Marenostrum 4, the supercomputer hosted in Barcelona, in the chapel of Torre Girona at UPC university campus, occupies position 38 in this list, not bad for us! (virtual visit).", "The Google system mentioned before, which includes 4096 TPUs and hundreds of CPU host machines connected via an ultra-fast interconnect, delivers over 430,000 TFLOPs of peak performance. Near the top one in the world (according to June 2020 list), and far from the second one and others!", "For creating AI, we need Supercomputers. Who can own and pay for these supercomputers? Nation-states and multi-national corporations only?", "Last week in the Spanish newspaper La Vanguardia appeared this article: \u201cThe digital world is the third polluter on the planet\u201d.", "Also, research by the University of Massachusetts advises about the unsustainable costs of Artificial Intelligence. They claim that the estimated carbon cost of training a common NLP model is comparable to the amount produced by 125 round-trip flights from New York to Beijing.", "These numbers are probably relative, as organizations can power their computing facilities with renewable energy sources. Then they can reduce the carbon footprint. For instance, Iceland\u2019s energy is sourced from 100 percent renewable geothermal and hydroelectric power, and its national grid is modern and reliable; this means that the Artificial Intelligence systems housed there operate more efficiently and deliver cleaner energy.", "But, even the numbers are inflated, the exponential growth of computing needs by Artificial Intelligent makes it difficult to think that we can power supercomputing with only green energy in the short term.", "At present, the vast majority of Artificial Intelligent research in algorithms is focused on achieving the highest levels of accuracy, without much concern paid to computational or energy efficiency. But as the world\u2019s attention has shifted to climate change, should the field of Artificial Intelligence begin to take note of its carbon footprint?", "Artificial Intelligence is definitely penetrating society, like electricity, what will we expect? The future we will \u201cinvent\u201d is a choice we make jointly, not something that happens.", "This is good! For instance, Genetics and genomics look for mutations and links to disease from the information in DNA and with the help of Artificial Intelligence, body scans can spot diseases early and predict the health issues people might face based on their genetics.", "But as in most things in life, where there is light, there is shadow. Artificial Intelligence algorithms propagate gender biases, and AI systems have monitored citizens at large without their informed consent. Among many other bad things!", "We must mull over the imminent adoption of Artificial Intelligence and its impact. Were we to go on to build Artificial Intelligence without regard to our responsibility of preventing its misuse, we can never expect to see Artificial Intelligence help humanity prosper.", "All of us, who are working or want to work on these topics, cannot shy away from our responsibility, because otherwise, we will regret it in the future.", "Thank you for reading this publication!", "Acknowledgment: Many thanks to Juan Luis Dom\u00ednguez, Alvaro Jover Alvarez, Miquel Escobar Castells and Raul Garcia Fuentes for their contributions to the proofreading of this document.", "Our research group at Barcelona Supercomputing Center and UPC Barcelona Tech is doing research on this topic.", "Real-world challenges, e.g., image processing in sectors such as health or banking, among others, are driving fundamental research to the creation of novel large deep and reinforcement learning models. However, the creation of these new models is only a part of the solution to these challenges. Training processes of these models require massive amounts of computation and execution time. But scaling up large deep and reinforcement learning models in today\u2019s parallel and distributed infrastructures has become a significant challenge as it requires great multidisciplinary expertise in machine learning and supercomputing. In general, these are two areas of research that so far have not they have gone together; and now it is required an effort in providing joint solutions in the parallelization of these algorithms, demanding not only reprogramming them, as knowing how to use the parallel and distributed resources efficiently. Because as Rich Sutton says recently, one of the leading researchers in Reinforcement Learning, \u201cgeneral methods augmented by massive computation are the most effective\u201d. Our research group aims to introduce solutions that overlap these two research worlds. The AI \u200b\u200brevolution is not only about new mathematical models; it\u2019s about how to take advantage of the unprecedented opportunities that HPC offers for next-generation deep and reinforcement learning methods.", "\u201cExplore, Discover and Learn: Unsupervised Discovery of State-Covering Skills\u201d presented in the 37th International Conference on Machine Learning (ICML2020). The paper presents a novel paradigm for unsupervised skill discovery in Reinforcement Learning. It is the last contribution of @vcampos7, one of our Ph.D. students co-advised with@DocXavi. This paper is co-authored with @alexrtrott, @CaimingXiong, @RichardSocher from Salesforce Research.", "The Barcelona Supercomputing Center (BSC) is a public research center located in Barcelona. It hosts MareNostrum, a 13.7 Petaflops supercomputer, which also includes clusters of emerging technologies. In June 2017, it ranked 13th in the world.", "The Polytechnic University of Catalonia (Universitat Polit\u00e8cnica de Catalunya), currently referred to as BarcelonaTech, and commonly known as UPC, is the largest engineering university in Catalonia, Spain. It also offers programs in other disciplines such as mathematics and architecture.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4b0edbc2888d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----4b0edbc2888d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/supercomputing-for-a-i", "anchor_text": "SUPERCOMPUTING FOR ARTIFICIAL INTELLIGENCE \u2014 01"}, {"url": "https://bsc.es/", "anchor_text": "BSC"}, {"url": "https://www.fib.upc.edu/en/studies/masters/master-innovation-and-research-informatics/curriculum/syllabus/SA-MIRI", "anchor_text": "Supercomputers Architecture"}, {"url": "https://www.upc.edu/en?set_language=en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://bsc.es", "anchor_text": "BSC"}, {"url": "https://bsc.es", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://www.bsc.es/discover-bsc/organisation/scientific-structure/emerging-technologies-artificial-intelligence/team-people", "anchor_text": "our research group at UPC & BSC"}, {"url": "https://arxiv.org/pdf/2010.00263.pdf", "anchor_text": "iven a video and a linguistic phrase, we show how to generate binary masks for the object to which the phrase refers"}, {"url": "https://arxiv.org/pdf/2010.00263.pdf", "anchor_text": "RefVOS: Referring Expressions For VOS, M. Bellver et all."}, {"url": "https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)", "anchor_text": "John McCarthy"}, {"url": "https://en.wikipedia.org/wiki/Marvin_Minsky", "anchor_text": "Marvin Minsky"}, {"url": "https://en.wikipedia.org/wiki/Frank_Rosenblatt", "anchor_text": "Frank Rosenblatt"}, {"url": "https://scholar.google.com/citations?hl=en&user=NkzyCvUAAAAJ&view_op=list_works&sortby=pubdate", "anchor_text": "Oriol Vinyals"}, {"url": "https://twitter.com/OriolVinyalsML/status/1253053130411032576", "anchor_text": "recent tweet"}, {"url": "https://twitter.com/OriolVinyalsML/status/1253053130411032576", "anchor_text": "https://twitter.com/OriolVinyalsML/status/1253053130411032576"}, {"url": "https://openai.com/blog/ai-and-efficiency/", "anchor_text": "doubling every 16 months over a period of 7 years"}, {"url": "https://openai.com/blog/ai-and-efficiency/", "anchor_text": "https://openai.com/blog/ai-and-efficiency/"}, {"url": "https://en.wikipedia.org/wiki/AlexNet", "anchor_text": "AlexNet"}, {"url": "https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge", "anchor_text": "ImageNet"}, {"url": "https://www.youtube.com/watch?v=-fdexQBpRas&t=721s", "anchor_text": "excellent presentation at MIT"}, {"url": "https://twitter.com/DocXavi/status/1208077661173665794", "anchor_text": "a good friend of our research group"}, {"url": "https://blog.openai.com/ai-and-compute", "anchor_text": "graphic from OpenAI"}, {"url": "https://blog.openai.com/ai-and-compute", "anchor_text": "data source"}, {"url": "https://en.wikipedia.org/wiki/FLOPS", "anchor_text": "petaflop"}, {"url": "https://en.wikipedia.org/wiki/Moore%27s_law", "anchor_text": "Moore\u2019s Law"}, {"url": "https://en.wikipedia.org/wiki/Gordon_Moore", "anchor_text": "Gordon Moore"}, {"url": "http://ai.eecs.umich.edu/people/conway/VLSI/BackgroundContext/SMErpt/AppB.pdf", "anchor_text": "Moore, G. Progress in digital integrated electronics. In Proceedings of the International Electronic Devices Meeting (Washington, D.C., Dec.). IEEE, New York, 1975, 1113."}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext#R26", "anchor_text": "of processors created by companies like Intel"}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext", "anchor_text": "The current expectation is that the gap will continue to grow as CMOS technology approaches fundamental limits!"}, {"url": "https://ieeexplore.ieee.org/document/1050511", "anchor_text": "Dennard Scaling"}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext", "anchor_text": "\u201cGrowth of computer performance\u201d, created by Hennessy and Patterson"}, {"url": "https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext", "anchor_text": "data source"}, {"url": "https://events.linuxfoundation.org/ray-summit/", "anchor_text": "Ray Summit"}, {"url": "https://people.eecs.berkeley.edu/~istoica/", "anchor_text": "Ion Stoica"}, {"url": "https://en.wikipedia.org/wiki/Graphics_processing_unit", "anchor_text": "Nvidia\u2019s GPUs"}, {"url": "https://en.wikipedia.org/wiki/Tensor_Processing_Unit", "anchor_text": "Google\u2019s TPUs"}, {"url": "https://www.nvidia.com/en-us/data-center/tensor-cores/", "anchor_text": "Tensor Cores"}, {"url": "https://blog.openai.com/ai-and-compute/", "anchor_text": "OpenAI graph"}, {"url": "https://www.bsc.es/user-support/power.php#systemoverview", "anchor_text": "BSC based on 4 GPUs"}, {"url": "https://bsc.es", "anchor_text": "https://bsc.es"}, {"url": "https://www.redbooks.ibm.com/redpapers/pdfs/redp5472.pdf", "anchor_text": "This server, provided by IBM, has two CPUs, power 9, and 4 NVIDIA V100 GPUs"}, {"url": "https://towardsdatascience.com/tensorflow-vs-pytorch-the-battle-continues-9dcd34bb47d4", "anchor_text": "TensorFlow of Pytorch"}, {"url": "https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html", "anchor_text": "torch.nn.parallel.DistributedDataParallel"}, {"url": "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy", "anchor_text": "tf.distribute.MirroredStrategy"}, {"url": "https://developer.nvidia.com/cudnn", "anchor_text": "cuDNN"}, {"url": "https://www.nvidia.com/en-us/data-center/tensor-cores/", "anchor_text": "Tensor Cores"}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "our supercomputer in Barcelona"}, {"url": "https://www.ibm.com/products/power-systems-ac922/details", "anchor_text": "in the case of a server with the Power 9 we are talking about, we can reach a maximum of 6 GPUs"}, {"url": "https://www.bsc.es/user-support/power.php", "anchor_text": "where 54 servers are linked together with an InfiniBand network on optical fiber"}, {"url": "https://www.redbooks.ibm.com/redpapers/pdfs/redp5472.pdf", "anchor_text": "two Power 9 processors and four NVIDIA GPUs with 512 GB of main memory"}, {"url": "https://www.mellanox.com/pdf/whitepapers/IB_Intro_WP_190.pdf", "anchor_text": "InfiniBand"}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004c b2", "anchor_text": "Horovod"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://pytorch.org/", "anchor_text": "PyTorch"}, {"url": "https://mxnet.apache.org/versions/1.7.0/", "anchor_text": "MXNet"}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004c b2", "anchor_text": "Horovod"}, {"url": "https://en.wikipedia.org/wiki/Message_Passing_Interface", "anchor_text": "Message Passing Interface (MPI)"}, {"url": "https://developer.nvidia.com/nccl", "anchor_text": "NVIDIA NCCL2 library"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer", "anchor_text": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer"}, {"url": "https://arxiv.org/pdf/2006.16668.pdf", "anchor_text": "recent paper from Google"}, {"url": "https://en.wikipedia.org/wiki/GPT-3", "anchor_text": "GTP-3"}, {"url": "https://cloud.google.com/tpu/docs/system-architecture", "anchor_text": "here"}, {"url": "https://arxiv.org/pdf/2104.04473.pdf", "anchor_text": "Efficient Large-Scale Language Model Training on GPU Clusters"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "series of posts"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/deep-q-network-dqn-i-bce08bdf2af", "anchor_text": "DQN"}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "Asynchronous Actor-Critic method"}, {"url": "https://arxiv.org/pdf/1802.01561.pdf", "anchor_text": "IMPALA"}, {"url": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html", "anchor_text": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html"}, {"url": "https://arxiv.org/pdf/1910.06591.pdf", "anchor_text": "SEED method"}, {"url": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html", "anchor_text": "some drawbacks present in IMPALA method"}, {"url": "https://grpc.io/", "anchor_text": "gRPC"}, {"url": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html", "anchor_text": "https://ai.googleblog.com/2020/03/massively-scaling-reinforcement.html"}, {"url": "https://arxiv.org/pdf/2005.04305.pdf", "anchor_text": "Measuring the Algorithmic Efficiency of Neural Networks Danny Hernandez, Tom B. Brown OpenAI."}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer", "anchor_text": "Google breaks AI performance records in the Industry-standard benchmark MLPerf"}, {"url": "https://mlperf.org", "anchor_text": "MLPerf benchmark"}, {"url": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer", "anchor_text": "https://cloud.google.com/blog/products/ai-machine-learning/google-breaks-ai-performance-records-in-mlperf-with-worlds-fastest-training-supercomputer"}, {"url": "https://twitter.com/eturner303/status/1266264358771757057", "anchor_text": "the following tweet"}, {"url": "https://twitter.com/eturner303/status/1266264358771757057", "anchor_text": "https://twitter.com/eturner303/status/1266264358771757057"}, {"url": "https://arxiv.org/abs/2011.12692", "anchor_text": "A new paper from Tencent"}, {"url": "https://www.top500.org", "anchor_text": "top500 list"}, {"url": "https://sc20.supercomputing.org", "anchor_text": "SC"}, {"url": "https://www.isc-hpc.com", "anchor_text": "ISC"}, {"url": "https://www.bsc.es/sites/default/files/public/gallery/2017_bsc_superordenador_marenostrum-4_barcelona-supercomputing-center.jpg", "anchor_text": "chapel of Torre Girona at UPC university campus"}, {"url": "https://www.bsc.es/discover-bsc/visit-our-supercomputer#virtual-visit-title", "anchor_text": "virtual visit"}, {"url": "https://www.lavanguardia.com/natural/20201101/4946606386/mundo-digital-nube-contaminante-electricidad-ranking-mundo.html", "anchor_text": "La Vanguardia appeared this article"}, {"url": "https://www.lavanguardia.com/natural/20201101/4946606386/mundo-digital-nube-contaminante-electricidad-ranking-mundo.html", "anchor_text": "The digital world is the third polluter on the planet"}, {"url": "https://arxiv.org/pdf/1906.02243.pdf", "anchor_text": "research by the University of Massachusetts"}, {"url": "https://www.scientific-computing.com/analysis-opinion/true-cost-ai-innov", "anchor_text": "Iceland\u2019s energy is sourced from 100 percent renewable geothermal and hydroelectric power, and its national grid is modern and reliable"}, {"url": "https://link.springer.com/article/10.1186/s13073-019-0689-8", "anchor_text": "with the help of Artificial Intelligence"}, {"url": "https://theconversation.com/artificial-intelligence-has-a-gender-bias-problem-just-ask-siri-123937", "anchor_text": "gender biases"}, {"url": "https://carnegieendowment.org/2019/09/17/global-expansion-of-ai-surveillance-pub-79847", "anchor_text": "monitored citizens at large without their informed consent"}, {"url": "https://www.aies-conference.com/2021/", "anchor_text": "Among many other bad things"}, {"url": "https://www.bsc.es/dominguez-bermudez-juan-luis", "anchor_text": "Juan Luis Dom\u00ednguez"}, {"url": "https://www.bsc.es/jover-alvarez-alvaro", "anchor_text": "Alvaro Jover Alvarez"}, {"url": "https://www.bsc.es/escobar-castells-miquel", "anchor_text": "Miquel Escobar Castells"}, {"url": "https://www.bsc.es/garcia-fuentes-raul", "anchor_text": "Raul Garcia Fuentes"}, {"url": "https://www.bsc.es/discover-bsc/organisation/scientific-structure/emerging-technologies-artificial-intelligence/team-people", "anchor_text": "Our research group at Barcelona Supercomputing Center"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html", "anchor_text": "Rich Sutton says recently"}, {"url": "https://arxiv.org/pdf/2002.03647.pdf", "anchor_text": "Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills"}, {"url": "https://icml.cc/", "anchor_text": "37th International Conference on Machine Learning (ICML2020)"}, {"url": "https://twitter.com/vcampos7", "anchor_text": "@vcampos7"}, {"url": "https://twitter.com/DocXavi", "anchor_text": "@DocXavi"}, {"url": "https://twitter.com/alexrtrott", "anchor_text": "@alexrtrott"}, {"url": "https://twitter.com/CaimingXiong", "anchor_text": "@CaimingXiong"}, {"url": "https://twitter.com/RichardSocher", "anchor_text": "@RichardSocher"}, {"url": "https://einstein.ai/", "anchor_text": "Salesforce Research"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://en.wikipedia.org/wiki/Barcelona", "anchor_text": "Barcelona"}, {"url": "https://en.wikipedia.org/wiki/MareNostrum", "anchor_text": "MareNostrum"}, {"url": "https://en.wikipedia.org/wiki/TOP500", "anchor_text": "13th in the world"}, {"url": "https://www.upc.edu/en", "anchor_text": "The Polytechnic University of Catalonia"}, {"url": "https://towardsdatascience.com/artificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d", "anchor_text": "Artificial Intelligence is a Supercomputing problem"}, {"url": "https://towardsdatascience.com/using-supercomputers-for-deep-learning-training-3f9cc3f51d3", "anchor_text": "Using Supercomputers for Deep Learning Training"}, {"url": "https://towardsdatascience.com/scalable-deep-learning-on-parallel-and-distributed-infrastructures-e5fb4a956bef", "anchor_text": "Scalable Deep Learning on Parallel and Distributed Infrastructures"}, {"url": "https://towardsdatascience.com/train-a-neural-network-on-multi-gpu-with-tensorflow-42fa5f51b8af", "anchor_text": "Train a Neural Network on multi-GPU with TensorFlow"}, {"url": "https://towardsdatascience.com/distributed-deep-learning-with-horovod-2d1eea004cb2", "anchor_text": "Distributed Deep Learning with Horovod"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained - Jordi TORRES.AI"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----4b0edbc2888d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----4b0edbc2888d---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4b0edbc2888d---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----4b0edbc2888d---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/tag/supercomputing-for-a-i?source=post_page-----4b0edbc2888d---------------supercomputing_for_a_i-----------------", "anchor_text": "Supercomputing For A I"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----4b0edbc2888d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----4b0edbc2888d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4b0edbc2888d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4b0edbc2888d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4b0edbc2888d--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fartificial-intelligence-is-a-supercomputing-problem-4b0edbc2888d&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}