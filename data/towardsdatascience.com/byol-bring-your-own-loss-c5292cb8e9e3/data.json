{"url": "https://towardsdatascience.com/byol-bring-your-own-loss-c5292cb8e9e3", "time": 1683009521.018296, "path": "towardsdatascience.com/byol-bring-your-own-loss-c5292cb8e9e3/", "webpage": {"metadata": {"title": "BYOL: Bring Your Own Loss. How we improve delivery time estimation\u2026 | by Pavel Kochetkov | Towards Data Science", "h1": "BYOL: Bring Your Own Loss", "description": "Dear connoisseurs, I invite you to take a look inside Careem\u2019s food delivery platform. Specifically, we are going to look at how we use machine learning to improve the customer experience for\u2026"}, "outgoing_paragraph_urls": [{"url": "http://careem.com/", "anchor_text": "Careem\u2019s", "paragraph_index": 0}, {"url": "https://heartbeat.fritz.ai/research-guide-advanced-loss-functions-for-machine-learning-models-aee68ed8a38c", "anchor_text": "post", "paragraph_index": 15}, {"url": "https://catboost.ai/docs/concepts/parameter-tuning.html", "anchor_text": "Catboost", "paragraph_index": 24}, {"url": "https://github.com/pashna/gbm_custom_loss", "anchor_text": "links", "paragraph_index": 38}, {"url": "https://github.com/pashna/custom_lose_medium", "anchor_text": "here", "paragraph_index": 41}, {"url": "https://github.com/pashna/gbm_custom_loss", "anchor_text": "library", "paragraph_index": 42}], "all_paragraphs": ["Dear connoisseurs, I invite you to take a look inside Careem\u2019s food delivery platform. Specifically, we are going to look at how we use machine learning to improve the customer experience for delivery time tracking.", "When planning a meal, timing is crucial. This is why we take a lot of care estimating the delivery time of our orders. However, delivery time depends on several complicated factors \u2014 for this reason, machine learning is the right choice to predict what the ETA will be.", "From a first glance, it is nothing but a typical regression problem: go get some features, train a reasonable model against historical delivery time to minimize RMSE, estimate expected decrease in average error with suitable cross-validation strategy and share it with leadership, deploy, announce it broadly and gain respect, trust, promotion\u2026", "In this post, I\u2019m going to try and explain what is wrong with this approach. I will describe our solution to the problem and the way we measured user impact. I will then show you how we built a custom loss function to better optimize for user order satisfaction.", "Disclaimer: For legal reasons all of the data shown here is synthetic, but it has been constructed in a way that represents real behavior.", "Let\u2019s start by finding a way to measure success. There are circumstances where measuring MAE makes sense. However, in the case of delivery time estimation, it is largely pointless. It tells us nothing about the impact on the customer.", "What we care about is customer experience. What you care about is the impact on your customer experience which might be measured in various ways: delivery rating, churn, retention, LTV, call center contact rate, NPS score. For example, at Careem, we use a weighted combination of call center contact rate, churn rate, delivery time, and delivery rating which reflects the business state.", "The problem is these are hard to optimize for. So often data scientists resort to RMSE, MAE, Huber, etc as proxies. Then only in tests the real user impact after deployment.", "Let\u2019s assume the goal of our project is to improve the average delivery rating. We need to understand what impact our model will have on delivery ratting. One way to do this is to look at historical delivery time estimates, and see what happens to customer ratings when various delivery time errors are made. Below we see a graph shows the average trip rating, for different degrees of error in the displayed delivery time error estimate:", "There are a number of intuitive insights:", "If the food was delivered 22+ minutes late, one should expect a score of ~1.9 (see disclaimer at the top). Using this we have a clear way of converting our delivery time errors, into an estimate for the ordering ratting. Then on a validation dataset, we are able to estimate the expected average rating of trips when using our model. Thus we have found a way of evaluating a meaningful metric for our model.", "Looking at the graph, it is also important to highlight \u2014 the best model is not necessarily the one that has its MAE equals to 0. If I had a perfect model, I would probably anyway shift the predictions a little bit towards left, to make sure we do what the customers like the most \u2014 delivering food a few minutes earlier than we initially promised.", "All right back on track, our objective is defined.", "A very reasonable next step is to run your favorite hyperparameter optimization algorithm against this objective. For the plot below, we are tunning Catboost with RMSE loss against this objective for 120 iterations. Let\u2019s first compare where these two will take us.", "Not great, not terrible. It is a small improvement, but it comes for free. Moreover, now, you have a much better understanding of the potential impact of your work, and instead of saying \u201cnew implementation improved delivery time estimation by U%\u201d, you would add a tiny and important suffix: \u201c\u2026 which is expected to bring an improvement of around K-stars in rating\u201d, or \u201cP% increase in retention\u201d, or \u201cH% increase in revenue\u201d, or \u201cpeople will be smiling 28% more often\u201d.", "Surely, it is not all we can do. Yes, we improved the score a bit, but still are facing the same problem with RMSE symmetry. It is pretty common to come up with a custom loss function, especially in various Deep Learning fields. I like this post that makes a good summary of advanced loss functions.", "But surprisingly enough, classical machine learning applications didn\u2019t widely adopt loss function customization, although, they definitely might benefit from it from time to time. Let\u2019s consider a generic form tweak around MSE:", "The function g could be as complex as a data scientist fantasy gets, as long as your application benefits from it\u2026 and the function g has some form of derivative. But even without going deep into the rabbit hole, we could pick the simplest version of g - step function of error:", "In this case, g becomes nothing but a dynamic weight, which is chosen for each of the objects based on the error. Let\u2019s call it Piecewise MSE. And the most obvious choice for the parameters to start with is to make them proportional to the ratings per error. This way, the model would push harder to avoid late deliveries than earlier deliveries, become less optimistic.", "Then, there is nothing easier than computing first and second derivatives with respect to predictions:", "And this is all we need to get it running on top of your favorite gradient boosting implementation with extensive hyperparameter optimization", "We\u2019ve got a little gain against the vanilla RMSE, but there is still a long journey ahead. The truth is, our new loss function shape is not optimal, even if it seems like.", "Consider a simple example. The dataset has one outlier object, that came completely out of nowhere: the bike of a courier (we call them captains) broke just one block away from the destination and the delivery took 3 hours. There is no feature in your dataset that can help to distinguish that particular case, and chances are, this object will hurt the overall algorithm\u2019s performance. If would be way better if the loss function would pay less attention to the outlier.", "And the second argument against this fixed custom loss is optimization. Most of the models in machine learning are optimized via some form of gradient descent. In this sequential (and often stochastic) process, any change: different initialization, learning rate, batch shuffle will lead to a different result. So, the same way as we change the learning rate adopting a model to a different dataset, the tweaking loss function\u2019s coefficient might help and improve the performance.", "Unfortunately, we can\u2019t make c_k parameters of the model. But making them hyperparameters is not too bad. Now, among with other model\u2019s parameters (I am using Catboost), we will be tunning c_k with the same hyperparameter optimization procedure.", "Now, the score is significantly higher. Not surprisingly. This happens because the loss is being customized for this particular problem and this particular dataset.", "Moreover, with this setup, the HPO directly (well, almost) optimizes for the metric you care about \u2014 the average expected rating, rather than abstract and business agnostic MSE.", "The next plot shows the shape of our winner loss function, which was discovered on iteration \u2116151.", "It looks a skewed MSE everywhere where error below 18. And then, it deprioritizes objects for which error is above 18. Looking at the barplot above, it might seem counter-intuitive: the error of 18 minutes is expected to be the most important to avoid!", "But we should not forget that the HPO process not only looks at the barplot, but considers holistically the combination of the dataset, model\u2019s performance, model\u2019s other hyperparameters, and the objective. There might not be a single object for which the model makes a mistake of such magnitude. So, instead, it concentrates on moving objects from basket 11\u201318 to basket 5\u201311. Or, there might be other reasons for such behavior. So, the loss function shape does not have to be intuitive in order to deliver the best score. And while considering all the factors together, this loss function rocks for in this particular context.", "So, we are already getting great improvement. Let\u2019s get one additional tiny twist. The previous loss function modified RSE by scaling it. Good, but not flexible enough. A step forward is to generalize it by adding a bias:", "Then, we'll be calling it Biased Piecewise MSE.", "Again, we perform the same operation of HPO, but now running together with our c and b among with the rest of the algorithm\u2019s hyperparameters.", "The best score is another additional increment of our previously best-achieved result. And the shape of newly obtained custom loss looks even more Frankenstein, but yet, beautiful.", "The above compares customized tunable objectives against RMSE, which despite being the most widely adopted one, for the current application has a major disadvantage \u2014 it is symmetric. But there are other types of non-symmetric loss functions. A popular example being Quintile loss. We are going to compare this to our piecewise function. And again, its main parameter alpha will be treated as a hyperparameter the same way as it was done in the previous experiments.", "The result of the quantile regression is better than any previously tunned RMSE, but still losing big margin to tunned Piecewise losses.", "Important to note, that all of the experiments above used the same hyperparameter set except for loss specific parameters. Additionally, for the HPO runs where the number of hyperparameters of the models was increased, the number of iterations also was increased to account for that (for example, for Piecewise requires 7 additional hyperparameters, while for Biased Piecewise MSE there are 14 additional hyperparameters).", "The truth is, here we\u2019ve been using a quite simple method to evaluate the expected rating, but nothing (well almost nothing) stops you from using a sophisticated ML model to evaluate your objective. Let\u2019s say, you want to minimize customer churn, and the churn depends on the delivery time error. Bingo! This is the perfect marriage, especially when the churn model is already developed. The only problem, likely, the whole training becomes way slower, but there are tons of applications where it is not a constraint at all.", "In this particular example, our main metric\u2014 rating is almost convex, that is why even though RMSE, Quintile is losing, the margin is not that drastic. If, for example, our delivery time experience distribution would look more like the one on the following image, the marginal benefit of Piecewise losses would increase remarkably. In the links section, you will be able to find a notebook and play around it.", "The loss function is only limited by your fantasy and the computation power you\u2019ve got, and pretty often, it is practically beneficial to implement and tune a custom one.", "Even though these particular values are synthetic, our production setup works in a pretty similar way and the real impact of the model on our platform is superb: we\u2019ve got over 25% of the orders shifted to the baskets with better delivery time experience. Not only it helped us to understand better what is important and why, but also it contributes a lot to the great experience of our customers (and we know by how much greater it got!).", "The code associated with the post is located here, among with the dataset (see the disclaimer).", "Additionally, if someone wants to use any of the loss functions, you could install this little library (which is btw, pretty open for any contribution too).", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc5292cb8e9e3&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@pavelkochetkov?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pavelkochetkov?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Pavel Kochetkov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7f4009feab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&user=Pavel+Kochetkov&userId=f7f4009feab5&source=post_page-f7f4009feab5----c5292cb8e9e3---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://careem.com/", "anchor_text": "Careem\u2019s"}, {"url": "https://heartbeat.fritz.ai/research-guide-advanced-loss-functions-for-machine-learning-models-aee68ed8a38c", "anchor_text": "post"}, {"url": "https://catboost.ai/docs/concepts/parameter-tuning.html", "anchor_text": "Catboost"}, {"url": "https://github.com/pashna/gbm_custom_loss", "anchor_text": "links"}, {"url": "https://github.com/pashna/custom_lose_medium", "anchor_text": "here"}, {"url": "https://github.com/pashna/gbm_custom_loss", "anchor_text": "library"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c5292cb8e9e3---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/loss-function?source=post_page-----c5292cb8e9e3---------------loss_function-----------------", "anchor_text": "Loss Function"}, {"url": "https://medium.com/tag/gradient-boosting?source=post_page-----c5292cb8e9e3---------------gradient_boosting-----------------", "anchor_text": "Gradient Boosting"}, {"url": "https://medium.com/tag/gradient-descent?source=post_page-----c5292cb8e9e3---------------gradient_descent-----------------", "anchor_text": "Gradient Descent"}, {"url": "https://medium.com/tag/delivery?source=post_page-----c5292cb8e9e3---------------delivery-----------------", "anchor_text": "Delivery"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&user=Pavel+Kochetkov&userId=f7f4009feab5&source=-----c5292cb8e9e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&user=Pavel+Kochetkov&userId=f7f4009feab5&source=-----c5292cb8e9e3---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc5292cb8e9e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c5292cb8e9e3---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c5292cb8e9e3--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pavelkochetkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@pavelkochetkov?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Pavel Kochetkov"}, {"url": "https://medium.com/@pavelkochetkov/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "26 Followers"}, {"url": "https://www.linkedin.com/in/paul-kochetkov/", "anchor_text": "https://www.linkedin.com/in/paul-kochetkov/"}, {"url": "https://github.com/pashna/", "anchor_text": "https://github.com/pashna/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff7f4009feab5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&user=Pavel+Kochetkov&userId=f7f4009feab5&source=post_page-f7f4009feab5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Ff7f4009feab5%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbyol-bring-your-own-loss-c5292cb8e9e3&user=Pavel+Kochetkov&userId=f7f4009feab5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}