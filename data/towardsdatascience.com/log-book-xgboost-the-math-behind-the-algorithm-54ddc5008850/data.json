{"url": "https://towardsdatascience.com/log-book-xgboost-the-math-behind-the-algorithm-54ddc5008850", "time": 1683006177.758585, "path": "towardsdatascience.com/log-book-xgboost-the-math-behind-the-algorithm-54ddc5008850/", "webpage": {"metadata": {"title": "Log Book \u2014 XGBoost, the math behind the algorithm | by dearC | Towards Data Science", "h1": "Log Book \u2014 XGBoost, the math behind the algorithm", "description": "XGBoost is a beautiful algorithm and the journey through it has been nothing short of illuminating. The concepts, often simple and beautiful gets lost in mathematical jargon. I had faced the same\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/log-book-adaboost-the-math-behind-the-algorithm-a014c8afbbcc", "anchor_text": "Recall that, in Adaboost,\u201cshortcomings\u201d are identified by high-weight data points.", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Taylor_expansion", "anchor_text": "Taylor expansion", "paragraph_index": 19}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs", "paragraph_index": 22}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs", "paragraph_index": 23}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs", "paragraph_index": 28}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs", "paragraph_index": 31}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs", "paragraph_index": 33}], "all_paragraphs": ["If you want to understand something well, try to explain it simply. \u2014 Feynman", "XGBoost is a beautiful algorithm and the journey through it has been nothing short of illuminating. The concepts, often simple and beautiful gets lost in mathematical jargon. I had faced the same challenges while understanding the math and this is an attempt to consolidate my understanding while helping others on a similar journey.", "In order to understand what XGBoost does we will need to understand what Gradient Boosting is, so let\u2019s first understand the concepts behind that. Please note that this post assumes familiarity with the boosting process in general and will just try to touch upon the intuition and math behind Gradient Boosting and XGBoost. Let\u2019s jump right in.", "As always let's start with a crude initial function F\u2080, something like average of all values in case of regression. It will give us some output, however bad.", "Next we will compute the Loss function given by L(y\u1d62,F\u209c(x\u1d62)).", "Now what is a loss function? It is nothing but a way to measure the difference between actual and predicted values. Few examples would be:", "And why this robustness to outliers is important can be understood from the below table:", "The idea is that lower the value of the Loss Function the more accurate our predictions are, so now getting better predictions has become a minimization problem of the Loss function.", "So far we have the built our initial model, taken its predictions. Next, we should have fit a new model on the residuals given by the Loss function, but there is a subtle twist: we will instead fit on the negative gradient of the loss function, the intuition of why we are doing that and why they are similar is given below:", "The gradient can be interpreted as the \u201cdirection and rate of fastest increase\u201d of a function, so the negative gradient tells us the direction of the minima of a function, in this case which is the minima of the loss function.", "We will follow the same approach of gradient descent here. Take steps towards the minima of the Loss function, and how big or small a step will be given by the learning rate of the algorithm. And at the minima of the loss function we will have the lowest error rate.", "So we will build a new model h\u209c\u208a\u2081 on the -ve gradient of the loss function.", "This process of fitting the model iteratively on the -ve gradient will continue till we have reached the minima or the limit of the number of weak learners given by T, this is called the additive approach", "Recall that, in Adaboost,\u201cshortcomings\u201d are identified by high-weight data points. In Gradient Boosting, \u201cshortcomings\u201d are identified by gradients.", "This is in short of the intuition as to how Gradient Boosting works. In case of regression and classification the only thing that differs is the loss function that is used.", "Next we will see how XGBoost differs from Gradient Boosting.", "XGBoost and GBM both follow the principle of gradient boosted trees, but XGBoost uses a more regularized model formulation to control over-fitting, which gives it better performance, which is why it\u2019s also known as \u2018regularized boosting\u2019 technique.", "So what is this Newton\u2019s method? In Stochastic Gradient Descent, we use less point to take less time to compute the direction we should go towards, in order to make more of them, in the hope we go there quicker. In Newton\u2019s method, we take more time to compute the direction we want to go into, in the hope we have to take fewer steps in order to get there.", "One important point to note is that, even in the case of Gradient Boosting while the regression problem was solved using gradient descent, the classification problem still uses Newton\u2019s method to solve the minimization problem. XGBoost uses this method in both the cases of classification and regression.", "Newton\u2019s method attempts to solve the minimization problem by constructing a sequence {x\u2096} from an initial guess (starting point) x\u2080\u2208 R that converges towards a minimizer x* of f by using a sequence of second-order Taylor approximations of f around the iterates. The second-order Taylor expansion of f around {x\u2096} is", "The 2nd order derivatives are important in speeding gradient descent because if your algorithm is leading you zig-zag across a valley so that you are making very little progress down the actual slope of the valley, instead just step wise ending up repeatedly crossing this valley, adjusting the direction by the 2nd order derivative will skew your direction of descent in the direction of this valley, thus converting a slow descent into a much more rapid one.", "We had seen how the Square Loss behaves in the gradient boosting framework, let's take a quick look what becomes of the square loss function in the XGBoost approach:", "The form of MSE is friendly, with a first order term (usually called the residual) and a quadratic term. For other losses of interest (for example, logistic loss), it is not so easy to get such a nice form. So in the general case, we take the Taylor expansion of the loss function up to the second order \u2014 XGBoost Docs", "This becomes our optimization goal for the new tree. One important advantage of this definition is that the value of the objective function only depends on p\u1d62 and q\u1d62. This is how XGBoost supports custom loss functions. We can optimize every loss function, including logistic regression and pairwise ranking, using exactly the same solver that takes p\u1d62 and q\u1d62 as input! \u2014 XGBoost Docs", "Next we will deal with the Regularization term, but before going there we need to understand how a decision tree is defined mathematically. Intuitively if you think a decision tree is or mainly its output is a combination of leaves and a function assigning the data point to those leaves. Mathematically it is written as:", "where JT is the number of leaves. This definition describes the prediction process on a tree as:", "In XGBoost, complexity is defined as:", "These hyper-parameters in XGBoost are described as below:", "Of course, there is more than one way to define the complexity, but this one works well in practice. The regularization is one part most tree packages treat less carefully, or simply ignore. This was because the traditional treatment of tree learning only emphasized improving impurity, while the complexity control was left to heuristics. By defining it formally, we can get a better idea of what we are learning and obtain models that perform well in the wild. \u2014 XGBoost Docs", "The last equation measures how good a tree structure is.", "If all this sounds a bit complicated, let\u2019s take a look at the picture, and see how the scores can be calculated.", "Basically, for a given tree structure, we push the statistics gi and hi to the leaves they belong to, sum the statistics together, and use the formula to calculate how good the tree is. This score is like the impurity measure in a decision tree, except that it also takes the model complexity into account. \u2014 XGBoost Docs", "Now that we have a way to measure how good a tree is, ideally we would enumerate all possible trees and pick the best one. In practice this is intractable, so we will try to optimize one level of the tree at a time. Specifically we try to split a leaf into two leaves, and the score it gains is", "This formula can be decomposed as 1) the score on the new left leaf 2) the score on the new right leaf 3) The score on the original leaf 4) regularization on the additional leaf. We can see an important fact here: if the gain is smaller than \u03b3, we would do better not to add that branch. This is exactly the pruning techniques in tree based models! By using the principles of supervised learning, we can naturally come up with the reason these techniques work. \u2014 XGBoost Docs", "Great now hopefully we have a preliminary understanding of what XGBoost and why it works the way it does. See you in the next post!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F54ddc5008850&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://dearc.medium.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "dearC"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb63742fc872&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&user=dearC&userId=eb63742fc872&source=post_page-eb63742fc872----54ddc5008850---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/", "anchor_text": "Source"}, {"url": "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf", "anchor_text": "Source"}, {"url": "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf", "anchor_text": "Outliers and its impact on Loss Function, here 5 is the outlier. Check the values of different Loss functions"}, {"url": "https://towardsdatascience.com/log-book-adaboost-the-math-behind-the-algorithm-a014c8afbbcc", "anchor_text": "Recall that, in Adaboost,\u201cshortcomings\u201d are identified by high-weight data points."}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "source"}, {"url": "https://en.wikipedia.org/wiki/Taylor_expansion", "anchor_text": "Taylor expansion"}, {"url": "https://en.wikipedia.org/wiki/Newton%27s_method", "anchor_text": "source"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs"}, {"url": "https://xgboost.readthedocs.io/en/latest/parameter.html", "anchor_text": "XGBoost Regularization Parameters"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "source"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "XGBoost Docs"}, {"url": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html", "anchor_text": "https://xgboost.readthedocs.io/en/latest/tutorials/model.html"}, {"url": "https://drive.google.com/file/d/1CmNhi-7pZFnCEOJ9g7LQXwuIwom0ZN_D/view", "anchor_text": "https://drive.google.com/file/d/1CmNhi-7pZFnCEOJ9g7LQXwuIwom0ZN_D/view"}, {"url": "https://arxiv.org/pdf/1603.02754.pdf", "anchor_text": "https://arxiv.org/pdf/1603.02754.pdf"}, {"url": "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf", "anchor_text": "http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf"}, {"url": "https://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/", "anchor_text": "https://mlexplained.com/2018/02/02/an-introduction-to-second-order-optimization-for-deep-learning-practitioners-basic-math-for-deep-learning-part-1/"}, {"url": "http://learningsys.org/papers/LearningSys_2015_paper_32.pdf", "anchor_text": "http://learningsys.org/papers/LearningSys_2015_paper_32.pdf"}, {"url": "https://www.youtube.com/user/joshstarmer", "anchor_text": "https://www.youtube.com/user/joshstarmer"}, {"url": "https://medium.com/tag/data-science?source=post_page-----54ddc5008850---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/mathematics?source=post_page-----54ddc5008850---------------mathematics-----------------", "anchor_text": "Mathematics"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----54ddc5008850---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&user=dearC&userId=eb63742fc872&source=-----54ddc5008850---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&user=dearC&userId=eb63742fc872&source=-----54ddc5008850---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F54ddc5008850&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----54ddc5008850---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----54ddc5008850--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----54ddc5008850--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----54ddc5008850--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----54ddc5008850--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----54ddc5008850--------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://dearc.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "dearC"}, {"url": "https://dearc.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "151 Followers"}, {"url": "https://book.thedatascienceinterviewproject.com/", "anchor_text": "https://book.thedatascienceinterviewproject.com/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feb63742fc872&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&user=dearC&userId=eb63742fc872&source=post_page-eb63742fc872--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9d95fcc3fdb1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flog-book-xgboost-the-math-behind-the-algorithm-54ddc5008850&newsletterV3=eb63742fc872&newsletterV3Id=9d95fcc3fdb1&user=dearC&userId=eb63742fc872&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}