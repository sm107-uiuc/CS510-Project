{"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "time": 1682994986.7103171, "path": "towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941/", "webpage": {"metadata": {"title": "Predict malignancy in cancer tumors with your own neural network | by Javier Ideami | Towards Data Science", "h1": "Predict malignancy in cancer tumors with your own neural network", "description": "Predict malignancy in breast cancer tumors using deep learning with a network you code from scratch in Python and the Wisconsin Cancer Dataset"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "In part 1 of this series", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "In part 2", "paragraph_index": 0}, {"url": "https://pandas.pydata.org/pandas-docs/stable/visualization.html", "anchor_text": "with this link", "paragraph_index": 21}, {"url": "https://www.kaggle.com/jprakashds/confusion-matrix-in-python-binary-class", "anchor_text": "this public Kaggle", "paragraph_index": 60}, {"url": "https://arxiv.org/abs/1712.09913", "anchor_text": "Visualizing the Loss Landscape of Neural Nets", "paragraph_index": 97}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1", "paragraph_index": 103}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2", "paragraph_index": 103}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3", "paragraph_index": 103}, {"url": "https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy", "anchor_text": "Github Repository with all the code of this project", "paragraph_index": 104}], "all_paragraphs": ["In part 1 of this series, we understood in depth the architecture of our neural network. In part 2, we built it using Python. We also understood in depth back-propagation and the gradient descent optimization algorithm.", "In the final part 3, we will use the Wisconsin Cancer data-set. We will learn to prepare our data, run it through our network and analyze the results.", "It\u2019s time to explore the loss landscape of our network.", "To switch on our network, we need some fuel, we need data.", "First, we download the data to our machine. Then, we use pandas to create a dataframe and we take a look at its first rows.", "A dataframe is a python data structure that allows us to work and visualize data very easily.", "The first thing we need to do is to understand the structure of the data. We find key information about it on its website.", "This is useful information that allows us to achieve some conclusions.", "We proceed to do these changes. First, we change the class values (at the column number 10) from 2 to 0 and from 4 to 1", "Then we proceed to eliminate all rows that hold missing values (represented by the ? character) at column 6, which we have identified as the column that holds them.", "The \u2018?\u2019 character causes Python to interpret column 6 as made of strings. Other columns are made of integers. We set the entire dataframe to be interpreted as made of float numbers. This helps our network perform complex computations.", "Next, let\u2019s deal with the range of the values in our data. Notice how the data within the 9 features is made of numbers that go beyond the 0 to 1 range. Real data-sets are often messy and come with a great diversity of range in their values: negative numbers, huge range differences within columns, etc.", "That\u2019s why data normalization is a key first step within the feature engineering phase of deep learning processes.", "Normalizing the data means preparing it in a way that is easier for the network to digest. We are helping the network converge easier and faster to that minima we are looking for. Typically, neural networks respond well to numerical data set in the 0 to 1 range, and also to data that has a mean of 0 and a standard deviation of 1.", "Feature engineering and normalization are not the focus of this article but let\u2019s quickly mention a couple of methods within this phase of the feature engineering process:", "Some data-sets and scenarios will benefit more than others from each of these techniques. In our case and after some tests, we decide to apply min-max normalization using the sklearn library:", "Let\u2019s take a look at the same 15 rows after all these changes.", "The process could continue as we explore more the data.", "These and more are part of the feature engineering process that takes place before the training begins.", "Another useful thing to do is to build charts to analyze the data in different ways. The myplotlib python library helps us study the data through different kinds of graphs.", "We first combine the normalized columns we want to study with the class column, and then begin to explore.", "Explore the full range of visualization options offered by Panda with this link", "To speed up the article, in our case we conclude that the 9 features are useful. Our objective is to predict the \u201cclass\u201d column with precision.", "So, how complex would the function be that would describe the connection between our 683 samples and their output?", "The relationship between the 9 features and the output is clearly multi-dimensional and non-linear.", "Let\u2019s run the data through the network and see what happens.", "Before we do that, we need to consider a key topic:", "This is the reason why the deep learning practitioner typically takes into account three kinds of data-sets:", "The size of the different sets in relation to each other is another topic that would take some time to describe. For our purposes, consider that most of the data forms the training set and a small percentage of it is typically extracted (and eliminated from the training set) to become the validation set.", "20% is a typical number that is often chosen as the percentage of the data that will form our validation set.", "To estimate the quality of the training of your network, it is useful to compare the performance of your training and validation sets:", "In general, realize that success with the validation set is your real target. Having the network perform fantastically well with the training data serves no purpose if it fails to perform well with new data it hasn\u2019t seen before.", "So your real target is to reach a good loss value and achieve good accuracy with your validation set.", "To get there, over-fitting is one of the most important issues we need to prevent, and that\u2019s why regularization is so important. Let\u2019s quickly and briefly recap 4 widely used regularization techniques.", "Dropout: During each training pass, we randomly disable some of the hidden units of our network. This prevents the network from putting too much emphasis on any specific weight and helps the network generalize better. It is as if we were running the data through different network architectures and then averaging their impact, which helps prevent over-fitting.", "L1 and L2: We add extra terms to the cost function that penalize the network when weights become too large. These techniques encourage the network to find a good balance between the loss value and the scale of the weights.", "Early stopping: Over-fitting can be a consequence of training for too long. If we monitor our validation error, we can stop the training process when the validation error stops improving.", "Data augmentation: Typically, more training data means a better network performance, but obtaining more data is not always possible. Instead, we can augment the existing data by artificially creating variations of it. For example, in the case of images, we can apply rotations, translations, cropping and other techniques to produce new variations of them.", "Back to our data. It\u2019s time to pick our training and validation sets. We will select part of the 683 rows as the training set and a different part of the data-set as our validation set.", "After the training, we will validate the quality of our network by running the process again through the validation set.", "We decide to build our training set with 500 of the 683 rows, and we pick them from the normalized scaled_df dataframe. We also make sure to eliminate the first column (the IDs) and to not include the last column (the class) in the input x to the network", "We declare the target output y using the class column that corresponds to the same 500 rows. We pick the class column from the original non-normalized df dataframe (as the class value should remain as a 0 or a 1).", "We then select the next 183 rows for our validation set, and store them in the variables xval and yval.", "We are ready. We will first train the network with the 500 rows of our x,y training set. Afterwards, we will test the trained network with the 183 rows of our xval,yval validation set, to see how well the network generalizes to data it has never seen before.", "We declare our network, set a learning rate and the number of nodes at each layer (the input has 9 nodes because we are using 9 features, and it\u2019s not counted as a layer of the network. The first hidden layer has 15 hidden units and the second and final layer has a single output node).", "We then run the gradient descent algorithm through a few thousand iterations. Let\u2019s get a feel of how well the network trains with a few seconds of gradient descent.", "Every x iterations, we display the loss value of the network. If the training proceeds well, the loss value should decrease after every cycle.", "After a number of iterations our loss begins to stabilize at a low level. We plot a chart that follows the loss of the network through the iterations.", "Our network seems to have trained quite well, reaching a low loss value (the distance between our predictions and the target outputs is low). But, how good is it? and most importantly, how good is it, not just on the whole training set, but way more important, on our validation set?", "To find out, we create a new function, pred(), that runs a set of inputs through the network and then compares systematically every obtained output to its corresponding target output in order to produce an average accuracy value.", "Notice below how the function studies if the prediction is above or below 0.5. We are doing binary classification and by default we consider that output values that are above 0.5 mean that the result belongs to one of the classes, and vice-versa.", "In this case, because 1 is the class value for malignant tumors, we consider that outputs above 0.5 predict a malignant result, and below 0.5 the opposite. We will talk in a bit about how, when and why we would want to change this 0.5 threshold value.", "We now proceed to compare the accuracy of the network when using the training and validation sets, by calling the pred function twice, once with our training set, and another time with our validation set.", "And we get these 2 results.", "The network has an accuracy of a 96% on the training set (first 500 rows) and of 100% when using the validation set (next 183 rows).", "The accuracy on the validation set is higher. This means that the network is not over-fitting and is generalizing well enough to be able to adapt to data it has never seen before.", "We can now use the nn.forward() function to compare directly the first few values of the validation set output in relation to the target output:", "Both match perfectly, because we have achieved 100% accuracy on our validation set.", "Therefore, the function learnt pretty well to adapt to both the training and validation sets.", "One great way to analyze the accuracy is by plotting a confusion matrix. First, we declare a custom plotting function.", "(This custom confusion matrix function comes from this public Kaggle created by JP)", "Then, we run the pred function again twice, and plot confusion matrices for both the training and validation sets.", "We can see even more clearly that our validation set has perfect accuracy on its 183 samples. As for the training set, there are 19 mistakes among the 500 samples.", "Now, at this point you may say that in a topic as delicate as diagnosing a tumor, setting our prediction to be 1 if the sigmoid output gives a value above 0.5 is not really good. The network should be really confident before giving a prediction of malignancy.", "I totally agree, that\u2019s very correct. And these are the kinds of decisions that you need to take depending on the nature of the challenge and topic you are dealing with.", "Let\u2019s then create a new variable called threshold. It will control our confidence threshold, how close to 1 the output of the network needs to be before we decide that a tumor is malignant. By default we set it to 0.5", "Out prediction function is now updated to use that confidence threshold.", "Let\u2019s now compare our results as we gradually raise the confidence threshold.", "Confidence threshold: 0.5 . Output values need to be higher than 0.5 for the output to be considered malignant. As seen previously, the validation accuracy is 100%, the training one is 96%.", "Confidence threshold: 0.7 . Output values need to be higher than 0.7 for the output to be considered malignant. The validation accuracy remains at 100%, the training one decreases a bit to 95%.", "Confidence threshold: 0.8 . Output values need to be higher than 0.8 for the output to be considered malignant. The validation accuracy for the first time decreases very, very slightly to 99.45%. In the confusion matrix we see that 1 single sample of the 183 is not recognized correctly. The training accuracy decreases a bit more till 94.2%", "Confidence threshold: 0.9. Finally, in the case of 0.9, output values need to be higher than 0.9 for the output to be considered malignant. We are looking for almost complete confidence. The validation accuracy decreases a bit more till 98.9%. In the confusion matrix we see that 2 samples of the 183 were not recognized correctly. The training accuracy decreases further till 92.6%.", "Therefore, by controlling the confidence threshold, we adapt to the specific needs of our challenge.", "If we want to lower the loss value related to our training set (because we are failing to recognize a small percentage of the training samples), we can try to train for longer, and also use different learning rates.", "For example, if we set the learning rate to 0.07 and train for 65000 iterations, we obtain:", "Now, with our confidence threshold set to 0.5, the network is accurate with every sample in both sets, except with one of each.", "If we raise the confidence threshold to 0.7, performance is still excellent, only 1 validation sample and 2 training samples are not predicted correctly.", "Finally, if we are really demanding and set the confidence threshold to 0.9, the network fails to guess correctly 1 of the validation samples and 10 of the training ones.", "Although we have done quite well, considering that we are using a basic network without regularization, it is typical for things to get much harder when you are dealing with more complex data.", "Often, the loss landscape gets very complex and it\u2019s easier to fall in the wrong local minima or fail to converge to a good enough loss.", "Also, depending on the initial conditions of the network, we may converge to a good minima or we may get stuck at a plateau somewhere and fail to get out of it. It\u2019s useful at this stage to picture again our initial animation.", "Picture that landscape, full of hills and valleys, places where the loss is really high, and places where the loss gets very low. The landscape of the loss function related to a complex scenario is often not uniform (though it can be made more smooth using different methods, but that\u2019s a whole different topic).", "It\u2019s full of hills and valleys of different depths and angles. The way you move around the landscape is by changing the loss value of the network when you run the gradient descent algorithm.", "And the speed at which you move is controlled by the learning rate:", "So there are some very delicate issues that have an enormous impact on how your network will perform.", "A lot of the progress achieved recently in improving the speed with which neural networks train is connected to different techniques that dynamically manage the learning rate and also to new ways of setting those initial conditions in better ways.", "It is therefore critical to set the initial values of our weights in the best way possible so that the computations of the units at the start of the training process produce outputs that fall within the best possible range of our activation functions.", "That could make the whole difference between beginning at a really high hill of the loss landscape or way lower.", "Managing the learning rate to prevent the training process from being too slow or too fast, and to adapt its value to the changing conditions of the process and of each parameter, is another complex challenge", "Talking about the many ways of dealing with the initial conditions and the learning rate would take a few articles. I will briefly describe some of them to give an idea of some of the methods experts use to deal with these challenges.", "If you are new to all these names, don\u2019t get overwhelmed. Behind most of them are the very same roots: back-propagation and gradient descent.", "Also, a lot of these methods are selected automatically for you within modern frameworks such as the fast.ai library. It is though really useful to understand how they work, as you are then in a better position to take your own decisions and even to research and test different variations and options.", "When we understand the core of the network, the basic back-propagation algorithm and the basic gradient descent process, we have more options to explore and experiment whenever we face hard challenges.", "Because we understand the process, we realize for example that in deep learning, the initial place where we drop the ball within the loss landscape is key.", "Some initial positions will soon push the ball (the training process) to get stuck in some part of the landscape. Others will quickly drive us to a good minima.", "When the mystery function becomes more complex, it is the time to incorporate some of the advanced solutions I mentioned earlier. It is also time to study in more depth the architecture of the entire network and to go deeper into the different hyper-parameters.", "The shape of our loss landscape is very much influenced by the design of the architecture of our networks as well as hyper-parameters like the learning rate, the size of our batches, the optimizer algorithm we use, etc.", "For a discussion about those influences, check the paper: Visualizing the Loss Landscape of Neural Nets by Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein.", "A very interesting point coming out of recent research is how the skip connections model in neural nets can smooth our loss landscape and make it dramatically simpler and more convex, increasing our chances to converge to a good result.", "Skip connections have helped a lot to train very deep networks. Basically, skip connections are extra connections that link nodes of separate layers, skipping one or more non-linear layers in between.", "As we experiment with different architectures and parameters, we are modifying our loss landscape, making it more rugged or smooth, increasing or decreasing the number of local optima. And as we optimize the way we initialize the parameters of the network, we are improving our starting position.", "Let\u2019s keep on exploring new ways to navigate the loss landscapes of the most fascinating challenges in the world.", "This article covered the basics and from here, the sky is the limit!", "Links to the 3 parts of this article:Part 1 | Part 2 | Part 3", "Github Repository with all the code of this project", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A multidisciplinary engineer, researcher, creative director, artist and entrepreneur, from augmented reality to deep learning, filmmaking, 3D and beyond."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F76271a05e941&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----76271a05e941--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ideami?source=post_page-----76271a05e941--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84----76271a05e941---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "In part 1 of this series"}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "In part 2"}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names", "anchor_text": "The Wisconsin Cancer Data-set"}, {"url": "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data", "anchor_text": "from this link"}, {"url": "https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy", "anchor_text": "Github link"}, {"url": "https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy", "anchor_text": "javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancyPredicting Cancer Malignancy with a 2 layer neural network coded from scratch in Python. \u2026github.com"}, {"url": "https://pandas.pydata.org/pandas-docs/stable/visualization.html", "anchor_text": "with this link"}, {"url": "https://www.kaggle.com/jprakashds/confusion-matrix-in-python-binary-class", "anchor_text": "this public Kaggle"}, {"url": "https://arxiv.org/abs/1712.09913", "anchor_text": "Visualizing the Loss Landscape of Neural Nets"}, {"url": "https://towardsdatascience.com/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504", "anchor_text": "Part 1"}, {"url": "https://towardsdatascience.com/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2", "anchor_text": "Part 2"}, {"url": "https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941", "anchor_text": "Part 3"}, {"url": "https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy", "anchor_text": "Github Repository with all the code of this project"}, {"url": "https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy", "anchor_text": "javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancyPredicting Cancer Malignancy with a 2 layer neural network coded from scratch in Python. \u2026github.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----76271a05e941---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----76271a05e941---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----76271a05e941---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/data-science?source=post_page-----76271a05e941---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----76271a05e941---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&user=Javier+Ideami&userId=7f7b5d730c84&source=-----76271a05e941---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&user=Javier+Ideami&userId=7f7b5d730c84&source=-----76271a05e941---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----76271a05e941--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F76271a05e941&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----76271a05e941---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----76271a05e941--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----76271a05e941--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----76271a05e941--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----76271a05e941--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----76271a05e941--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ideami?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Javier Ideami"}, {"url": "https://medium.com/@ideami/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f7b5d730c84&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&user=Javier+Ideami&userId=7f7b5d730c84&source=post_page-7f7b5d730c84--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4c3e5da2cfc1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpredict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941&newsletterV3=7f7b5d730c84&newsletterV3Id=4c3e5da2cfc1&user=Javier+Ideami&userId=7f7b5d730c84&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}