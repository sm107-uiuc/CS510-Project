{"url": "https://towardsdatascience.com/how-neural-networks-are-learning-to-write-d631b249b499", "time": 1682997612.627499, "path": "towardsdatascience.com/how-neural-networks-are-learning-to-write-d631b249b499/", "webpage": {"metadata": {"title": "How Neural Networks Are Learning to Write | by Erick Fonseca | Towards Data Science", "h1": "How Neural Networks Are Learning to Write", "description": "Shortly after, they receive a five paragraph-long reply. Before elaborating a complex answer, it starts with a reasonable introductory explanation: Some troll replies the post with a useless hi, and\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.reddit.com/r/SubSimulatorGPT2/comments/c7g4qp/did_the_average_soviet_citizen_have_a_sense_of/", "anchor_text": "The post", "paragraph_index": 5}, {"url": "https://www.reddit.com/r/SubSimulatorGPT2/", "anchor_text": "r/SubSimulatorGPT2", "paragraph_index": 5}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT2", "paragraph_index": 5}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "r/SubredditSimulator", "paragraph_index": 6}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "r/SubredditSimulator", "paragraph_index": 8}, {"url": "https://books.google.com/ngrams", "anchor_text": "Google n-grams", "paragraph_index": 17}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "subreddit simulator", "paragraph_index": 17}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax", "paragraph_index": 20}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "first neural language model", "paragraph_index": 22}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "great post", "paragraph_index": 24}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer", "paragraph_index": 36}, {"url": "https://www.reddit.com/r/SubSimulatorGPT2/", "anchor_text": "subreddit simulator", "paragraph_index": 44}, {"url": "https://blog.swiftkey.com/swiftkey-debuts-worlds-first-smartphone-keyboard-powered-by-neural-networks/", "anchor_text": "that\u2019s", "paragraph_index": 46}, {"url": "https://ai.googleblog.com/2017/05/the-machine-intelligence-behind-gboard.html", "anchor_text": "right", "paragraph_index": 46}, {"url": "https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html", "anchor_text": "Speech recognition", "paragraph_index": 47}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "original paper on transformers", "paragraph_index": 48}], "all_paragraphs": ["In some online forum of historians, a user asks a question:", "Did the average Soviet citizen have a sense of humor?", "Shortly after, they receive a five paragraph-long reply. Before elaborating a complex answer, it starts with a reasonable introductory explanation:", "In the Soviet Union, there was a sort of universal sense of humor that was often associated with Soviet identity and with the Communist Party.", "Some troll replies the post with a useless hi, and is promptly reproached by a moderator. At first sight, there's nothing particularly unusual about this thread. Except\u2026 the posts on this forum are not from humans \u2014 but from artificial intelligence agents, or bots.", "The post I referred to is but an example from r/SubSimulatorGPT2, a subreddit dedicated exclusively to posts automatically generated by a bot based on OpenAI's GPT2. If you think the passage above about Soviet humor was just something it memorized and copied, that\u2019s most likely not the case: a Google search shows no occurrence of even half that sentence.", "Having bots talk to each other in Reddit is not new: there was already r/SubredditSimulator doing that for a few years. The difference here is a huge leap in the quality of the generated text. While the older bots can amuse you with some random nonsense, the GPT2-fueled ones can actually make you believe for a while they\u2019re actual people \u2014 which often makes it even funnier when they finally let out some absurd statement.", "How come there are AI models so smart out there? Well, things did not come to this overnight. The research on developing models capable of replicating human language has been going on for a few decades, but only recently it became really impressive. Here I\u2019m going to show an overview of the language modeling evolution, and why it is so useful besides creating amusing bots.", "The bots from the older r/SubredditSimulator use Markov chains, a well-established technique for generating sequences. Like modern neural networks, they learn from data, but are much simpler.", "A model based on Markov chains assumes that each word in a sentence depends only on the last few ones before it. Thus, it models the probability of observing a given sentence as the combined probability of all n-grams \u2014 sequences of n words \u2014 that compose it. The probability of an n-gram can be approximated as the number of times you saw that particular sequence out of all possible n-grams you could find in your language.", "Training a Markov chain model consists basically of estimating these probabilities from text data. The following figure illustrates the concept:", "In the example above, a Markov chain based on sequences of three words (or trigrams) determines the probability of the token chocolate following I like. Next, it would determine the probability of seeing the token cake following like chocolate, but not regarding the token I anymore.", "Then, to generate a new text, we just sample one word at a time from the probabilities that the model gives us. This kind of procedure, in which the result for each step depends on what the model predicted in the previous one, is called autoregressive.", "Sampling is what gives us some variety in the model output. If we always pick the most likely word according to the model, it will always produce the same texts.", "The problem is that, if we decompose the problem like this, we would never find anything odd about a sentence as I like chocolate cake and I like chocolate cake and I like chocolate cake: every sequence of three tokens sounds perfectly normal, and the model does not care beyond that.", "If we try to look at longer sequences, we see an exponential explosion. Considering a small vocabulary size of 10 thousand (10\u2074) words, we have 100 million (10\u2078) possible bigrams, one trillion (10\u00b9\u00b2) possible trigrams and ten quadrillion (10\u00b9\u2076) possible four-grams! Most of these are pointless, such as banana banana maelstrom banana, but this is part of the problem: a lot of perfectly fine 4-grams will also not appear in the training data, and the model has no way telling apart what sequences are absurd and what is fine but was unlucky not to be seen before.", "A way to circumvent the sparsity of longer n-grams is to use a language model that combines them with shorter ones. So, maybe our training corpus has no occurrences of universal sense of humor, but it does have a few of sense of humor. Meanwhile, even the bigrams banana banana and banana maelstrom are unheard of. That surely helps \u2014 but with the price that we can\u2019t trust the longer n-grams so much.", "N-gram counts were the best we had for language modeling for many years, and they got quite good with the massive Google n-grams. Still, we can see they are very brittle, and sentences generated by a Markov chain longer than five or six words hardly ever make any sense \u2014 again, you can check the older subreddit simulator.", "Word embeddings are one of the first things learned by anyone in NLP nowadays: a projection of words into a multidimensional space. Their great advantage is that words with similar usage/meaning get similar vectors, as measured by cosine similarity. Thus, matrix multiplications involving word vectors of similar words tend to give similar results.", "This is the basis for neural network-based language models. Instead of viewing each word as an atomic symbol, we can now treat them as dense vectors with a few hundred dimensions and perform numeric operations on them.", "In its simplest form, a neural language model looks at an n-gram, maps each of its words to their corresponding embedding vector, concatenates these vectors and feeds them to one or more hidden layers. The output layer determines the probability of each word in the vocabulary to come next, computed as a softmax over scores.", "It\u2019s interesting to see that the neural model does not count occurrences to determine probabilities, but instead learns parameters (weight matrices and biases) that can compute them for any input. This way, we don\u2019t need to fall back to shorter n-grams when our count of longer ones is not reliable \u2014what's best, we can even compute a reasonable probability distribution for the next word after an n-gram we never saw in the training data!", "The first neural language model was proposed in 2003, one decade before the deep learning era. Back then, no one ran neural nets on GPUs, computers were slower, and we hadn\u2019t discovered yet a lot of tricks commonly used nowadays. These models would slowly become more popular, but the real breakthrough only happened with Recurrent Neural Networks.", "Arguably the greatest improvement in language generation came with the advent of Recurrent Neural Networks (RNNs), and more specifically, Long Short-Term Memories (LSTMs). Unlike the simplest networks I mentioned before, the context of an RNN is not limited to only n words; it does not even have a theoretical limit.", "There's a great post by Andrej Karpathy which explains how they work and shows a lot of examples in which they learn to produce text resembling Shakespearian plays, Wikipedia articles, and even C code.", "The main improvement of an RNN in comparison with the simple networks I showed before is that they keep an internal state \u2014 that is, a matrix denoting the memory of the network. So, instead of looking only at a fixed window, an RNN can read continuously word after word, updating its internal state to reflect the current context.", "Text generation with RNNs follows a similar rationale than Markov chains, in an autoregressive fashion. We sample the first word, feed it to the neural network, get the probabilities of the next word, sample one, and so on, until we sample a special sentence end sign.", "RNNs became very popular in NLP around 2014 and 2015, and are still very widely used. However, their elegant architecture with a mindful internal state is a bit of a liability sometimes. Let me illustrate with this example:", "The boys that came with John are very smart.", "See that are agrees with boys, not with John, despite it being closer. In order to properly generate a sentence like this, an RNN needs to keep track of the plural noun until it generates a matching verb. This example is rather simple, but things get harder once you have longer sentences, especially in languages more fond of inflections and long-distance dependencies. It would be great if we didn\u2019t need to cram so much information into a limited-space memory, and instead, just look back at the previous words to check what\u2019s still missing.", "To alleviate this, the attention mechanism was introduced. It allows an RNN to do exactly that: to look back at all previous words before producing its next output. Computing attention essentially means computing some distribution over past words (that is, weighting each of them such that the sum of weights equals 1), and then aggregating the vectors of those words proportionally to the received attention. The following image illustrates the concept.", "But there\u2019s still an inconvenience. Training an RNN does not take advantage of parallelizable hardware operations, since we need to process each word i before looking at word i+1.", "Besides that, did you notice how the intermediate output vectors in the figure above get more colorful after each word? That\u2019s to symbolize that words further down in the sentence had access to the ones before it, while the ones at the beginning could not see the words coming afterward. Modeling a word conditioned on what comes after it may sound counterintuitive at first, but we do it all the time. Consider noun-noun compounds in English, such as the chocolate cake in the first example. As soon as we hear or read the word cake, we know that chocolate is just describing it, and is not the head of the phrase. We do it because we can backtrack the information in our minds, but a linear RNN can\u2019t do that. So, while RNNs can do a great job, there\u2019s still room for improvement.", "The Transformer is a neural network architecture introduced in 2017 to address the shortcomings of RNNs. Its key idea is to rely heavily on attention, to the point of not needing an internal state or recurrency at all.", "Each hidden layer in a Transformer has an attention component followed by a default feedforward (or MLP) transformation. First, the Transformer computes how much attention each word should pay to all words, including itself and the ones after it. As in the RNN, the vectors of these words are scaled by the proportion of attention received and summed, yielding a context-aware vector. This resulting vector then goes through a feedforward transformation, and the result is the output of that Transformer layer.", "Transformers usually have a high number of layers, which allows them to learn increasingly more complex interactions among words. And in order to take word order into account, input word embeddings are augmented with additional vectors encoding their positions.", "This was a rather simplified description of the Transformer, and one such layer is illustrated below. The actual architecture is quite complex, and you can find a great explanation in more detail in the post The Illustrated Transformer.", "Stacking some six, 12 or even 24 layers gives us very richly encoded vectors. On top of the last one, we can place our final output layer to produce a probability distribution for the next word, as in the other models.", "When generating a new sentence, we have to rerun the whole Transformer after each new word. This is necessary in order to allow the previous words to take into account the new one, something the RNNs aren't capable of.", "In the chocolate cake example, a Transformer can make chocolate attend over cake on its very first layer. As such, its vector encodes more precise meaning information, which will be propagated to the upper attention layers.", "I also mentioned before that RNNs didn't let us take advantage of parallel operations. You may have noticed that since we also have to generate new words with the Transformer step by step, that would also be the case here. Well, it is when running your model, but not when training.", "During training, we may have a Transformer predict all the words in a sentence in parallel. We only need to mask the word at the position i we want to predict and the ones following it, so the network can only look at past positions. There is no hidden state \u2014 all the hidden layers in the Transformer are recomputed at each time step regardless of the previous one.", "Transformers are currently the state of the art in many NLP tasks, including language modeling. Researchers are constantly proposing some improvements here and there, but the overall architecture has remained nearly unchanged.", "The GPT-2 model I mentioned at the beginning of this post is a Transformer instance. That particular one had 345 million parameters, was trained by OpenAI on a huge collection of texts, and then fine-tuned on a few tens of megabytes of text from Reddit.", "The pre-trained models provided by OpenAI are great at generating text in general, but to make something more specific, you need to fine-tune it \u2014 that is, train it further with some data you are interested in. In the subreddit simulator, there are versions of GPT-2 fine-tuned on over 100 subreddits talking to each other, and you can clearly see how each of them learned their own style and idiosyncrasies. That can be particularly fun in threads with bots fine-tuned in different subreddits!", "By the way, that pre-trained model was not even OpenAI\u2019s best. They had trained an even more powerful one, with 1.5 billion parameters, but decided not to publish it for fear of malicious usage. That sparks some interesting questions concerning what dangers AI pose: not the killing robots from science fiction, but a prolific writer that can argue or create stories tirelessly. Many people in machine learning and outside it disagree such language models are dangerous, but this episode at least shows that the repercussions of AI are becoming a more important topic of debate.", "Ok, but what use are these models, besides creating amusing Reddit threads? Well, a lot of NLP applications, for sure! You probably already realized that a very straightforward one is in your cell phone\u2019s keyboard, and that\u2019s right.", "Any NLP task involving text generation can benefit from these models. Speech recognition is one example: here, the language model has to find the most likely word sequence conditioned on some voice input, instead of just previous words.", "Another natural application is translation: this is a task of finding the most likely word sequence given some text in another language. In fact, machine translation was exactly the application that the original paper on transformers dealt with.", "The usefulness of neural language models doesn\u2019t need to be restricted to language generation. Once a model is capable of generating text that looks almost human, it means it learned a lot about how words interact. This knowledge, encoded in its many parameters, is very useful to initialize new NLP models for a lot of different tasks: text classification, entity recognition, question answering, and many others.", "Taking advantage of pre-trained models to initialize new ones is called transfer learning, and it is being used very often nowadays with great results \u2014 in fact, it is the main motivation for training these huge neural language models. This is a very interesting line of work, that deserves another post by itself.", "Data Scientist at Kaufland, Germany. Doing Natural Language Processing stuff."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd631b249b499&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----d631b249b499--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Erick Fonseca"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F232a4085d8e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=post_page-232a4085d8e9----d631b249b499---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd631b249b499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d631b249b499---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd631b249b499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=-----d631b249b499---------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://www.reddit.com/r/SubSimulatorGPT2/comments/c7g4qp/did_the_average_soviet_citizen_have_a_sense_of/", "anchor_text": "The post"}, {"url": "https://www.reddit.com/r/SubSimulatorGPT2/", "anchor_text": "r/SubSimulatorGPT2"}, {"url": "https://openai.com/blog/better-language-models/", "anchor_text": "GPT2"}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "r/SubredditSimulator"}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "r/SubredditSimulator"}, {"url": "https://books.google.com/ngrams", "anchor_text": "Google n-grams"}, {"url": "https://www.reddit.com/r/SubredditSimulator/", "anchor_text": "subreddit simulator"}, {"url": "https://en.wikipedia.org/wiki/Softmax_function", "anchor_text": "softmax"}, {"url": "http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf", "anchor_text": "first neural language model"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "great post"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "The Illustrated Transformer"}, {"url": "https://www.reddit.com/r/SubSimulatorGPT2/", "anchor_text": "subreddit simulator"}, {"url": "https://blog.swiftkey.com/swiftkey-debuts-worlds-first-smartphone-keyboard-powered-by-neural-networks/", "anchor_text": "that\u2019s"}, {"url": "https://ai.googleblog.com/2017/05/the-machine-intelligence-behind-gboard.html", "anchor_text": "right"}, {"url": "https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html", "anchor_text": "Speech recognition"}, {"url": "https://arxiv.org/abs/1706.03762", "anchor_text": "original paper on transformers"}, {"url": "https://github.com/openai/gpt-2", "anchor_text": "GPT-2 models"}, {"url": "https://transformer.huggingface.co", "anchor_text": "online platform"}, {"url": "https://svilentodorov.xyz/blog/gpt-345M-finetune/", "anchor_text": "tutorial of fine-tuning GPT-2"}, {"url": "https://pmbaumgartner.github.io/blog/gpt2-jokes/", "anchor_text": "Another tutorial"}, {"url": "https://github.com/karpathy/char-rnn", "anchor_text": "Code"}, {"url": "https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt", "anchor_text": "Shakespearean plays"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d631b249b499---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/nlp?source=post_page-----d631b249b499---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d631b249b499---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd631b249b499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d631b249b499---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd631b249b499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d631b249b499---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd631b249b499&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----d631b249b499--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F232a4085d8e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=post_page-232a4085d8e9----d631b249b499---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F232a4085d8e9%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d631b249b499---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Written by Erick Fonseca"}, {"url": "https://medium.com/@erickrfonseca/followers?source=post_page-----d631b249b499--------------------------------", "anchor_text": "146 Followers"}, {"url": "https://towardsdatascience.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F232a4085d8e9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=post_page-232a4085d8e9----d631b249b499---------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F232a4085d8e9%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-neural-networks-are-learning-to-write-d631b249b499&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d631b249b499---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/ensina-ai/ensinando-portugu\u00eas-ao-gpt-2-d4aa4aa29e1d?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Erick Fonseca"}, {"url": "https://medium.com/ensina-ai?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Ensina.AI"}, {"url": "https://medium.com/ensina-ai/ensinando-portugu\u00eas-ao-gpt-2-d4aa4aa29e1d?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Ensinando portugu\u00eas ao GPT-2O GPT-2 leu a Wikip\u00e9dia em portugu\u00eas e aprendeu a inventar pessoas, cidades e eventos hist\u00f3ricos"}, {"url": "https://medium.com/ensina-ai/ensinando-portugu\u00eas-ao-gpt-2-d4aa4aa29e1d?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "13 min read\u00b7Aug 20, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fensina-ai%2Fd4aa4aa29e1d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fensina-ai%2Fensinando-portugu%25C3%25AAs-ao-gpt-2-d4aa4aa29e1d&user=Erick+Fonseca&userId=232a4085d8e9&source=-----d4aa4aa29e1d----0-----------------clap_footer----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/ensina-ai/ensinando-portugu\u00eas-ao-gpt-2-d4aa4aa29e1d?source=author_recirc-----d631b249b499----0---------------------5f66465e_e715_484a_9c24_899f931e286f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "8"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd4aa4aa29e1d&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fensina-ai%2Fensinando-portugu%25C3%25AAs-ao-gpt-2-d4aa4aa29e1d&source=-----d631b249b499----0-----------------bookmark_preview----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://barrmoses.medium.com/?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Barr Moses"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Zero-ETL, ChatGPT, And The Future of Data EngineeringThe post-modern data stack is coming. Are we ready?"}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "9 min read\u00b7Apr 3"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&user=Barr+Moses&userId=2818bac48708&source=-----71849642ad9c----1-----------------clap_footer----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/zero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c?source=author_recirc-----d631b249b499----1---------------------5f66465e_e715_484a_9c24_899f931e286f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "21"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F71849642ad9c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fzero-etl-chatgpt-and-the-future-of-data-engineering-71849642ad9c&source=-----d631b249b499----1-----------------bookmark_preview----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----2-----------------clap_footer----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=author_recirc-----d631b249b499----2---------------------5f66465e_e715_484a_9c24_899f931e286f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d631b249b499----2-----------------bookmark_preview----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Erick Fonseca"}, {"url": "https://towardsdatascience.com/?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "State-of-the-art Multilingual LemmatizationAn analysis of state-of-the-art lemmatizers that work for tens of languages"}, {"url": "https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": "\u00b711 min read\u00b7Mar 11, 2019"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&user=Erick+Fonseca&userId=232a4085d8e9&source=-----f303e8ff1a8----3-----------------clap_footer----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/state-of-the-art-multilingual-lemmatization-f303e8ff1a8?source=author_recirc-----d631b249b499----3---------------------5f66465e_e715_484a_9c24_899f931e286f-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff303e8ff1a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fstate-of-the-art-multilingual-lemmatization-f303e8ff1a8&source=-----d631b249b499----3-----------------bookmark_preview----5f66465e_e715_484a_9c24_899f931e286f-------", "anchor_text": ""}, {"url": "https://medium.com/@erickrfonseca?source=post_page-----d631b249b499--------------------------------", "anchor_text": "See all from Erick Fonseca"}, {"url": "https://towardsdatascience.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "See all from Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://medium.com/@mattchapmanmsc?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Matt Chapman"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "The Portfolio that Got Me a Data Scientist JobSpoiler alert: It was surprisingly easy (and free) to make"}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b710 min read\u00b7Mar 24"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&user=Matt+Chapman&userId=bf7d13fc53db&source=-----513cc821bfe4----0-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/the-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "42"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F513cc821bfe4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-portfolio-that-got-me-a-data-scientist-job-513cc821bfe4&source=-----d631b249b499----0-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Artificial Corner"}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fartificial-corner%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&user=The+PyCoach&userId=fb44e21903f3&source=-----886a50dabc54----1-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://artificialcorner.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "275"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F886a50dabc54&operation=register&redirect=https%3A%2F%2Fartificialcorner.com%2Fyoure-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54&source=-----d631b249b499----1-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://dwiuzila.medium.com/?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Albers Uzila"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Wanna Break into Data Science in 2023? Think Twice!It won\u2019t be smooth sailing for you"}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b711 min read\u00b7Dec 23, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&user=Albers+Uzila&userId=159e5ce51250&source=-----26842e9a87fe----0-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/wanna-break-into-data-science-in-2023-think-twice-26842e9a87fe?source=read_next_recirc-----d631b249b499----0---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F26842e9a87fe&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwanna-break-into-data-science-in-2023-think-twice-26842e9a87fe&source=-----d631b249b499----0-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://medium.com/@timothymugayi?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Timothy Mugayi"}, {"url": "https://betterprogramming.pub/?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Better Programming"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "How To Build Your Own Custom ChatGPT With Custom Knowledge BaseFeed your ChatGPT bot with custom data sources"}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b711 min read\u00b7Apr 7"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fbetter-programming%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&user=Timothy+Mugayi&userId=34774d6cac27&source=-----4e61ad82427e----1-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://betterprogramming.pub/how-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e?source=read_next_recirc-----d631b249b499----1---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "83"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4e61ad82427e&operation=register&redirect=https%3A%2F%2Fbetterprogramming.pub%2Fhow-to-build-your-own-custom-chatgpt-with-custom-knowledge-base-4e61ad82427e&source=-----d631b249b499----1-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://alexcancode.medium.com/?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Alexander Nguyen"}, {"url": "https://levelup.gitconnected.com/?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Level Up Coding"}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Why I Keep Failing Candidates During Google Interviews\u2026They don\u2019t meet the bar."}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b74 min read\u00b7Apr 13"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fgitconnected%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&user=Alexander+Nguyen&userId=a148fd75c2e9&source=-----dc8f865b2c19----2-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://levelup.gitconnected.com/why-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19?source=read_next_recirc-----d631b249b499----2---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "89"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdc8f865b2c19&operation=register&redirect=https%3A%2F%2Flevelup.gitconnected.com%2Fwhy-i-keep-failing-candidates-during-google-interviews-dc8f865b2c19&source=-----d631b249b499----2-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://wolfecameron.medium.com/?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Cameron R. Wolfe"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Towards Data Science"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "Language Models: GPT and GPT-2How smaller language models inspired modern breakthroughs"}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": "\u00b713 min read\u00b7Nov 24, 2022"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&user=Cameron+R.+Wolfe&userId=28aa6026c553&source=-----8bdb9867c50a----3-----------------clap_footer----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/language-models-gpt-and-gpt-2-8bdb9867c50a?source=read_next_recirc-----d631b249b499----3---------------------3e7c4b13_b664_4a49_88d4_777cc4684f8c-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8bdb9867c50a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-gpt-and-gpt-2-8bdb9867c50a&source=-----d631b249b499----3-----------------bookmark_preview----3e7c4b13_b664_4a49_88d4_777cc4684f8c-------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d631b249b499--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----d631b249b499--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}