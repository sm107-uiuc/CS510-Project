{"url": "https://towardsdatascience.com/forecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a", "time": 1683001079.4669921, "path": "towardsdatascience.com/forecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a/", "webpage": {"metadata": {"title": "Forecasting Stock Prices using XGBoost (Part 1/5) | by Yibin Ng | Towards Data Science", "h1": "Forecasting Stock Prices using XGBoost (Part 1/5)", "description": "There are many machine learning techniques in the wild, but extreme gradient boosting (XGBoost) is one of the most popular. Gradient boosting is a process to convert weak learners to strong learners\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001?source=friends_link&sk=fe1f1ab52fec793c433028fc63c7a965", "anchor_text": "previous article", "paragraph_index": 1}, {"url": "https://finance.yahoo.com/quote/VTI/", "anchor_text": "yahoo finance", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/forecasting-stock-prices-using-prophet-652b31fb564e", "anchor_text": "previous article", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Feature_engineering", "anchor_text": "Wikipedia", "paragraph_index": 14}, {"url": "https://en.wikipedia.org/wiki/Andrew_Ng", "anchor_text": "Andrew Ng", "paragraph_index": 16}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "here", "paragraph_index": 30}, {"url": "https://github.com/NGYB/Stocks/blob/master/StockPricePrediction_fh21/StockPricePrediction_v6c_xgboost.ipynb", "anchor_text": "here", "paragraph_index": 42}, {"url": "https://github.com/NGYB/Stocks/blob/master/StockPricePrediction_fh21/StockPricePrediction_v6d_xgboost.ipynb", "anchor_text": "here", "paragraph_index": 42}, {"url": "https://ngyibin.medium.com/forecasting-stock-prices-using-xgboost-part-2-2-5fa8ce843690?sk=29401a9fa0647fe94068eb13e77306a0", "anchor_text": "here", "paragraph_index": 43}], "all_paragraphs": ["There are many machine learning techniques in the wild, but extreme gradient boosting (XGBoost) is one of the most popular. Gradient boosting is a process to convert weak learners to strong learners, in an iterative fashion. The name XGBoost refers to the engineering goal to push the limit of computational resources for boosted tree algorithms. Ever since its introduction in 2014, XGBoost has proven to be a very powerful machine learning technique and is usually the go-to algorithm in many Machine Learning competitions.", "In this article, we will experiment with using XGBoost to forecast stock prices. We have experimented with XGBoost in a previous article, but in this article, we will be taking a more detailed look at the performance of XGBoost applied to the stock price prediction problem. We list down the main differences between this article and the previous one below:", "In the rest of this article, we will walk through the standard steps of a machine learning project, with a focus on our stock price prediction problem:", "It is worthwhile to be aware that there are other steps of a machine learning project not mentioned here such as data cleaning (not an issue here), missing values imputation (no missing values here), and feature selection (well, we don\u2019t have a lot of features here). These topics are important as well, but are not an issue in our problem, as you will see below.", "It is very important to define the problem statement clearly before you start any work. Here, we aim to predict the daily adjusted closing prices of Vanguard Total Stock Market ETF (VTI), using data from the previous N days. In this experiment, we will use 6 years of historical prices for VTI from 2013\u201301\u201302 to 2018\u201312\u201328, which can be easily downloaded from yahoo finance. After downloading, the dataset looks like this:", "Altogether, we have 1509 days of data to play with. Note that Saturdays and Sundays are not included in the dataset above. A plot of the adjusted closing price in the entire dataset is shown below:", "To effectively evaluate the performance of XGBoost, running one forecast at a single date is not enough. Instead, we will perform various forecasts at different dates in this dataset, and average the results.", "To evaluate the effectiveness of our methods, we will use the root mean square error (RMSE), mean absolute percentage error (MAPE) and mean absolute error (MAE) metrics. For all metrics, the lower the value, the better the prediction. Similar to our previous article, we will use the Last Value method to benchmark our results.", "EDA is an essential part of a machine learning project to help you get a good \u2018feel\u2019 for a dataset. If you participate in Machine Learning competitions (or plan to), extensive EDA may help you generate better features or even discover \u2018information leaks\u2019 which can help you climb the leaderboard rankings. As we will see below, the EDA process involves creating visualizations to help you understand the dataset better.", "The plot below shows the average adjusted closing price for each month. We can infer that based on our dataset, on average, later months have a higher value than earlier months.", "The plot below shows the average adjusted closing price for each day of the month. On average, there is an upward sloping trend, where the later days of the month have a higher price than the earlier days.", "The plot below shows the average adjusted closing price for each day of the week. On average, adjusted closing prices are higher for Thursdays and Fridays than other days of the week.", "The heatmap below shows the correlation of previous days\u2019 adjusted closing prices with the current day\u2019s. It is clear that the nearer the adjusted closing price is to the current day, the more highly correlated they are. Therefore, features relating to adjusted closing prices of the previous 10 days should be used in the prediction.", "Based on the EDA above, we infer that features related to dates might be helpful to the model. Further, adjusted closing prices of the previous 10 days are highly correlated to the target variable. These are important information that we will use for feature engineering below.", "Feature engineering is a creative process and is one of the most important parts of any machine learning project. To highlight the importance of feature engineering, there is a nice quote from Andrew Ng which is worth sharing (from Wikipedia):", "Coming up with features is difficult, time-consuming, requires expert knowledge. \u201cApplied machine learning\u201d is basically feature engineering.", "\u2014 Andrew Ng, Machine Learning and AI via Brain simulations", "In this project, we will generate the following features:", "The features relating to dates are easily generated using the fastai package as such:", "After using the code above the dataframe looks like the below. The column adj_close will be the target column. Features relating to adjusted closing prices of the last N days are omitted for brevity.", "The heatmap below shows the correlation of the features with the target column. The feature year is quite highly correlated with the adjusted closing price. This is unsurprising, because in our dataset, there is an upward-sloping trend where the larger the year, the higher the adjusted closing price. Other features do not exhibit a high correlation with the target variable. From the below, we also found that the feature is_year_start has all NaNs. This is because the first day of the year is never a trading day, and so we remove this feature from the model.", "Below is a bar chart showing the importance scores of the top 10 most important features. This is obtained for the forecast of 2017\u201301\u201303, and forecasts on other dates may have different ranking of the feature importance. As expected, the adjusted closing price of the previous day is the most important feature.", "To perform a forecast, we need training and validation data. We will use 3 years of data as the train set, which corresponds to 756 days since there are about 252 trading days in a year (252*3 = 756). We will use the next 1 year of data to perform validation, which corresponds to 252 days. In other words, for each forecast we make, we need 756+252 = 1,008 days of data for model training and validation. The model will be trained using the train set, and model hyperparameters will be tuned using the validation set.", "To tune the hyperparameters, we will use the moving window validation method. The moving window validation method has been described in detail in one of our previous articles:", "An example is illustrated below, for the case of train size of 756 days, validation size of 40 days, and a forecast horizon of 40 days.", "It is very important in time series prediction that the train, validation, test splits have to be in chronological order. Failure to do so will result in \u2018information leak\u2019 in the model, which is defined as the scenario where the model is trained on data that provides information about the test set. For example, if we have the open price for today and we are trying to predict for the closing price yesterday, immediately we can set our prediction to be equal to the open price of today and we should get pretty good results. The end result is that our model will give better performance than can be expected in real life. Therefore, when building a machine learning model, we need to be very careful about information leaks.", "In what follows, we will use XGBoost to perform forecasts on several days in our test set, namely:", "For each of the 12 forecasts above, we will use a forecast horizon of 21 days. We will use the 1008 days immediately prior to the forecast date as training and validation set, with a 756:252 split as mentioned above.", "Feature scaling is important here because if you were to look at the plot of adjusted closing prices above, splitting the train and test sets in a chronological split almost always results in the adjusted closing prices of the test set having a higher value than the train set. What this means is that a model trained without scaling the adjusted closing prices will only output predictions around the range of the prices in the train set. This has also been explained in our previous article:", "We have experimented with various techniques and in this article, we will use the method found from the above that has the best performance. For each feature group of adjusted closing prices (the lag features) of each sample, we will scale them to have mean 0 and variance 1. For example, if we are doing predictions on day T, we will take the adjusted closing prices of the last N days (days T-N to T-1) and scale them to have mean 0 and variance 1. The same is done on the train, validation, and test sets for the lag features. The date features are not scaled. We then use these scaled lag features and dates features to do prediction. The predicted values will also be scaled and we inverse transform them using their corresponding mean and variance.", "We perform hyperparameter tuning on the validation set. For XGBoost, there are several hyperparameters that can be tuned including n_estimators, max_depth, learning_rate, min_child_weight, subsample, gamma, colsample_bytree, and colsample_bylevel. For a definition of each of these hyperparameters, see here.", "To look at the effectiveness of hyperparameter tuning, we can look at the predictions on our validation set for the forecast of 2018\u201311\u201301. Below show the predictions without hyperparameter tuning, where we just use the default values from the package:", "Below shows the predictions on the same validation set after hyperparameter tuning. You can see the wild prediction on 18 Jan is much more stable now.", "Below shows the hyperparameters before and after tuning:", "Clearly, the tuned hyperparameters differ a lot from the default values. Also, after tuning the RMSE, MAPE and MAE of the validation drops as expected. For example, RMSE drops from 3.395 to 2.984.", "Having performed EDA, feature engineering, feature scaling, and hyperparameter tuning as explained above, we are now ready to perform our forecasts on the test set. In this case, we have a forecast horizon of 21 days which means we need to generate 21 predictions for each forecast. We cannot generate all 21 predictions at one go, because after generating the prediction for day T, we need to feedback this prediction into our model to generate the prediction for day T+1, and so on until we have all 21 predictions. This is known as recursive forecasting. Therefore, we implement a logic like the flowchart below:", "For each day in the forecast horizon, we need to predict, unscale the prediction, compute the new mean and standard deviation of the last N values, scale the adjusted closing prices of the last N days, and predict again.", "Below shows the RMSE, MAPE, and MAE of each forecast, along with their corresponding (selected) optimum hyperparameters tuned using their respective validation sets.", "The results of applying XGBoost on our test set using the moving window validation method are shown below.", "Another way of visualizing the forecasts is to plot each forecast with its actual value. This is shown in the plot below. If we have perfect accuracy, each forecast should lie on the diagonal y=x line.", "Finally, here are the results of our model benchmarked against the Last Value method:", "Using XGBoost with or without the date features give a better performance over the Last Value method. Interestingly, omitting the date features gives a slightly lower RMSE than including the date features (2.32 vs. 2.42). As we have found earlier, the date features have a low correlation with the target variable and likely do not help the model much.", "You can check out the Jupyter notebooks for XGBoost without date features here, and XGBoost with date features here.", "We hope you enjoyed the article above, where we worked and pondered through a real-life dataset rather than a simple textbook example. The intricacies of the recursive forecasting mechanism took us longer than expected, but it was fun. Another important point of this article is to show that there are many decisions to make in building a machine learning model, which makes it very much an art as well as a science. Feel free to leave your comments below, and check out part 2 here!", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7817c1ff536a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://ngyibin.medium.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": ""}, {"url": "https://ngyibin.medium.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Yibin Ng"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7bfe365f9382&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&user=Yibin+Ng&userId=7bfe365f9382&source=post_page-7bfe365f9382----7817c1ff536a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@jamie452?utm_source=medium&utm_medium=referral", "anchor_text": "Jamie Street"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001?source=friends_link&sk=fe1f1ab52fec793c433028fc63c7a965", "anchor_text": "previous article"}, {"url": "https://finance.yahoo.com/quote/VTI/", "anchor_text": "yahoo finance"}, {"url": "https://towardsdatascience.com/forecasting-stock-prices-using-prophet-652b31fb564e", "anchor_text": "previous article"}, {"url": "https://en.wikipedia.org/wiki/Feature_engineering", "anchor_text": "Wikipedia"}, {"url": "https://en.wikipedia.org/wiki/Andrew_Ng", "anchor_text": "Andrew Ng"}, {"url": "https://towardsdatascience.com/forecasting-stock-prices-using-prophet-652b31fb564e", "anchor_text": "Forecasting Stock Prices using ProphetForecasting is a hard science and requires substantial expertise. For these reasons Facebook open-sourced Prophet\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001", "anchor_text": "Machine Learning Techniques applied to Stock Price PredictionMachine learning has many applications, one of which is to forecast time series. One of the most interesting (or\u2026towardsdatascience.com"}, {"url": "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn", "anchor_text": "here"}, {"url": "https://github.com/NGYB/Stocks/blob/master/StockPricePrediction_fh21/StockPricePrediction_v6c_xgboost.ipynb", "anchor_text": "here"}, {"url": "https://github.com/NGYB/Stocks/blob/master/StockPricePrediction_fh21/StockPricePrediction_v6d_xgboost.ipynb", "anchor_text": "here"}, {"url": "https://ngyibin.medium.com/forecasting-stock-prices-using-xgboost-part-2-2-5fa8ce843690?sk=29401a9fa0647fe94068eb13e77306a0", "anchor_text": "here"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7817c1ff536a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7817c1ff536a---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/stock-market?source=post_page-----7817c1ff536a---------------stock_market-----------------", "anchor_text": "Stock Market"}, {"url": "https://medium.com/tag/predictions?source=post_page-----7817c1ff536a---------------predictions-----------------", "anchor_text": "Predictions"}, {"url": "https://medium.com/tag/forecasting?source=post_page-----7817c1ff536a---------------forecasting-----------------", "anchor_text": "Forecasting"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&user=Yibin+Ng&userId=7bfe365f9382&source=-----7817c1ff536a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&user=Yibin+Ng&userId=7bfe365f9382&source=-----7817c1ff536a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7817c1ff536a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7817c1ff536a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7817c1ff536a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7817c1ff536a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7817c1ff536a--------------------------------", "anchor_text": ""}, {"url": "https://ngyibin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://ngyibin.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Yibin Ng"}, {"url": "https://ngyibin.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "877 Followers"}, {"url": "https://www.linkedin.com/in/ngyibin/", "anchor_text": "https://www.linkedin.com/in/ngyibin/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7bfe365f9382&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&user=Yibin+Ng&userId=7bfe365f9382&source=post_page-7bfe365f9382--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff7ac9e5346c4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fforecasting-stock-prices-using-xgboost-a-detailed-walk-through-7817c1ff536a&newsletterV3=7bfe365f9382&newsletterV3Id=f7ac9e5346c4&user=Yibin+Ng&userId=7bfe365f9382&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}