{"url": "https://towardsdatascience.com/balancing-who-handles-data-inconsistency-72779a1404b8", "time": 1682997327.0995572, "path": "towardsdatascience.com/balancing-who-handles-data-inconsistency-72779a1404b8/", "webpage": {"metadata": {"title": "Balancing Who Handles Data Inconsistency | by Randy Au | Towards Data Science", "h1": "Balancing Who Handles Data Inconsistency", "description": "If there\u2019s one thing you learn over the years while working with data, it\u2019s that data quality \u201cguarantees\u201d are essentially well-intentioned fictions. Any time I get to touch an unfamiliar production\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.postgresql.org/docs/9.4/ddl-constraints.html", "anchor_text": "things", "paragraph_index": 3}, {"url": "https://dev.mysql.com/doc/refman/8.0/en/constraints.html", "anchor_text": "typically", "paragraph_index": 3}, {"url": "https://www.bignerdranch.com/blog/guaranteed-consistency-the-case-for-database-constraints/", "anchor_text": "constraints", "paragraph_index": 3}, {"url": "https://ai.google/research/pubs/pub39966", "anchor_text": "Spanner", "paragraph_index": 8}], "all_paragraphs": ["If there\u2019s one thing you learn over the years while working with data, it\u2019s that data quality \u201cguarantees\u201d are essentially well-intentioned fictions. Any time I get to touch an unfamiliar production database, I inevitably do some simple consistency checking, simple things like \u201cdoes this unique ID actually show up only once?\u201d \u201cAre there duplicated events?\u201d \u201cAre these events that should be co-occur exactly 1-to-1 really 1-to-1?\u201d", "Every time I check on a system that\u2019s been running for a year or more, there\u2019s usually a very good chance at least some of those truths have been broken in a limited fashion. And in my opinion, within some reasonable bounds, this is okay. Because the cost of dealing with/preventing issues differs depending on the team.", "The thing about contracts (both in life and in software) is that unless they\u2019re enforced, unless there\u2019s actual teeth and consequences, they\u2019re often ignored whenever it is convenient for one or both parties. There\u2019s also a cost to enforcement that might not be worth paying.", "In the production systems I\u2019ve worked with, things that typically enforce consistency constraints, such as foreign key and check constraints, were explicitly not used in the database for performance reasons or don\u2019t exist (like those dark NoSQL \u201cACID? that\u2019s a drug right?\u201d days). It\u2019s all just a social contract \u201cenforced\u201d within the application layer, not the DB layer. So the only reason there\u2019s no duplicate row entry in the the event log table is because the app knows to not write a duplicated one.", "Other times, there were built-in features such as auto-incrementing IDs that made sense in a single-host environment but eventually had complexity bolted on, like sharding, then multi-regional sharding. As complexity grew, that logic started shifting into the application layer and becomes harder to enforce.", "Most of the time, this all works surprisingly well thanks to the engineers who architect the systems and anticipate the obvious issues. The expected behavior is verified in tests and QA, life generally proceeds.", "Problems only tend to creep up when systems run in production for a time and the system starts accumulating unforeseen issues. Inevitably, a system will go down, or a bug in the app is introduced. Things get even more insane when you have to deal with a distributed system which is much more likely to have unexpected and utterly bizarre failure modes. This is how wonky data gets around these soft \u201cguarantees\u201d and into your data set. Sometimes even \u201chard\u201d guarantees get blown up because there was an unexpected failure mode that uncovers a bug.", "Sure? But here\u2019s the thing, what happens if there really was a duplicated row (or a missing one) in your data? If you\u2019re a bank and this is your transaction log, it\u2019s a huge deal, even if it happens just once every 10 trillion transactions. If you\u2019re just tracking how many users clicked a button, you\u2019re not likely to even notice.", "Meanwhile, the cost for maintaining consistency contracts can be significant. Database foreign key constraints might be okay, or not, depending on your exact hardware, db workload and schema. You need to do actual tests and benchmarks to figure out what the cost for FKs are. If you needed to guarantee read/write transaction consistency across the whole planet, you either had to wait for the speed of light, or come up with crazy new algorithms like with Spanner.", "You could do after-the-fact checks to sound alarm bells if an issue is detected, but that\u2019s a whole new suite of tests you need to write and maintain as the app evolves. Plus, it requires you to proactively anticipate the consistency issues to check for.", "For big NoSQL systems where you might only have eventual consistency, there might not be something you can do about it without switching to an entirely different system architecture (and all the costs involved in that).", "\u2014 What? But my data is messed up! I literally have a duplicate ID in a table where that field that\u2019s supposed to be unique (true story). My world is all lies. How can this ever be \u201cfine\u201d? You\u2019re crazy.", "All that\u2019s happened in you\u2019ve found evidence of a bug, take a breath and look at what was affected. Start the debugging process. Did you make a mistake? What are it\u2019s main effects? Are there critical systems that are affected? Is it an ongoing issue, or has it stopped already? Is there a signature you can use to identify problem areas?", "Once you have a sense of what\u2019s going on (and that it\u2019s not a mistake), file a bug with an appropriate severity. If it\u2019s a critical core business data\u00a0source, it might warrant a P0.", "Next, determine if you can work around the damage caused by the incident. For things that have a strong human element in them, like ad-hoc and exploratory work, can you work around the issue by identifying the faulty rows somehow? Can you do your analysis in degraded state with less fine-grained metrics like using unique counts and correlated proxy stats that are unaffected?", "Things are more complicated with automated data pipelines since those aren\u2019t as flexible. But still, depending on what your models and systems are doing, it might be within acceptable error. You have to test to find out.", "Murphy\u2019s Law applies to all things, every system will fail eventually. Perfectly clean data exists only in toy problem sets from school. The question is how many resources are you willing to invest in to guard against progressively rarer events.", "As an organization, you\u2019re balancing the ability the ability to prevent/catch data issues with engineering before things happen, with the time and risk of having to deal with the aftermath when something does happen.", "The engineering of consistency checks and constraints can be\u00a0expensive, both from raw computational resources and also in engineering hours to build and maintain. Plus it\u2019s another system that can potentially fail! Sure there are a few relatively cheap things you can architect in from the start, but let\u2019s admit that many of these efforts are typically\u2026 bolted on later.", "On the other hand, if you don\u2019t do a lot of consistency checking up front, when you find bad data you as the data scientist will be forced to deal with it.", "As I mentioned before, it depends on the type of DS work that\u2019s done. Compared to a production system or pipeline, analysis and exploratory research is a very human and flexible thing. Since there\u2019s lots of experimentation and exploration built into the process, it\u2019s inherently more capable of handling bizarre data errors that would take a ton of engineering work to prevent or detect.", "The power to use approximations is probably the strongest weapon we have when dealing with messed up data. Given what we know about how everything works and the nature of the bug creating bad data, it\u2019s possible to at least derive useful approximations, upper or lower bounds that can at least provide partial information.", "It\u2019s thanks to this ability to use degraded data sets that is why if I were forced to choose a side, I\u2019d devote less resources on ensuring up front data consistency on data sets that aren\u2019t mission critical. It\u2019s more efficient to work around incidental bugs during\u00a0occasional\u00a0consumption.", "You still need the data to be reasonably clean and reliable! But you shouldn\u2019t be trying to plan ahead for every single black swan event you can imagine. Things will go wrong and you\u2019ll deal with it when it happens.", "Depending on the specific situation, data issues can have a huge effect on a model, or no effect at all. Only you as model designer will be able to know. But even with the huge range of possibilities there are a few common situations we can talk about.", "If the data issue has been around a long time, you might not need to do anything immediately because the model was literally trained on the bad data and was launched after being judged to be giving reasonable output. Ironically, you need to test if your model can handle receiving clean and bugfixed data. This brings up all sorts of epistemological questions about the original validity of the model, but from a pure black-box output viewpoint it works.", "Similarly, since many models prioritize the use of recent data, old and bad data have a tendency to \u201cage out\u201d of the system. It might not even be worth fixing the historic data if a fix would take longer than the month or whatever it is for things to self correct.", "It\u2019s also rare (but not unheard of) that a data issue will trigger a catastrophic re-analysis and rewrite of a model because the models tend to pull in many inputs together. A data bug would need to affect a really wide area to affect a bunch of model inputs because almost all models have a feature reduction step that tries to minimize the correlation between variables.", "One example of such a bug would be if important data points were dropped in a very biased manner, introducing a bunch of skew into distributions. Something like a system that systematically fails to identify purchases from large customers that are only from the state of California. Hopefully these sorts of bugs are big enough that they are found very quickly because they should have big effects on other things in the business.", "What I\u2019m saying is the data science process has more natural shock absorbance. The marginal cost of improvement curves for data quality and vs human-in-the-loop analytics processes are very different and there is a point where it makes sense to say \u201cok our data consistency safeguards are as good as we can afford to make them now\u201d.", "The responsible answer is when you realize that some process and piece of data has become more important in your business and therefore you should invest more in it. This means you need to do periodic reviews and be proactive. It is really hard to do well and I only really do it when something is in active development and I\u2019m actively thinking about these issues.", "The honest answer is that when you find a bug and are made to realize how important something is. These things can easily sneak up on even\u00a0the\u00a0best\u00a0of us\u00a0as\u00a0systems\u00a0change. We might not have visibility into all possible changes and we never have 100% clarity over the side effects of every last aspect of a system.", "It\u2019s always a balanced dance. You\u2019ll never be fully comfortable with what you have, and that discomfort is probably a good sign that you\u2019re close to a manageable spot.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I stress about data quality a lot. Data nerd/scientist, camera junkie. Quant UXR @Google Cloud. Formerly @bitly, @Meetup, @primarydotcom. Opinions are my own."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F72779a1404b8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://randy-au.medium.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": ""}, {"url": "https://randy-au.medium.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Randy Au"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bd01667d4e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&user=Randy+Au&userId=5bd01667d4e5&source=post_page-5bd01667d4e5----72779a1404b8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.postgresql.org/docs/9.4/ddl-constraints.html", "anchor_text": "things"}, {"url": "https://dev.mysql.com/doc/refman/8.0/en/constraints.html", "anchor_text": "typically"}, {"url": "https://www.bignerdranch.com/blog/guaranteed-consistency-the-case-for-database-constraints/", "anchor_text": "constraints"}, {"url": "https://ai.google/research/pubs/pub39966", "anchor_text": "Spanner"}, {"url": "https://medium.com/tag/data-science?source=post_page-----72779a1404b8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-engineering?source=post_page-----72779a1404b8---------------data_engineering-----------------", "anchor_text": "Data Engineering"}, {"url": "https://medium.com/tag/analytics?source=post_page-----72779a1404b8---------------analytics-----------------", "anchor_text": "Analytics"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&user=Randy+Au&userId=5bd01667d4e5&source=-----72779a1404b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&user=Randy+Au&userId=5bd01667d4e5&source=-----72779a1404b8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F72779a1404b8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----72779a1404b8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----72779a1404b8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----72779a1404b8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----72779a1404b8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----72779a1404b8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----72779a1404b8--------------------------------", "anchor_text": ""}, {"url": "https://randy-au.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://randy-au.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Randy Au"}, {"url": "https://randy-au.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5bd01667d4e5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&user=Randy+Au&userId=5bd01667d4e5&source=post_page-5bd01667d4e5--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc29d9b065bd4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbalancing-who-handles-data-inconsistency-72779a1404b8&newsletterV3=5bd01667d4e5&newsletterV3Id=c29d9b065bd4&user=Randy+Au&userId=5bd01667d4e5&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}