{"url": "https://towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8", "time": 1683015647.610753, "path": "towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8/", "webpage": {"metadata": {"title": "How To Build An Artificial Neural Network From Scratch In Julia | by Bernard Brenyah | Towards Data Science", "h1": "How To Build An Artificial Neural Network From Scratch In Julia", "description": "One of the popular requests that I usually get from readers who love the \u201cbuilding algorithms from scratch\u201d series is coverage on deep learning or neural networks. Neural network models can be quite\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/GPT-3", "anchor_text": "billions, in the case of GPT-3", "paragraph_index": 6}], "all_paragraphs": ["One of the popular requests that I usually get from readers who love the \u201cbuilding algorithms from scratch\u201d series is coverage on deep learning or neural networks. Neural network models can be quite complex but at their core, most architectures have a common base from which new logics have emerged. This core architecture (which I refer to as Vanilla Neural Network) will be the main focus of this post. As a result, this post aims to build a vanilla neural network WITHOUT any ML package but instead implement these mathematical concepts with Julia (a language I feel is the perfect companion for numerical computing). The goal is to implement a neural network model from scratch to solve a binary classification problem.", "So what is even a neural network? The genesis of neural networks is widely attributed to the paper of Warren McCulloch (neurologist) and Walter Pitts (mathematician) in 1934. In this paper, they modelled a simple neural network using electrical circuits with inspiration on how the human mind works.", "At some basic level, Neural networks can be seen as a system which tries to take inspiration from how biological neurons share information with each other in an attempt to discover patterns within some supplied data. Neural networks are typically composed of interconnected layers. The layers of a vanilla Neural Network are;", "The input layer accepts the raw data which are passed onto the hidden layer(s) whose job is to try to find some underlying patterns in this raw data. Finally, the output layer(s) combines these learned patterns and spits out a predicted value.", "How do these interconnected layers share information? At the heart of neural networks is the concept of neurons. All these layers have one thing in common, they have are composed of neurons. These neurons are responsible for the transmission of information across the network but what sort of mechanism is used for this data transmission?", "Instead of pouring out some machine learning theories to explain this mechanism, let us simply explain it with a very simple yet practical example involving a single neuron. Assuming that we have a simple case where we want to train a neural network to distinguish between a dolphin and a shark. Luckily for us, we have thousands or millions of information gathered from dolphins and sharks around the world. For simplicity\u2019s sake, let\u2019s say the only variables recorded for each of the observed are \u2018size(X1)\u2019 and \u2018weight(X2)\u2019. Every neuron has its own parameters \u2014 \u2018weights(W1)\u2019 and \u2018bias(b1)\u2019 \u2014 which are used to process the incoming data (X1 & X2) with these parameters and then share the output of this process with the next neuron. This process is visually represented below;", "A typical network is composed of so many layers of neurons (billions, in the case of GPT-3). The aggregate of these neurons make up the main parameters of a vanilla neural network. Ok, we know how a neuron works, but how does an army of neurons learn how to make good predictions on some supplied data?", "To understand this learning process, we will compose our own neural network architecture to solve the task at hand. To do this, we would need to breakdown the sequence of operations within a typical vanilla network via 5 components;", "The nature of the problem for which a network is designed to solve normally dictates the range of choices available in these 5 components. In a way, these networks are like LEGO systems where users plug in different activation functions, loss/cost functions, optimization techniques. It is of no surprise that this deep learning is one of the most experimental fields in scientific computing. With all these in mind, let us now move over the fun part and implement our network to solve the binary classification task.", "NB: The implementation will be vectorized with input data of dimensions (n, m) examples mapping some ouptut target of dimensions (m, 1) where n is the number is the number of features and m is the number of training examples.", "These networks are typically composed by stacking up many layers of neurons. The number of layers and neurons are fixed and the elements of these neurons (weights and bias) initially normally populated some random values (for symmetry-breaking since we have an optimisation problem). This snippet randomly initialiases the parameters of a given network dimensions:", "With the parameters of the model/network initialised, let us build the functions that will run the input data through the network to generate a predicted value. As explained earlier, activation function functions play a crucial role in this flow of information. Our network will use the ReLU activator for activations upto the output layer and the sigmoid activator. These activators are visualised below;", "These two functions implements these activators;", "With our activators implemented, our attention now switches to the movement of data from the input layer to the output layer via the forward pass or propagation sequence. First, a linear output is calcluated and then one of the activation functions is used to convert the linear output into a non-linear one.", "NB: The inputs and outputs of the activations are \u2018cached\u2019 or saved for later retrieval and usage.", "These two functions below provide the utilities for moving the data through the network are;", "The sequence for whole forward propagation algorithm is pretty straightforward. We simply loop over the network dimensions in such a way that the output of a layer becomes the input of the next layer until the last layer which spits out the a predicted value or score (a probability score when sigmoid activation is used). This snippet below implements this sequence;", "We now have a network which is capable of making some predictions for our training examples based on the randomly initialised parameters. The next step is to see how good those predictions are. For this objective, the network needs a cost or loss function. This cost/loss function has a simple (crucial) task which is to give a summary of how close the predictions are to the actual output values for all the training examples. Log Loss (Binary Cross Entropy) function is suited for a binary classification problems so this next function implements that cost function;", "Our network can generate some random values as predictions and it can also know how far off these predictions are from the acutal outputs so the next logical step will be figuring out how to improve its decision making capabilities beyond making random predictions, right?", "At the heart of the \u2018learning\u2019 process with neural networks is the backpropagation algorithm. Unsurprisingly, that is the most difficult part for newbies to wrap their heads around. Let\u2019s try to unpack this step with some high level intuition before implementing it in our network.", "The goal of the backpropagation sequence is to understand the how each of the parameters (all the weights & biases of all the layers) of the network changes with respect to the cost/loss function. But why would we want to do that? This process allows us to turn the learning strategy into a minimisation objective where we want that cost/loss output to be a low as possible. Additionally, we also know and can measure how changes in the bolts and knobs (weights and biases) of our network affect the loss function. Ponder over this simplified analogy for a second and realise how powerful this concept is! Ultimately, all we have to do is tweak the parameters \u2014 weights and biases \u2014 of the network such that it delivers the lowest value in terms of the loss/cost function and voila, we are close to make predictions just like in Sci-Fi movies!", "At the heart of implementation side of backpropagation, is the concept of chain rule borrowed from multivariate calculus. In fact, the whole sequence for backpropagation can be seen as a long chain of partial derivatives of each of the layers (and their weights \u2018W\u2019 and biases \u2018b\u2019 as well as linear outputs, Z) with respect to the cost/loss function. As the name connotes, this chain of sequences computes the partial derivatives, using caches that were stored during the forward propagation sequence, by going \u2018backwards\u2019 from the output layer to the input layer. For brevity\u2019s sake, here are the vectorized formulas needed to implement backpropagation in our network.", "These two utility functions implements the derivatives of our sigmoid and ReLu activations:", "With the derivative of the activation functions, let\u2019s focus on unpacking the stored components of a linear activated output (weight, bias, & activated output of the previous layer) and computing the partial derivatives of each of them with respect to the loss function with these two functions.", "Combining the derivate of the activation functions and the stored components of each layers activated outputs, backpropagation algorithm is composed for the network with this function.", "We are almost there! Our network is taking shape now. When given some layers as an architecture, it can initiate the parameters/weights of these layers randomly, generate some random predictions thereof and quantify how right or wrong overall it is from the ground truth using this function;", "Additionally, backpropagation steps now allows the network to measure how each of these weights changes with respect to loss function. Thus, all the networks needs to do is to tweak these different weights such that its predictions match as much as possible with the actual outputs of the data given to this network.", "In theory, one could try all possible ranges of all the parameters in the network and select the network that gives the best possible loss (as determined by the output value of the loss function). In practice, this inefficient approach is impractical for the simple fact that it would be computationally too expensive for finding the best parameters of a network with so many parameters. For this and other reasons, an optimization technique is used to find the best parameters. Gradient Descent is one of the most popular optimization techniques for find the best parameters of such a network. Let\u2019s now try to explain this optimization technique with a simple practical analogy.", "Suppose you are placed on the top mountain Everest given a challenge of finding the lowest part of the moutain (where a bag full of gold has been placed as the price) while wearing a blindfold. The only help one can get is that at every point in this journey, he/she can talk to a guide via a walkie talkie and ONLY ask about the current direction relative to the highest point of the mountain (gradient) at that point. Gradient descent, as an optimization technique, proposes a simple but effective way of helping one to navigate this challenge (given the constraints). A potential solution offered by this technique is that, one starts at some point and at different steps uses the walkie-talkie to get the relative direction to the highest point of Everest and then move some steps (learning rate) in the opposite direction given. Repeat these technique until one gets to the bag of gold at the lowest part of Everest.", "This snippet applies Gradient descent in our network to update the parameters of the network.", "The training of a neural neural is basically the chain of sequence where input data are forwarded through the network using available parameters of this network, predictions compared to the actual training data predictions and then finally the tweaking of the parameters as a method of improving predictions. Each run of this sequence is referred to as an \u2018epoch\u2019 in machine learning lingo. The core idea of this training phase is to try to improve predictions generated by the network at various attempts or iterations (epoch). This snippet combines all the various components that we have generated so far to do just that:", "With all the components in place, it is time to test if the implementation works. For the sake of brevity, let us just generate some simple synthetic binary classification data for this demonstration. This is easily done as:", "As the plots show the network starts with a low accuracy and high cost values but as it keeps learning and updating the parameters of the network, accuracy rises (while cost plateaus) until it achieves a perfect score on this super simple and perfect dummy dataset. Sadly, datasets are rarely this perfect in real life!", "As always, I look forward to feedback! Until the next post, go ahead and play with this implementation and maybe implement other loss functions and optimization algorithms. In the next post, we will go through the process of making and registering a Julia package using this implementation as an example.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I have a love/hate relationship with numbers"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc839219b3ef8&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@bbrenyah?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bbrenyah?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Bernard Brenyah"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff21ca351a4aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&user=Bernard+Brenyah&userId=f21ca351a4aa&source=post_page-f21ca351a4aa----c839219b3ef8---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://alexlenail.me/NN-SVG/LeNet.html", "anchor_text": "NN-SVG"}, {"url": "https://en.wikipedia.org/wiki/GPT-3", "anchor_text": "billions, in the case of GPT-3"}, {"url": "http://alexlenail.me/NN-SVG/LeNet.html", "anchor_text": "NN-SVG"}, {"url": "https://medium.com/tag/julia?source=post_page-----c839219b3ef8---------------julia-----------------", "anchor_text": "Julia"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c839219b3ef8---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c839219b3ef8---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c839219b3ef8---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c839219b3ef8---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&user=Bernard+Brenyah&userId=f21ca351a4aa&source=-----c839219b3ef8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&user=Bernard+Brenyah&userId=f21ca351a4aa&source=-----c839219b3ef8---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc839219b3ef8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c839219b3ef8---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c839219b3ef8--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c839219b3ef8--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c839219b3ef8--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bbrenyah?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@bbrenyah?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Bernard Brenyah"}, {"url": "https://medium.com/@bbrenyah/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "970 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff21ca351a4aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&user=Bernard+Brenyah&userId=f21ca351a4aa&source=post_page-f21ca351a4aa--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7dbf902a4f4d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8&newsletterV3=f21ca351a4aa&newsletterV3Id=7dbf902a4f4d&user=Bernard+Brenyah&userId=f21ca351a4aa&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}