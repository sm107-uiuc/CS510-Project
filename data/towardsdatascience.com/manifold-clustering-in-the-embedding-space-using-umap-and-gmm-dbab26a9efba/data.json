{"url": "https://towardsdatascience.com/manifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba", "time": 1683018412.42244, "path": "towardsdatascience.com/manifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba/", "webpage": {"metadata": {"title": "Manifold clustering in the embedding space using UMAP and GMM | by Gianmario Spacagna | Towards Data Science", "h1": "Manifold clustering in the embedding space using UMAP and GMM", "description": "In the previous article Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL we have seen how to represent pictures into a multi-dimensional numerical embedding space. We\u2026"}, "outgoing_paragraph_urls": [{"url": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/", "anchor_text": "Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668", "anchor_text": "How exactly UMAP works", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Silhouette_(clustering)", "anchor_text": "Silhouette score", "paragraph_index": 16}, {"url": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/", "anchor_text": "Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL", "paragraph_index": 21}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html", "anchor_text": "exact subset of labels match as defined in scikit-learn", "paragraph_index": 23}, {"url": "https://en.wikipedia.org/wiki/Rand_index#:~:text=(true%20negatives).-,Adjusted%20Rand%20index,specified%20by%20a%20random%20model.", "anchor_text": "Adjusted Rand Index", "paragraph_index": 25}, {"url": "https://hdbscan.readthedocs.io/en/latest/", "anchor_text": "HDBSCAN", "paragraph_index": 30}, {"url": "https://github.com/gm-spacagna/docem", "anchor_text": "https://github.com/gm-spacagna/docem", "paragraph_index": 32}, {"url": "https://datasciencevademecum.com/2014/02/27/data-clustering-dont-worry-about-the-algorithm/", "anchor_text": "Data Clustering? don\u2019t worry about the algorithm", "paragraph_index": 33}, {"url": "https://datasciencevademecum.com/2014/02/27/a-distributed-genetic-evolutionary-tuning-for-data-clustering/", "anchor_text": "A Distributed Genetic Evolutionary Tuning for Data Clustering: Part 1", "paragraph_index": 33}, {"url": "https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/", "anchor_text": "https://datasciencevademecum.com", "paragraph_index": 34}], "all_paragraphs": ["In the previous article Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL we have seen how to represent pictures into a multi-dimensional numerical embedding space. We have also seen the effectiveness of the embedding space to represent similar pictures closely to each other. In this tutorial, we will see a few clustering techniques that are suitable for discovering and identifying the manifolds in our dataset. Moreover, the proposed clustering technique is also motivated by the \u201cdocument embedding averaging\u201d that will be described in the next article.", "Dimensionality reduction is not just used for data visualization, but it is a fundamental step for clustering algorithms due to the \u201c curse of dimensionality \u201c. In other words, if the number of dimensions increases then most of the points will start to look as similar and as different from each other across at least a few of those dimensions. The effect is that there is no clear structure to follow resulting in a random grouping of the data points.", "The unsupervised dimensionality reduction techniques are divided into two families: Linear Projection and Manifold Learning.", "The main difference of manifold learning with linear projections (e.g. PCA, SVD) is that it can handle non-linear relationships in the data and it is very effective for clustering groups of similar data points preserving their relative proximities. For our purposes and given the nature of our data (the embedding space generated by a deep convolutional neural network), we do not consider any linear projection technique as suitable.", "In the manifold learning family, there are two main competitors: t-SNE and UMAP.", "t-SNE vs UMAP 3d projection of COCO detection 2017 pictures colored by supercategory", "Uniform Manifold Approximation and Projection (UMAP) is a general-purpose manifold learning and dimension reduction algorithm.T-distributed Stochastic Neighbour Embedding (t-SNE) is an algorithm that generates a low-dimensional graph trying to keep similar instances close and dissimilar instances apart.", "They sound similar, and in fact from a lot of aspects they are. Nevertheless, let\u2019s summarize a few reasons to prefer UMAP over t-SNE for clustering purposes:", "For a more comprehensive comparison of t-SNE vs. UMAP please refer to the following article: How exactly UMAP works.", "For the reasons discussed above, we can conclude that t-SNE is a great visualization tool but UMAP is a more suitable technique for clustering purposes in the case of manifold structures.", "GMM is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It can be seen as a generalization of the more popular k-means model. The advantage of using GMM over k-means is that it can represent clusters of different sizes and shapes based on a parametric covariance matrix.In k-means the clusters are spherical over all of the dimensions and share the same diameter. This is a big limitation when considering a manifold feature space as is the case of transforming a deep convolutional neural network embedding space with UMAP.", "Another distinction is in the interpretation of the clustering output. k-means divide the space into voronoi cells and hard assign each point to the cluster of the closest centroid. GMM, on the other hand, gives us an interpretable output modeling the probability that each data point belong to each cluster. The latter is a desired probability for dealing with fuzzy situations in presence of overlapping clusters or outliers.", "As with k-means, also GMM requires the number of clusters k to be specified.", "Moreover, in order for GMM to be able to model arbitrary elliptic shapes in the feature space, the covariance matrix should be \u201cfull\u201d. The problem with \u201cfull\u201d GMM models is that the degrees of freedom increase quadratically with the dimension of the feature space, risking to overfit the data. There are a few constrained versions of GMM that impose certain properties to the covariance matrix., namely: spherical, diagonal, and tied.", "We are now left with two major parameters to tune: the number of clusters k and the covariance type among the 4 options listed above.", "Likely, since GMM is a probabilistic model we can calculate the Bayesian Information Criterion (BIC) that is a statistics calculated as the sum of the negative log-likelihood of the model and a penalty term that is a function of the number of data samples and the number of free parameters of the model. The smaller the BIC value the more preferable is the model. Nonetheless, searching for the minimum BIC score may suggest selecting a model with a lot of clusters in front of tiny decreases of the score. That is why a preferred approach is to identify the elbow of the curve that corresponds to the minimum of the second derivative. The BIC score is comparable among different clustering outputs only if they are representing the same points in the same feature space. That is, we cannot compare data points reduced via PCA with data points reduced via UMAP.", "Another technique is the Silhouette score that is an empirical method measuring the consistency of the clusters by comparing how much a point is similar to its cluster (cohesion) compared to the other clusters (separation). The silhouette ranges from \u22121 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate, otherwise the clustering configuration may have too many or too few clusters.", "If we plot both the BIC and Silhouette curves as function of the number of clusters k for the 4 different covariance types we obtain the following graph:", "The dot points correspond, respectively, to the elbow and the maximum of the BIC and Silhouette curves.", "We can conclude that the ideal number of clusters should be between 30 and 50. In terms of the covariance type, the tied type minimizes the BIC while there is not strong evidence of worsening results in the Silhouette curve. The lower BIC score can be explained by the good trade-off between low model complexity and the high likelihood of the points. Moreover, given the nature of the feature space, it does make sense to consider manifolds of irregular but similar shapes.", "The selected configuration for us will be tied covariance type and 40 clusters.", "In order to visualize the clusters we will re-use the 3D projections that were calculated in Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL but we will color the points based on the assigned clusters rather than the COCO super categories of the pictures.", "In order to measure the predictive power of the embedding space, let\u2019s try to predict the COCO annotations. We can train a random forest classifier with default parameters on a multi-labels task. Since each picture can have none, one, or many categories annotated, the task consists of predicting whether a given label is present or not in the picture.", "I have used 4000 pictures for training and 1000 for the test stratifying on the most frequent label in each picture. The multilabel accuracy ( exact subset of labels match as defined in scikit-learn) would be 16.7% which is not bad considering the high cardinality of the task. If we micro-average all of the label predictions we can perform a binary evaluation:", "We have achieved 89% precision and 31% recall on all of the possible picture annotations, not bad and not great. We should consider that we only trained on a very small sample of pictures and a few labels had very few occurrences. Nonetheless, the purpose of this tutorial is not to predict COCO categories but rather to show the effectiveness of the embedding features in identifying correct manifolds.", "Since we have grouped the COCO pictures into 40 unsupervised clusters, let\u2019s compare our grouping with the categories provided in the COCO taxonomy. We can use the Adjusted Rand Index that is a measure of similarity between two clustering outputs. The higher the score, the higher the consistency between the two groupings.", "As we could have already observed from the 3D projections, we can conclude that the discovered topicality defined by the manifolds in the data does not match the COCO taxonomy.", "What topic is each cluster representing then? Let\u2019s print the pictures closest to the centroid in a few sample clusters.", "In this tutorial, we have learned how to cluster pictures in their latent embedding space. We first have used UMAP for isolating manifolds and projecting them into a lower-dimensional space. We then used GMM for discovering the high-density areas in the UMAP space. The BIC elbow and Silhouette techniques were used to find the ideal number of clusters as well as the constraints to the covariance matrix. Through the AdjustedRand test, we demonstrated that the data is intrinsically organized into major topics that do not match with the COCO taxonomy. For instance, we found clusters for horses, bears, towers, watersports, people\u2019s dining, and more.", "The presented methodology can be used to cluster any dataset that presents high-dimensional manifolds, not just pictures. It is in general suitable for embeddings produced by neural network models.", "If instead of using a pre-trained network, you are training your own, you may want to consider a small dimensionality (below 50) such that you may not need any dimensionality reduction before clustering. Other clustering algorithms that work with any kind of shapes, and are not constrained by the Gaussian mixture assumption, are the hierarchical density-based models such as HDBSCAN.", "Stay tuned for the next article on how to exploit embedding features and the manifolds clusters to average and represent collections of datapoints (documents) into the same latent space.", "You can find the code and the notebooks at https://github.com/gm-spacagna/docem.", "If you want to learn more about tuning techniques for data clustering algorithms you can read those articles: Data Clustering? don\u2019t worry about the algorithm and A Distributed Genetic Evolutionary Tuning for Data Clustering: Part 1.", "Originally published at https://datasciencevademecum.com on January 2, 2021.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Director of Artificial Intelligence at Brainly"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdbab26a9efba&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://gm-spacagna.medium.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": ""}, {"url": "https://gm-spacagna.medium.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Gianmario Spacagna"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb66d34d8e1ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&user=Gianmario+Spacagna&userId=b66d34d8e1ba&source=post_page-b66d34d8e1ba----dbab26a9efba---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@toamna?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "DIMA VALENTINA"}, {"url": "https://unsplash.com/s/photos/flock?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/", "anchor_text": "Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL"}, {"url": "https://en.wikipedia.org/wiki/Curse_of_dimensionality", "anchor_text": "curse of dimensionality"}, {"url": "https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668", "anchor_text": "How exactly UMAP works"}, {"url": "https://towardsdatascience.com/mixture-modelling-from-scratch-in-r-5ab7bfc83eef", "anchor_text": "source"}, {"url": "https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html", "anchor_text": "scikit-learn"}, {"url": "https://en.wikipedia.org/wiki/Silhouette_(clustering)", "anchor_text": "Silhouette score"}, {"url": "https://datasciencevademecum.com/2020/12/02/extracting-rich-embedding-features-from-pictures-using-pytorch-and-resnext-wsl/", "anchor_text": "Extracting rich embedding features from pictures using PyTorch and ResNeXt-WSL"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html", "anchor_text": "exact subset of labels match as defined in scikit-learn"}, {"url": "https://en.wikipedia.org/wiki/Rand_index#:~:text=(true%20negatives).-,Adjusted%20Rand%20index,specified%20by%20a%20random%20model.", "anchor_text": "Adjusted Rand Index"}, {"url": "https://hdbscan.readthedocs.io/en/latest/", "anchor_text": "HDBSCAN"}, {"url": "https://github.com/gm-spacagna/docem", "anchor_text": "https://github.com/gm-spacagna/docem"}, {"url": "https://datasciencevademecum.com/2014/02/27/data-clustering-dont-worry-about-the-algorithm/", "anchor_text": "Data Clustering? don\u2019t worry about the algorithm"}, {"url": "https://datasciencevademecum.com/2014/02/27/a-distributed-genetic-evolutionary-tuning-for-data-clustering/", "anchor_text": "A Distributed Genetic Evolutionary Tuning for Data Clustering: Part 1"}, {"url": "https://datasciencevademecum.com/2021/01/02/manifold-clustering-in-the-embedding-space-using-umap-and-gmm/", "anchor_text": "https://datasciencevademecum.com"}, {"url": "https://medium.com/tag/manifold-learning?source=post_page-----dbab26a9efba---------------manifold_learning-----------------", "anchor_text": "Manifold Learning"}, {"url": "https://medium.com/tag/clustering?source=post_page-----dbab26a9efba---------------clustering-----------------", "anchor_text": "Clustering"}, {"url": "https://medium.com/tag/umap?source=post_page-----dbab26a9efba---------------umap-----------------", "anchor_text": "Umap"}, {"url": "https://medium.com/tag/gmm?source=post_page-----dbab26a9efba---------------gmm-----------------", "anchor_text": "Gmm"}, {"url": "https://medium.com/tag/dimensionality-reduction?source=post_page-----dbab26a9efba---------------dimensionality_reduction-----------------", "anchor_text": "Dimensionality Reduction"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&user=Gianmario+Spacagna&userId=b66d34d8e1ba&source=-----dbab26a9efba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&user=Gianmario+Spacagna&userId=b66d34d8e1ba&source=-----dbab26a9efba---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdbab26a9efba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----dbab26a9efba---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----dbab26a9efba--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----dbab26a9efba--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----dbab26a9efba--------------------------------", "anchor_text": ""}, {"url": "https://gm-spacagna.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://gm-spacagna.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Gianmario Spacagna"}, {"url": "https://gm-spacagna.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "134 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb66d34d8e1ba&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&user=Gianmario+Spacagna&userId=b66d34d8e1ba&source=post_page-b66d34d8e1ba--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fd208a609588c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmanifold-clustering-in-the-embedding-space-using-umap-and-gmm-dbab26a9efba&newsletterV3=b66d34d8e1ba&newsletterV3Id=d208a609588c&user=Gianmario+Spacagna&userId=b66d34d8e1ba&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}