{"url": "https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca", "time": 1683000198.8138509, "path": "towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca/", "webpage": {"metadata": {"title": "A Hands-On Guide To Text Classification With Transformer Models (XLNet, BERT, XLM, RoBERTa) | by Thilina Rajapakse | Towards Data Science", "h1": "A Hands-On Guide To Text Classification With Transformer Models (XLNet, BERT, XLM, RoBERTa)", "description": "A step-by-step tutorial on using Transformer Models for Text Classification tasks. Includes ready-to-use code for BERT, XLNet, XLM, and RoBERTa models from Pytorch-Transformers."}, "outgoing_paragraph_urls": [{"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 0}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3", "anchor_text": "SimpleTransformers", "paragraph_index": 1}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification", "anchor_text": "Github repo", "paragraph_index": 2}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "here", "paragraph_index": 5}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04", "anchor_text": "Using BERT for Binary Text Classification", "paragraph_index": 6}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification", "anchor_text": "Github repo", "paragraph_index": 27}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 41}], "all_paragraphs": ["Please consider using the Simple Transformers library as it is easy to use, feature-packed, and regularly updated. The article still stands as a reference to BERT models and is likely to be helpful with understanding how BERT works. However, Simple Transformers offers a lot more features, much more straightforward tuning options, all the while being quick and easy to use! The links below should help you get started quickly.", "The Pytorch-Transformers (now Transformers) library has moved on quite a bit since this article was written. I recommend using SimpleTransformers as it is kept up to date with the Transformers library and is significantly more user-friendly. While the ideas and concepts in this article still stand, the code and the Github repo are no longer actively maintained.", "I highly recommend cloning the Github repo for this article and running the code while you follow the guide. It should help you understand both the guide and the code better. Reading is great, but coding is better. \ud83d\ude09", "Special thanks to Hugging Face for their Pytorch-Transformers library for making Transformer Models easy and fun to play with!", "Transformer models have taken the world of Natural Language Processing by storm, transforming (sorry!) the field by leaps and bounds. New, bigger, and better models seem to crop up almost every month, setting new benchmarks in performance across a wide variety of tasks.", "This post is intended as a straightforward guide to utilizing these awesome models for text classification tasks. As such, I won\u2019t be talking about the theory behind the networks, or how they work under the hood. If you are interested in diving into the nitty-gritty of Transformers, my recommendation is Jay Alammar\u2019s Illustrated Guides here.", "This also serves as an update to my earlier guide on Using BERT for Binary Text Classification. I\u2019ll be using the same dataset (Yelp Reviews) that I used the last time to avoid having to download a new dataset because I\u2019m lazy and I have terrible internet. The motivation behind the update is down to several reasons, including the update to the HuggingFace library I used for the previous guide, as well as the release of multiple new Transformer models which have managed to knock BERT off its perch.", "With the background set, let\u2019s take a look at what we\u2019ll be doing.", "I\u2019ll be using two Jupyter Notebooks, one for data preparation, and one for training and evaluation.", "Most online datasets will typically be in .csv format. Following the norm, the Yelp dataset contains two csv files train.csv and test.csv.", "Kicking off our first (data preparation) notebook, let\u2019s load the csv files in with Pandas.", "However, the labels used here break the norm by being 1 and 2 instead of the usual 0 and 1. I\u2019m all for a bit of rebellion, but this just puts me off. Let\u2019s fix this so that the labels are 0 and 1, indicating a bad review and good review respectively.", "We need to do some final bit of retouching before our data is ready for the Pytorch-Transformer models. The data needs to be in tsv format, with four columns, and no header.", "So, let\u2019s get the data in order, and save it in tsv format.", "This marks the end of the data preparation Notebook, and we\u2019ll continue with the training Notebook from the next section.", "Before we can start the actual training, we need to convert our data from text into numerical values that can be fed into neural networks. In the case of Transformer models, the data will be represented as InputFeature objects.", "To make our data Transformer-ready, we\u2019ll be using the classes and functions in the file utils.py. (Brace yourself, a wall of code incoming!)", "Let\u2019s look at the important bits.", "The InputExample class represents a single sample of our dataset;", "The DataProcessor and BinaryProcessor classes are used to read in the data from tsv files and convert it into InputExamples.", "The InputFeature class represents the pure, numerical data that can be fed to a Transformer.", "The three functions convert_example_to_feature, convert_examples_to_features, _truncate_seq_pair are used to convert InputExamples into InputFeatures which will finally be sent to the Transformer model.", "The conversion process includes tokenization, and converting all sentences to a given sequence length (truncating longer sequences, and padding shorter sequences). During tokenization, each word in the sentence is broken apart into smaller and smaller tokens (word pieces) until all the tokens in the dataset are recognized by the Transformer.", "As a contrived example, let\u2019s say we have the word understanding. The Transformer we are using does not have a token for understanding but it has separate tokens for understand and ing. Then, the word understanding would be broken into the tokens understand and ing. The sequence length is the number of such tokens in the sequence.", "The convert_example_to_feature function takes a single sample of data and converts it into an InputFeature. The convert_examples_to_features function takes a list of examples and returns a list of InputFeatures by using the convert_example_to_feature function. The reason behind there being two separate functions is to allow us to use Multiprocessing in the conversion process. By default, I\u2019ve set the process count to cpu_count() - 2, but you can change it by passing a value for the process_count parameter in the convert_examples_to_features function.", "Now, we can go to our training notebook and import the stuff we\u2019ll use and configure our training options.", "Go through the args dictionary carefully and note all the different settings you can configure for training. In my case, I am using fp16 training to lower memory usage and speed up training. If you don\u2019t have Nvidia Apex installed, you will have to turn off fp16 by setting it to False.", "In this guide, I am using the XL-Net model with a sequence length of 128. Please refer to the Github repo for the full list of available models.", "Now, we are ready to load our model for training.", "The coolest thing about the Pytorch-Transformers library is that you can use any of the MODEL_CLASSES above, just by changing the model_type and model_name in the arguments dictionary. The process for fine-tuning, and evaluating is basically the same for all the models. All hail HuggingFace!", "Next, we have functions defining how to load data, train a model, and to evaluate a model.", "Finally, we have everything ready to tokenize our data and train our model.", "It should be fairly straightforward from here.", "This will convert the data into features and start the training process. The converted features will be automatically cached, and you can reuse them later if you want to run the same experiment. However, if you change something like the max_seq_length, you will need to reprocess the data. Same goes for changing the model used. To reprocess the data, simply set reprocess_input_data to True in the args dictionary.", "For comparison, this dataset took about 3 hours for training on my RTX 2080.", "Once training completes, we can save everything.", "Evaluation is quite easy as well.", "Without any parameter tuning, and with one training epoch, my results are as follows.", "Transformer models have displayed incredible prowess in handling a wide variety of Natural Language Processing tasks. Here, we\u2019ve looked at how we can use them for one of the most common tasks, which is Sequence Classification.", "The Pytorch-Transformers library by HuggingFace makes it almost trivial to harness the power of these mammoth models!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd370944b50ca&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----d370944b50ca---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@tetrakiss?utm_source=medium&utm_medium=referral", "anchor_text": "Arseny Togulev"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3?source=---------16------------------", "anchor_text": "Binary Classification"}, {"url": "https://medium.com/swlh/simple-transformers-multi-class-text-classification-with-bert-roberta-xlnet-xlm-and-8b585000ce3a?source=---------15------------------", "anchor_text": "Multi-Class Classification"}, {"url": "https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5?source=---------13------------------", "anchor_text": "Multi-Label Classification"}, {"url": "https://towardsdatascience.com/simple-transformers-named-entity-recognition-with-transformer-models-c04b9242a2a0?source=---------14------------------", "anchor_text": "Named Entity Recognition (Part-of-Speech Tagging)"}, {"url": "https://towardsdatascience.com/question-answering-with-bert-xlnet-xlm-and-distilbert-using-simple-transformers-4d8785ee762a?source=---------12------------------", "anchor_text": "Question Answering"}, {"url": "https://medium.com/swlh/solving-sentence-pair-tasks-using-simple-transformers-2496fe79d616?source=---------9------------------", "anchor_text": "Sentence-Pair Tasks and Regression"}, {"url": "https://towardsdatascience.com/how-to-train-your-chatbot-with-simple-transformers-da25160859f4?source=---------6------------------", "anchor_text": "Conversational AI"}, {"url": "https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee?source=---------4------------------", "anchor_text": "Language Model Fine-Tuning"}, {"url": "https://towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d?source=---------2------------------", "anchor_text": "ELECTRA and Language Model Training from Scratch"}, {"url": "https://medium.com/skilai/to-see-is-to-believe-visualizing-the-training-of-machine-learning-models-664ef3fe4f49?source=---------10------------------", "anchor_text": "Visualising Model Training"}, {"url": "https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3", "anchor_text": "SimpleTransformers"}, {"url": "https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee?source=---------4------------------", "anchor_text": "Language Model Fine-Tuning"}, {"url": "https://towardsdatascience.com/understanding-electra-and-training-an-electra-language-model-3d33e3a9660d?source=---------2------------------", "anchor_text": "ELECTRA and Language Model Training from Scratch"}, {"url": "https://medium.com/skilai/to-see-is-to-believe-visualizing-the-training-of-machine-learning-models-664ef3fe4f49?source=---------10------------------", "anchor_text": "Visualising Model Training"}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification", "anchor_text": "Github repo"}, {"url": "http://jalammar.github.io/illustrated-transformer/", "anchor_text": "here"}, {"url": "https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04", "anchor_text": "Using BERT for Binary Text Classification"}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification/blob/master/data_download.sh", "anchor_text": "here"}, {"url": "https://course.fast.ai/datasets", "anchor_text": "here"}, {"url": "https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz", "anchor_text": "direct download link"}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification", "anchor_text": "Github repo"}, {"url": "https://github.com/ThilinaRajapakse/pytorch-transformers-classification", "anchor_text": "Github repo"}, {"url": "https://medium.com/tag/data-science?source=post_page-----d370944b50ca---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d370944b50ca---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/pytorch?source=post_page-----d370944b50ca---------------pytorch-----------------", "anchor_text": "Pytorch"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----d370944b50ca---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----d370944b50ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----d370944b50ca---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd370944b50ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d370944b50ca---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d370944b50ca--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d370944b50ca--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d370944b50ca--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d370944b50ca--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d370944b50ca--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhttps-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}