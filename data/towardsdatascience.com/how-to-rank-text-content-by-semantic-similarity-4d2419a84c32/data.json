{"url": "https://towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32", "time": 1683007139.582859, "path": "towardsdatascience.com/how-to-rank-text-content-by-semantic-similarity-4d2419a84c32/", "webpage": {"metadata": {"title": "How to Rank Text Content by Semantic Similarity | by Rupert Thomas | Towards Data Science", "h1": "How to Rank Text Content by Semantic Similarity", "description": "Introduction to ranking text content by document similarity: using TF-idf and Semantic Similarity using GloVe embeddings. Python with Scikit-Learn and Gensim"}, "outgoing_paragraph_urls": [{"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe algorithm", "paragraph_index": 2}, {"url": "http://www.tfidf.com/", "anchor_text": "summary here", "paragraph_index": 7}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TF-idf vectorizer model", "paragraph_index": 8}, {"url": "https://dictionary.cambridge.org/dictionary/english/lemma", "anchor_text": "lemma", "paragraph_index": 13}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe", "paragraph_index": 18}, {"url": "https://twitter.com/rupertthomas", "anchor_text": "Rupert Thomas", "paragraph_index": 29}, {"url": "https://twitter.com/rupertthomas", "anchor_text": "@rupertthomas", "paragraph_index": 29}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe: Global Vectors for Word Representation", "paragraph_index": 30}], "all_paragraphs": ["Search is a standard tool for finding content \u2014 whether it's online or on-device \u2014 but what if you wanted to go deeper and find content based on the meaning of words? Tools developed for Natural Langage Processing can help.", "In this article we will cover two methods of calculating the similarity of text:", "For the first part, we\u2019ll use the TF-idf implementation in scikit-learn in isolation, as its really simple and only requires a couple of lines of code. For semantic similarity, we\u2019ll use a number of functions from gensim (including its TF-idf implementation) and pre-trained word vectors from the GloVe algorithm. Also, we\u2019ll need a few tools from nltk. These packages can be installed using pip:", "Alternatively, you can grab the example code from the GitHub repository and install from the requirements file:", "Document: a piece of text, in the form of a string. This could be just a few words, or a whole novel.", "Term: a word in a document.", "Term Frequency-inverse document frequency (or TF-idf) is an established technique for scoring document similarity based on the importance of the words that they share. Very high level summary:", "A TF-idf score estimates the trade-off between those two heuristics, based on how frequently a word appears. There is a more detailed summary here.", "We can create and fit a TF-idf vectorizer model from scikit-learn with only a few lines of code:", "Here, we create the model and \u2018fit\u2019 using the text corpus. TfidfVectorizer handles the pre-processing using its default tokenizer \u2014 this converts strings into lists of single word \u2018tokens\u2019. It produces a sparse matrix of document vectors containing the term frequencies.", "We then take the dot product (linear kernel) of the first vector (that contains the search terms) with the documents to determine the similarity. We have to ignore the first similarity result ([1:]) as that is comparing the search terms to themselves.", "This works well for a simple example, but may fall down in real-world cases for a number of reasons.", "Firstly, it doesn\u2019t make much sense to involve words such as [\u2018and\u2019, \u2018on\u2019, \u2018the\u2019, \u2018are\u2019] in the matching process, as these stopwords don\u2019t contain contextual information. These words should be stripped out before determining similarity.", "Secondly, we would want \u2018fruit\u2019 and \u2018fruits\u2019 to be recognised as related, although the model above only find exact matches. One approach to this problem is to reduce each word down to its simplest lemma \u2014 for that we need a lemmatizer.", "In this case, \u2018tomatoes\u2019 in the second document is reduced to \u2018tomato\u2019 by the lemmatizer (tokenizer), which is then matched to the same word in the search terms.", "Conceptually, each document is a point in a high-dimensional space, where the number of dimensions is equal to the number of unique words in the text corpus. This space will usually be quite empty, with large distances between points (documents) as documents contain a wide variety of words. In this scenario, the angle between points makes more sense as a metric of similarity than, for example, a distance-related metric such as Euclidean distance.", "The similarity here is referred to as the cosine similarity. The output from TfidfVectorizer is (by default) L2-normalized, so then the dot product of two vectors is the cosine of the angle between the points denoted by the vectors.", "A move advanced approach is to compare documents based on how similar their words are. For example, \u2018apples\u2019 and \u2018oranges\u2019 might be regarded as more similar than \u2018apples\u2019 and \u2018Jupiter\u2019. Judging word similarity at scale is difficult \u2014 one widely used approach is to analyse a large corpus of text and rank words that appear together often as being more similar.", "This is the basis of the word embedding model GloVe: it maps words into numerical vectors \u2014 points in a multi-dimensional space so that words that occur together often are near each other in space. It is an unsupervised learning algorithm, developed at Stanford University in 2014.", "In the previous example, we used WordNetLemmatizer from the nltk package to stem and tokenize (convert to a list of single word strings) our data. Here we do similar preprocessing in gensim, and also remove any HTML tags that may be present, such as if we have scraped data from the web:", "Then we create a similarity matrix, that contains the similarity between each pair of words, weighted using the term frequency:", "Finally, we calculate the soft cosine similarity between the query and each of the documents. Unlike the regular cosine similarity (which would return zero for vectors with no overlapping terms), the soft cosine similarity considers word similarity as well.", "There is a full example in the notebook in the code repository. There is also a self-contained DocSim class, that can be imported as a module and used to run semantic similarity queries without additional code:", "The GloVe word embedding models can be quite large \u2014 on my machine it took about 40 seconds to load from disk. Once it's loaded, however, subsequent operations are faster. The multi-threaded version of the class loads the model in the background, to avoid locking the main thread for a significant period of time. It is used in a similar way, although will raise an exception if the model is still loading so the status of the model_ready property should be checked first.", "We\u2019ve looked at two methods for comparing text content for similarity, such as might be used for search queries or content recommender systems. The first (TF-idf) scores document relationships based on the frequency of occurrence of shared words. It is fast and works well when documents are large and/or have lots of overlapping terms. The second technique looks for shared words that address similar concepts, but it does not require an exact match: for example, it links \u2018fruit and vegetables\u2019 with the word \u2018tomato\u2019. This is slower, and can sometimes give less clear-cut results, but is good with shorter search queries or documents with low word overlap.", "I examined both of these methods for a real-world application: I needed to know if any of a <collection of documents> related to a small number of <search terms>. Initially, I thought I would need to use semantic similarity matching \u2014 the search terms came from uncontrolled user input and so there might be some pretty tenuous relationships. Document ranking was a useful feature, but the overall aim was to apply a decision threshold so that a binary yes/no result could be generated.", "For that particular application, I found that using TF-idf alone was sufficient. The collections of documents were usually large enough that \u2014 in the cases when a match should indeed be found\u2014there was sufficient overlap with the search terms. TF-idf gave a clearer separation between the positive and negative cases, although it did miss a few documents that would also be relevant.", "Its worth taking a data-driven approach, and seeing what works best in your application.", "What are your thoughts? Have you used these tools in your work, or do you favor a different approach? Leave your comments below!", "Rupert Thomas is a technology consultant specialising in machine learning, machine vision, and data-driven products. @rupertthomas", "GloVe: Global Vectors for Word Representation", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Technology consultant from Cambridge, UK specialising in software engineering, machine learning, machine vision, and building data-driven products"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4d2419a84c32&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@rupertt?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rupertt?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Rupert Thomas"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ea46cd46fb3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&user=Rupert+Thomas&userId=6ea46cd46fb3&source=post_page-6ea46cd46fb3----4d2419a84c32---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Patrick Tomasso"}, {"url": "https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText", "anchor_text": "Unsplash"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe algorithm"}, {"url": "https://github.com/4OH4/doc-similarity", "anchor_text": "https://github.com/4OH4/doc-similarity"}, {"url": "https://github.com/4OH4/doc-similarity", "anchor_text": "4OH4/doc-similarityRanking documents using semantic similarity in Python - 4OH4/doc-similaritygithub.com"}, {"url": "http://www.tfidf.com/", "anchor_text": "summary here"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "TF-idf vectorizer model"}, {"url": "https://dictionary.cambridge.org/dictionary/english/lemma", "anchor_text": "lemma"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "https://twitter.com/rupertthomas", "anchor_text": "Rupert Thomas"}, {"url": "https://twitter.com/rupertthomas", "anchor_text": "@rupertthomas"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "Sci-kit Learn: TfidfVectorizer"}, {"url": "https://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe: Global Vectors for Word Representation"}, {"url": "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb", "anchor_text": "Gensim: Soft Cosine Tutorial"}, {"url": "https://medium.com/tag/introduction?source=post_page-----4d2419a84c32---------------introduction-----------------", "anchor_text": "Introduction"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4d2419a84c32---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/python?source=post_page-----4d2419a84c32---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4d2419a84c32---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4d2419a84c32---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&user=Rupert+Thomas&userId=6ea46cd46fb3&source=-----4d2419a84c32---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&user=Rupert+Thomas&userId=6ea46cd46fb3&source=-----4d2419a84c32---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4d2419a84c32&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4d2419a84c32---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4d2419a84c32--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4d2419a84c32--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4d2419a84c32--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rupertt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@rupertt?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Rupert Thomas"}, {"url": "https://medium.com/@rupertt/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "272 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6ea46cd46fb3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&user=Rupert+Thomas&userId=6ea46cd46fb3&source=post_page-6ea46cd46fb3--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F998c7c4c7a1d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-rank-text-content-by-semantic-similarity-4d2419a84c32&newsletterV3=6ea46cd46fb3&newsletterV3Id=998c7c4c7a1d&user=Rupert+Thomas&userId=6ea46cd46fb3&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}