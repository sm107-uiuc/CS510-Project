{"url": "https://towardsdatascience.com/what-is-shannons-entropy-5ad1b5a83ce1", "time": 1683016503.04301, "path": "towardsdatascience.com/what-is-shannons-entropy-5ad1b5a83ce1/", "webpage": {"metadata": {"title": "Do You Know What is Shannon\u2019s Entropy? | by Abhishek Verma | Towards Data Science", "h1": "Do You Know What is Shannon\u2019s Entropy?", "description": "Entropy forms the basis of the universe and everything in it. Why should deep learning be any different? It is highly used in information theory (the variant of entropy that\u2019s used there is Shannon\u2019s\u2026"}, "outgoing_paragraph_urls": [], "all_paragraphs": ["Entropy forms the basis of the universe and everything in it. Why should deep learning be any different? It is highly used in information theory (the variant of entropy that\u2019s used there is Shannon\u2019s Entropy) and has made way into deep learning (Cross-Entropy Loss and KL Divergence) also. Let\u2019s understand the concept of Shannon\u2019s Entropy.", "In layman terms, you describe entropy as:", "The most basic example you get is of a fair coin: when you toss it, what will you get? Heads (1) or Tails (0).", "Because the probability of both events is the same (1/2). There is no way to tell. So, if you are playing this coin-tossing game on the phone. Then, you must tell the other a single outcome or you need a single bit (0 or 1) to convey the information about this.", "Let\u2019s consider the case of a completely biased coin (would always end up on heads):", "Would you need to tell the other person the result? The simple answer is NO. Because both of you know the answer(if you are not cheating though \ud83d\ude04). Hence, you need not say anything. You are not using even a single bit.", "In order to digest entropy intuitively, let\u2019s again revisit its definition:", "There is one pertinent thing to notice, the word \u2018system\u2019 in the layman definition and the occurrence of probability term two times though differently.", "Let\u2019s revisit the definition of probability. What is probability? The likelihood of an event to occur. The definition of probability talks about a single event, not the whole system. Thus, what probability can give us is a local picture, a limited picture of the whole system.", "Probability gives a local picture of the whole system", "In order for us to get a sense of the whole system, we need to come up with a way that tells us a global picture of the whole system. We need to evaluate the parts of the system and see their effect in summation.", "How do you evaluate a part of the system for the randomness it will contribute to the system?", "The p\u1d62 part of the Entropy\u2019s formula tells us about the importance of the event in the whole system. How? The numerator of a probability value tells the number of times the event happens, that\u2019s how.", "For the next part, let us look at the formula of entropy in a new light:", "We can transform the second part of Entropy\u2019s formula into the reciprocal of the probability. What would this give us? If probability tells you the certainty, what would its inverse signify? The uncertainty.", "Therefore, the formula of Entropy can be interpreted as:", "And that is what, entropy can be interpreted as the product of the importance of an event and the uncertainty the event has. The more important an event in the system, the more uncertainty i.e. randomness, it can introduce into the system. Therefore, if we derive this randomness for each event and sum it up, we can then get to understand the randomness in the whole system.", "The mathematical formula of Shannon\u2019s entropy is:", "Here, c is the number of different classes you have. In the case of a coin, we have heads (1) or tails (0). Hence, c = 2.", "So, the entropy of a fair coin is:", "So, the entropy for the fair coin case comes out to be 1. Utter uncertainty (remember, the layman definition of entropy). We are totally uncertain about the result.", "Now, let\u2019s consider the case of the completely biased coin. For it, entropy is:", "First of all, 0log(0) in all calculations concerning entropy is assumed to be 0. Now, we have quelled that, it can be observed for this case, entropy is 0. We are completely certain about the result, no matter what.", "Let\u2019s plot the resultant entropy when the probability of getting a head is between 0 and 1.", "We see that Entropy becomes maximum at probability 1/2.", "In the general case, the maximum value of entropy for any system is log\u2082c. This happens when probability is 1/2. Why?", "There are two explanations for it. Let\u2019s see.", "In the case of a coin, thus, the maximum entropy will be log\u20822 = 1 bit.", "When everything is equally likely (at probability 1/2), the entropy is highest because you don\u2019t know what\u2019s going to happen.", "To convey all states the coin can take i.e. 0 or 1, you need 1 bit.", "Feel free to skip this, if not interested.", "Remember how to find the value at which a given function\u2019s value will be maximum.", "Step 1: Take the derivative of the function and equate it to zero, the values you get will be either the minimum or maximum of the function. How to know if the values found are minimum or maximum?", "Step 2: You take the double-derivative of the function and then, substitute the values you found out in the previous step. If the value of the second derivative is less than 0, then, the value will the maximum of the function and if it is more than 0, then, the value will be the minimum of the function.", "Now, let\u2019s apply this procedure for the formula of Shannon\u2019s Entropy.", "The derivative of Shannon\u2019s Entropy is:", "Equating the derivative found to 0,", "For the last step, we raise both sides to the power of 2, and 2 raised to power log\u2082x is x.", "Now, let\u2019s verify whether the value is maximum or minimum. First, we find the double-derivative of Entropy:", "Now, we substitute the value found from equating derivative to 0 into this double-derivative,", "Finally, we get the value of double-derivative which is negative, which hence, confirms that the maximum value of Entropy is at probability 1/2.", "We thus prove that the maximum value of Entropy occurs at probability 1/2.", "In this post, we understand Shannon\u2019s entropy both mathematically and intuitively. We understand what the bounds of Shannon\u2019s entropy are mathematically. We also derived the probability at which Shannon\u2019s entropy is maximum. We then understood the concept of entropy intuitively.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "I am a Data Scientist. I like to write about concepts related to deep learning. Leave in the comments what you want me to write on next."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F5ad1b5a83ce1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://deeptechtalker.medium.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Abhishek Verma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18373f6fd34a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&user=Abhishek+Verma&userId=18373f6fd34a&source=post_page-18373f6fd34a----5ad1b5a83ce1---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.pexels.com/@fotografierende?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "fotografierende"}, {"url": "https://www.pexels.com/photo/person-about-to-catch-four-dices-1111597/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels", "anchor_text": "Pexels"}, {"url": "https://medium.com/tag/data-science?source=post_page-----5ad1b5a83ce1---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----5ad1b5a83ce1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----5ad1b5a83ce1---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----5ad1b5a83ce1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/technology?source=post_page-----5ad1b5a83ce1---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&user=Abhishek+Verma&userId=18373f6fd34a&source=-----5ad1b5a83ce1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&user=Abhishek+Verma&userId=18373f6fd34a&source=-----5ad1b5a83ce1---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F5ad1b5a83ce1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----5ad1b5a83ce1---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----5ad1b5a83ce1--------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://deeptechtalker.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Abhishek Verma"}, {"url": "https://deeptechtalker.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "396 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18373f6fd34a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&user=Abhishek+Verma&userId=18373f6fd34a&source=post_page-18373f6fd34a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F6854bb98687&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-is-shannons-entropy-5ad1b5a83ce1&newsletterV3=18373f6fd34a&newsletterV3Id=6854bb98687&user=Abhishek+Verma&userId=18373f6fd34a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}