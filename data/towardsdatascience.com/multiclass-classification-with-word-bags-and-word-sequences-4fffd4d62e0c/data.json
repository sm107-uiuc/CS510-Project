{"url": "https://towardsdatascience.com/multiclass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c", "time": 1682995117.941592, "path": "towardsdatascience.com/multiclass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c/", "webpage": {"metadata": {"title": "Multiclass Classification with Word Bags and Word Sequences | by Ashok Chilakapati | Towards Data Science", "h1": "Multiclass Classification with Word Bags and Word Sequences", "description": "Document is a specific sequence of words. But not all sequences of words are documents. Teaching the difference to an algorithm is a tall order. Taking the sequence of words into account for text\u2026"}, "outgoing_paragraph_urls": [{"url": "http://xplordat.com/2018/01/23/stacks-of-documents-and-bags-of-words/", "anchor_text": "shred the document and make bags by word", "paragraph_index": 1}, {"url": "http://xplordat.com/2019/01/13/word-bags-vs-word-sequences-for-text-classification/", "anchor_text": "synthetic documents", "paragraph_index": 2}, {"url": "http://xplordat.com/2019/01/28/sentiment-analysis-with-word-bags-and-word-sequences/", "anchor_text": "binary sentiment classification", "paragraph_index": 2}, {"url": "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#", "anchor_text": "scikit-learn api", "paragraph_index": 3}, {"url": "https://github.com/ashokc/Multiclass-classification-with-word-bags-and-word-strings", "anchor_text": "github", "paragraph_index": 3}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "Word-embeddings", "paragraph_index": 5}, {"url": "https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip", "anchor_text": "Pre-trained fasttext", "paragraph_index": 5}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "generated", "paragraph_index": 5}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim", "paragraph_index": 5}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "Tf-Idf Vectorizer", "paragraph_index": 8}, {"url": "https://keras.io/preprocessing/text/", "anchor_text": "processor", "paragraph_index": 10}, {"url": "https://keras.io/layers/recurrent/", "anchor_text": "LSTM", "paragraph_index": 11}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC", "anchor_text": "SVM", "paragraph_index": 11}, {"url": "https://github.com/ashokc/Multiclass-classification-with-word-bags-and-word-strings", "anchor_text": "github", "paragraph_index": 17}, {"url": "http://xplordat.com/2019/01/13/word-bags-vs-word-sequences-for-text-classification/", "anchor_text": "word strings beat out word bags", "paragraph_index": 24}, {"url": "http://a%20simple%20lstm%20model%20for%20binary%20classification./", "anchor_text": "score was even.", "paragraph_index": 24}], "all_paragraphs": ["Document is a specific sequence of words. But not all sequences of words are documents. Teaching the difference to an algorithm is a tall order. Taking the sequence of words into account for text analysis is in general computationally expensive. Deep learning approaches such as LSTM allow us to model a document as a string- of-words and they have indeed found some success in NLP tasks recently.", "On the other hand when we shred the document and make bags by word, we end up with a vector of weights/counts of these words. The mapping from a document to this vector can be many-to-one as all possible sequences of words yield the same vector. So the deciphering of the meaning of the original document (much less resurrecting it!), from this vector is not possible. Nevertheless this decades old bags-of-words approach to modeling documents has been the main stay for NLP tasks.", "When the sequence of words is important in determining the class of a document, string-of-words approaches will outshine the bags-of-words. We have demonstrated this with synthetic documents where LSTM trounced the bags-of-words approach (Naive Bayes working with tf-idf vectors) for classification. But for a real text corpus of movie reviews for binary sentiment classification, we have shown that both LSTM and SVM (with tf-idf vectors) were comparable in quality even while the former took much longer.", "The objective of this post is to further evaluate \u201cbags vs strings\u201d for a multiclass situation. We will work with the 20-newsgroups text corpus that is available from scikit-learn api. We will also look at the impact of using word-embeddings \u2014 both pre-trained and custom. We go through some code snippets here, but the complete code to reproduce the results can be downloaded from github.[LatexPage]", "This corpus consists of posts made to 20 news groups so they are well-labeled. There are over 18000 posts that are more or less evenly distributed across the 20 topics. In the code snippet below we fetch these posts, clean and tokenize them to get ready for classification.", "Word-embeddings are short (of length p that is much, much shorter than the size of the vocabulary nWords) numerical vector representations for words. They allow us to reduce the dimensionality of the word-space from the length of the corpus vocabulary (about 107, 000 here) to a much shorter length like 300 used here. Pre-trained fasttext word vectors are downloaded, and the custom fasttext ones for the movies corpus are generated offline via Gensim. In either case once in hand they are simply read off of the disk.", "The end result is a matrix where each row represents a 300 long vector for a word. The words/rows are ordered as per the integer index in the word_index dictionary \u2014 {word:index}. In case of Keras, the words are ordered based on their frequency. In case of tf-idf vectorizer a word gets its index based on its alphabetical order in the vocabulary. Just book keeping, nothing complex.", "LSTM works with word sequences as input while the traditional classifiers work with word bags such as tf-idf vectors. Having each document in hand as a list of tokens we are ready for either.", "We use scikit-learn Tf-Idf Vectorizer to build the vocabulary (the word_index dict variable in Line #6 below) and the document vectors (Line #7) from the tokens.", "Xencoded is a sparse nDocs x nWords matrix. When using word-embeddings we convert that to a dense nDocs x 300 matrix by multiplying with the embedding matrix we computed in Section 2. These shorter 300-long dense vectors are then classified.", "The text processor in Keras turns each document into a sequence/string of integers, where the integer value indicates the actual word as per the {word:index} dictionary that the same processing generates. The index values start at 1, skipping 0 which is reserved for padding. We use 200-long sequences as the stats on the tokens show that over 92% of the documents have less than 200 words. In Line # 7 in the code below, the documents with fewer than 200 words are \u2018post\u2019 padded with the index value 0 that is ignored by the embedding layer (mask_zero=True is set for in the definition of embedding layer in Section 4).", "LSTM is implemented via Keras while SVM is implemented via scikit-learn. Both work with the same train/test split so a comparison would be fair. Twenty percent of the overall corpus (i.e 3660 documents) are set aside for test while training on the remaining 14636 documents.", "As in the earlier articles in this series, we use the simplest possible LSTM model, with an embedding layer, one LSTM layer and the output layer. When using external word-embeddings the embedding layer will not be trained i.e., the weights will be what we have read from the disk in Section 2.", "The embedding layer in Figure 1 reduces the number of features from 107196 (the number of unique words in the corpus) to 300. The LSTM layer outputs a 150-long vector that is fed to the output layer for classification. The model itself is defined quite simply below.", "Training is done with early stopping to prevent over training in Line #5 in the code below. The final output layer yields a vector that is as long as the number of labels, and the argmax of that vector is the predicted class label.", "The model for SVM is much less involved as there are far fewer moving parts and parameters to decide upon. That is always a good thing of course.", "The confusion matrix and the F1-scores obtained are what we are interested in. With the predicted labels in hand from either approach we use scikit-learn API to compute them.", "While we have gone through some snippets in different order, the complete code for lstm-20news.py for running LSTM and svm-20news.py for running SVM is on github. As indicated in the earlier articles various random seeds are initialized for repeatability. The simulations are carried out with the help of a shell script below that loops over the variations we are considering.", "The embedding layer should contribute to", "This matches the number of non-trainable parameters in Line # 11 below for an LSTM run with external word-embeddings.", "The run takes over 2 hrs, stops due to the early stopping criteria and obtains an F1-score of 0.73. Figure 2 shows the rate of convergence flattening out a good bit by about 20 epochs or so.", "SVM has far fewer moving parts and it finishes much more quickly as well. With fasttext embeddings, it works with a 18296 x 300 dense matrix (Line# 7 below), and obtains F1-score of 0.68.", "We have the results in hand to not only compare bag & sequences for multiclass classification but also the impact of using pre-trained and custom word-embeddings. Figure 3 shows the F1-scores obtained and the time taken in all cases. SVM with direct tf-idf vectors does the best both for quality & performance. Pre-trained word-embeddings help LSTM improve its F1-score. The larger run times for LSTM are expected and they are in line with what we have seen in the earlier articles in this series.", "Figure 4 below compares the best confusion matrices obtained by either approach.", "So what are we to make of the results obtained in this three part series? For a synthetic text corpus dominated by sequences, word strings beat out word bags handily. For a binary classification task, the score was even. In this multiclass classification task, the scale has tilted towards word bags. Given that the deep learning approaches have so many knobs one can never be sure if the obtained results cannot be improved by tweaking some (which ones, pray tell me\u2026 units/layers/batches/cells/\u2026 and by how much too while you are at it\u2026). So here are some loose assessments for whatever they are worth.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4fffd4d62e0c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29----4fffd4d62e0c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "http://xplordat.com/2018/01/23/stacks-of-documents-and-bags-of-words/", "anchor_text": "shred the document and make bags by word"}, {"url": "http://xplordat.com/2019/01/13/word-bags-vs-word-sequences-for-text-classification/", "anchor_text": "synthetic documents"}, {"url": "http://xplordat.com/2019/01/28/sentiment-analysis-with-word-bags-and-word-sequences/", "anchor_text": "binary sentiment classification"}, {"url": "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#", "anchor_text": "scikit-learn api"}, {"url": "https://github.com/ashokc/Multiclass-classification-with-word-bags-and-word-strings", "anchor_text": "github"}, {"url": "http://xplordat.com/2018/09/27/word-embeddings-and-document-vectors-part-1-similarity/", "anchor_text": "Word-embeddings"}, {"url": "https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip", "anchor_text": "Pre-trained fasttext"}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "generated"}, {"url": "https://radimrehurek.com/gensim/index.html", "anchor_text": "Gensim"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html", "anchor_text": "Tf-Idf Vectorizer"}, {"url": "https://keras.io/preprocessing/text/", "anchor_text": "processor"}, {"url": "https://keras.io/layers/recurrent/", "anchor_text": "LSTM"}, {"url": "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC", "anchor_text": "SVM"}, {"url": "https://github.com/ashokc/Multiclass-classification-with-word-bags-and-word-strings", "anchor_text": "github"}, {"url": "http://xplordat.com/2019/01/13/word-bags-vs-word-sequences-for-text-classification/", "anchor_text": "word strings beat out word bags"}, {"url": "http://a%20simple%20lstm%20model%20for%20binary%20classification./", "anchor_text": "score was even."}, {"url": "http://xplordat.com/2018/10/09/word-embeddings-and-document-vectors-part-2-classification/", "anchor_text": "Word Embeddings and Document Vectors: Part 2. Classification"}, {"url": "https://medium.com/tag/keras?source=post_page-----4fffd4d62e0c---------------keras-----------------", "anchor_text": "Keras"}, {"url": "https://medium.com/tag/lstm?source=post_page-----4fffd4d62e0c---------------lstm-----------------", "anchor_text": "Lstm"}, {"url": "https://medium.com/tag/svm?source=post_page-----4fffd4d62e0c---------------svm-----------------", "anchor_text": "Svm"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4fffd4d62e0c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/scikit-learn?source=post_page-----4fffd4d62e0c---------------scikit_learn-----------------", "anchor_text": "Scikit Learn"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----4fffd4d62e0c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=-----4fffd4d62e0c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4fffd4d62e0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4fffd4d62e0c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4fffd4d62e0c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@ashok.chilakapati?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Ashok Chilakapati"}, {"url": "https://medium.com/@ashok.chilakapati/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "244 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcc37b40eae29&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&user=Ashok+Chilakapati&userId=cc37b40eae29&source=post_page-cc37b40eae29--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F5ab4b71672c9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmulticlass-classification-with-word-bags-and-word-sequences-4fffd4d62e0c&newsletterV3=cc37b40eae29&newsletterV3Id=5ab4b71672c9&user=Ashok+Chilakapati&userId=cc37b40eae29&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}