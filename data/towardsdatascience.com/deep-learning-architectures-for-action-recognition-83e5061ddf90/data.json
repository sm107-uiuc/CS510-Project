{"url": "https://towardsdatascience.com/deep-learning-architectures-for-action-recognition-83e5061ddf90", "time": 1683007596.3489778, "path": "towardsdatascience.com/deep-learning-architectures-for-action-recognition-83e5061ddf90/", "webpage": {"metadata": {"title": "Deep Learning Architectures for Action Recognition | by Shivam Sharma | Towards Data Science", "h1": "Deep Learning Architectures for Action Recognition", "description": "Deep learning approaches have empirically demonstrated remarkable success in learning image representations for tasks like object recognition, image captioning, and semantic segmentation\u2026"}, "outgoing_paragraph_urls": [{"url": "http://nofreeshivam.com", "anchor_text": "nofreeshivam.com", "paragraph_index": 42}], "all_paragraphs": ["Deep learning approaches have empirically demonstrated remarkable success in learning image representations for tasks like object recognition, image captioning, and semantic segmentation. Convolutional neural networks have enabled us to efficiently capture the hypothesis of spatial locality of data structure in images through parameter sharing convolutions, and local invariance-building max-pooling neurons. In this literature review, I would like to explore the impact of deep learning techniques on video tasks, specifically action recognition.", "I would like to explore how spatiotemporal features are aggregated through various deep architectures, the role of optical flow as an input, the impacts on real-time capabilities, and the compactness & interpretability of the learned features.", "I will then propose areas of future research that I believe could help bias our deep learning architectures in a way that better captures temporal hypotheses of the real-world; the natural manifold.", "Before 2014, the state of the art techniques focused on hand crafted features formed from sparsely or densely sampled trajectories. For example, a popular approach called Improved Dense Trajectories (iDT) [1], extracted trajectories and features for a dense set of interest points, encodes them in a fixed sized video description, then a classifier like SVM is trained on the resulting \u201cbag of words\u201d representation. In this approach, there is a lot of preprocessing that needs to be done for each frame. The optical flow must be calculated between frames, gradients for optical flow are also calculated, and mean-subtracted histograms are produced for both. These are all considered input features that are encoded into the fixed size video description. In deep learning approaches to video representation, we will observe how preprocessing has an effect on end-to-end trainability and on real-time capability.", "After 2014, deep learning architectures prevailed with state of the art performance on landmark video action recognition datasets like UCF101, Sports-1M, and HMDB51. In 2014, two important breakthrough papers gave deep learning the start in video recognition. Large-scale Video Classification with Convolutional Neural Networks by Karpathy et. al. [2] and Two-Stream Convolutional Networks for Action Recognition in Videos by Simonyan and Zisserman [3] gave rise to the popularity of single stream and two stream networks in action recognition.", "Karpathy et. al. explored how to fuse temporal data with a single stream 2D convolutional neural net. They tested the architectures proposed in Figure 1.", "The single frame network is essentially an image classification network with no temporal features. Late fusion uses two far apart frames and fuses deep hierarchal features at the densely connected layers by flattening features processed from the two frames. Early fusion stacks frames as channels and learns video descriptors via 2D Convolution on the entire stack of frames. Slow fusion attempts to concatenate features in a hierarchal fashion from a stack of frames, so as the network gets deeper more temporal features are learnt. Their results are summarised in Table 1.", "The strength of the single stream strategy is that we can use transfer learning from models trained on large scale image datasets. We also have no need to pre-process the images for optical flow as we directly use the RGB image data in this architecture. This makes single stream networks a candidate for real-time processing. In the fusion architectures that the authors propose, the number of parameters increase significantly from a deep 2D CNN. To mitigate this, the authors propose using a multi-resolution stream. One that embeds a high-resolution fovea stream on a center crop of the video combined with a low-resolution context stream from the whole video, as seen in Figure 2.", "Empirically, this drastically reduced the number of parameters to an order of magnitude comparable to the single frame architecture.", "From the results published in Table 1, we see that the architectures proposed failed to effectively boost the performance from just using a single frame. The weakness of these models is that they did not capture motion features well. However, this paper did reveal that transfer learning is very useful for action recognition. Models that were pre-trained on Sports-1M and then finely tuned on the top 3 layers boosted accuracy on the UCF101 dataset by over 20% when compared to a model trained from scratch on UCF101 [2].", "In 2014, Simonyan and Zisserman proposed a two stream architecture that processes spatial features and temporal feature separately [3] as in Figure 3.", "A single frame for the video is passed to a 2D convolution net while preprocessed multi-frame optical flow is passed to a separate 2D convolution net. Each stream forms a prediction and the class score is determined by their fusion. The drawback of this architecture is that it is not end-to-end trainable as optical flow needs to be calculated separately and both streams need to be trained separately. The spatial stream can take learnings from large image datasets, whereas the temporal stream must be trained on a video dataset. In this way, transfer learning is not completely applicable for this architecture. Furthermore, the preprocessing required for computing optical flow makes it difficult for this algorithm to have real-time capabilities. The strength of this approach is found in its ability to match the state of the art techniques of the time like IDT, as seen in Table 2 below.", "This opened the door for further research into deep learning for video classification. This research showed that convolutional deep nets could effectively capture some motion features and combine that with spatial features to form accurate predictions for action classes.", "The deep learning architectures developed in the next 5 years beyond 2014 to 2019 largely follow variations around the architectures depicted in Figure 4 below.", "The first two approaches a) and b), using an LSTM and a 3D ConvNet respectively, share the strengths of being end-to-end trainable and real-time capable. This is because they do not rely on optical flow and instead must learn features that encode this information. This allows for the network to learn spatiotemporal features directly in end-to-end training. Approaches c) \u2014 e) are not real-time capable nor end-to-end trainable because they require optical flow calculations over the raw data. Approaches b), d), and e) use 3D convolutions. This creates a magnitude of more parameters from traditional 2D ConvNets. For a single 3D convolution neural network trained for the UCF101 dataset can have 33M + parameters, compared to just 5M+ parameters in the 2D case [4]. This significantly affects the training cost as 3D ConvNet models trained on Sports-1M take approximately 2 months. This makes it difficult to search for the right architecture for video data. The large number of parameters also creates a risk of overfitting.", "The LSTM architecture for videos was popularised in the 2014 paper Long-term Recurrent Convolutional Networks for Visual Recognition and Description by Donahue et. al [5]. The architecture is known as LRCN. It is a direct extension of the encoder-decoder architecture but for video representations. The strength of the LRCN network is that it can handle sequences of various lengths. It can also be adapted to other video tasks like image captioning and video description. The weakness was that the LRCN was not able to beat the state of the art at the time, however it did provide improvements over single frame architectures as noted in Table 3.", "Temporal modelling of spatial features is tough for a hidden recurrent layer to learn. Empirically, adding more hidden units to the RGB models did not improve past 256 hidden units. However, adding more hidden units while using Flow input yielded an accuracy boost of 1.7% from 256 units to 1024 units. This shows that the LRCN has a tough time learning optical flow or a similar representation of motion natively.", "3D ConvNets were established as the new state of the art in the 2015 research paper Learning Spatiotemporal Features with 3D Convolutional Networks by Du Tran et. al [6]. In this paper, they establish that the 3D convolution net (C3D) with a 3x3x3 kernel is the most effective in learning spatiotemporal features. Interestingly, de-convolutions reveal that the network is learning spatial appearance for the first few frames followed by salient motion in the later frames of a clip. This architecture is powerful in that many videos can be processed in real time as C3D processes at up to 313fps. The video descriptors generated by this network are also compact and discriminative as we can project the features generated by convolutions to 10 dimensions via PCA and still achieve 52.8% accuracy on the UCF101 dataset.", "In 2016, the focus shifted back to two stream networks. In Convolutional Two-Stream Network Fusion for Video Action Recognition by Zisserman et. al. [7], the authors tackled how to effectively fuse spatial and temporal data across streams and create multi-level loss that could handle long term temporal dependencies. The motivating idea here was that in order to discriminate between similar motions in different parts of the image, like brushing hair and brushing teeth, the network will need to take a combination of spatial features and motion features at a pixel location. Theoretically, methods that fuse the streams before densely connected layers could achieve this. In the proposed architecture, the authors fuse the two streams at two locations as shown in Figure 5. This network was able to better capture motion and spatial features in distinct subnetworks and beat the state of the art IDT and C3D approaches. The multi-level loss is formed by a spatiotemporal loss at the last fusion layer and a separate temporal loss that is formed from output of the temporal net. This allowed the researchers to create spatiotemporal features and model long term temporal dependencies. This method still suffers from the weaknesses of the original two stream network, but performs better due to an enhanced architecture that better serves our real-world biases.", "In 2017, Zhu et. al. took two stream networks a step forward by introducing a hidden stream that learns optical flow called MotionNet [8]. This end-to-end approach allowed the researchers to skip explicitly computing optical flow. This means that two streams approaches could now be real-time and errors from misprediction could also be propagated into MotionNet for more optimal optical flow features.", "The researchers find that hidden two stream CNN\u2019s perform at a similar accuracy to non-hidden approaches but can now process up to 10x more frames per second, as seen in Table 6. This enables real-time capabilities for the two stream method.", "The MotionNet subnetwork is extensible and can be applied to other deep learning methods where calculating optical flow is necessary. This is important because it allows us to make other approaches in real-time.", "In 2017, Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset by Zisserman et. al. takes C3D another step forward by merging it with learnings from two stream networks [4]. The researchers propose a novel two stream inflated 3D ConvNet (I3D). Filters and pooling kernels from 2D ConvNets are expanded into 3D, endowing them with an extra temporal dimension. This enables the researchers to take successful architectures for 2D classification and apply them to 3D. The researchers also bootstrap these 3D filters with parameters from 2D ConvNet models trained on massive image datasets like ImageNet.", "Using 3D ConvNets on sequential RGB frames and sequential optical flow frames in a two stream architecture enabled the researchers to beat the state of the art on UCF101. The researchers established the clear importance of transfer learning with the use of the Kinetics dataset. Unfortunately, the model architecture they used is not end-to-end trainable and does not have real-time capabilities.", "In 2017 to 2018, many advances in deep residual learning led to novel architectures like 3DResNet and pseudo-residual C3D (P3D) [9]. Unfortunately, I will not cover these papers in this literature review, but I do respectfully acknowledge their impact on the state of the art.", "Most recently, in June 2019, Du Tran et. al. propose channel separated convolution networks (CSN) for the task of action recognition in Video Classification with Channel-Separated Convolutional Networks [10]. The researchers build on the ideas of group convolution and depth-wise convolution that received great success in Xception and MobileNet models.", "Fundamentally, group convolutions introduce regularisation and less computations by not being fully connected. Depth-wise convolutions are the extreme case of group convolutions where the input and output channels equal the number of groups, as seen in Figure 7. Conventional convolutional networks model channel interactions and local interactions (both spatial or spatiotemporal) jointly in their 3D convolutions.", "The researchers propose to decompose 3x3x3 convolution kernels into two distinct layers, where the first layer is a 1x1x1 convolution for local channel interaction and the second layer is a 3x3x3 depth-wise convolution for local spatiotemporal interactions. By using these blocks, the researchers significantly decrease the number of parameters in the network and introduce a strong form of regularisation. The channel separated blocks allow for the network to locally learn spatial and spatiotemporal features in distinct layers.", "As shown in Table 8, The CSN improves on state of the art RGB methods like R(2+1)D, C3D, and P3D on the Sports-1M dataset. The network is also 2\u20134x faster during inference. The model is also trained from scratch, where the rest of the models in the table are pretrained on ImageNet or Kinetics dataset. This novel architecture improves on previous factorized networks while reducing overfitting, being exceptionally fast, and producing state of the art accuracy on benchmark datasets.", "The current state of the art for action recognition (August 2019) is the channel separated network. This network effectively captures spatial and spatiotemporal features in their own distinct layers. The channel separated convolution blocks learns these features distinctly but combines them locally at all stages of convolution. This alleviates the need to perform slow fusion of temporal and spatial two stream networks. The network also does not need to decide between learning spatial or temporal features as in C3D where the network can decide to learn features that are mixed between the two dimensions. This network effectively captures the bias that 2D spatial slices should form a natural image, whereas a 2D slice in the temporal direction has different temporal properties and does not fall in the natural manifold. In this way, the researchers enforce this bias by creating two separate distinct layers to process each direction. Channel separation is an important step forward in action recognition and has beat state of the art results even when trained from scratch. It is also capable of real time inference. For these reasons, I believe CSN\u2019s are the current state of the art.", "We have learned that deep learning has revolutionized the way we process videos for action recognition. Deep learning literature has come a long way from using improved Dense Trajectories. Many learnings from the sister problem of image classification has been used in advancing deep networks for action recognition. Specifically, the usage of convolution layers, pooling layers, batch normalization, and residual connections have been borrowed from the 2D space and applied in 3D with substantial success. Many models that use a spatial stream are pretrained on extensive image datasets. Optical flow has also had an important role in representing temporal features in early deep video architectures like the two stream networks and fusion networks. Optical flow is our mathematical definition of how we believe movement in subsequent frames can be described as densely calculated flow vectors for all pixels. Originally, networks bolstered performance by using optical flow. However, this made networks unable to be end-to-end trained and limited real-time capabilities. In modern deep learning, we have moved beyond optical flow, and we instead architect networks that are able to natively learn temporal embeddings and are end-to-end trainable.", "We have also learned that action recognition is a truly unique problem with its own set of complications. The first source of friction is the high computation and memory cost associated with 3D convolutions. Some models take over 2 months to train on Sports-1M on modern GPU\u2019s. The second source of friction is that there is no standard benchmark for video architecture search [11]. Sports-1M and UCF101 are highly correlated and false-label assignment is common when a portion of a video is selected to be trained on but actually may not contain the actual action as it may be in another part of the video. The last source of friction is that designing a video deep neural network is nontrivial. The choice of layers, how to preprocess the input, and how to model the temporal dimension is an open problem. The authors of the papers above attempt to tackle these issues in an empirical fashion and propose novel architectures that resolve temporal modelling in videos.", "For future research, I recommend looking into how to include more biases we have of the real world in deep video network architecture. An interesting vertical to study is how depth modelling can relate to better video classifications. Current approaches to video classification have to learn that the videos are taken in a 3D environment. Depth forms an important part of our spatial perception. It could be that current approaches have to learn how to express depth in their spatiotemporal modelling of 2D features. Perhaps using monocular depth estimation networks can aid the current video networks in creating a better understanding of the environment itself. An important observation is that any spatial changes in a video come from two sources: a transformation of an external object we are observing, or the observer itself changing viewing angle or position. Both these sources of movement have to be learned by the current networks. It would be interesting to investigate how depth fields could be used to model either sources of change.", "[1] Heng Wang, Alexander Kl\u00e4ser, Cordelia Schmid, Liu Cheng-Lin. Action Recognition by Dense Trajectories. CVPR 2011 \u2014 IEEE Conference on Computer Vision", "[2] Karpathy, Andrej, et al. \u201cLarge-scale video classification with convolutional neural networks.\u201d Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2014.", "[3] Simonyan, Karen, and Andrew Zisserman. \u201cTwo-stream convolutional networks for action recognition in videos.\u201d Advances in neural information processing systems. 2014.", "[4] Carreira, Joao, and Andrew Zisserman. \u201cQuo vadis, action recognition? a new model and the kinetics dataset.\u201d proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.", "[5] Donahue, Jeffrey, et al. \u201cLong-term recurrent convolutional networks for visual recognition and description.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.", "[6] Tran, Du, et al. \u201cLearning spatiotemporal features with 3d convolutional networks.\u201d Proceedings of the IEEE international conference on computer vision. 2015.", "[7] Feichtenhofer, Christoph, Axel Pinz, and Andrew Zisserman. \u201cConvolutional two-stream network fusion for video action recognition.\u201d Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.", "[9] Qiu, Zhaofan, Ting Yao, and Tao Mei. \u201cLearning spatio-temporal representation with pseudo-3d residual networks.\u201d proceedings of the IEEE International Conference on Computer Vision. 2017.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Deep learning researcher. Builder of software. Follow me on twitter @nofreeshivam :) full bio @ nofreeshivam.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F83e5061ddf90&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@nofreeshivam?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nofreeshivam?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Shivam Sharma"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9768c95f2c90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&user=Shivam+Sharma&userId=9768c95f2c90&source=post_page-9768c95f2c90----83e5061ddf90---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/tag/action-recognition?source=post_page-----83e5061ddf90---------------action_recognition-----------------", "anchor_text": "Action Recognition"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----83e5061ddf90---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----83e5061ddf90---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/shivam-sharma?source=post_page-----83e5061ddf90---------------shivam_sharma-----------------", "anchor_text": "Shivam Sharma"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----83e5061ddf90---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&user=Shivam+Sharma&userId=9768c95f2c90&source=-----83e5061ddf90---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&user=Shivam+Sharma&userId=9768c95f2c90&source=-----83e5061ddf90---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F83e5061ddf90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----83e5061ddf90---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----83e5061ddf90--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----83e5061ddf90--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----83e5061ddf90--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nofreeshivam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@nofreeshivam?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shivam Sharma"}, {"url": "https://medium.com/@nofreeshivam/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "59 Followers"}, {"url": "http://nofreeshivam.com", "anchor_text": "nofreeshivam.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9768c95f2c90&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&user=Shivam+Sharma&userId=9768c95f2c90&source=post_page-9768c95f2c90--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F59e544556814&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeep-learning-architectures-for-action-recognition-83e5061ddf90&newsletterV3=9768c95f2c90&newsletterV3Id=59e544556814&user=Shivam+Sharma&userId=9768c95f2c90&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}