{"url": "https://towardsdatascience.com/quickprop-an-alternative-to-back-propagation-d9a78069e2a7", "time": 1683013033.074347, "path": "towardsdatascience.com/quickprop-an-alternative-to-back-propagation-d9a78069e2a7/", "webpage": {"metadata": {"title": "Quickprop, an Alternative to Back-Propagation | by Johanna Appel | Towards Data Science", "h1": "Quickprop, an Alternative to Back-Propagation", "description": "Due to the slowly converging nature of the vanilla back-propagation algorithms of the \u201980s/\u201990s, Scott Fahlman invented a learning algorithm dubbed Quickprop [1] that is roughly based on Newton\u2019s\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Newton's_method", "anchor_text": "Newton\u2019s method", "paragraph_index": 0}, {"url": "https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/", "anchor_text": "this useful blog post", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my last article", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor series", "paragraph_index": 13}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my earlier article", "paragraph_index": 40}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my series of articles", "paragraph_index": 44}, {"url": "https://github.com/ephe-meral/cascor", "anchor_text": "available on Github", "paragraph_index": 45}, {"url": "https://medium.com/@johannaappel/membership", "anchor_text": "sign up for a medium membership", "paragraph_index": 46}, {"url": "https://medium.com/subscribe/@johannaappel", "anchor_text": "follow my account", "paragraph_index": 46}, {"url": "http://www.it.uu.se/edu/course/homepage/mil/vt11/handouts/fahlman.quickprop-tr.pdf", "anchor_text": "An empirical study of learning speed in back-propagation networks", "paragraph_index": 47}, {"url": "http://web.cs.iastate.edu/~honavar/fahlman.pdf", "anchor_text": "The cascade-correlation learning architecture", "paragraph_index": 48}], "all_paragraphs": ["Due to the slowly converging nature of the vanilla back-propagation algorithms of the \u201980s/\u201990s, Scott Fahlman invented a learning algorithm dubbed Quickprop [1] that is roughly based on Newton\u2019s method. His simple idea outperformed back-propagation (with various adjustments) on problem domains like the \u2018N-M-N Encoder\u2019 task \u2014 i.e. training a de-/encoder network with N inputs, M hidden units and N outputs.One of the problems that Quickprop specifically tackles is the issue of finding a domain-specific optimal learning rate, or rather: an algorithm that adjusts it appropriately dynamically.", "In this article, we\u2019ll look at the simple mathematical idea behind Quickprop. We\u2019ll implement the basic algorithm and some improvements that Fahlman suggests \u2014 all in Python and PyTorch.", "A rough implementation of the algorithm and some background can already be found in this useful blog post by Giuseppe Bonaccorso. We are going to expand on that \u2014 both on the theory and code side \u2014 but if in doubt, have a look at how Giuseppe explains it.", "The motivation to look into Quickprop came from writing my last article on the \u201cCascade-Correlation Learning Architecture\u201d [2]. There, I used it to train the neural network\u2019s output and hidden neurons, which was a mistake I realized only later and which we\u2019ll also look into here.", "To follow along with this article, you should be familiar with how neural networks can be trained using back-propagation of the loss gradient (as of 2020, a widely used approach). That is, you should understand how the gradient is usually calculated and applied to the parameters of a network to try to iteratively achieve convergence of the loss to a global minimum.", "We\u2019ll start with the mathematics behind Quickprop and then look at how it can be implemented and improved step by step.To make following along easier, any equations used and inference steps done are explained in more detail than in the original paper.", "The often used learning method of back-propagation for neural networks is based on the idea of iteratively \u2018riding down\u2019 the slop of a function, by taking short steps in the inverse direction of its gradient.", "These \u2018short steps\u2019 are the crux here. Their length usually depends on a learning rate factor, and that is kept intentionally small to not overshoot a potential minimum.", "Back in the days when Fahlman developed Quickprop, choosing a good learning rate was something of a major problem. As he actually mentions in his paper, in the best performing algorithm, the scientist chose the learning rate \u2018by eye\u2019 (i.e. manually and based on experience) every step along the way! [1]", "Faced with this, Fahlman came up with a different idea: Solving a simpler problem.", "Minimizing the loss function L, especially for deep neural networks, can become extremely difficult analytically (i.e. in a general way on the entire domain).In back-propagation, for instance, we only calculate it point-wise and then do the small steps in the right direction. If we would know how the \u2018terrain\u2019 of the function looks like in general, we could \u2018jump\u2019 to the minimum directly.", "But what if we could replace the loss function with a simpler version, of which we know its terrain? This is exactly Fahlmans\u2019 assumption taken in Quickprop: He presumes that L can be approximated by a simple parabola that opens in the positive direction. This way, calculating the minimum (of the parabola) is as simple as finding the intersection of a line with the x-axis.", "And if that point is not yet a minimum of the loss function, the next parabola can be approximated from there, like in the graphic below.", "So\u2026 How exactly can we approximate L? Easy \u2014 using a Taylor series, and a small trick.", "Note that for the following equations, we consider the components of the weight vector w to be trained independently, so w is meant to be seen as a scalar. But we can still exploit the SIMD architecture of GPU\u2019s, using component-wise computations.", "We start off with the second order Taylor expansion of L, giving us a parabola (without an error term):", "(To understand how this was created, check out the Wikipedia article on Taylor series linked above \u2014 it\u2019s as simple as inputting L into the general Taylor formula up to the second term and dropping the rest.)", "We can now define the update rule for the weights based on a weight difference, and input that into T:", "Quickprop now further approximates L\u2019\u2019 linearly using the difference quotient (this is the small trick mentioned above):", "Using this, we can rewrite the Taylor polynomial to this \u2018Quickprop\u2019 adjusted version and build its gradient:", "And that last equation, finally, can be used to calculate the stationary point of the parabola:", "That\u2019s it! Now, to put things together, given a previous weight, a previous weight difference and the loss slope at the previous and current weight, Quickprop calculates the new weight simply by:", "Before starting with the actual Quickprop implementation, let\u2019s import some foundational libraries:", "With the last two lines of the mathematical equation from earlier, we can start with Quickprop! If you read the first article on Cascade-Correlation, you might be already familiar with this \u2014 here, we\u2019ll concentrate on essential parts of the algorithm first, and put it all together in the end.", "Note that we use PyTorch to do the automatic gradient calculation for us. We also assume to have defined an activation and loss function beforehand.", "This is the simplest Quickprop version for one epoch of learning. To actually make use of it, we\u2019ll have to run it several times and see if the loss converges (we\u2019ll cover that bit later).", "However, this implementation is flawed in several ways, which we are going to investigate and fix in the following sections:", "The first simple fix we can apply is using gradient descent (with a very small learning rate) to prepare the dw_prev and dL_prev variables. This will give us a good first glimpse of the loss function terrain, and kick-starts Quickprop in the right direction.", "Gradient descent is easily implemented using pytorch again \u2014 we\u2019ll also use the opportunity to refactor the code above a bit as well:", "Sometimes, the weight deltas become vanishingly small when using the Quickprop parabola approach. To prevent that from happening when the gradient is not zero, Fahlman recommends conditionally adding the slope to the weight delta.The idea can be described like this: Go further if you have been moving in that direction anyway, but don\u2019t push on if your previous update sent you in the opposite direction (to prevent oscillation).", "With a little piece of decider code, this can be implemented quite easily:", "With this, we can put it all into one small function:", "As a second step, we\u2019ll fix the issue of exploding weight deltas near some function features (e.g. near singularities).To do that, Fahlman suggests to clip the weight update, if it would be bigger than the last weight update times a maximum grow factor:", "On some occasions, the previous and current computed slope can be the same. The result is that we\u2019ll try to divide by zero in the weight update rule, and will afterward continue having to work with NaN's, which obviously breaks the training.The simple fix here is to do a gradient descent step instead.", "Besides the last factor, they look similar, no?Which means we can go branchless again (i.e. save us some if-clauses), stay element-wise and pack everything in one formula:", "The attentive reader probably noted the \u2018learning rate\u2019 factor we used above \u2014 a parameter we thought we could get rid of\u2026Well, actually we sort of did, or at least we did get rid of the problem of having to adjust the learning rate over the course of the training. The Quickprop learning rate can stay fixed throughout the process. It only has to be adjusted once per domain in the beginning. The actual dynamic step sizes are chosen through the parabola jumps, which in turn depend heavily on the current and last calculated slope.", "If you think this sounds awfully familiar to how back-propagation learning rate optimizers work (think: momentum), you\u2019d be on the right track. In essence, Quickprop achieves something very similar to them \u2014 just that it doesn\u2019t use back-propagation at its core.", "Coming back to the code: Since we already implemented gradient descent earlier on, we can build on that and re-use as much as possible:", "With all of these functions in place, we can put it all together. The bit of boilerplate code still necessary just does the initialization and checks for convergence of the mean loss per epoch.", "Quickprop has one major caveat that greatly reduces its usefulness: The mathematical \u2018trick\u2019 we used, i.e. the approximation of the second order derivative of the loss function with a simple difference quotient relies on the assumption that this second order derivative is a continuous function.This is not given for activation functions like e.g. the rectified linear unit, or ReLU for short. The second order-derivative is discontinuous and the behavior of the algorithm might become unreliable (e.g. it might diverge).", "Looking back at my earlier article covering the implementation of Cascade-Correlation, we trained the hidden units of the network using Quickprop and used the covariance function as a way to estimate loss in that process. However, the covariance (as implemented there) is wrapped in an absolute value function. I.e. its second-order derivative is discontinuous and therefore, Quickprop should not be used. The careful reader of Fahlman et al.\u2019s Cascade-Correlation paper [2] may have also noticed that they are actually using gradient ascent to calculate this maximum covariance.", "Apart from that, it also seems that Quickprop delivers better results on some domains rather than others. An interesting summary by Brust et al. showed that it achieved better training results compared to the quality of back-propagation based techniques on some simple image classification tasks (classifying basic shapes) while at the same time doing worse on more realistic image classification tasks [3].I haven\u2019t done any research in that direction, but I wonder if this could imply that Quickprop might work better on less fuzzy and more structured data (think data frames/tables used in a business context). That would surely be interesting to investigate.", "This article covered Scott Fahlman\u2019s idea of improving back-propagation. We had a look at the mathematical foundations and a possible implementation.", "Now go about and try it out for your own projects \u2014 I\u2019d love to see what Quickprop can be used for!", "If you would like to see variants of Quickprop in action, check out my series of articles on the Cascade-Correlation Learning Architecture.", "All finished notebooks and code of this series are also available on Github. Please feel encouraged to leave feedback and suggest improvements.", "Finally, if you\u2019d like to support the creation of this and similarly fascinating articles, you can sign up for a medium membership and/or follow my account.", "[1] S. E. Fahlman, An empirical study of learning speed in back-propagation networks (1988), Carnegie Mellon University, Computer Science Department", "[2] S. E. Fahlman and C. Lebiere, The cascade-correlation learning architecture (1990), Advances in neural information processing systems (pp. 524\u2013532)", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Tech Entrepreneur, interested in Machine Intelligence, New Rationality, Maths, Physics \u2026 you name it."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd9a78069e2a7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@johannaappel?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johannaappel?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Johanna Appel"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fede7381126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&user=Johanna+Appel&userId=ede7381126aa&source=post_page-ede7381126aa----d9a78069e2a7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://en.wikipedia.org/wiki/Newton's_method", "anchor_text": "Newton\u2019s method"}, {"url": "https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/", "anchor_text": "this useful blog post"}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my last article"}, {"url": "https://en.wikipedia.org/wiki/Taylor_series", "anchor_text": "Taylor series"}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my earlier article"}, {"url": "https://towardsdatascience.com/cascade-correlation-a-forgotten-learning-architecture-a2354a0bec92", "anchor_text": "my series of articles"}, {"url": "https://github.com/ephe-meral/cascor", "anchor_text": "available on Github"}, {"url": "https://medium.com/@johannaappel/membership", "anchor_text": "sign up for a medium membership"}, {"url": "https://medium.com/subscribe/@johannaappel", "anchor_text": "follow my account"}, {"url": "http://www.it.uu.se/edu/course/homepage/mil/vt11/handouts/fahlman.quickprop-tr.pdf", "anchor_text": "An empirical study of learning speed in back-propagation networks"}, {"url": "http://web.cs.iastate.edu/~honavar/fahlman.pdf", "anchor_text": "The cascade-correlation learning architecture"}, {"url": "https://arxiv.org/pdf/1606.04333.pdf", "anchor_text": "Neither Quick Nor Proper \u2014 Evaluation of QuickProp for Learning Deep Neural Networks"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d9a78069e2a7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/quickprop?source=post_page-----d9a78069e2a7---------------quickprop-----------------", "anchor_text": "Quickprop"}, {"url": "https://medium.com/tag/artifical-intelligence?source=post_page-----d9a78069e2a7---------------artifical_intelligence-----------------", "anchor_text": "Artifical Intelligence"}, {"url": "https://medium.com/tag/python?source=post_page-----d9a78069e2a7---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----d9a78069e2a7---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&user=Johanna+Appel&userId=ede7381126aa&source=-----d9a78069e2a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&user=Johanna+Appel&userId=ede7381126aa&source=-----d9a78069e2a7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd9a78069e2a7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d9a78069e2a7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d9a78069e2a7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johannaappel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@johannaappel?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Johanna Appel"}, {"url": "https://medium.com/@johannaappel/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "130 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fede7381126aa&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&user=Johanna+Appel&userId=ede7381126aa&source=post_page-ede7381126aa--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff0c35e7946b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquickprop-an-alternative-to-back-propagation-d9a78069e2a7&newsletterV3=ede7381126aa&newsletterV3Id=f0c35e7946b&user=Johanna+Appel&userId=ede7381126aa&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}