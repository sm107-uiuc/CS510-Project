{"url": "https://towardsdatascience.com/tell-me-a-story-ai-one-that-i-like-4c0bc60f46ae", "time": 1683005655.896296, "path": "towardsdatascience.com/tell-me-a-story-ai-one-that-i-like-4c0bc60f46ae/", "webpage": {"metadata": {"title": "Tell me a story, AI. One that I like. | by Andre Ye | Towards Data Science", "h1": "Tell me a story, AI. One that I like.", "description": "Now that we\u2019re all in isolation, we need, more than ever, stories. But not all stories are interesting to us \u2014 someone that loves romance may turn their nose at a mystery story, and vice versa. Who\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/jrobischon/wikipedia-movie-plots", "anchor_text": "movie plots", "paragraph_index": 5}, {"url": "https://www.kaggle.com/washingtongold/tell-me-a-story", "anchor_text": "here", "paragraph_index": 31}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-of-higher-dimensions-ab180e0bb45f?source=---------21------------------", "anchor_text": "here", "paragraph_index": 72}, {"url": "https://www.kaggle.com/washingtongold/tell-me-a-story-1-2?scriptVersionId=31773396", "anchor_text": "here", "paragraph_index": 103}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership", "paragraph_index": 105}], "all_paragraphs": ["Now that we\u2019re all in isolation, we need, more than ever, stories. But not all stories are interesting to us \u2014 someone that loves romance may turn their nose at a mystery story, and vice versa. Who better than AI to tell us stories tailored to us?", "In this article, I\u2019ll guide you through the process and code of writing an AI to make the quarantine a bit more fun and interesting by telling us stories \u2014 ones that are tailored to our tastes.", "The article will be broken down into several segments:", "We\u2019d like for the AI to tell us stories. Ideally, in true technology-Renaissance fashion, we\u2019d like to train a Recurrent Neural Network or some other generative method. However, in my experience working with text generation, they either take a very, very long time to train or end up overfitting to the data and defeat the purpose of \u2018original text generation\u2019. As an additional note, the time it takes to train a well-performing model takes more than 8 hours, the operating time limit, on Kaggle, which (as of my knowledge) is the most efficient free platform for training deep learning models.", "Instead, I\u2019d like this project to be quick and universal, something that everyone can implement. Instead of training a RNN or a generative model, this AI will search for human-created stories in a \u2018story database\u2019 and find the best one. Not only does this guarantee a basic level of common sense and quality of story (made by humans, for humans), it\u2019s faster.", "For our \u2018story database\u2019, we\u2019ll use the Wikipedia movie plots dataset on Kaggle. With stories of 35k movies of all genres, nationalities, and times, it is the best story database one could find.", "Among the columns are release year, title, nationality of the film, genre, and a text description of the plot.", "Now that we have our data, we can begin devising a rough outline/blueprint.", "Note that the ~five or so most discriminating stories are chosen such that the model is given the most information possible with a limited amount of data.", "Initially, the program requests your opinion on three story snapshots. These three stories are, in the program\u2019s mind, most representative of each of the clusters in the data.", "After you answer the first three starter questions to gauge roughly where your tastes are, the model will begin generating stories it thinks you\u2019ll like.", "If you are interested in the snippet from a story, the program will print out the entire story for you to read.", "Whether you enjoy a story or not is added to the model\u2019s training data to improve its recommendations. The model continually learns as you read stories. If you dislike a snippet, the program will not print out the full length story and move on to generating a new one.", "After responding \u20181\u2019 to a few murder and police snippets, the program began learning and began to recommend more and more stories in that direction.", "The program, like a Monte Carlo Tree Search, begins going in a direction that optimizes its reward and retreats when it\u2019s gone too far (perhaps it has strayed too far from genres of stories that you like) to optimize your experience.", "We\u2019ll load the data with pandas\u2019 load_csv.", "We have the release year, the title, nationality of the movie, director, cast, genre, a URL to the Wikipedia page for that movie, and a text description of the plot. We can get rid of the director and the cast \u2014 there are simply too many classes (to be exact, 12593 unique directors and 32182 casts) for our recommendation algorithm or clustering methods to get anything from it. However, the genre is of use, with a smaller amount of classes \u2014 30 for movies with over 100 movies in its category \u2014 representative of 80% of the movies (the rest belong to smaller sub-categories that can simply be labelled as \u2018other\u2019). Hence, we can drop the director and the cast.", "Another issue we encounter is of bracket referencing. As you may be familiar with, Wikipedia references its sources by putting a bracket with a number inside it corresponding to a source (e.g. [3]).", "For example, in the string above, we\u2019d like to remove the [1]. The simplest solution to this is creating a list with every bracket value ([1], [2], [3], \u2026, [98], [99]) and removing each one of them from the string. This works because we can be sure that no article has more than 99 citations. While not the most efficient, it gets the job done with messy string indexing or splitting.", "This creates the \u2018blacklist\u2019 \u2014 a list full of the citations we don\u2019t need or want.", "Using the blacklist, we can create a function remove_brackets to be applied on the column.", "This concludes our elementary round of data cleaning.", "A key element of our system is summarizing plots. Because stories are often too long to read, it\u2019s important to summarize the stories so users can choose if they want to keep on reading or not.", "We\u2019ll be using graph-based summarization, which is the most popular method for text summarization. It creates a graph of document units (whereas most other methods utilize sentences as base units) and then selects nodes with a PageRank version adapted for this scenario. The original PageRank version by Google operates on a similar graph-based method to find webpage nodes.", "The PageRank algorithm calculates node \u2018centrality\u2019 in a graph, which is useful in measuring relevant information content in the sentences. The graph is constructed using Bag of Words feature sequences and cosine-similarity-based edge weights.", "We\u2019ll use the gensim library to summarize long texts. It is simple, to implement, as with the example before:", "The summarization makes sense (if you bothered to read the entire text). Graph summarization is one of the most effective methods of summarization, and we\u2019ll be using that for our plots. Let\u2019s create a function summary that takes in a text and outputs a summary. However, we need to set into place two conditions:", "If it does not meet any of those two conditions, it will return the summary of the text. Then, we\u2019ll create a column Summary that summarizes the plots.", "This takes a few hours to run. However, it\u2019s a one-time cost, and having a summary will save time later.", "Let\u2019s take a look at how it performs on some example texts from the dataset:", "Why, that\u2019s a great teaser of what the full story yields! These summaries are quick to read and give a great glimpse of important sentences from the movie plot. This has worked out nicely.", "The code in a forkable notebook applied to the real dataset can be found here.", "In order to find the most discriminating story plots, we\u2019ll cluster the text of the plots using K Means into the optimal number of clusters. The cluster label of the text, along with the nationality, genre, and year of the movie, will be clustered to find clusters among movies. The movies closest to each of the centroids (cluster centers) should be most representative of their cluster, hence the most discriminatory. The main idea behind this idea is that", "Asking users about how much they like those most discriminatory movies should provide the most information for a model with no previous information on the user\u2019s tastes.", "The nationality, genre, and year of the movie all represent aspects of the movie that may be communicated in the text, which helps us find suitable recommendations quickly. The most \u2018accurate\u2019 recommendation would be, theoretically, some sort of similarity between very, very, very long vectorizations of the plots, but that would take a long time. Instead, a plot can be \u2018represented\u2019 by its attributes.", "Clustering the text is a one-time thing that not only gives us an additional feature to use for clustering movies, but also features to use as movie attributes when we actually make recommendations.", "Let\u2019s get started! To begin, we\u2019ll need to remove any punctuation and make all text lowercase. We can use regular expressions to create a function clean() that does this.", "Using pandas\u2019 .apply(), the function can be applied to all of the plots.", "Now, let\u2019s vectorize the data. We\u2019ll use a Term-Frequency Inverse-Document-Frequency (TF-IDF) vectorizer. This vectorizer further serves the purpose of helping differentiate important words from unimportant ones to have better clusters. The vectorizer emphasizes words that appear more in one document but less across the overall corpus (more discriminating) and deemphasizes words that appear across all documents.", "We\u2019ll save the very, very sparse matrix to a variable X. Because K-Means is distance-based, meaning it is subject to the Curse of Dimensionality, we should do our best to reduce the dimensionality of the vectorized text by setting the maximum number of elements in a vector to 500. (When I didn\u2019t implement the max_features limit, the text classified everything but one text in cluster 2 and one text in cluster 3 as cluster 1. This is a result of the Curse of Dimensionality on K-Means \u2014 distance is so distorted that with the presumably hundreds of thousands of dimensions in a TF-IDF\u2019s vocabulary, everything except the outliers are grouped into one category.", "For the same reason, it\u2019s a good idea to scale the data before we feed it into the K-Means model. We\u2019ll use StandardScaler, which puts the data on a range between -1 to 1.", "Now, it\u2019s time to train the K-Means models. Ideally, the number of clusters (number of questions we need to ask) will be between 3 and 6, inclusive.", "Hence, we\u2019ll run a K-Means model with each number of clusters within the list [3, 4, 5, 6]. We\u2019ll evaluate the silhouette score of each and find which number of clusters is the best for our data.", "First, let\u2019s initialize two lists to store the number of clusters and the silhouette scores (the x and the y of our plot):", "Next, we\u2019ll import sklearn\u2019s KMeans and silhouette_score.", "Then, for each number of clusters within the four possible clusters, we\u2019ll fit a KMeans model with n amount of clusters, then append the silhouette score for that number of clusters to the list scores.", "From here, I\u2019ll click \u2018Commit\u2019 on Kaggle and let it run by itself \u2014 it takes a couple hours to run through.", "Finally, it turns out that the number of clusters that performs the best is three clusters, with the highest silhouette score.", "Now that we have the text labels, we can start clustering the movies as a whole. However, we do have to take some steps to clean the data.", "For example, the Release Year starts counting from the 1900s. If taken at literal integer value, it will confuse the model. Instead, let\u2019s create a column Age that returns the age of a movie, which is simply 2017 (the youngest movie in the database) minus whatever year the movie was released.", "Now, the Age starts counting from 0 and actually means something.", "The Origin/Ethnicity column is an important ones \u2014 the style and flavor of a story often can be traced to where it comes from. However, this column is categorical, meaning that it is in the form [\u2018American\u2019, \u2018Telegu\u2019, \u2018Chinese\u2019], for example. To convert it into something machine readable, we will need to One-Hot encode it, which can be easily done with sklearn\u2019s OneHotEncoder.", "Now, nation stores the one-hot encoded values for each of the rows. Each index of a row represents a unique value \u2014 for instance, the first column (the first index in each row) represents \u2018American\u2019.", "However, currently, it\u2019s simply an array \u2014 we will need to create columns in the data to actually translate the information into our data. So, for each column, we\u2019ll name whatever country that column of the vector corresponds to (enc.categories_[0] returns an array of original columns, and nation[:,i] indexes the ith value of each row in an array).", "We\u2019ve successfully added data about the nationality of each story into our data. Let\u2019s do the same with the genre of the story. This may be even more important than nationality, because it conveys information about what the story discusses on a level that is simply impossible for a machine learning model to recognize.", "It seems that much of the genres are unknown. Don\u2019t worry \u2014 we will address this later. Right now, our goal is to one hot encode the genre. We\u2019ll follow a similar process as before, but slightly altered \u2014 because there are so many genres that are considered different because they are named differently (e.g. \u2018drama comedy\u2019 and \u2018romantic comedy\u2019) but are pretty much the same, we will only select the top 20 most populous genres and the impute the rest into one of the 20 genres.", "Note that we end up removing \u2018unknown\u2019 from the list of columns, which is why initially the top 21 populous genres are selected. Next, let\u2019s process the genre according to the top_genres such that if a genre is not in the top 20 most populous genres, it will be replaced with a string value \u2018unknown\u2019.", "Then, like before, we\u2019ll create an instance of a one-hot encoder and save its transformation on the data in an array form into variable genres.", "To integrate the array into the data, we\u2019ll once again create columns where each column in the data is filled with one column in the array.", "Great! Our data is one-hot encoded \u2014 but we still have an issue with unknown values. Now that all the data has been one-hot encoded, we know that the rows where the column unknown has value 1 need to have their genre imputed. Hence, for each index whose genre needs to be imputed, we\u2019ll replace their genres with a nan value so the KNN imputer we will use later can recognize that it is a missing value.", "Now that all the missing values are marked as missing, we could use a KNN imputer. However, I\u2019m afraid there\u2019s not much data to impute the data on except for the year released and nationality. Let\u2019s use a TF-IDF vectorizer and select the top 30 words from the story just as an additional supplement of information for KNN to properly assign genres.", "The text must always be cleaned beforehand, so we\u2019ll use regular expressions to remove all punctuation and make everything lowercase.", "We\u2019ll set English standard stop words and set the maximum number of features to 30. The cleaned, vectorized content will be stored to variable X in the form of an array.", "Like previously, we will transfer each column of information in array X into a column in our data, naming each column whatever word the column in X corresponds to.", "These words will provide more context for what a story is about to help impute the genre. Finally, it\u2019s time to impute the genre!", "Our imputer will recognize the np.nan values marked as missing values and automatically uses surrounded data on the nationality and the words within the data, as well as the age of a movie, to impute the genre. The results are saved to a variable imputed in the form of an array. Like always, we\u2019ll translate the data over:", "After removing columns that we\u2019ve one-hot encoded or don\u2019t need anymore, like the Unknown column for Genre or the categorical Genre variable\u2026", "\u2026the data is ready and free from missing values. Another interesting aspect of KNN imputation is that it can give decimal values \u2014 that is, a movie is 20% Western and some other percent of another genre, or several genres.", "These features will all serve well to be clustered. Combined with the text clustering labels achieved earlier, these features should be a great indicator of where a story stands in the realm of people\u2019s preferences. Finally, we can begin clustering \u2014 like before, we\u2019ll cluster the stories into 3, 4, 5, or 6 clusters, and see which one performs the best.", "\u2026it seems, like before, that three clusters performs the best, having the highest silhouette score. Let\u2019s train our KMeans on three clusters only:", "It\u2019s good that each cluster has a relatively even spread of movies. Let\u2019s get the cluster centers, which can be accessed with the .cluster_centers_ method:", "First, let\u2019s assign labels to each item.", "For each cluster, we want to find the data point that is the closest in terms of Euclidean distance to the cluster centers. This will be the one that is the most representative of that cluster. The distance between two points p and q is given by the square root of the sum of the squares of differences of corresponding dimensions of p and q. You can find my proof of Euclidean distance (Pythagorean formula in multiple dimensions) here.", "Because Euclidean distance is the l2 norm, this can be easily calculated with numpy\u2019s linear algebra capabilities: np.linalg.norm(a-b).", "Let\u2019s look at the entire code to calculate the code and find stories that have the smallest Euclidean distances with the cluster .", "This initializes the search. First, we store the stories whose label is equal to the cluster that we are currently searching for. We drop \u2018Label\u2019 from the subset afterwards. To store the original index for later referencing, we\u2019ll store the indices into a variable indexes. Afterward, we\u2019ll reset the index on subset to ensure smooth row indexing. Then, we\u2019ll select the center point of the current cluster, and initiate a dictionary with two columns: a list for storing the index of a story within the master dataset, and a list for storing the score/distance.", "This code runs through every row within the subset, records the current index, and calculates and records the distance between it and the center.", "This converts scores into a pandas DataFrame for analysis, and prints out the indexes of stories whose distance from the centers were the closest.", "It looks like there are four stories for the first cluster that had the closest Euclidean distance, but only one for clusters 1 and 2. Let\u2019s check out which stories these are.", "Great! We have gotten our three most discriminating story plots. While it may not seem as discriminating to humans, in our machine learning model\u2019s mind these will give it the most information to work with off the bat.", "The recommendation engine will simply be a machine learning model, predicting which movie plots have a higher chance of being rated highly by the user. The engine will take in the features of the movie, such as its age or nationality, as well as a TF-IDF vectorized version of the summary, limited to 100 features (distinct words).", "The target for each movie plot will be either 1 or 0. A model will be trained on the available data (the stories the user has rated) and predict probabilities of the user rating a story positively. The highest probability-plot will be recommended to the user next, and the user\u2019s rating on that story will be recorded and the story added to a list of training data.", "As training data, we\u2019ll simply use attributes from the data on each movie.", "We\u2019ll probably want a Decision Tree classifier because it makes efficient predictions, it\u2019s quick to train, and develops a high-variance solution, something that recommender systems strive for.", "In the next section, we\u2019ll bring all of our previous work together.", "Let\u2019s start out by coding the user\u2019s ratings on the three most discriminatory movies. For each input, the program ensures that the output is either 0 or 1.", "This works well. Next, let\u2019s store the data into a training dataset DataFrame and remove those indices from the data.", "Now, it\u2019s time to create a loop. We\u2019ll train a Decision Tree classifier on the current training set.", "Then, for every index in the data, we\u2019ll make a probability prediction.", "To ensure a quick selection, we\u2019ll randomly select about 1/3 of the data by shuffling the data and selecting the top 10,000 rows. The code saves the indices to a DataFrame.", "Initially, it rates many movies with probability 1, but as we progress and the model learns more, it will begin to make more advanced choices.", "We\u2019ll save the index of the top-performing movie to a variable index.", "Now, we need to fetch information about index from the data and display it.", "After verifying that the user\u2019s input is either 0, 1, or -1 to quit\u2026", "\u2026we can begin adding to our training data. First, however, if the user wants to quit, we must break from the loop.", "Additionally, regardless of if the user likes or dislikes the movie, we are still going to add it to the training data (the target will just be different):", "Finally, if the response is equal to 0, we\u2019ll append 0 to the y. The user didn\u2019t want like the story.", "However, if the user likes it, the program will print the full story.", "We will once again collect the user\u2019s input and ensure it is either 0 or 1,", "\u2026and add 0 or 1 to the y correspondingly.", "Finally, we\u2019ll remove the story from the data so the user does not see the same story again.", "And we\u2019ve completed! Each iteration, the training data will be updated and the model more and more accurate.", "I hope you enjoyed! Maybe you\u2019ll use this program to read a couple of interesting plots, or maybe check out the movies to whom plots belong. It was fun to deal with problems and challenges in the data to create something to entertain us during this period of isolation.", "If you\u2019d like to toy a bit with the program, you can find it here. It still can be significantly improved, so I\u2019d be interested in what you all can do with the code!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "ML enthusiast. Join Medium through my referral link: https://andre-ye.medium.com/membership."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4c0bc60f46ae&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://andre-ye.medium.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006----4c0bc60f46ae---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.kaggle.com/jrobischon/wikipedia-movie-plots", "anchor_text": "movie plots"}, {"url": "https://www.kaggle.com/washingtongold/tell-me-a-story", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/understanding-the-mathematics-of-higher-dimensions-ab180e0bb45f?source=---------21------------------", "anchor_text": "here"}, {"url": "https://www.kaggle.com/washingtongold/tell-me-a-story-1-2?scriptVersionId=31773396", "anchor_text": "here"}, {"url": "https://medium.com/tag/ai?source=post_page-----4c0bc60f46ae---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----4c0bc60f46ae---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----4c0bc60f46ae---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/technology?source=post_page-----4c0bc60f46ae---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/tag/data?source=post_page-----4c0bc60f46ae---------------data-----------------", "anchor_text": "Data"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&user=Andre+Ye&userId=be743a65b006&source=-----4c0bc60f46ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&user=Andre+Ye&userId=be743a65b006&source=-----4c0bc60f46ae---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4c0bc60f46ae&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4c0bc60f46ae---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4c0bc60f46ae--------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://andre-ye.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andre Ye"}, {"url": "https://andre-ye.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "9.8K Followers"}, {"url": "https://andre-ye.medium.com/membership", "anchor_text": "https://andre-ye.medium.com/membership"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fbe743a65b006&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&user=Andre+Ye&userId=be743a65b006&source=post_page-be743a65b006--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff44a966e4ff1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftell-me-a-story-ai-one-that-i-like-4c0bc60f46ae&newsletterV3=be743a65b006&newsletterV3Id=f44a966e4ff1&user=Andre+Ye&userId=be743a65b006&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}