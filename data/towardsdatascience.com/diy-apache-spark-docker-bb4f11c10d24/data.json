{"url": "https://towardsdatascience.com/diy-apache-spark-docker-bb4f11c10d24", "time": 1683006941.051347, "path": "towardsdatascience.com/diy-apache-spark-docker-bb4f11c10d24/", "webpage": {"metadata": {"title": "DIY: Apache Spark & Docker. Set up a Spark cluster in Docker from\u2026 | by Shane De Silva | Towards Data Science", "h1": "DIY: Apache Spark & Docker", "description": "Apache Spark provides users with a way of performing CPU intensive tasks in a distributed manner. It\u2019s adoption has been steadily increasing in the last few years due to its speed when compared to\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html", "anchor_text": "Spark won the Gray Sort Benchmark test", "paragraph_index": 1}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/TUTORIAL.md", "anchor_text": "github repo", "paragraph_index": 4}, {"url": "http://localhost:8080.", "anchor_text": "http://localhost:8080.", "paragraph_index": 30}, {"url": "http://localhost:8080.", "anchor_text": "http://localhost:8080.", "paragraph_index": 33}, {"url": "http://localhost:4040.", "anchor_text": "http://localhost:4040.", "paragraph_index": 36}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080", "paragraph_index": 40}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_master", "anchor_text": "dockerfile", "paragraph_index": 40}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_worker", "anchor_text": "dockerfile here", "paragraph_index": 43}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080", "paragraph_index": 44}, {"url": "http://localhost:8081.", "anchor_text": "http://localhost:8081.", "paragraph_index": 44}, {"url": "https://www.informit.com/articles/article.aspx?p=2928186", "anchor_text": "article", "paragraph_index": 51}, {"url": "http://localhost:4040.", "anchor_text": "http://localhost:4040.", "paragraph_index": 53}, {"url": "https://github.com/sdesilva26/docker-spark/tree/master/docker", "anchor_text": "Dockerfiles", "paragraph_index": 60}, {"url": "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/", "anchor_text": "part 1", "paragraph_index": 75}, {"url": "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/", "anchor_text": "part 2", "paragraph_index": 75}, {"url": "https://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/", "anchor_text": "blog post", "paragraph_index": 75}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/docker-compose.yml", "anchor_text": "created", "paragraph_index": 107}, {"url": "https://github.com/juanfrans/notes/wiki/Copying-Files-Between-Local-Computer-and-Instance-(AWS)", "anchor_text": "here", "paragraph_index": 113}], "all_paragraphs": ["Two technologies that have risen in popularity over the last few years are Apache Spark and Docker.", "Apache Spark provides users with a way of performing CPU intensive tasks in a distributed manner. It\u2019s adoption has been steadily increasing in the last few years due to its speed when compared to other distributed technologies such as Hadoop. In 2014 Spark won the Gray Sort Benchmark test in which they sorted 100TB of data 3x faster using 10x fewer machines then a Hadoop cluster previously did.", "Docker on the other hand has seen widespread adoption in a variety of situations. Docker provides users the ability to define minimal specifications of environments meaning you can easily develop, ship, and scale applications. Furthermore, due to its use of linux containers users are able to develop Docker containers that can run be run simultaneously on a single server whilst remaining isolated from each other. Finally, Docker provides an abstraction layer called the Docker Engine that guarantees compatibility between machines that can run Docker solving the age-old headache of \u201cit works on my machine, I don\u2019t know why it doesn\u2019t on yours\u201d.", "With the rise of Big Data these two technologies are a matched made in heaven. Apache Spark providing the analytics engine to crunch the numbers and Docker providing fast, scalable deployment coupled with a consistent environment.", "I assume knowledge of Docker commands and terms as well as Apache Spark concepts. Therefore, I do not recommend this article if either of these two technologies are new to you. With considerations of brevity in mind this article will intentionally leave out much of the detail of what is happening. For a full drawn out description of the architecture and a more sequential walk through of the process I direct the reader to my github repo.", "I also assume that you have at least basic experience with a cloud provider and as such are able to set up a computing instance on your preferred platform.", "The rest of this article is going to be a fairly straight shot at going through varying levels of architectural complexity:", "First we need to get to grips with some basic Docker networking. We will do this with the containers running on the same machine in the first instance.", "NOTE: For the purpose of this section any images will do.", "2. Create a bridge network by running", "3. Run two containers on this user-defined bridge network by running", "4. Inspect the network and find the IP addresses of the two containers", "The output of the above should look like the image below", "5. Attach to the spark-master container and test it\u2019s communication to the spark-worker container using both it\u2019s IP address and then using its container name", "Because the containers have been deployed into the same Docker bridge network they are able to resolve the IP address of other containers using the container\u2019s name. This is called automatic service discovery and will be a great help to us later.", "With the above commands we have created the following architecture.", "4. Open up the following ports for the instance to communicate with docker hub (inbound and outbound);", "For example, on AWS, my security group which my two instances are deployed in have the following security group settings", "6. On instance 1, pull a docker image of your choice.", "7. Pull another image onto instance 2.", "8. Initialise a docker swarm and make instance 1 the swarm manager by running", "9. Copy the output of the command above from instance 1 and run it on instance 2 to join the swarm as a worker node", "10. On instance 1 (the swarm manager) create an overlay network", "11. On instance 1, run a container", "12. On instance 2, run a container within the overlay network created by the swarm manager", "13. From inside the container on instance 2 check the container communication by pinging the container running on instance 1", "14. Similarly, check the backwards connection from the container in instance 1 to the container in instance 2", "As before, the containers are able to resolve each other\u2019s IP address using only the container name since they are within the same overlay network.", "After following the instructions above, you have created an architecture similar to the one below.", "Now that we have a handle on how to get two different docker hosts to communicate, we will get started on creating a Spark cluster on our local machine.", "4. Check your master node has successfully been deploy by navigating to http://localhost:8080. You should see the following", "5. Attach a worker node to the cluster", "where the two flags define the amount of cores and memory you wish this worker to have. The last input is the address and port of the master node prefixed with \u201cspark://\u201d because we are using spark\u2019s standalone cluster manager", "6. Check that the worker was successfully registered with the master node by going back to http://localhost:8080. You should now see the worker node as a resource of the cluster. (You can also check the UI of the worker by going to http://localhost:8081)", "7. Test the cluster by opening a scala shell from the bin directory of your spark installation", "This will return an estimate of the value of pi.", "8. Check the UI of the application by going to http://localhost:4040. You should see something similar to", "You now have a fully functioning spark cluster!", "Now it\u2019s time to start tying the two together. We will now learn to walk before running by setting up a Spark cluster running inside Docker containers on your local machine", "2. Create a Spark master node inside of the bridge network", "3. Check the container successfully started a Spark master node by navigating to http://localhost:8080. I have set the sdesilva26/spark_master:0.0.2 image to by default set up a master node. See the dockerfile.", "4. Create a Spark worker node inside of the bridge network", "By default the sdesilva26/spark_worker:0.0.2 image, when run, will try to join a Spark cluster with the master node located at spark://spark-master:7077.", "If you change the name of the container running the Spark master node (step 2) then you will need to pass this container name to the above command, e.g. -e <MASTER_CONTAINER_NAME>. See the dockerfile here.", "5. Again, verify the worker has successfully registered with the master node by navigating to http://localhost:8080 and http://localhost:8081.", "6. Attach a second spark worker to the cluster", "The only change we had to make from the command in step 4 was that we had to give the container a unique name and also we had to map port 8081 of the container to port 8082 of the local machine since the spark-worker1 container is already using your local machines port 8081.", "7. Spin up a Spark submit node", "You should now be inside of the spark-submit container.", "8. Open a scala shell and connect to the Spark cluster", "As before, if you have a different name than spark-master for the container running your Spark master node then you would change the above command with \u2014 master spark://<YOUR_MASTER_CONTAINER>:7077.", "The above command also asks that on your cluster, you want each executor to contain 2G of memory and 1 core. The Spark master node will allocate these executors, provided there is enough resource available on each worker to allow this. For an explanation of executors and workers see the following article.", "9. Run an example job in the interactive scala shell", "10. Check the application UI by navigating to http://localhost:4040. You should see the following", "You have just run a Spark job inside of Docker containers. Spocker is born!", "What we have done in the above is created a network within Docker in which we can deploy containers and they can freely communicate with each other.", "The white arrows in the diagram below represents open communication between containers. Ports on the containers are shown in green and the ports of your local machine are shown in yellow.", "You can see that all the container are deployed within the bridge network. If we now deploy a container outside of this network, it would not be able to resolve the IP addresses of the other containers just by using their container names.", "Now let\u2019s wrap everything together to form a fully distributed Spark cluster running inside of Docker containers.", "NOTE: For this part you will need to use the 3 images that I have created.", "You can also build them yourself by downloading the Dockerfiles", "2. Copy and paste the output of the above command to at least 2 other instances. I have done this on 4 other instances \u2014 3 will act as Spark workers and 1 will be my Spark submit node", "3. On instance 1, create an overlay network as we did before", "4. Run the spark_master image to create a container that will be the Spark master node", "5. Open up ports 8080\u20138090 and 4040 by adding the following to your security group\u2019s inbound rules", "NOTE: In AWS security groups are stateful, so the return traffic from the instance to users is allowed automatically, so you don\u2019t need to modify the security group\u2019s outbound rules. On other cloud providers you may have to add a similar rule to your outbound rules.", "My inbound security group rules now look like this", "6. Check the Spark master node UI at http://<PUBLIC_IPv4_ADDRESS_OF_INSTANCE>:8080. You should see the same UI that we saw earlier.", "7. Now on another one of your instances run the following to attach a Spark worker node to the cluster", "NOTE: As a general rule of thumb start your Spark worker node with memory = memory of instance-1GB, and cores = cores of instance - 1. This leaves 1 core and 1GB for the instance\u2019s OS to be able to carry out background tasks.", "8. Again, check the master node\u2019s web UI to make sure the worker was added successfully.", "9. Rinse and repeat step 7 to add as many Spark workers as you please. Make sure to increment the name of the container though from spark-worker1 to spark-worker2, and so on.", "I have connected 3 workers and my master node\u2019s web UI looks like this", "10. In another instance, fire up a Spark submit node", "11. Launch a pyspark interactive shell and connect to the cluster", "NOTE: You specify the resources you would like each executor to have when connecting an application to the cluster by using the \u2014 conf flag. The topic of Spark tuning is a whole post in itself so I will not go into any detail here. This two part Cloudera blog post I found to be a good resource for understanding resource allocation: part 1 & part 2. Also a blog post by Anthony Shipman from C2FO.io I found very useful and also includes a handy excel sheet to work out settings for memory, cores, and parallelization.", "12. Check the submit node has successfully connected to the cluster by checking both the Spark master node\u2019s UI and the Spark submit node\u2019s UI. They should look like the images below.", "13. Run a sample job from the pyspark shell", "The more common way to submit jobs to a Spark cluster is by using the spark-submit script which is included with your spark installation. Let\u2019s also do this.", "14. Exit out of pyspark and submit a program to executor on the cluster", "It\u2019s pretty much the same syntax as before except we are calling the spark-submit script and we are passing it a .py file along with any other configurations for the file to execute.", "Happy days! We have now created a fully distributed Spark cluster running inside of Docker containers and submitted an application to the cluster.", "The architecture we have just created looks like the following", "Each Spark worker node and the master node is running inside a Docker container located on its own computing instance. The Spark driver node (spark submit node) is also located within its own container running on a separate instance. All the Docker daemons are connected by means of an overlay network with the Spark master node being the Docker swarm manager in this case.", "Within the overlay network, containers can easily resolve each other\u2019s addresses by referencing container names which utilises automatic service discovery.", "More Spark worker nodes can be fired up on additional instances if needed.", "In this tutorial we have managed to sequentially step through the varying levels of complexity in setting up a Spark cluster running inside of Docker containers.", "We first started off with some simple Docker networking principles both on our local machine, in which we used a bridge network, and then on distributed machines using an overlay network with a docker swarm.", "Next we set up a Spark cluster running on our local machine to get to grips with adding workers to a cluster.", "Then we introduced Docker back in to the mix and set up a Spark cluster running inside of Docker containers on our local machine.", "Finally, we brought everything together to construct a fully distributed Spark cluster running in Docker containers.", "Hopefully you\u2019ve gained a clear understanding of how these two technologies link together and could provide benefit to your specific problem or project.", "Now is the time for you to start experimenting and see what you can learn using this architecture.", "The above steps of manually creating a cluster was more informative than practical as it required a lot of manual typing and repeated commands.", "For small(ish) problems where you only need the resources of maybe 4 or 5 computing instances this amount of effort is probably below your pain threshold.", "However, as the data becomes truly large and the computing power needed starts to increase, following the above steps will turn you into a full-time cluster creator.", "A much more practical and elegant way of setting up a cluster is by taking advantage of Docker compose", "For those of you new to Docker compose, it allows you to launch what are called \u201cservices\u201d.", "A service is made up of a single Docker image, but you may want multiple containers of this image to be running. For example, running multiple Spark worker containers from the docker image sdesilva26/spark_worker:0.0.2 would constitute a single service.", "To launch a set of services you create a docker-compose.yml file which specifies everything about the various services you would like to run.", "However, Docker compose is used to create services running on a single host. It does not support deploying containers across hosts.", "The Docker stack is a simple extension to the idea of Docker compose. Instead of running your services on a single host, you can now run your services on multiple hosts which are connected as a Docker swarm.", "Best of all, if you have a Docker compose file very little modifications need to be made in order for it to work with the Docker stack commands.", "Let\u2019s see how to create our distributed Spark cluster running inside of Docker containers using a compose file and docker stack.", "You should get a similar output to the image below.", "2. For any instances you wish to be Spark workers, add a label to them", "3. Now label the instance you wish to run the Spark master node with the role of master", "4. Create a docker-compose.yml file or pull the one I have created.", "In this compose file I have defined two services \u2014 spark-master and spark-worker.", "The first service deploys a single container onto any node in the swarm that has the label \u201crole=master\u201d.", "The second service will deploy 3 containers of the sdesilva26/spark_worker:0.0.2 image onto nodes with the label \u201crole=worker\u201d. If 3 suitable nodes in the swarm aren\u2019t found it will deploy as many as is possible.", "Finally, all these containers will be deployed into an overlay network called spark-net which will be created for us.", "5. Copy the docker-compose.yml into the instance which is the swarm manager.", "[see here for alternative ways of doing this]", "6. Finally, run your docker stack from the swarm manager and give it a name.", "NOTE: the name of your stack will be prepended to all service names. So the service \u201cspark-master\u201d becomes \u201csparkdemo_spark-master\u201d. You can check the services that are running by using", "7. Check that the spark_worker image is running on the instances you labelled as \u2018worker\u2019 and that the spark_master image is running on the node labelled as \u2018master\u2019.", "Congratulations, we have just simplified all of the work from this article into a few commands from the Docker swarm manager.", "The best thing about Docker services is that it is very easy to scale up. For example, if later on we added another instance to the Docker swarm and we then wished to scale up the \u201csparkdemo_spark-worker\u201d service, we can simply run", "and you now have 4 Spark workers in your cluster!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD student interested in the application of statistical learning, DS, ML, and DL to real world problems"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fbb4f11c10d24&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@shanedesilva?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shanedesilva?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Shane De Silva"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb843df4e54f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&user=Shane+De+Silva&userId=b843df4e54f4&source=post_page-b843df4e54f4----bb4f11c10d24---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html", "anchor_text": "Spark won the Gray Sort Benchmark test"}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/TUTORIAL.md", "anchor_text": "github repo"}, {"url": "https://giphy.com/gifs/reaction-typing-unpopular-opinion-13GIgrGdslD9oQ/links", "anchor_text": "source"}, {"url": "https://github.com/sdesilva26/docker-spark/tree/master/docker", "anchor_text": "Dockerfiles"}, {"url": "https://eu-west-2.console.aws.amazon.com/ec2/v2/home?region=eu-west-2#SecurityGroup:groupId=sg-0140fc8be109d6ecf", "anchor_text": "sg-0140fc8be109d6ecf"}, {"url": "https://spark.apache.org/downloads.html", "anchor_text": "website"}, {"url": "http://localhost:8080.", "anchor_text": "http://localhost:8080."}, {"url": "http://localhost:8080.", "anchor_text": "http://localhost:8080."}, {"url": "http://localhost:4040.", "anchor_text": "http://localhost:4040."}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080"}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_master", "anchor_text": "dockerfile"}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/Dockerfile_worker", "anchor_text": "dockerfile here"}, {"url": "http://localhost:8080", "anchor_text": "http://localhost:8080"}, {"url": "http://localhost:8081.", "anchor_text": "http://localhost:8081."}, {"url": "https://www.informit.com/articles/article.aspx?p=2928186", "anchor_text": "article"}, {"url": "http://localhost:4040.", "anchor_text": "http://localhost:4040."}, {"url": "https://github.com/sdesilva26/docker-spark/tree/master/docker", "anchor_text": "Dockerfiles"}, {"url": "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/", "anchor_text": "part 1"}, {"url": "https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-2/", "anchor_text": "part 2"}, {"url": "https://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet/", "anchor_text": "blog post"}, {"url": "https://unsplash.com/@jon_chng?utm_source=medium&utm_medium=referral", "anchor_text": "Jonathan Chng"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://giphy.com/gifs/goodbye-back-to-the-future-marty-mcfly-12xvz9NssSkaS4/links", "anchor_text": "source"}, {"url": "https://github.com/sdesilva26/docker-spark/blob/master/docker/docker-compose.yml", "anchor_text": "created"}, {"url": "https://github.com/juanfrans/notes/wiki/Copying-Files-Between-Local-Computer-and-Instance-(AWS)", "anchor_text": "here"}, {"url": "https://unsplash.com/@preciousjfm?utm_source=medium&utm_medium=referral", "anchor_text": "J E W E L M I T CH E L L"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://medium.com/tag/big-data?source=post_page-----bb4f11c10d24---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/spark?source=post_page-----bb4f11c10d24---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/docker?source=post_page-----bb4f11c10d24---------------docker-----------------", "anchor_text": "Docker"}, {"url": "https://medium.com/tag/data-science?source=post_page-----bb4f11c10d24---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&user=Shane+De+Silva&userId=b843df4e54f4&source=-----bb4f11c10d24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&user=Shane+De+Silva&userId=b843df4e54f4&source=-----bb4f11c10d24---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fbb4f11c10d24&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----bb4f11c10d24---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----bb4f11c10d24--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shanedesilva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@shanedesilva?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Shane De Silva"}, {"url": "https://medium.com/@shanedesilva/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "163 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb843df4e54f4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&user=Shane+De+Silva&userId=b843df4e54f4&source=post_page-b843df4e54f4--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7b227aa56f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdiy-apache-spark-docker-bb4f11c10d24&newsletterV3=b843df4e54f4&newsletterV3Id=e7b227aa56f&user=Shane+De+Silva&userId=b843df4e54f4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}