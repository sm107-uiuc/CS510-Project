{"url": "https://towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac", "time": 1682994833.2980428, "path": "towardsdatascience.com/maximum-likelihood-estimation-984af2dcfcac/", "webpage": {"metadata": {"title": "Fundamentals of Machine Learning (Part 2) | by William Fleshman | Towards Data Science", "h1": "Fundamentals of Machine Learning (Part 2)", "description": "This is part two in a series on fundamental concepts of machine learning. We discuss the basics of modeling data and fitting parameters with Maximum Likelihood Estimation."}, "outgoing_paragraph_urls": [], "all_paragraphs": ["This is part two in a series of topics I consider fundamental to machine learning. Part one covered probability theory, which we will build on heavily in this post. If you need to refresh on probability check out part one:", "Maximum Likelihood Estimation (MLE) is a frequentist approach for estimating the parameters of a model given some observed data. The general approach for using MLE is:", "We\u2019re going to cover the basics of creating models, understanding likelihood, and using the Maximum Likelihood Estimation process for fitting our parameters.", "A model is a formal representation of our beliefs, assumptions, and simplifications surrounding some event or process. Let\u2019s look at a couple of examples to make this idea clear.", "We\u2019d like to build a model for flipping a specific coin. What do we know?", "Let\u2019s take a first stab at writing down a model without simplifications:", "The real world can be complicated. Sometimes, a simplified model can do just as well or better. Let\u2019s make a simplified model:", "Our simplified model only has a single parameter! In part one, we learned that we can estimate this parameter by simply flipping the coin a few times and counting the number of heads we get. Fitting the complicated model would require many more flips and difficult calculations. So which model is right? My favorite quote in all of statistics is from George Box:", "\u201cAll models are wrong, but some are useful.\u201d", "Usefulness is the key metric when designing models. In this case, I\u2019d rather use the simplified model even though I know it\u2019s wrong. How do I know it\u2019s wrong? I\u2019ve assigned 0 probability to the coin landing on its edge. I\u2019ve never seen this happen in real life, so I\u2019ve made the simplifying assumption that it can\u2019t occur.", "In this example, we\u2019ll work with the above figure. We\u2019d like to build a model of the data in order to predict future values of y given x. The data almost looks like a line, so let\u2019s start with that as our model.", "If the underlying relationship is actually linear, how do we account for the deviations we\u2019re observing? Imagine we\u2019re using a sensor to collect this data. Most sensors have some amount of error in their measurements. Similarly, we can think of the deviations from our model as being caused by an error prone sensor. It\u2019s very common to model the error as being drawn from a Gaussian distribution with mean zero and variance \u03c3\u00b2. A mean of zero distributes the error equally on both sides of the line. The larger the variance, the larger the deviations. Let\u2019s add a Gaussian noise term to our model. The resulting model has three parameters: the slope, the intercept, and the variance of the Gaussian.", "In both examples, we wrote our model down in terms of some parameters. In general, we might have any number of parameters, so let\u2019s refer to the entire collection of them as \u03b8 (theta). How do we know what the values of \u03b8 should be? This is where likelihood comes into play. In part one, we talked about the likelihood of a continuous random variable taking on a specific value. We got this likelihood from the probability density function (PDF) for the distribution with the parameters fixed at some value. Instead of having fixed parameters, let\u2019s think of the likelihood as a function of \u03b8 given the data we\u2019ve observed. For a continuous distribution with PDF f(x|\u03b8), the likelihood function becomes:", "Similarly, for a discrete distribution with probability mass function (PMF) P(x|\u03b8), the likelihood function becomes:", "We normally have more than a single data point to inform our decisions. How do we calculate the likelihood with respect to an entire collection of data X? Let\u2019s look at the typical situation, where the data is independent and identically distributed (iid). Identically distributed means that each data point comes from the same distribution with the same parameters. Independence was covered in part one, and means that for any two data points x and y, P(x, y)=P(x)P(y). Coin flips are a good example of iid data. Every flip uses the same coin, and the outcome of a flip is independent of the flips before it. For iid X, we rewrite the likelihood function as:", "Notice the product operator in the likelihood function. Often times, individual likelihoods are very small numbers. Taking the product of small numbers creates smaller numbers, which computers can struggle to represent with finite precision. To alleviate these numerical issues (and for other conveniences mentioned later), we often work with the log of the likelihood function, aptly named the log-likelihood. Why does taking the log help? The product rule of logarithms says that log(x\u22c5y) = log(x) + log(y). So taking the log turns our product into a summation! The log-likelihood can then be written as:", "Logarithms are also monotone, which means that larger inputs produce larger outputs. Therefore, the maximum of the log-likelihood function will occur at the same location as the maximum for the likelihood function. Many probability distributions are written in the form of an exponential or contain exponents. The log and the exponential cancel each other out, and logs allow us to turn exponents into products. Both of these leave us with much simpler equations to maximize.", "You\u2019ve probably already put the pieces together, but let\u2019s revisit our goal once more. We can write down a model for our data in terms of probability distributions. Next, we can write down a function over the parameters of our model which outputs the likelihood (or log-likelihood) that those parameters generated our data. The purpose of MLE is to find the maximum of that function, i.e. the parameters which are most likely to have produced the observed data.", "Imagine we have some data generated from a Gaussian distribution with a variance of 4, but we don\u2019t know the mean. I like to think of MLE as taking the Gaussian, sliding it over all possible means, and choosing the mean which causes the model to fit the data best. I\u2019ve created a visualization of this process below, where we see the maximum of the log-likelihood occur at a mean of 2. In fact, that is the true mean of the distribution which created the histogram!", "How do we do this process mathematically? If you\u2019re familiar with calculus, you\u2019ll know that you can find the maximum of a function by taking its derivative and setting it equal to 0. The derivative of a function represents the rate of change of the original function. If you look at the log-likelihood curve above, we see that initially it\u2019s changing in the positive direction (moving up). It reaches a peak, and then it starts changing in a negative direction (moving down). The key is that at the peak, the rate of change is 0. So if we know the functional form of the derivative, we can set it equal to 0 and solve for the best parameters.", "Let\u2019s derive the MLE estimator for our coin flip model from before. I\u2019ll cover the MLE estimator for our linear model in a later post on linear regression.", "Recall that we\u2019re modeling the outcome of a coin flip by a Bernoulli distribution, where the parameter p represents the probability of getting a heads. First, let\u2019s write down the likelihood function for a single flip:", "I\u2019ve written the probability mass function of the Bernoulli distribution in a mathematically convenient way. Take a second to verify for yourself that when x=1 (heads), the probability is p, and when x=0 (tails), the probability is (1-p).", "Now, let\u2019s assume we see the following sequence of flips:", "X = heads, heads, tails, heads, tails, tails, tails, heads, tails, tails.", "Since the coin flips are iid, we can write the likelihood of seeing a particular sequence as the product of each individual flip:", "Plugging in our data, we get:", "Notice, that for every heads we get a factor of p, and for every tails a factor of (1-p). Let\u2019s generalize this to n coin flips with h heads:", "We want to find the p that maximizes this function. To make our job easier let\u2019s take the log of both sides. This will bring the exponents down, and will turn the product into a sum. Taking the derivative of sums is easier than products (another convenience of log-likelihood). Remember, we can do this because the p that maximizes the log-likelihood is the same as the p that maximizes the likelihood. Our log-likelihood is:", "To find the maximum we\u2019re going to take the derivative of this function with respect to p. If you\u2019re not comfortable with calculus, the important thing is that you know the derivative is the rate of change of the function. In this case, the derivative is:", "We set the derivative equal to 0 to find the maximum of the function (where the rate of change is 0). Setting the above equation equal to 0 and solving for p (try doing this yourself) gives us:", "It turns out that the Maximum Likelihood Estimate for our coin is simply the number of heads divided by the number of flips! This makes perfect intuitive sense, if you flipped a fair coin (p = 0.5) 100 times, you\u2019d expect to get about 50 heads and 50 tails.", "Maximum Likelihood Estimation is a powerful technique for fitting our models to data. The solutions provided by MLE are often very intuitive, but they\u2019re completely data driven. This means, that the more data we have, the more accurate our solutions become and vice versa. In a future post, we\u2019ll look at methods for including our prior beliefs about a model, which will help us in low data situations.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F984af2dcfcac&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@william.fleshman?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&user=William+Fleshman&userId=797753643492&source=post_page-797753643492----984af2dcfcac---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/probability-fundamentals-of-machine-learning-part-1-a156b4703e69", "anchor_text": "Probability \u2014 Fundamentals of Machine Learning (Part 1)An overview of the concepts of Probability Theory needed for Machine Learningtowardsdatascience.com"}, {"url": "https://unsplash.com/@ryanthomasang?utm_source=medium&utm_medium=referral", "anchor_text": "Ryan Thomas Ang"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/fundamentals-of-machine-learning-part-3-b305933f00cd", "anchor_text": "Fundamentals of Machine Learning (Part 3)Information Theorytowardsdatascience.com"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----984af2dcfcac---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/statistics?source=post_page-----984af2dcfcac---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/data-science?source=post_page-----984af2dcfcac---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/probability?source=post_page-----984af2dcfcac---------------probability-----------------", "anchor_text": "Probability"}, {"url": "https://medium.com/tag/william-fleshman?source=post_page-----984af2dcfcac---------------william_fleshman-----------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&user=William+Fleshman&userId=797753643492&source=-----984af2dcfcac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&user=William+Fleshman&userId=797753643492&source=-----984af2dcfcac---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F984af2dcfcac&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----984af2dcfcac---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----984af2dcfcac--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----984af2dcfcac--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----984af2dcfcac--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@william.fleshman?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "William Fleshman"}, {"url": "https://medium.com/@william.fleshman/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "865 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F797753643492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&user=William+Fleshman&userId=797753643492&source=post_page-797753643492--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbf0fad40b9b5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmaximum-likelihood-estimation-984af2dcfcac&newsletterV3=797753643492&newsletterV3Id=bf0fad40b9b5&user=William+Fleshman&userId=797753643492&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}