{"url": "https://towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628", "time": 1682997455.696065, "path": "towardsdatascience.com/10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628/", "webpage": {"metadata": {"title": "10 Lessons I Learned Training GANs for one Year | by Marco Pasini | Towards Data Science", "h1": "10 Lessons I Learned Training GANs for one Year", "description": "A year ago I decided to begin my journey into the world of Generative Adversarial Networks, or GANs. I\u2019ve always been intrigued by them since the beginning of my interest in Deep Learning, mainly for\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/abs/1811.09567", "anchor_text": "recent paper", "paragraph_index": 14}, {"url": "https://arxiv.org/abs/1903.06048", "anchor_text": "the paper", "paragraph_index": 27}, {"url": "https://arxiv.org/abs/1706.08500", "anchor_text": "paper", "paragraph_index": 28}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "this paper", "paragraph_index": 31}], "all_paragraphs": ["A year ago I decided to begin my journey into the world of Generative Adversarial Networks, or GANs. I\u2019ve always been intrigued by them since the beginning of my interest in Deep Learning, mainly for the incredible results that they could produce. When I think of the term Artificial Intelligence, GAN is one of the first words that come to my mind.", "But only when I started training them for the first time I discovered the double face of this interesting kind of algorithm: it is incredibly difficult to train. And yes, I knew that before trying myself from papers and other people that tried before me, but I\u2019ve always thought that they were exaggerating an otherwise small and easy to overcome problem.", "As soon as I tried to generate something different from the traditional MNIST example I found out the huge instability problem that affects GANs and, as the hours spent trying to find a solution increase, becomes extremely annoying.", "Now, after spending countless days researching known solutions and trying to come up with new ones, I can finally say I have at least more control over the stability of convergence in my GAN projects, and so can you. I\u2019m not promising you a 10 minute solution to achieve perfect convergence (or in game theory words, Nash Equilibrium) in each one of your projects, but I would love to give you some tips and techniques you can follow to make your GAN journey a bit easier, less time-consuming and above all, less annoying.", "Since the birth of Generative Adversarial Networks and consequently their stability problems, a lot of research has been conducted. Nowadays we have a large number of papers proposing methods to stabilize convergence, with long and difficult mathematical proofs besides them. Moreover, some practical techniques and heuristics surfaced the Deep Learning world: I noticed that often these unproven tricks with no mathematical thinking behind are surprisingly very effective and must not be discarded.", "With improvements in stability, major leaps in generated image realism also came. You can just look at the results from StyleGAN from Nvidia and from BigGAN from Google to really understand how far GANs have come.", "Having read and tried many of these techniques from both literature and practicioners, I have compiled a list of tips you should consider and also not consider when training your GANs, hoping to further your perspective even a little bit on this complex and sometimes tedious subject.", "As soon as I started my first independent GAN project, I noticed that at some early point in the training process the Discriminator adversarial loss always went to zero, while the Generator loss was really high. I immediately concluded that one network had not enough \u201ccapacity\u201d (or number of parameters) to keep up with the other one: so I rushed to change the Architecture of the Generator to add some more filters to the convolutional layers, but to my surprise, nothing changed.", "Having explored the effect of changes in capacity of the networks on stability in training a little more, I couldn\u2019t find any obvious correlation. There surely is some kind of connection, but it is not as important as you may think when you are starting out.", "So if you find yourself with an unbalanced training process and you don\u2019t have any network that is obviously surpassing the other in term of capacity, I wouldn\u2019t suggest you adding or removing filters as the primary solution.", "Naturally, if you are extremely unsure about your networks capacity, you can check some online examples of architectures used for a similar case scenario as yours.", "Another frequent mistake that you may encounter in GANs training is to stop the training as soon as you see the Generator or Discriminator loss increasing or decreasing abruptly. I\u2019ve done it myself countless times: immediately after seeing the loss going up, I thought that the entire training was ruined and the blame was on some not-perfectly-tuned hyperparameter.", "Only later I realized that often the losses go up or down almost randomly, and there is nothing wrong with that. I achieved some great and realistic results while the generator loss was way higher than the discriminator loss, which is perfectly normal. Thus, when you encounter a sudden instability in your training process, I recommend leaving the training going for a bit more, keeping an eye on the quality of the generated images during training, as a visual understanding is often more meaningful than some loss numbers.", "When faced with the selection of the loss function used to train our GAN with, which one should we choose?", "This question was tackled by a recent paper, in which all the different loss functions were benchmarked and compared: some extremely interesting results emerged. Apparently it doesn\u2019t really matter which loss function is chosen: no function absolutely prevailed over the others, and the GAN was able to learn in every different case.", "Thus, my recommendation would be to start off with the simplest loss function for you, leaving a more specific and \u201cstate of the art\u201d option as a possible last step, as we know from literature that it is very possible that you could end up with a worse result.", "In a number of GAN papers, especially some early ones, it\u2019s not rare to read in the implementation section that the authors used a double or triple update of the generator for each update of the discriminator.", "In my first attempts I\u2019ve noticed that in the case of unbalanced training, the discriminator was nearly every time the network surpassing the other (having the loss heavily decreasing). Thus, reading that even the authors of famous papers had a similar problem and implemented an incredibly easy solution to overcome it, gave me confidence that what I was doing was right.", "Unfortunately, balancing training through different network weights updates is a very shortsighted solution in my opinion. Almost never changing how often the generator must update its weights ended up as the final solution to stabilize my training: it sometimes could postpone the \u201cinstability\u201d but could never solve it until convergence. When I noticed the un-effectiveness of this technique, I even tried to make it more dynamic, changing the weight update schedule based on the current state of the two network losses; only later I found out that I wasn\u2019t the only one trying to go up that route, and as many others, I wasn\u2019t succeeding in overcoming instabilities.", "I have only later understood that other techniques, later explained in the article, have a far greater effect on improving training stability.", "If you are dealing with GANs, you will know for sure what Mode Collapse is. It consists in the generator \u201ccollapsing\u201d and always generating a single image for every possible latent vector fed as input. It is a fairly common obstacle in GAN training, and in some cases it can become quite annoying.", "The most straight forward solution that I would recommend if you find yourself in this situation is to try and tune the learning rate of the GAN, as in my personal experiences I could always overcome this obstacle changing this particular hyperparameter. As a rule of thumb, when dealing with Mode Collapse, try and use a lower learning rate and restart the training from scratch.", "The learning rate is one of the most important hyperparameter, if not the most, as even small changes of it can lead to radical changes during training. Usually you can allow higher learning rates when using larger batch sizes, but being on the conservative side was almost always a safe choice in my experience.", "There are other ways to combat Mode Collapse, such as Feature Matching and Minibatch Discrimination that I never ended up implementing in my own code, as I have always found another way to avoid this particular hassle, but feel free to give them a little attention if needed.", "It is well know that making the training of the discriminator more difficult is beneficial for the overall stability. One of the most known methods to increase the complexity of the discriminator training is adding noise to both the real and synthetic data (for example the images generated by the generator); in the mathematical world this should work because it helps giving some stability to the data distributions of the two competing networks. It really is a simple solution that I recommend trying as it can work quite well in practice (even if it does not magically solve any instability problem that you may encounter), while requiring minimal effort to set up. With that said, I started using this technique but ended up ditching it after some time, preferring some other and in my opinion more effective techniques.", "Another method to achieve the same goal is label smoothing, which is even more easy and straight forward to understand and implement: if the label set for real images is 1, we change it to a lower value, like 0.9. This solution discourages the discriminator from being overconfident about its classification, or in other words from relying on a very limited set of features to determine if an image is real or fake. I completely endorse this little trick as it has shown to work very well in practice and it only requires to change one or two characters in your code.", "When working with images that are not too small (like the ones in MNIST) you must look at Multi Scale Gradient. It is a special GAN implementation that makes the gradient flow from the discriminator to the generator thanks to multiple skip connections between the two networks, similarly to what happens in a U-Net used traditionally for semantic segmentation.", "The authors of the Multi Scale Gradient paper were able to train a GAN to directly generate High Definition 1024x1024 images without any particular obstacle (Mode Collapse etc\u2026), while before it was only possible with progressively growing GANs (ProGAN by Nvidia). I have implemented it myself for my projects and I noticed a more stable training and convincing results. Check the paper for more details and try it out!", "When I say Two Time-Scale Update Rule, or TTUR, you may think that I reference a complex and articulated technique employed in GAN training, and you would be totally wrong. It only consists in choosing different learning rates for the generator and discriminator and that\u2019s it. In the paper where TTUR was first introduced, the authors provided a mathematical proof of convergence to Nash equilibrium and showed that implementing famous GANs (DCGAN, WGAN-GP) using different learning rates achieved state-of-the-art results.", "But when I say \u2018use different learning rates\u2019, what do I really mean in practice? Generally, I recommend choosing a higher learning rate for the discriminator and a lower one for the generator: in this way the generator has to make smaller steps to fool the discriminator and does not choose fast, not precise and not realistic solutions to win the adversarial game. To give a practical example, I often choose 0.0004 for the discriminator and 0.0001 for the generator, and I found that these values worked well for some of my projects. Keep in mind that you may notice a higher loss for the generator when using TTUR.", "In a number of papers, such as the one presenting SAGAN (or Self Attention GAN), it is shown that Spectral Normalization, a particular kind of normalization applied on the convolutional kernels, can greatly help the stability of the training. First used only in the discriminator, it later showed to be effective if also used in the convolutional layers of the generator, and I can completely endorse this decision!", "I can almost say that the discovery and implementation of Spectral Normalization in my GANs changed the direction of my GAN journey, and I frankly don\u2019t see any reason not to use it yourself: I can almost guarantee it will take you to a dramatically better and more stable training, while letting you focus on other, more fun, aspects of your Deep Learning projects! (Refer to this paper for more details.)", "Plenty of other tricks, more sophisticated techniques and architectures promise to put an end to GANs training problems: in this article I wanted to tell you about the ways I personally found and implemented to overcome the obstacles that I encountered.", "Thus, there is a lot more material to research if you find yourself stuck while knowing about each method and trick presented here. I can only say that after spending countless hours studying and trying out every possible solution to my GAN related problems, I feel much more confident in my projects and I really hope you can do the same.", "Finally, I would love to sincerely thank you for reading and giving your attention to this article, in the hope that you can walk out with something valuable.", "Thank you so much, and have fun in your GAN adventures!", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Studying Energy Engineering. Artificial Intelligence is pretty cool too."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc9071159628&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9071159628--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9071159628--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@marco.pasini?source=post_page-----c9071159628--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marco.pasini?source=post_page-----c9071159628--------------------------------", "anchor_text": "Marco Pasini"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9bed459710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&user=Marco+Pasini&userId=5c9bed459710&source=post_page-5c9bed459710----c9071159628---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/abs/1811.09567", "anchor_text": "recent paper"}, {"url": "https://arxiv.org/abs/1811.09567", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1903.06048", "anchor_text": "the paper"}, {"url": "https://arxiv.org/abs/1706.08500", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1802.05957", "anchor_text": "this paper"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c9071159628---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c9071159628---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c9071159628---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c9071159628---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----c9071159628---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&user=Marco+Pasini&userId=5c9bed459710&source=-----c9071159628---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&user=Marco+Pasini&userId=5c9bed459710&source=-----c9071159628---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c9071159628--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc9071159628&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c9071159628---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c9071159628--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c9071159628--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c9071159628--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c9071159628--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c9071159628--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c9071159628--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c9071159628--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c9071159628--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marco.pasini?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@marco.pasini?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Marco Pasini"}, {"url": "https://medium.com/@marco.pasini/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "342 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5c9bed459710&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&user=Marco+Pasini&userId=5c9bed459710&source=post_page-5c9bed459710--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F4a9556d568a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F10-lessons-i-learned-training-generative-adversarial-networks-gans-for-a-year-c9071159628&newsletterV3=5c9bed459710&newsletterV3Id=4a9556d568a3&user=Marco+Pasini&userId=5c9bed459710&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}