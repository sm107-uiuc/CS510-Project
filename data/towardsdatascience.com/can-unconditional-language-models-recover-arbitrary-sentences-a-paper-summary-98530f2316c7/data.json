{"url": "https://towardsdatascience.com/can-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7", "time": 1683015010.677855, "path": "towardsdatascience.com/can-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7/", "webpage": {"metadata": {"title": "Can Unconditional Language Models Recover Arbitrary Sentences? \u2014 A paper summary | by Devansh Goenka | Towards Data Science", "h1": "Can Unconditional Language Models Recover Arbitrary Sentences? \u2014 A paper summary", "description": "These days BERT, ELMo and Ernie reminds one of pre-trained generative models rather than Sesame Street characters, such has been their hegemony over the Natural Language Processing landscape. These\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1905.07129.pdf", "anchor_text": "Ernie", "paragraph_index": 0}, {"url": "http://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences", "anchor_text": "this", "paragraph_index": 0}, {"url": "https://deepgenerativemodels.github.io/notes/autoregressive/", "anchor_text": "autoregressively", "paragraph_index": 1}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM", "paragraph_index": 3}, {"url": "https://deepai.org/machine-learning-glossary-and-terms/softmax-layer#:~:text=The%20softmax%20function%20is%20a,can%20be%20interpreted%20as%20probabilities.", "anchor_text": "softmax", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "soft-attention", "paragraph_index": 6}, {"url": "https://en.wikipedia.org/wiki/Beam_search#:~:text=In%20computer%20science%2C%20beam%20search,that%20reduces%20its%20memory%20requirements.", "anchor_text": "beam search", "paragraph_index": 9}, {"url": "https://catalog.ldc.upenn.edu/LDC2011T07", "anchor_text": "English Gigaword", "paragraph_index": 10}], "all_paragraphs": ["These days BERT, ELMo and Ernie reminds one of pre-trained generative models rather than Sesame Street characters, such has been their hegemony over the Natural Language Processing landscape. These models can serve as general purpose encoders, and can even perform some tasks like text classification without requiring further modification. However, limited research has been conducted on the reverse-case, exploiting these models for use as general purpose decoders. This article is a summary of this paper by researchers at New York University which tries to ascertain exactly this, whether these models can recover an arbitrary sentence from its encoded representation.", "In order to prove the existence of encoded representations that can be used for recovering a sentence, the paper introduces methods to feed these representations into a recurrent language model trained autoregressively as well as map sentences into and out of this \u201creparametrized\u201d space, while keeping the main language model parameters frozen.", "Before we begin, let us quickly look at recurrent language models and how they can be trained autoregressively.", "Recall that in an autoregressive model, we take as input all the previous tokens, combine it with the previous hidden state and compute the next token. This hidden state is often implemented as a LSTM recurrent network, and the final output is nothing but a softmax function, which indicates the dedicated probability of a particular word being the next token.", "Right, so now that we are aware of the how recurrent language models work, we try to imagine how a particular sentence looks like in the space. If we assume that the hidden state has a dimension \u201cd\u201d (which itself is based on the dimension of the LSTM units), then a sentence can be imagined as a trajectory in this d-dimensional space, as this trajectory evolves after taking into account the previous emitted token.", "With this given trajectory, it can become difficult to recover a particular sentence from this space primarily due to 1) the sentence length not being encoded in the trajectory, and 2) the fact that a single misrepresented token can totally deviate the trajectory. So, the researchers looked to transform this trajectory into a flat vector representation, and they did so by adding an additional hidden bias to the previous and cell state in the network.", "Under the proposed reparameterization, a trajectory of hidden states in the sentence space maps to a vector, (of say dimension \u201cd\u2081\u201d)in the transformed sentence space, and ultimately serves as the vessel to go back and forth in this sentence space. One thing to note is that if this dimension \u201cd\u2081\u201d is lesser than the model dimension, a random matrix projects the bias onto the hidden states. Whereas, if \u201cd\u2081\u201d is greater, then soft-attention is used to downsize the projection to the model dimension (as illustrated below).", "So with the concept in place for going back and forth into this space, the researchers looked to describe the mechanism to do so, which comprised of going forward \u2014 projecting a sentence from the model to a point in the newly described space, and backward \u2014 recovering a sentence from this point in that space.", "For forward estimation, they started with an arbitrary bias that is added to all states as previously explained. They then optimized on this bias (say z) to enhance the probability of this bias producing the exact same sentence, while keeping the language model parameters frozen. This optimization is achieved through any gradient-based algorithm such as nonlinear conjugate descent.", "To go back from this point (represented as z), the same procedure described above is followed, with the only difference being that the optimization is reversed to produce the same sentence sequence, rather than on z. Since backward estimation is not a straightforward procedure, beam search, a heuristic algorithm which detects the most promising sequence, is employed.", "To demonstrate the above hypothesis, a 2-layer neural model with 256, 512 and 1024 LSTM units was constructed for a small, medium and large language model respectively. The models are trained on 50M sentences from the English Gigaword news corpus, yielding a final vocabulary of 20,234 subword tokens.", "For a measure of the quality of recoverability, three approaches are discussed.", "Finally, the dimension of the bias z (represented as d\u2081 above) is set to 128, 256, 512, 1024 \u2026 to 32768 for each language model and the recoverability of 100 sentences is evaluated, with the results as shown below.", "It is observed that recoverability increases as dimension of z increases until it is equal to the model dimension, after which it plateaus. It also observed that recoverability is nearly perfect for the large model, achieving Exact Match \u2265 99, and very high for the medium model, achieving Exact Match \u2265 84. Further, is observed that sentence recovery fails for sentences with length > 30, indicating the limitation that fixed vector representations of longer sequences cannot adequately include all essential information from that sequence.", "To study the recoverability on out-of-domain sentences, the models are tested on TED talk transcripts, and a nearly-perfect BLEU metric is observed on both the medium and large models. This exceptional performance on out-of-domain sentences indicates that the model does not just memorize the examples, and actually learns information about the language to perform this recovery.", "Thus, the methodology described above demonstrates that there can exist a particular encoded representation from which we can recover an arbitrary sentence. The low recoverability for the small model and near-perfect recoverability for the large model indicates the requirement of a fairly adequate model to achieve high recoverability.", "With degrading recoverability for increasing sentence length, an overall difficulty to recover longer sentences is also observed. However, from the high recoverability of even out-of-domain sentences, it can be surmised that with a further increase in model size, the degradation point can be stretched out.", "Finally, the implicit sentence space of the model can be regularized, whereby increasing the recoverability as well as the applicability of an unconditional language model as a general-purpose decoder.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Full-time Software Engineer, Machine Learning enthusiast and foodie."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F98530f2316c7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://devanshgoenka97.medium.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": ""}, {"url": "https://devanshgoenka97.medium.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Devansh Goenka"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F501c107ab746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&user=Devansh+Goenka&userId=501c107ab746&source=post_page-501c107ab746----98530f2316c7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://arxiv.org/pdf/1810.04805.pdf", "anchor_text": "BERT"}, {"url": "https://arxiv.org/pdf/1802.05365.pdf", "anchor_text": "ELMo"}, {"url": "https://arxiv.org/pdf/1905.07129.pdf", "anchor_text": "Ernie"}, {"url": "http://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences", "anchor_text": "this"}, {"url": "https://deepgenerativemodels.github.io/notes/autoregressive/", "anchor_text": "autoregressively"}, {"url": "https://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences.pdf", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Long_short-term_memory", "anchor_text": "LSTM"}, {"url": "https://deepai.org/machine-learning-glossary-and-terms/softmax-layer#:~:text=The%20softmax%20function%20is%20a,can%20be%20interpreted%20as%20probabilities.", "anchor_text": "softmax"}, {"url": "https://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/abs/1409.0473", "anchor_text": "soft-attention"}, {"url": "https://en.wikipedia.org/wiki/Beam_search#:~:text=In%20computer%20science%2C%20beam%20search,that%20reduces%20its%20memory%20requirements.", "anchor_text": "beam search"}, {"url": "https://catalog.ldc.upenn.edu/LDC2011T07", "anchor_text": "English Gigaword"}, {"url": "https://www.aclweb.org/anthology/P02-1040.pdf", "anchor_text": "BLEU"}, {"url": "https://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences.pdf", "anchor_text": "paper"}, {"url": "https://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences.pdf", "anchor_text": "paper"}, {"url": "http://papers.nips.cc/paper/9661-can-unconditional-language-models-recover-arbitrary-sentences", "anchor_text": "N Subramani, S Bowman, K Cho (2019). Can Unconditional Language Models Recover Arbitrary Sentences? Advances in Neural Information Processing Systems 32 (NIPS 2019)"}, {"url": "https://medium.com/tag/data-science?source=post_page-----98530f2316c7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----98530f2316c7---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----98530f2316c7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----98530f2316c7---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&user=Devansh+Goenka&userId=501c107ab746&source=-----98530f2316c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&user=Devansh+Goenka&userId=501c107ab746&source=-----98530f2316c7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F98530f2316c7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----98530f2316c7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----98530f2316c7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----98530f2316c7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----98530f2316c7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----98530f2316c7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----98530f2316c7--------------------------------", "anchor_text": ""}, {"url": "https://devanshgoenka97.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://devanshgoenka97.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Devansh Goenka"}, {"url": "https://devanshgoenka97.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "14 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F501c107ab746&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&user=Devansh+Goenka&userId=501c107ab746&source=post_page-501c107ab746--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F501c107ab746%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcan-unconditional-language-models-recover-arbitrary-sentences-a-paper-summary-98530f2316c7&user=Devansh+Goenka&userId=501c107ab746&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}