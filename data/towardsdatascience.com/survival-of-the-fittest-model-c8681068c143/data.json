{"url": "https://towardsdatascience.com/survival-of-the-fittest-model-c8681068c143", "time": 1682997404.4279099, "path": "towardsdatascience.com/survival-of-the-fittest-model-c8681068c143/", "webpage": {"metadata": {"title": "Survival of the Fittest\u2026Model. An Evolutionary View of Classification | by Dan Lans | Towards Data Science", "h1": "Survival of the Fittest\u2026Model", "description": "Only the best-fit model will survive! This has been a major task for data scientists and statisticians for many years \u2014 figuring out a way to capture the predictive essence within data. Granted, one\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/dslans/model-evo", "anchor_text": "See my github repo for the full python code used for this simulation.", "paragraph_index": 25}], "all_paragraphs": ["Only the best-fit model will survive! This has been a major task for data scientists and statisticians for many years \u2014 figuring out a way to capture the predictive essence within data. Granted, one could argue that the best predictive models are a result of excellent experimental design, data collection, and feature engineering, but the advances in machine learning concepts and algorithms have also contributed to our ability to make better predictions.", "It\u2019s interesting to view the arsenal of predictive methods at our disposal in an evolutionary sense. Advancements in the predictive modeling space have made the algorithms smarter, more efficient, and adaptable to more scenarios. This post isn\u2019t meant to be a thorough dissection of each method, but rather to show that studying the advancements in the modeling space is a fantastic way to understand the mechanics of each method. At the end, I\u2019ll perform a small bake-off to compare the performance of each model.", "In my evolutionary hierarchy of predictive models, I don\u2019t cover every single method out there \u2014that would take too long. Rather, I\u2019ve selected a few very popular predictive modeling techniques that best illustrate an \u201cevolution\u201d of predicting outcomes.", "Let\u2019s start off with the good old coin flip! Who can\u2019t say they\u2019ve never made a decision by the flip of a coin? While not a predictive model in the traditional sense, a coin flip can still be used as a decision process. We all know that a coin has a 50/50 chance of ending up heads or tails, and therefore, a 50% probability of assigning classes as 0 or 1 in binary cases.", "The class is purely determined by random chance, so not really a great classifier. Think of this as the beginning of our evolutionary chain \u2014 our paramecium \u2014 and the baseline for which the other, more advanced species of models will improve upon.", "In the first stage of evolution comes a mathematical approach, and the de-facto statistical approach for solving classification problems \u2014 logistic regression. It\u2019s possible to build an equation using a linear combination of covariates to predict the response, which is the log-odds of the outcome. The logit function is able to fit a regression line that best describes the variance in the log-odds of the outcome.", "Logistic regression is a very stable and efficient method with easy-to-interpret output which makes it a valid go-to model to use for classification problems. The biggest limitations of logistic regression come when you face larger amounts of data \u2014 it will be more difficult to specify all the possible interactions and capture more complex relationships (although, not impossible), and it may run into convergence issues when trying to estimate the parameters of the model. Also, it\u2019s still a parametric model, so the data must meet certain assumptions for logistic regression to be valid.", "Therefore, in the next stage of evolution, we need to find a way to methodically handle cases where we can easily capture nonlinear relationships within the data, but still achieve a high level of classification accuracy.", "With Classification and Regression Trees (CART), we start to get away from statistics and venture into the world of machine learning. A decision tree is built to classify the outcomes by recursively partitioning the predictor space. CART greedily searches for variable splits that will minimize a loss function, which, in the case of classification, is a purity metric called the Gini Index. This measures how pure the nodes (or leaves) of the tree are for each outcome class.", "This can easily be visualized by creating a quick model with the ever-so-popular iris dataset.", "As you can see, the decision tree is really just a bunch of \u201cif-then\u201d statements. Notice how the root node starts out with the balanced group of data and with each subsequent split, the decision rules are made such that it maximizes the homogeneity of the ensuing nodes. We are left with terminal nodes that are made up of 100% setosa, 91% versicolor, and 98% virginica.", "Decision tree algorithms are great for data consisting of more complex relationships. It\u2019s a non-parametric, non-linear method of modeling data, and is very easy to implement. The biggest shortcoming of CART is that it has a tendency to overfit the data, which could cause the model to perform poorly on unseen data. Pruning methods (i.e. setting the maximum depth of the tree) can help the model to be more generalizable, but we can also feed our model some rare candy and watch it evolve into the next stage of models: ensembles.", "Imagine there were 10, 100, or even 1000 trees available at our disposal to make predictions. Combining multiple prediction tools like this into a predictive model is called an ensemble method. Multiple models are produced, and the class predictions are averaged across the ensemble through a method called bootstrap aggregating, or \u201cbagging.\u201d", "This means that each tree has to have some sort of random process within its assembly in order to contribute something different to the predictive process \u2014 it would do no good if each tree were identical! This is achieved by sampling the training data with replacement. That way, each tree is different because they are built by sampling different parts of the dataset.", "The trees are very deep and designed to overfit the data in order to learn more complex relationships, but each tree is different because they are built by sampling different parts of the dataset. If you think about the bias-variance tradeoff, each tree is able to greatly reduce bias, but at the cost of creating a larger amount of variance among the estimators. Averaging the results of these estimators then helps reduce the variance.", "A random forest takes the idea of bagging and enhances the algorithm by randomly selecting a subset of variables at each tree split in addition to sampling the training data. This helps make the trees within the ensemble less alike and allows different variables to influence the outcome, thereby reducing the variance even further.", "So this is great! We\u2019re able to reduce the problem of overfitting in CART by bagging multiple trees. But what if we didn\u2019t want to overfit the estimators? Is there a way we could construct the ensemble in a different way?", "Through evolution, species undergo changes in their genetic makeup. In this next stage, a tree ensemble method is still used, only this time, the way the trees are engineered has changed. Remember when I said back in CART when we could prune the tree to get a more generalizable estimator? Boosting algorithms use the idea that an ensemble of weak learners (very small trees or \u201cdecision stumps\u201d) can perform better than deep learners like in random forest.", "The trees don\u2019t only differ in size, but also in how they interact with one another. With bagging, the individual classifiers can be built in parallel and averaged at the end. In boosting, the classifiers are built sequentially, with each subsequent tree improving the errors produced by its predecessor. That way, there is a systematic way to improve where the previous model struggled and minimize error when all the models are combined in the overall classifier.", "There are a few popular boosting methods, namely adaptive boosting and gradient boosting. If ensembles were a species, think of boosting as one type of genus, followed by these separate families of classifiers. They all share the same intuition as described above, but take different approaches to minimize modeling error. Briefly put: In adaptive boosting, observations that are misclassified by one model get weighted heavier for the next model and the data is trained on the re-weighted data set. In gradient boosting, instead of fitting subsequent models on re-weighted training data, the errors of the previous classifier are modeled directly with the goal of minimizing a loss function. Think of it this way \u2014 if an additional classifier is able to correct some of the errors of the previous model, that classifier\u2019s error will be less than the previous model\u2019s. At the end of the ensemble, error should be minimized.", "Now we have come to the final stage of evolution: the neural network. For lack of a better transition into this type of model, imagine an alien species landed into our predictive model space and starting feeding off data. The more data it eats up, the more it is able to learn, and the stronger it becomes.", "Let\u2019s say you want to use a set of variables to predict an outcome. These are called \u201cinputs\u201d and \u201coutputs,\u201d respectively, in the neural network space. The inputs travel through the network, passing through different layers of nodes, called hidden layers, until they arrive at the end of the network at the output layer.", "Each node in subsequent layers receives a \u201csignal\u201d from all the nodes in the previous layer. For example, each value (or signal) from the input layer gets sent to all the nodes in the first hidden layer, and every connection gets assigned a weight (not unlike a coefficient in a regression equation). An activation function / non-linear transformation is applied to the sum of the weighted values to give the following node (or neuron) a binary decision on whether to activate or not. Another term, called a bias, is added to the sum of the weighted values before the activation function is applied and acts sort of like an intercept in regression, controlling how large the sum of weights needs to be before the neuron becomes active.", "This process ultimately determines the strength of a node in the output layer, where the strongest output node becomes the assigned class decided by the model. The neural network then measures errors in the response and recursively \u201cback-propagates\u201d the errors to readjust the weights at each preceding node, and in essence, \u201clearns\u201d the best weights needed to more accurately signal the correct outcome.", "So, with a huge amount of training data, a neural network can learn useful patterns to classify outcomes. This is especially useful for analyzing very high-dimensional data as you would be doing with image recognition. A small 64x64 image would have 4,096 features alone to add to the input layer of a neural network.", "I\u2019ve presented a hierarchy of models here, but whichever one will be the \u201cfittest\u201d will depend largely on your data. Assuming we only care about predictive accuracy, I designed a small simulation to determine the survival of the fittest model. See my github repo for the full python code used for this simulation.", "In the first round, I simulated a dataset with 10,000 observations and 50 variables \u2014 including a handful of informative features, a couple of redundant features to introduce some collinearity, and a large group of useless variables to test the models\u2019 abilities to use the informative features. The data is split into 2 classes and I introduced a little bit of noise in the data to make the problem a little more difficult.", "For the first part of the simulation, I just wanted to get an idea of how each method handled larger amounts of data. I plotted accuracies in each model\u2019s ability to predict the test set of data based on the number of training samples available.", "Logistic regression was very stable, but probably failed to pick up any of the nonlinear features of the data without further tuning. CART did slightly better in most situations, but varies quite a bit and could not reach the accuracy obtained by the ensemble models. Random Forest and XGBoost performed very similarly with this data, and the neural net caught up with these models, but only when lots more training data was available.", "For the bake-off, I created 1,000 different random data sets similar to the one used in round 1, but with varying amounts of rows and columns, and compared point estimates of model accuracy among the different classifiers. There wasn\u2019t a lot of complexity added to the data, so each method scored pretty good, on average.", "I used all these models with minimum tuning, which isn\u2019t the best representation of the abilities of each method, but it made for easy comparison for the purposes of this post.", "While I presented an \u201cevolution\u201d of models and titled this post, \u201cSurvival of the Fittest,\u201d none of the models here are in danger of going extinct. Depending on the goals of the analysis (prediction vs inference) and nature of the data, one model may be better suited than others. Hopefully, through this post, I was able to illustrate the benefits of each method and how they build upon each other to tackle predictions.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A statistician tackling world of data science one blog at a time"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fc8681068c143&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----c8681068c143--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@dslans?source=post_page-----c8681068c143--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslans?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Dan Lans"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24b0d0cc772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&user=Dan+Lans&userId=24b0d0cc772e&source=post_page-24b0d0cc772e----c8681068c143---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://github.com/dslans/model-evo", "anchor_text": "See my github repo for the full python code used for this simulation."}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c8681068c143---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/predictive-modeling?source=post_page-----c8681068c143---------------predictive_modeling-----------------", "anchor_text": "Predictive Modeling"}, {"url": "https://medium.com/tag/data-science?source=post_page-----c8681068c143---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/classification?source=post_page-----c8681068c143---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&user=Dan+Lans&userId=24b0d0cc772e&source=-----c8681068c143---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&user=Dan+Lans&userId=24b0d0cc772e&source=-----c8681068c143---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----c8681068c143--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fc8681068c143&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----c8681068c143---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----c8681068c143--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c8681068c143--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c8681068c143--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----c8681068c143--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----c8681068c143--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslans?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@dslans?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Dan Lans"}, {"url": "https://medium.com/@dslans/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "17 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F24b0d0cc772e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&user=Dan+Lans&userId=24b0d0cc772e&source=post_page-24b0d0cc772e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F24b0d0cc772e%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsurvival-of-the-fittest-model-c8681068c143&user=Dan+Lans&userId=24b0d0cc772e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}