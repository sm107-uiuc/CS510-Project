{"url": "https://towardsdatascience.com/bart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c", "time": 1683011989.125172, "path": "towardsdatascience.com/bart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c/", "webpage": {"metadata": {"title": "BART for Paraphrasing with Simple Transformers | by Thilina Rajapakse | Towards Data Science", "h1": "BART for Paraphrasing with Simple Transformers", "description": "Don\u2019t worry if that sounds a little complicated; we are going to break it down and see what it all means. To add a little bit of background before we dive into BART, it\u2019s time for the now-customary\u2026"}, "outgoing_paragraph_urls": [{"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "paragraph_index": 1}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Transformer model", "paragraph_index": 3}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "BART paper", "paragraph_index": 3}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers", "paragraph_index": 16}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers", "paragraph_index": 16}, {"url": "https://www.anaconda.com/distribution", "anchor_text": "here", "paragraph_index": 17}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/seq2seq/paraphrasing", "anchor_text": "here", "paragraph_index": 25}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "here", "paragraph_index": 34}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/", "paragraph_index": 43}], "all_paragraphs": ["BART is a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.", "- BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension -", "Don\u2019t worry if that sounds a little complicated; we are going to break it down and see what it all means. To add a little bit of background before we dive into BART, it\u2019s time for the now-customary ode to Transfer Learning with self-supervised models. It\u2019s been said many times over the past couple of years, but Transformers really have achieved incredible success in a wide variety of Natural Language Processing (NLP) tasks.", "BART uses a standard Transformer architecture (Encoder-Decoder) like the original Transformer model used for neural machine translation but also incorporates some changes from BERT (only uses the encoder) and GPT (only uses the decoder). You can refer to the 2.1 Architecture section of the BART paper for more details.", "BART is pre-trained by minimizing the cross-entropy loss between the decoder output and the original sequence.", "MLM models such as BERT are pre-trained to predict masked tokens. This process can be broken down as follows:", "Importantly, BERT models can \u201csee\u201d the full input sequence (with some tokens replaced with [MASK]) when attempting to predict the original tokens. This makes BERT a bidirectional model, i.e. it can \u201csee\u201d the tokens before and after the masked tokens.", "This is suited for tasks like classification where you can use information from the full sequence to perform the prediction. However, it is less suited for text generation tasks where the prediction depends only on the previous words.", "Models used for text generation, such as GPT2, are pre-trained to predict the next token given the previous sequence of tokens. This pre-training objective results in models that are well-suited for text generation, but not for tasks like classification.", "BART has both an encoder (like BERT) and a decoder (like GPT), essentially getting the best of both worlds.", "The encoder uses a denoising objective similar to BERT while the decoder attempts to reproduce the original sequence (autoencoder), token by token, using the previous (uncorrupted) tokens and the output from the encoder.", "A significant advantage of this setup is the unlimited flexibility of choosing the corruption scheme; including changing the length of the original input. Or, in fancier terms, the text can be corrupted with an arbitrary noising function.", "The corruption schemes used in the paper are summarized below.", "The authors note that training BART with text infilling yields the most consistently strong performance across many tasks.", "For the task we are interested in, namely paraphrasing, the pre-trained BART model can be fine-tuned directly using the input sequence (original phrase) and the target sequence (paraphrased sentence) as a Sequence-to-Sequence model.", "This also works for tasks like summarization and abstractive question answering.", "We will use the Simple Transformers library, based on the Hugging Face Transformers library, to train the models.", "1. Install Anaconda or Miniconda Package Manager from here.", "2. Create a new virtual environment and install packages.", "We will be combining three datasets to serve as training data for our BART Paraphrasing Model.", "The bash script below can be used to easily download and prep the first two datasets, but the MSRP dataset has to be downloaded manually from the link. (Microsoft hasn\u2019t provided a direct link \ud83d\ude1e )", "Make sure you place the files in the same directory ( data ) to avoid annoyances with file paths in the example code.", "We also have a couple of helper functions, one to load data, and one to clean unnecessary spaces in the training data. Both of these functions are defined in utils.py.", "Some of the data have spaces before punctuation marks that we need to remove. clean_unnecessary_spaces() function is used for this purpose.", "Once the data is prepared, training the model is quite simple.", "Note that you can find all the code in the Simple Transformers examples here.", "First, we import all the necessary stuff and set up logging.", "Then, we set up the model and hyperparameter values. Note that we are using the pre-trained facebook/bart-large model, and fine-tuning it on our own dataset.", "Finally, we\u2019ll generate paraphrases for each of the sentences in the test data.", "This will write the predictions to the predictions directory.", "The hyperparameter values are set to general, sensible values without doing hyperparameter optimization. For this task, the ground truth does not represent the only possible correct answer (nor is it necessarily the best answer). Because of this, tuning the hyperparameters to nudge the generated text as close to the ground truth as possible doesn\u2019t make much sense.", "Our aim is to generate good paraphrased sequences rather than to produce the exact paraphrased sequence from the dataset.", "If you are interested in Hyperparameter Optimization with Simple Transformers (particularly useful with other models/tasks like classification), do check out my guide here.", "The decoding algorithm (and the relevant hyperparameters) used has a considerable impact on the quality and nature of the generated text. The values I\u2019ve chosen (shown below) are generally suited to produce \u201cnatural\u201d text.", "For more information, please refer to the excellent Hugging Face guide here.", "You can use the script below to test the model on any sentence.", "Let\u2019s look at some of the paraphrased sequences generated by the model for the test data. For each input sequence, the model will generate three ( num_return_sequences ) paraphrased sequences.", "As can be seen from these examples, our BART model has learned to generate paraphrases quite well!", "The generated paraphrases can sometimes have minor issues, some of which are listed below.", "Encouragingly, these issues seem to be quite rare and can most likely be averted by using better training data (the same problems can sometimes be seen in the training data ground truth as well).", "Sequence-to-Sequence models like BART are another arrow in the quiver of NLP practitioners. They are particularly useful for tasks involving text generation such as paraphrasing, summarization, and abstractive question answering.", "Paraphrasing can be used for data augmentation where you can create a larger dataset by paraphrasing the available data.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI researcher, avid reader, fantasy and Sci-Fi geek, and fan of the Oxford comma. www.linkedin.com/in/t-rajapakse/"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F7c9ea3dfdd8c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e----7c9ea3dfdd8c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@alex_tsl?utm_source=medium&utm_medium=referral", "anchor_text": "Alexandra"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"}, {"url": "https://arxiv.org/pdf/1706.03762.pdf", "anchor_text": "Transformer model"}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "BART paper"}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "paper"}, {"url": "https://arxiv.org/pdf/1910.13461.pdf", "anchor_text": "paper"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers", "anchor_text": "Simple Transformers"}, {"url": "https://github.com/huggingface/transformers", "anchor_text": "Transformers"}, {"url": "https://www.anaconda.com/distribution", "anchor_text": "here"}, {"url": "https://github.com/google-research-datasets/paws#paws-wiki", "anchor_text": "Google PAWS-Wiki Labeled (Final)"}, {"url": "https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs", "anchor_text": "Quora Question Pairs Dataset"}, {"url": "https://msropendata.com/datasets/e235323f-f23c-4246-b2e6-27d7a654d6cc", "anchor_text": "Microsoft Research Paraphrase Corpus"}, {"url": "https://github.com/ThilinaRajapakse/simpletransformers/tree/master/examples/seq2seq/paraphrasing", "anchor_text": "here"}, {"url": "https://towardsdatascience.com/hyperparameter-optimization-for-optimum-transformer-models-b95a32b70949", "anchor_text": "Hyperparameter Optimization for Optimum Transformer ModelsHow to tune your hyperparameters with Simple Transformers for better Natural Langauge Processing.towardsdatascience.com"}, {"url": "https://huggingface.co/blog/how-to-generate", "anchor_text": "here"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----7c9ea3dfdd8c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----7c9ea3dfdd8c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----7c9ea3dfdd8c---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/nlp?source=post_page-----7c9ea3dfdd8c---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----7c9ea3dfdd8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=-----7c9ea3dfdd8c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F7c9ea3dfdd8c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----7c9ea3dfdd8c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----7c9ea3dfdd8c--------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://chaturangarajapakshe.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Thilina Rajapakse"}, {"url": "https://chaturangarajapakshe.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.6K Followers"}, {"url": "http://www.linkedin.com/in/t-rajapakse/", "anchor_text": "www.linkedin.com/in/t-rajapakse/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6b1e2355088e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&user=Thilina+Rajapakse&userId=6b1e2355088e&source=post_page-6b1e2355088e--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fecf622989264&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbart-for-paraphrasing-with-simple-transformers-7c9ea3dfdd8c&newsletterV3=6b1e2355088e&newsletterV3Id=ecf622989264&user=Thilina+Rajapakse&userId=6b1e2355088e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}