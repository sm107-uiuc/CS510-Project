{"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-three-optimization-db04ea415507", "time": 1682993522.080771, "path": "towardsdatascience.com/machine-learning-kaggle-competition-part-three-optimization-db04ea415507/", "webpage": {"metadata": {"title": "Machine Learning Kaggle Competition: Part Three Optimization | by Will Koehrsen | Towards Data Science", "h1": "Machine Learning Kaggle Competition: Part Three Optimization", "description": "How best to describe a Kaggle contest? It\u2019s a machine learning education disguised as a competition! Although there are valid criticisms of Kaggle, overall, it\u2019s a great community that provides\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.quora.com/Why-did-Kaggle-start-having-a-bad-reputation", "anchor_text": "valid criticisms", "paragraph_index": 0}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle", "paragraph_index": 0}, {"url": "https://www.kaggle.com/competitions", "anchor_text": "interesting problems", "paragraph_index": 0}, {"url": "https://www.kaggle.com/learn/overview", "anchor_text": "exploring new ideas", "paragraph_index": 0}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/", "anchor_text": "Kaggle Home Credit contest", "paragraph_index": 0}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426", "anchor_text": "Part One: Getting Started", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "Part Two: Improving)", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "first part of this series", "paragraph_index": 4}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "Part two", "paragraph_index": 5}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "As shown in this great paper,", "paragraph_index": 8}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization", "anchor_text": "evolutionary", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Gradient-based_optimization", "anchor_text": "gradient-based", "paragraph_index": 9}, {"url": "http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/", "anchor_text": "Gradient Boosting Machine (GBM) model", "paragraph_index": 10}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api", "anchor_text": "the documentation", "paragraph_index": 10}, {"url": "https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "anchor_text": "In the first notebook", "paragraph_index": 13}, {"url": "https://www.kaggle.com/willkoehrsen/automated-model-tuning", "anchor_text": "this notebook", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this article", "paragraph_index": 24}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "this article", "paragraph_index": 24}, {"url": "https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf", "anchor_text": "basic concept of Bayesian optimization", "paragraph_index": 25}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization", "anchor_text": "aspirations of Bayesian optimization", "paragraph_index": 25}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt", "anchor_text": "the final notebook", "paragraph_index": 26}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt", "anchor_text": "the third notebook", "paragraph_index": 27}, {"url": "http://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf", "anchor_text": "bias/variance", "paragraph_index": 40}, {"url": "https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/", "anchor_text": "learning rate", "paragraph_index": 51}, {"url": "https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features", "anchor_text": "this kernel", "paragraph_index": 54}, {"url": "https://www.kaggle.com/jsaguiar", "anchor_text": "the author", "paragraph_index": 54}, {"url": "https://www.featurelabs.com/blog/secret-to-data-science-success/", "anchor_text": "value of feature engineering", "paragraph_index": 57}, {"url": "https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219", "anchor_text": "automated feature engineering", "paragraph_index": 59}, {"url": "https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics", "anchor_text": "automated feature engineering", "paragraph_index": 59}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will", "paragraph_index": 62}], "all_paragraphs": ["How best to describe a Kaggle contest? It\u2019s a machine learning education disguised as a competition! Although there are valid criticisms of Kaggle, overall, it\u2019s a great community that provides interesting problems, thousands of data scientists willing to share their knowledge, and an ideal environment for exploring new ideas. As evidence of this, I never would have learned about the Gradient Boosting Machine, or, one of the topics of this article, automated model optimization, were it not for the Kaggle Home Credit contest.", "In this article, part three of a series (Part One: Getting Started and Part Two: Improving) documenting my work for this contest, we will focus on a crucial aspect of the machine learning pipeline: model optimization through hyperparameter tuning. In the second article, we decided on the Gradient Boosting Machine as our model of choice, and now we have to get the most out of it through optimization. We\u2019ll do this primarily with two methods: random search and automated tuning with Bayesian optimization.", "All the work presented here is available to run on Kaggle in the following notebooks. The article itself will highlight the key ideas but the code details are all in the notebooks (which are free to run with nothing to install!)", "For this article we will skip the background, so if at any time you feel lost, I encourage you to go to the previous articles and notebooks. All of the notebooks can be run on Kaggle without the need to download anything, so I highly recommend checking them out or competing!", "In the first part of this series, we got familiar with the dataset, performed exploratory data analysis, tried our hand at feature engineering, and built a few baseline models. Our public leaderboard score from this round was 0.678 (which at the moment would place us lower than 4000 on the leaderboard).", "Part two involved in-depth manual feature engineering, followed by feature selection and more modeling. Using the expanded (and then contracted) set of features, we ended up with a score of 0.779, quite an improvement from the baseline, but not quite in the top 50% of competitors. In this article, we will once again better our score and move up 1000 places on the leaderboard.", "Optimization in the context of machine learning means finding the set of model hyperparameter values that yield the highest cross validation score for a given dataset. Model hyperparameters, in contrast to model parameters that are learned during training, are set by the data scientist before training. The number of layers in a deep neural network is a model hyperparameter while the splits in a decision tree are model parameters. I like to think of model hyperparameters as settings that we need to tune for a dataset: the ideal combination of values is different for every problem!", "There are a handful of ways to tune a machine learning model:", "These are presented in order of increasing efficiency, with manual search taking the most time (and often yielding the poorest results) and automated methods converging on the best values the quickest, although, as with many topics in machine learning, this is not always the case! As shown in this great paper, random search does surprisingly well (which we\u2019ll see shortly).", "(There are also other hyperparameter tuning methods such as evolutionary and gradient-based. There are constantly better methods being developed, so make sure to stay up to date on the current best practice!)", "In the second part of this series, we decided to use the Gradient Boosting Machine (GBM) model because of its superior performance on structured data with many features. The GBM is extremely powerful (yet easy to implement in Python), but it has dozens of hyperparameters that significantly affect performance and which must be optimized for a problem. If you want to feel overwhelmed, check out the documentation on the LightGBM library:", "I don\u2019t think there is anyone in the world who can look at all of these and pick the best values! Therefore, we need to implement one of the four methods for choosing the hyperparameters.", "For this problem, I didn\u2019t even try manual tuning, both because this was my first time working with the GBM and because I didn\u2019t want to waste any time. e\u2019ll skip straight to random search and automated techniques using Bayesian optimization.", "In the first notebook, we go through an implementation of grid and random search covering the four parts of an optimization problem:", "These four parts also form the basis of Bayesian optimization, so laying them out here will help when it comes to that implementation. For the code details, refer to the notebooks, but here we\u2019ll briefly touch on each concept.", "The objective function takes in a set of inputs and returns a score that we want to maximize. In this case, the inputs are the model hyperparameters, and the score is the 5-fold cross validation ROC AUC on the training data. The objective function in pseudo-code is:", "The goal of hyperparameter optimization is to find the hyperparameters that return the best value when passed into the objective function. That seems pretty simple, but the problem is evaluating the objective function is very expensive in terms of time and computational resources. We cannot try every combination of hyperparameter values because we have a limited amount of time, hence the need for random search and automated methods.", "The domain is the set of values over which we search. For this problem with a GBM, the domain is as follows:", "We can visualize two of these distributions, the learning rate, which is logarithmic normal, and the number of leaves which is uniform normal:", "Although we don\u2019t generally think of them as such, both grid and random search are algorithms. In the case of grid search, we input the domain and the algorithm selects the next value for each hyperparameter in an ordered sequence. The only requirement of grid search is that it tries every combination in a grid once (and only once). For random search, we input the domain and each time the algorithm gives us a random combination of hyperparameter values to try. There are no requirements for random search other than that the next values are selected at random.", "Random search can be implemented as in the following:", "This is in effect a very simple algorithm!", "The results history is a data structure that contains the hyperparameter combinations and the resulting score on the objective function. When we get to Bayesian Optimization, the model actually uses the past results to decide on the next hyperparmeters to evaluate. Random and grid search are uninformed methods that do not use the past history, but we still need the history so we can find out which hyperparameters worked the best!", "In this case, the results history is simply a dataframe.", "Automated hyperparameter tuning with bayesian optimization sounds complicated, but in fact it uses the same four parts as random search with the only difference the Algorithm used. For this competition, I used the Hyperopt library and the Tree Parzen Estimator (TPE) algorithm with the work shown in this notebook. For a conceptual explanation, refer to this article, and for an implementation in Python, check out the notebook or this article.", "The basic concept of Bayesian optimization is that it uses the previous evaluation results to reason about which hyperparameters perform better and uses this reasoning to choose the next values. Hence, this method should spend fewer iterations evaluating the objective function with poorer values. Theoretically, Bayesian optimization can converge on the ideal values in fewer iterations than random search (although random search can still get lucky)! The aspirations of Bayesian optimization are:", "This is a powerful method that promises to deliver great results. The question is, does the evidence in practice show this to be the case? To answer that, we turn to the final notebook, a deep dive into the model tuning results!", "After the hard work of implementing random search and Bayesian optimization, the third notebook is a fun and revealing exploration of the results. There are over 35 plots, so if you like visuals, then check it out.", "Although I was trying to do the whole competition on Kaggle, for these searches, I did 500 iterations of random search and 400 of Bayesian optimization which took about 5 days each on a computer with 64 GB of RAM (thanks to Amazon AWS). All the results are available, but you\u2019ll need some serious hardware to redo the experiments!", "First off: which method did better? The image below summarizes the results from 501 iterations of random search and 402 of Bayesian optimization (called opt in the dataframe):", "Going by max score, random search does slightly better but if we measure by average score, bayesian optimization wins.", "The good news is this is almost exactly what we expect: random search can happen upon a great set of values because it thoroughly explores the search space, but Bayesian optimization will \u201cfocus\u201d on the highest scoring hyperparameter values by reasoning from previous results. Let\u2019s take a look at a very revealing plot, the value of the cross validation score versus the number of iterations:", "We see absolutely no trend for random search, while Bayesian Optimization (again shown by opt ) improves over the trials. If you can understand this graph, then you can see the benefits of both methods: random search explores the search domain but Bayesian optimization gets better over time. We can also see that Bayesian optimization appears to reach a plateau, indicating diminishing returns to further trials.", "Second major question: what were the best hyperparameter values? Below are the optimal results from Bayesian optimization:", "We can use these results to build a model and submit predictions to the competition, or they can be used to inform further searches by allowing us to define a concentrated search space around the best values.", "One interesting aspect to consider is the values tried by each search method for each hyperparameter. The plots below show kernel density estimate (KDE) functions for each of the search methods as well as the sampling distribution (the hyperparameter grid). The dashed vertical lines indicate the optimal value found for each method. First is the learning rate:", "Even though the learning rate distribution stretched over several orders of magnitude, both methods found optimal values quite low in the domain. We can use this knowledge to inform further hyperparameter searches by concentrating our search in this region. In most cases, a lower learning rate increases the cross validation performance but at the cost of increased run-time, which is a trade-off we have to make.", "Let\u2019s look at some other graphs. For most of the hyperparameters, the optimal values from both methods are fairly close, but not for colsample_bytree :", "This refers to the fraction of columns used when building each tree in the GBM, and random search found the optimal value was higher than Bayesian optimization. Again, these results can be used for further searches, as we see that the Bayesian method tended to concentrate on values around 0.65.", "We\u2019ll show two more plots from this analysis because they are fairly interesting, involving two regularization parameters:", "What\u2019s noteworthy here is that these two hyperparameters appear to be complements of one another: if one regularization value is high, then we want the other to be low and vice versa. Maybe this helps the model achieve a balance between bias/variance, the most common issue in machine learning.", "While random search does not change the distribution of values over the search, Bayesian optimization does, by concentrating on where it thinks the best values are in the search domain. We can see this by graphing the hyperparameter values against the iteration:", "The hyperparameters with the clearest trend are colsample_bytree and learning_rate which both continue downward over the trials. The reg_lambda and reg_alpha are diverging, which confirms our earlier hypothesis that we should decrease one of these while increasing the other.", "We want to be careful about placing too much value in these results, because the Bayesian optimization might have found a local minimum of the cross validation loss that it is exploiting. The trends here are pretty small, but it\u2019s encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve.", "These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs because we are not changing one hyperparameter at a time and there can be complex interactions between multiple hyperparameters. A truly accurate graph would be 10-dimensional and show the values of all hyperparameters and the resulting score. If we could understand a 10-dimensional graph, then we might be able to figure out the optimal combination of hyperparameters!", "Here the random search is in blue and the Bayesian in green:", "The only clear distinction is that the score decreases as the learning rate increases. We cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of estimators shortly).", "There are not any strong trends here. The score versus subsample is a little off because boosting_type = 'goss' cannot use subsample (it must be set equal to 1.0). While we can\u2019t look at all 10 hyperparameters at one time, we can view two at once if we turn to 3D plots!", "To try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. (See the code for details, 3D plotting in Python is surprisingly straightforward). First we can show the reg_alpha and reg_lambda , the regularization hyperparameters versus the score (for Bayesian opt):", "It\u2019s a little hard to make sense, but if we remember the best scores occurred around 0.5 for reg_alpha and 0.4 for reg_lambda , we can see generally better scores in this region.", "Next is the learning_rate and n_estimators (number of decision trees trained in the ensemble). While the learning rate was a hyperparameter in the grid, the number of decision trees (also called the number of boosting rounds) was found by early stopping with 5-fold cross validation:", "This time there is a clear trend: a lower learning rate and higher number of estimators increases the score. The relationship is expected because a lower learning rate means the contribution of each tree is reduced which necessitates training more trees. The expanded number of trees increases the model\u2019s capacity to fit the training data (while also increasing the time to run). Moreover, as long as we use early stopping with enough folds, we don\u2019t have to be concerned about overfitting with more trees. It\u2019s nice when the results agree with our understanding (although we probably learn more when they don\u2019t agree!)", "For the final plot, I wanted to show the correlations between each hyperparameter and each other and the score. These plots can\u2019t prove causation, but they can tell us which variables are correlated:", "We already found most of the information from the graphs, but we can see the negative correlation between the learning rate and the score, and the positive correlation between the number of estimators and the score.", "The final step of the exploration was to implement both the random search best hyperparameters and the Bayesian optimization best hyperparameters on a full dataset (the dataset comes from this kernel and I would like to thank the author for making it public). We train the model, make predictions on the testing set, and finally upload to the competition to see how we do on the public leaderboard. After all the hard work, do the results hold up?", "If we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it\u2019s possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results \u2014 including cross validation and public leaderboard \u2014 suggest that both methods produce similar outcomes when run for enough iterations. All we can say for sure is that either method is better than hand-tuning! Our final model is enough to move us 1000 places up the leaderboard compared to our previous work.", "Finally, to end with one more plot, we can take a look at the feature importances from the trained GBM:", "NEW_CREDIT_TO_ANNUITY_RATIO and NEW_EXT_SOURCES_MEAN were features derived by the data science community on Kaggle and not in the original data. It\u2019s reassuring to see these so high up in the importance because it shows the value of feature engineering.", "The major takeaways from this work are:", "Where to go from here? Well, there are always plenty of other methods to try such as automated feature engineering or treating the problem as a time-series. I\u2019ve already done a notebook on automated feature engineering so that will probably be where my focus will turn next. We can also try other models or even journey into the realm of deep learning!", "I\u2019m open to suggestions, so let me know on Kaggle or on Twitter. Thanks for reading, and if you want to check out any of my other work on this problem, here are the complete set of notebooks:", "There should be enough content there to keep anyone busy for a little bit. Now it\u2019s off to do more learning/exploration for the next post! The best part about data science is that you\u2019re constantly on the move, looking for the next technique to conquer. Whenever I find out what that is, I\u2019ll be sure to share it!", "As always, I welcome feedback and constructive criticism. I can be reached on Twitter @koehrsen_will.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Cortex Intel, Data Science Communicator"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fdb04ea415507&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9----db04ea415507---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.quora.com/Why-did-Kaggle-start-having-a-bad-reputation", "anchor_text": "valid criticisms"}, {"url": "https://www.kaggle.com/", "anchor_text": "Kaggle"}, {"url": "https://www.kaggle.com/competitions", "anchor_text": "interesting problems"}, {"url": "https://www.kaggle.com/learn/overview", "anchor_text": "exploring new ideas"}, {"url": "https://www.kaggle.com/c/home-credit-default-risk/", "anchor_text": "Kaggle Home Credit contest"}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-one-getting-started-32fb9ff47426", "anchor_text": "Part One: Getting Started"}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "Part Two: Improving)"}, {"url": "https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "anchor_text": "Random and Grid Search"}, {"url": "https://www.kaggle.com/willkoehrsen/automated-model-tuning", "anchor_text": "Automated Hyperparameter tuning"}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt/notebook", "anchor_text": "Tuning Results"}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "first part of this series"}, {"url": "https://towardsdatascience.com/machine-learning-kaggle-competition-part-two-improving-e5b4d61ab4b8", "anchor_text": "Part two"}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "As shown in this great paper,"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization", "anchor_text": "evolutionary"}, {"url": "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Gradient-based_optimization", "anchor_text": "gradient-based"}, {"url": "http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/", "anchor_text": "Gradient Boosting Machine (GBM) model"}, {"url": "http://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api", "anchor_text": "the documentation"}, {"url": "https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "anchor_text": "In the first notebook"}, {"url": "https://www.kaggle.com/willkoehrsen/automated-model-tuning", "anchor_text": "this notebook"}, {"url": "https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f", "anchor_text": "this article"}, {"url": "https://towardsdatascience.com/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a", "anchor_text": "this article"}, {"url": "https://sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf", "anchor_text": "basic concept of Bayesian optimization"}, {"url": "https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization", "anchor_text": "aspirations of Bayesian optimization"}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt", "anchor_text": "the final notebook"}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt", "anchor_text": "the third notebook"}, {"url": "http://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf", "anchor_text": "bias/variance"}, {"url": "https://machinelearningmastery.com/tune-learning-rate-for-gradient-boosting-with-xgboost-in-python/", "anchor_text": "learning rate"}, {"url": "https://www.kaggle.com/jsaguiar/updated-0-792-lb-lightgbm-with-simple-features", "anchor_text": "this kernel"}, {"url": "https://www.kaggle.com/jsaguiar", "anchor_text": "the author"}, {"url": "https://www.featurelabs.com/blog/secret-to-data-science-success/", "anchor_text": "value of feature engineering"}, {"url": "https://towardsdatascience.com/automated-feature-engineering-in-python-99baf11cc219", "anchor_text": "automated feature engineering"}, {"url": "https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics", "anchor_text": "automated feature engineering"}, {"url": "https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction", "anchor_text": "A Gentle Introduction"}, {"url": "https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering", "anchor_text": "Manual Feature Engineering Part One"}, {"url": "https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2", "anchor_text": "Manual Feature Engineering Part Two"}, {"url": "https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics", "anchor_text": "Introduction to Automated Feature Engineering"}, {"url": "https://www.kaggle.com/willkoehrsen/tuning-automated-feature-engineering-exploratory", "anchor_text": "Advanced Automated Feature Engineering"}, {"url": "https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection", "anchor_text": "Feature Selection"}, {"url": "https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search", "anchor_text": "Intro to Model Tuning: Grid and Random Search"}, {"url": "https://www.kaggle.com/willkoehrsen/automated-model-tuning", "anchor_text": "Automated Model Tuning"}, {"url": "https://www.kaggle.com/willkoehrsen/model-tuning-results-random-vs-bayesian-opt", "anchor_text": "Model Tuning Results"}, {"url": "http://twitter.com/@koehrsen_will", "anchor_text": "@koehrsen_will"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----db04ea415507---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----db04ea415507---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----db04ea415507---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/education?source=post_page-----db04ea415507---------------education-----------------", "anchor_text": "Education"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----db04ea415507---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----db04ea415507---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&user=Will+Koehrsen&userId=e2f299e30cb9&source=-----db04ea415507---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fdb04ea415507&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----db04ea415507---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----db04ea415507--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----db04ea415507--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----db04ea415507--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----db04ea415507--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----db04ea415507--------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://williamkoehrsen.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Will Koehrsen"}, {"url": "https://williamkoehrsen.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "38K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe2f299e30cb9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&user=Will+Koehrsen&userId=e2f299e30cb9&source=post_page-e2f299e30cb9--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe7d4a87a913e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmachine-learning-kaggle-competition-part-three-optimization-db04ea415507&newsletterV3=e2f299e30cb9&newsletterV3Id=e7d4a87a913e&user=Will+Koehrsen&userId=e2f299e30cb9&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}