{"url": "https://towardsdatascience.com/the-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0", "time": 1683014250.2897532, "path": "towardsdatascience.com/the-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0/", "webpage": {"metadata": {"title": "The K-bandit Problem with Reinforcement Learning | by Sebasti\u00e1n Gerard Aguilar Kleimann | Towards Data Science", "h1": "The K-bandit Problem with Reinforcement Learning", "description": "A very important part in reinforcement learning is how to evaluate the actions an agent performs. In this post we will use the K-bandit problem to show different ways of evaluating these actions\u2026"}, "outgoing_paragraph_urls": [{"url": "https://datasciencestreet.com/the-k-bandit-problem-with-reinforcement-learning/", "anchor_text": "https://datasciencestreet.com", "paragraph_index": 18}], "all_paragraphs": ["A very important part in reinforcement learning is how to evaluate the actions an agent performs. In this post we will use the K-bandit problem to show different ways of evaluating these actions. It\u2019s important to have in mind that the K-bandit problem is just a simple version of the many reinforcement learning situations. It\u2019s simple because the actions that the agent performs are evaluated individually and are not like other techniques where the evaluation is done over a series of actions.", "Imagine that you are in a casino surrounded by 4 different slot machines. Each machine gives you a different prize under a different probability law. Your job will be to maximize the prize those machines give you and discover which is the machine with the better prizes. You will do this by experimenting and playing on the machines a thousand times! The prizes and the probability of getting these prizes don\u2019t change over time.", "What strategy would you use to maximize the prize and discover the best machine? Maybe a first approach would be to play in the machines an equal amount of times (250 each). However, wouldn\u2019t it be better to play more on the machine that is giving us the better prizes? What if we choose the wrong machine?", "It\u2019s because of these questions that we need to coordinate our actions. Sometimes it will be better to explore (try playing on different machines) and sometimes it will be better to exploit our knowledge (play on the machine that we think is best).", "Before we continue to solve this questions, I will introduce some notation that will be used throughout the problem:", "The expected prize q\u2217(a) is the most important value in the problem, cause if we discover its true value we would know in which machine to play all the time. It\u2019s common that this value is unknown, that is why we need to explore (play) on the different machines to estimate the expected value. As we proceed in time we will get a better approximation of the expected value (if we could play infinitely we would get the exact value). The notation used for the approximate value is Q\u209c(a).This is the estimation of the expected value of a at time t.", "There are many ways to estimate the expected values of the actions (Q\u209c(a)). Here we will use the approach that for me is the most natural. The approach sums all the prizes that were obtained performing a certain action, this will appear in the numerator and it will be divided by the number of times this action was performed:", "With the help of this equation and as we advance in time the value of Q\u209c(a) will get closer to q\u2217(a) but it will be important to explore and exploit the different set of possible actions. In this article the \u03b5-greedy method will be used to explore. This is a very simple method since the only thing we\u2019re going to do is choose the option that has been calculated as the better option but with a certain probability(\u03b5) try a different action at random. So let\u2019s say that \u03b5=.1 is established, that means that for a thousand steps in time, 900 times the best option will be chosen(exploit) and 100 times exploration will be done.", "Now that the problem has been defined and the methods for solving it described we will solve it using python. So it\u2019s time to open your favorite IDE and start coding! I will describe and explain the code piece by piece and put it fully at the end. You can copy it fully and in case of doubt check the specific piece. For this program we will only use two libraries so we will proceed to import them:", "The second step will be to create two auxiliary functions. The first auxiliary function will help us emulate probabilistic outcomes. This function will throw a random number between 0 and 1. If the result of the random number is lower than the number we defined, the function will return true, and it will return false otherwise.", "The second function is the \u03b5-greedy algorithm which will decide on which machine to play. It can be the machine with highest expected value or a random one.", "Given the construction of the problem 3 for loops will be necessary:", "Finally we will use the matplot library to visualize the results in a plot.", "Observing \u201cFigure 1\u201d we realize that the maximum prize is reached when we set \u03b5=0.15", "this means that it is convenient to explore 15% of the time and be greedy the other 85%. Now, for the second part of the problem we asked the algorithm to tell us which machine it thought was best. Let\u2019s see this graphically too:", "Figure 2 shows us that with more exploration the algorithm tends to choose the best machine more accurately. This is a somewhat obvious result since by having more information about the different machines we would choose better. However, this comes with a prize since doing more exploration comes with the cost of reducing the prize.", "This article is just a first approach to the k-bandit problem so for the interested reader I would like to leave some questions for you to ponder upon:", "Thank you very much for your attention, and I hope to see you again. You can find and run the code ahead.", "Originally published at https://datasciencestreet.com on September 22, 2020.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Master in International Business and Entrepreneurship (Universita degli studi di Pavia) Bachelor in Actuary (Universidad Nacional Aut\u00f3noma de M\u00e9xico)"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F440b2f3ddee0&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@sgerardak?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgerardak?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Sebasti\u00e1n Gerard Aguilar Kleimann"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d3c136f9f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&user=Sebasti%C3%A1n+Gerard+Aguilar+Kleimann&userId=7d3c136f9f7c&source=post_page-7d3c136f9f7c----440b2f3ddee0---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pixabay.com/es/vectors/m%C3%A1quina-tragaperras-casino-frutas-159972/", "anchor_text": "Pixabay"}, {"url": "https://pixabay.com/es/photos/casino-juego-de-azar-3260387/", "anchor_text": "Pixabay"}, {"url": "https://datasciencestreet.com/the-k-bandit-problem-with-reinforcement-learning/", "anchor_text": "https://datasciencestreet.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----440b2f3ddee0---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/gambling?source=post_page-----440b2f3ddee0---------------gambling-----------------", "anchor_text": "Gambling"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----440b2f3ddee0---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----440b2f3ddee0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----440b2f3ddee0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&user=Sebasti%C3%A1n+Gerard+Aguilar+Kleimann&userId=7d3c136f9f7c&source=-----440b2f3ddee0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&user=Sebasti%C3%A1n+Gerard+Aguilar+Kleimann&userId=7d3c136f9f7c&source=-----440b2f3ddee0---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F440b2f3ddee0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----440b2f3ddee0---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----440b2f3ddee0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgerardak?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@sgerardak?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Sebasti\u00e1n Gerard Aguilar Kleimann"}, {"url": "https://medium.com/@sgerardak/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7d3c136f9f7c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&user=Sebasti%C3%A1n+Gerard+Aguilar+Kleimann&userId=7d3c136f9f7c&source=post_page-7d3c136f9f7c--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F7d3c136f9f7c%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-k-bandit-problem-with-reinforcement-learning-440b2f3ddee0&user=Sebasti%C3%A1n+Gerard+Aguilar+Kleimann&userId=7d3c136f9f7c&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}