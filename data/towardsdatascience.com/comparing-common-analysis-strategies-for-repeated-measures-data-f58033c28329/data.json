{"url": "https://towardsdatascience.com/comparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329", "time": 1682995064.842735, "path": "towardsdatascience.com/comparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329/", "webpage": {"metadata": {"title": "Comparing common analysis strategies for repeated measures data | by Eshin Jolly | Towards Data Science", "h1": "Comparing common analysis strategies for repeated measures data", "description": "My hope with this post is to provide a conceptual overview of how to deal with a specific type of dataset commonly encountered in the social sciences (and very common in my own disciplines of\u2026"}, "outgoing_paragraph_urls": [{"url": "https://pdfs.semanticscholar.org/fe95/9879dbf2b06f33b9ef07b67897135be57abf.pdf", "anchor_text": "Ugrinowitsch et al, 2004", "paragraph_index": 2}, {"url": "https://www.wikiwand.com/en/False_positives_and_false_negatives", "anchor_text": "false-positives", "paragraph_index": 2}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/3615759", "anchor_text": "Vasey & Thayer, 1987", "paragraph_index": 2}, {"url": "https://www.wikiwand.com/en/Simpson%27s_paradox", "anchor_text": "Simpson\u2019s Paradox", "paragraph_index": 3}, {"url": "https://www.sciencedirect.com/science/article/pii/S0749596X07001398", "anchor_text": "Baayen et al, 2008", "paragraph_index": 5}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/", "anchor_text": "Barr, et al, 2013", "paragraph_index": 6}, {"url": "https://arxiv.org/abs/1506.04967", "anchor_text": "Bates, et al 2015", "paragraph_index": 6}, {"url": "https://www.sciencedirect.com/science/article/pii/S0749596X17300013", "anchor_text": "Matuschek et al, 2017", "paragraph_index": 6}, {"url": "https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html", "anchor_text": "this guide", "paragraph_index": 6}, {"url": "https://ourcodingclub.github.io/2017/03/15/mixed-models.html", "anchor_text": "this post", "paragraph_index": 6}, {"url": "https://psycnet.apa.org/record/2017-52405-001", "anchor_text": "Brauer & Curtin, 2018", "paragraph_index": 6}, {"url": "https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r", "anchor_text": "ton", "paragraph_index": 6}, {"url": "http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf", "anchor_text": "resources", "paragraph_index": 6}, {"url": "http://www.stat.columbia.edu/~gelman/arm/", "anchor_text": "available", "paragraph_index": 6}, {"url": "http://projects.iq.harvard.edu/files/gov2001/files/sesection_5.pdf", "anchor_text": "This lecture", "paragraph_index": 7}, {"url": "http://econweb.umd.edu/~sarzosa/teach/2/Disc2_Cluster_handout.pdf", "anchor_text": "this presentation", "paragraph_index": 7}, {"url": "http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf", "anchor_text": "Cameron & Miller, 2015", "paragraph_index": 7}, {"url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780111304", "anchor_text": "Frison & Pocock, 1992", "paragraph_index": 8}, {"url": "https://www.fil.ion.ucl.ac.uk/spm/doc/papers/aph_rfx.pdf", "anchor_text": "Holmes & Friston, 1998", "paragraph_index": 8}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/19463958/", "anchor_text": "Mumford & Nichols, 2009", "paragraph_index": 8}, {"url": "http://www.stat.columbia.edu/~gelman/research/published/459.pdf", "anchor_text": "Gelman, 2005", "paragraph_index": 8}, {"url": "http://eshinjolly.com/pymer4/", "anchor_text": "pymer4", "paragraph_index": 13}, {"url": "https://www.apa.org/research/action/multitask", "anchor_text": "switch cost", "paragraph_index": 13}, {"url": "https://cran.r-project.org/web/packages/lme4/index.html", "anchor_text": "lme4", "paragraph_index": 13}, {"url": "http://joss.theoj.org/papers/10.21105/joss.00862", "anchor_text": "Jolly, 2018", "paragraph_index": 13}, {"url": "https://www.wikiwand.com/en/Best_linear_unbiased_prediction", "anchor_text": "BLUPs", "paragraph_index": 15}, {"url": "http://www.stat.columbia.edu/~gelman/research/published/459.pdf", "anchor_text": "Gelman, 2005", "paragraph_index": 29}, {"url": "https://www.wikiwand.com/en/False_positives_and_false_negatives", "anchor_text": "false negatives", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/gradient-descent-d3a58eb72404", "anchor_text": "find the lowest point", "paragraph_index": 33}, {"url": "https://www.reed.edu/economics/parker/s11/312/notes/Notes13.pdf", "anchor_text": "nice quick guide", "paragraph_index": 37}, {"url": "https://cran.r-project.org/web/packages/lmerTest/index.html", "anchor_text": "lmerTest R package", "paragraph_index": 37}, {"url": "http://eshinjolly.com/pymer4/usage.html", "anchor_text": "pymer4 also offers", "paragraph_index": 37}, {"url": "https://github.com/ejolly/pymer4/tree/dev", "anchor_text": "development branch on github", "paragraph_index": 37}, {"url": "http://eshinjolly.com/2019/02/18/rep_measures/", "anchor_text": "eshinjolly.com", "paragraph_index": 38}, {"url": "http://eshinjolly.com", "anchor_text": "eshinjolly.com", "paragraph_index": 40}], "all_paragraphs": ["My hope with this post is to provide a conceptual overview of how to deal with a specific type of dataset commonly encountered in the social sciences (and very common in my own disciplines of experimental psychology and cognitive neuroscience). My goal is not to provide mathematical formalisms, but rather build some intuitions and try to avoid as much jargon as possible. Specifically, I\u2019d like to compare some of the more common analysis strategies one can use and how they vary by situation, ending with some takeaways that hopefully guide your future decision-making. But to do so we need to start at the beginning\u2026", "Datasets come in all different shapes and sizes. In many introductory tutorials, classes, and even real world examples, folks are usually dealing with datasets that are referred to as satisfying the \u201ci.i.d assumption\u201d of many common statistical models. What does this mean in English? It refers to the fact that each data-point is largely independent of other data points in the complete dataset. More specifically, it means that the residuals of a model (i.e. what\u2019s left over that the model can\u2019t explain) are independent of each other and that they all come from the same distribution which has a mean of 0 and a standard deviation of sigma\u00b2 . In other words, knowing something about one error that the model makes tells you little about any other error the model makes, and by extension, knowing something about one data point tells you little about any other datapoint.", "However, many types of data contain \u201crepeats\u201d or \u201creplicates\u201d such as measuring the same people over time or under different conditions. These data notably violate this assumption. In these cases, some data points are more similar to each other than other data points. Violations of these assumptions can lead to model estimates that are not as accurate as they could possibly be (Ugrinowitsch et al, 2004). The more insidious issue is that inferences made using these estimates (e.g. computing t-statistics and by extension p-values) can be wildly inaccurate and produce false-positives (Vasey & Thayer, 1987). Let\u2019s try to make this more concrete by considering two different datasets.", "In case 1 (left) we give 21 people a survey 1 time each and try to see if their survey responses share any relationship with some demographic about them. 21 total data points, pretty straightforward. In case 2 (right), we give 3 people a survey 7 times each and do the same thing. 21 total data points again, but this time each data point is not independent of every other. In the first case, each survey response is independent of any other. That is, knowing something about one person\u2019s response tells you little about another person\u2019s response. However, in the second case this is not true. Knowing something about person A\u2019s survey response the first time you survey them tells you a bit more about person A\u2019s survey response the second time you survey them, whereas it does necessarily give you more information about any of person B\u2019s responses. Hence the non-independence. In the most extreme case estimating a model ignoring these dependencies in the data can completely reverse the resulting estimates, a phenomenon known as Simpson\u2019s Paradox.", "So what do we typically do? Well there are a few different analysis \u201ctraditions\u201d that have dealt with this in different ways. This isn\u2019t by any means an exhaustive list, but approaches that are reasonably common across many different literatures.", "Like many other researchers in psychology/neuroscience, I was first taught that repeated-measures ANOVAs are the only way to analyze these type of data. However, this has fallen a bit out of practice in favor of the more flexible approach of multi-level/mixed effects modeling (Baayen et al, 2008). I don\u2019t want to focus on why multi-level modeling is often far more preferable, as that\u2019s a different discussion (e.g. better handling of missing data, different numbers of repeats, additional levels of replicates, different numbers of replicates, etc), but suffice to say that it once you start using this approach there\u2019s essentially no reason to ever run a repeated measured ANOVA again. Going into all the details of how these models work is beyond the scope of this post, but I\u2019ll link to a few resources throughout. Conceptually, multi-level modeling simultaneously estimates coefficients that describe a relationship across the entire dataset, as well as within each group of replicates. In our example above, this amounts to estimating the relationship between survey responses and demographics for the entire population of survey respondents, but also the degree to which individual people deviate from these estimates. This has the net effect of \u201cpooling\u201d estimates and their associated errors together and works in a manner not entirely unlike using a prior if you are familiar with Bayesian terminology or regularization/smoothing if machine-learning is more your thing. The result of estimating a model this way means that estimates can \u201chelp each other out\u201d such that we can impute values if some of our survey respondents didn\u2019t fill out the survey each time we asked them to, or we can \u201cclean-up\u201d noisy estimates we get from specific individuals by assuming that individuals\u2019 estimates all come from the same population, thereby restricting wonky values they may take on.", "In practice using these models can be a bit tricky. This is due to the fact that it\u2019s not immediately obvious how to set these models up for estimation. For example, should we assume that each respondent has a different relationship between their survey results and demographics? Or should we simply assume that their survey results differ on average but vary with their demographics in the same way? Specifically, users have a variety of choices for how to specify what\u2019s referred to as the \u201crandom effects\u201d (deviation estimates) part of the model. You may have come across terminology like \u201crandom intercepts\u201d or \u201crandom slopes.\u201d In our example, this is the difference between allowing a model to learn a unique mean estimate for each individual\u2019s survey responses and learning a unique regression estimate for the relationship between each individual\u2019s survey responses and demographic outcome measure. In many cases, computing the complete set of coefficients one could compute (intercepts, slopes, and the correlations between them for every predictor) (Barr, et al, 2013) leads the model to fail to converge, leaving a user with unreliable estimates. This has lead to suggestions to keep models relatively \u201csimple\u201d with respect to the inference one is trying to make (Bates, et al 2015), or compare different model structures and use a model selection criteria to adjudicate between them before performing inferences (Matuschek et al, 2017). Pretty tricky huh? Try this guide to help you out if you venture down this path or check out this post for a nice visual treatment. Brauer & Curtin, 2018 is a particularly good one-stop-shop for review, theory, practice, estimation issues, and code snippets. There are a ton of resources available if multi-level models have got you excited.", "In other academic fields/areas, there is an entirely different tradition for handling these types of data. For example, in some economics disciplines \u201crobust/sandwich/huber-white\u201d standard errors are computed for what is otherwise a standard linear regression model. This lecture provides a nice math-ish overview of what these techniques are, but the general takeaway is that this approach entails computing the regression coefficients in a \u201ctypical\u201d manner using ordinary least squares (OLS) regression, but \u201ccorrecting\u201d the variance of these estimators (i.e. the standard errors) for how heteroscedastic they are. That is, how much their variances differ. There are several ways to account for heteroscedasticicty that incorporate things like small-sample and auto-correlation correction, but another one is to compute these robust estimates with respect to \u201cclusters\u201d or grouping factors in the data. In the example above, clusters would comprise survey respondents and each survey response would comprise a data point within that cluster. Therefore, this approach completely ignores the fact that there are repeated measurements when computing the regression coefficients, but takes the repeated measures data into account when making inferences on these coefficients by adjusting their standard errors. For an overview of this calculation see this presentation and for a more formal treatment see Cameron & Miller, 2015.", "Finally, a third approach we can use is what has been referred to as a two-stage-regression or the summary statistics approach (Frison & Pocock, 1992; Holmes & Friston, 1998). This approach is routine in the analysis of functional MRI data (Mumford & Nichols, 2009). Conceptually, this looks like fitting a standard OLS regression model to each survey respondent separately, and then fitting a second OLS model to the coefficients from each individual subject\u2019s fit. In the simplest case this equivalent to calculating a one-sample t-test over individuals\u2019 coefficients. You might notice that this approach \u201cfeels\u201d similar to the multi-level approach, and in colloquial English, there are in fact multiple levels of modeling going on. However, notice how each first-level model is estimated completely independently of every other model and how their errors or the variance of their estimates are not aggregated in any meaningful way. This means that we lose out on some of the benefits we gain from the formal multi-level modeling framework described above. Yet what we might lose in benefits we gain back in simplicity: there are no additional choices to be made such as choosing an appropriate \u201crandom effects\u201d structure. In fact, Gelman, 2005 notes that two-stage-regression can be viewed as a special case of multi-level modeling in which we assume that the distribution from which individual/cluster level coefficients comes has infinite variance.", "Having all these tools at our disposal can sometimes make it tricky to figure out which approach is preferable for what situation and whether there is one approach that is always better than the others (spoiler: there isn\u2019t). To better understand when we might use each approach let\u2019s consider some of the most common situations we might encounter. I\u2019ll refer to these as the \u201cdimensions\u201d along which our data can vary.", "The most common thing that varies about different datasets is simply their size, i.e. how many observations we\u2019re really dealing with. In the case of non-independent data, an analyst may most often be interested in making inferences about a particular \u201clevel\u201d of the data. In our survey example, this is generalizing to \u201cpeople\u201d rather than specific instances of the survey. So this dimension varies based on how many individuals we sampled, irrespective of how many times we sampled any given individual.", "Another dimension in which our repeated-measures data may vary, is how many repeats we\u2019re dealing with. In our example above, this is the number of observations we have about any given individual. Did each person fill out the survey 5 times? 10? 100? This dimension therefore varies based on how often we sample any given individual, irrespective of how many total individuals we sample.", "A key way in which each of these analysis approaches varies is how they handle (or don\u2019t) variability between clusters of replicates. In our example above, this is the variance between individuals. Do different people really respond differently from each other? At one extreme we can treat every individual survey response as entirely independent ignoring the fact that we surveyed individuals multiple times and pretending each survey is totally unique. At the other end, we can assume that the relationship between survey responses and demographics come from a higher-level distribution and specific people\u2019s estimates are instances of this distribution, preserving the fact that each person\u2019s own responses are more similar to each other than they are to anyone else\u2019s responses. I\u2019ll return to this a bit more below.", "Often in cases like this we can use simulated data, designed to vary in particular ways, to help us gain some insight as to how these things influence our different analysis strategies. So let\u2019s see how that looks. I\u2019m going to be primarily using the pymer4 Python package that I wrote to simulate some data and compare these different models. I wrote this package originally so I could reduce the switch cost I kept experiencing bouncing between R and Python for my work. I quickly realized that my primary need for R was using the fantastic lme4 package for multi-level modeling and so I wrote this Python package as a way to use lme4 from within Python while playing nicely with the rest of the scientific Python stack (e.g. pandas, numpy, scipy, etc). Since then the package has grown quite a bit (Jolly, 2018), including the ability to fit the different types of models discussed above and simulate different kinds of data. Ok let\u2019s get started:", "Let\u2019s start out with a single simulated dataset and fit each type of model discussed above. Below I\u2019m generating multi-level data similar to our toy example above. The dataset is comprised of 50 \u201cpeople\u201d with 50 \u201creplicates\u201d each. For each person, we measured 3 independent variables (e.g. 3 survey questions) and would like to relate them to 1 dependent variable (e.g. 1 demographic outcome).", "We can see that the overall dataset is generated as described above. Simulating data this way also allows us to generate the best-linear-unbiased-predictions (BLUPs) for each person in our dataset. These are the coefficients for each individual person.", "Finally, we can also checkout the \u201ctrue\u201d coefficients that generated these data. These are the \u201ccorrect answers\u201d we hope that our models can recover. Since these data have been simulated using the addition of noise to each individual\u2019s data (mu = 0, sigma\u00b2 = 1), and with variance across individuals (pymer4\u2019s default is sigma\u00b2 = 0.25) we don\u2019t expect perfect recovery of these parameters, but something pretty close (we\u2019ll explore this more below).", "Ok time to evaluate some modeling strategies. For each model type I\u2019ll fit the model to the data as described, and then compute 3 metrics:", "Let\u2019s begin with fitting a multi-level model specifying the complete set of all possible parameters we can estimate. This has the effect of letting each individual have their own set of regression estimates while still treating these estimates as coming from a common distribution. You can see below we can recover the parameters pretty well and as we expect all our are results are \u201csignificant.\u201d", "Next, let\u2019s see what happens when we fit a multi-level model with the simplest possible \u201crandom effects\u201d structure. Notice that by not letting each individual be free to have their own estimates (aside from their own mean/intercept), our coefficient recovery drops a little bit, but our t-statistics increase dramatically. This looks to be driven by the fact that the variance estimates of the coefficients (standard errors) are quite a bit smaller. All else being equal, we would be much more likely to identify \u201csignificant\u201d relationships using a simpler, or in this case, \u201cmisspecified\u201d multi-level model, since we know that the data were generated such that each individual did in fact, have different BLUPs.", "Next, let\u2019s evaluate the cluster-robust-error modeling approach. Remember, this involves estimating a single regression model to obtain coefficient estimates, but then applying a correction factor to the SEs, and thereby the t-statistics to adjust our inferences. It looks like our coefficient recovery is about the same as our simple multi-level model above, but our inferences are far more conservative due to the larger standard-errors and smaller t-statistics. In fact, these are even a bit more conservative than the fully specified multi-level model we estimated first.", "Lastly, let\u2019s use the two-stage-regression approach. We\u2019ll fit a separate regression to each of our 50 people and then compute another regression on those 50 coefficients. In this simple example, we\u2019re really just computing a one-sample t-test on these 50 coefficients. Notice that our coefficient recovery is a tiny bit better than our fully-specified multi-level model and our inferences (based on T-stats and SEs) would largely be similar. This suggests that for this particular dataset we could have gone with either strategy and walked away with the same inference.", "Now, this was only one particular dataset with a particular size and particular level of between-person variability. Remember the dimensions outlined above? The real question we want to answer is how these different modeling strategies vary with respect to each of those dimensions. So let\u2019s expand our simulation here. Let\u2019s generate a \u201cgrid\u201d of settings such that we simulate every combination of dimensions we can in a reasonable amount of time. Here\u2019s the grid we\u2019ll try to simulate:", "Going down the rows we\u2019ll vary dimension 1 the sample size of the units we\u2019re making inferences over (number of people) from 5 -> 100. Going across columns we\u2019ll vary dimension 2, the sample size of the units nested within the units we\u2019re making inferences over (number of observations per person) from 5 -> 100. Going over the z-plane we\u2019ll vary dimension 3 the variance between the units we\u2019re making inferences over (between-person variability) from 0.10 -> 4 standard deviations.", "Since varying dimension 1 and dimension 2 should make intuitive sense (they\u2019re different aspects of the sample size of our data) let\u2019s explore what varying dimension 3 looks like. Here are plots illustrating how changing our between person variance influences coefficients. Each figure below depicts a distribution of person level coefficients; these are the BLUPs we discussed above. When simulating a dataset with two parameters described by an intercept and a slope (IV1), notice how each distribution is centered on the true value of the parameter, but the width of the distribution increases as we increase the between-group variance. These distributions are the distributions that our person level parameters come from. So while they average out to be the same value, they are increasingly dispersed around that value. As these distributions become wider it becomes more challenging to recover the true coefficients of the data if a dataset is too small, as models need more data in order to stabilize their estimates.", "For the sake of brevity I\u2019ve removed the plotting code for the figures below, but am happy to share them on request.", "The next code block sets up this parameter grid and defines some helper functions to compute the metrics defined above. Since this simulation took about ~50 minutes to run on a 2015 quad-core Macbook Pro, I also defined some functions to save each simulation to a csv file.", "For the sake of brevity I\u2019ve removed the plotting code for the figures below, but am happy to share them on request!", "Ok, let\u2019s take first take a look at our coefficient recovery. If we look from the top left of the grid to the bottom right the first thing to jump out is that when we increase our overall sample size (number of clusters * number of observations per cluster), and our between cluster variability is medium to low, all model types do a similarly good job of recovering the true data generating coefficients. In other words, under good conditions (lots of data that isn\u2019t too variable) we can\u2019t go wrong picking any of the analysis strategies. In the converse, going from bottom left to top right, when between cluster variability is high, we quickly see the importance of having more clusters rather than more observations per cluster; without enough clusters to observe, even a fully specified multi-level model does a poor job of recovering the true coefficients.", "When we have small to medium sized datasets and lots of between-cluster variability all models tend to do a poor job of recovering the true coefficients. Interestingly, having particularly few observations per cluster (left-most column) disproportionately affects two-stage-regression estimation (orange boxplots). This is consistent with Gelman, 2005 who suggests that with few per cluster observations, the first-level OLS estimates are pretty poor with high-variance and there are none of the multi-level modeling benefits to help offset the situation. This situation also seems to favor fully-specified multi-level models the most (green boxplots), particularly when between cluster variability is high. It\u2019s interesting to note that cluster-robust, and misspecified (simple) multi-level models seem to perform similarly in this situation.", "In medium data situations (middle column) cluster-robust models seem to do a slightly worse job across the board of recovering coefficients. This is most likely due to the fact that the estimates completely ignore the clustered nature of the data and have no smoothing/regularization applied to them either through averaging (in the case of the two-stage-regression models) or through random-effects estimation (in the case of the multi-level models).", "Finally, in the high observations per cluster situation (right-most column), all models seem to perform rather similarly suggesting that each modeling strategy is about as good as any other when we densely sampling the unit of interest (increasing number of observations per cluster) even if the desire is to make inferences about the clusters themselves.", "Next, let\u2019s look at both standard errors and t-statistics to see how our inferences might vary. The effect of increased between cluster variance has a very notable effect on SEs and t-stats values generally making it less likely to identify a statistically significant relationship regardless of the size of the data. Interestingly, what two-stage-regression models exhibit in terms of poorer coefficient recovery in situations with few observations per cluster, they make up for with higher standard error estimates. We can see that their t-statistics are low in these situations suggesting that in these situations this approach may tip the scales towards lower false-positive, higher false-negative inferences. However, unlike other model types, they do not necessarily benefit from more clusters overall (bottom left panel) and run the risk of an inflated level of false negatives. Misspecified multilevel models seem to have the opposite properties: they have higher t-stats and lower SEs in most situations with medium to high between-cluster variability and benefit the most from situations with a high number of observations per cluster. This suggests they might run the risk of introducing more false positives in situations where other models may behave more conservatively, but also may be more sensitive to detecting true relationships in the face of high between-cluster variance. They also seem to benefit most from increasing observations within cluster. Inferences from cluster-robust and full-specified multi-level models seem to be largely comparable which is consistent with the proliferate use of both these model types across multiple literatures.", "Finally, we can take a brief look at what situations most often cause convergence failures for our fully-specified multi-level models (note: the simple multi-level models examined earlier never failed to converge in these simulations). In general, this seems to occur when between cluster variability is low, or the number of observations per cluster is very small. This makes sense because even though the data were generated in a multi-level manner, clusters are quite similar and simplifying models by discarding terms which try to model variance that may not be exhibited by the data in a meaningful way (e.g. dropping \u201crandom-slopes\u201d) achieve better estimation overall. In other words, the model may be trying to fit a variance parameter that is small enough to cause it to run out of optimizer iterations before it reaches a suitably small change in error. This is like trying to find the lowest point on a \u201chill\u201d that has a very shallow declining slope by comparing the height of your current step to the height of your previous step.", "So what have we learned? Here are some intuitions that I think this exercise has helped flesh out:", "I don\u2019t want to end this post with the feeling that we\u2019ve figured everything out and are expert analysts now, but rather appreciate that there are some limitations to this exercise that are worth keeping in mind. While we can build some general intuitions, there are conditions under which these intuitions may not always hold and it\u2019s incredibly important to be aware of them:", "I hope this was useful for some folks out there and even if it wasn\u2019t, it certainly helped me build some intuitions about the different analysis strategies that are available. Moreover, I hope if nothing else, this might motivate people who feel like they have limited formal training in statistics/machine-learning to take a more tinker/hacker approach to their own learning. I remember when as a kid, breaking things and taking them apart was one of my favorite ways to learn about how things worked. With the mass availability of free and open-source tools like scientific Python and R, I see no reason why statistical education can\u2019t be the same.", "This is a nice quick guide that defines much of the terminology across different fields and reviews a lot of the concepts covered here (plus more) in a much more pithy way. For those interested, p-values for multi-level models were computed using the lmerTest R package using Satterthwaite approximation for degrees of freedom calculation; note that based on the random-effects structure specified, these degrees of freedom can change dramatically. P-values for other model types were computed using a standard t-distribution, but pymer4 also offers non-parametric permutation testing and boot-strapped confidence intervals for other styles of inference. At the time of this writing, fitting two-stage-regression models is only available in development branch on github, but should be incorporated in a new release in the future.", "Originally published at eshinjolly.com on February 18, 2019.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "PhD Candidate in Computational Social Neuroscience @ Dartmouth College || Data/Stats Nerd || eshinjolly.com"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Ff58033c28329&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----f58033c28329--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@eshjolly?source=post_page-----f58033c28329--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@eshjolly?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Eshin Jolly"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf073cc9e036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&user=Eshin+Jolly&userId=cf073cc9e036&source=post_page-cf073cc9e036----f58033c28329---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://pdfs.semanticscholar.org/fe95/9879dbf2b06f33b9ef07b67897135be57abf.pdf", "anchor_text": "Ugrinowitsch et al, 2004"}, {"url": "https://www.wikiwand.com/en/False_positives_and_false_negatives", "anchor_text": "false-positives"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/3615759", "anchor_text": "Vasey & Thayer, 1987"}, {"url": "https://www.wikiwand.com/en/Simpson%27s_paradox", "anchor_text": "Simpson\u2019s Paradox"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0749596X07001398", "anchor_text": "Baayen et al, 2008"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/", "anchor_text": "Barr, et al, 2013"}, {"url": "https://arxiv.org/abs/1506.04967", "anchor_text": "Bates, et al 2015"}, {"url": "https://www.sciencedirect.com/science/article/pii/S0749596X17300013", "anchor_text": "Matuschek et al, 2017"}, {"url": "https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html", "anchor_text": "this guide"}, {"url": "https://ourcodingclub.github.io/2017/03/15/mixed-models.html", "anchor_text": "this post"}, {"url": "https://psycnet.apa.org/record/2017-52405-001", "anchor_text": "Brauer & Curtin, 2018"}, {"url": "https://www.jaredknowles.com/journal/2013/11/25/getting-started-with-mixed-effect-models-in-r", "anchor_text": "ton"}, {"url": "http://www.bodowinter.com/tutorial/bw_LME_tutorial1.pdf", "anchor_text": "resources"}, {"url": "http://www.stat.columbia.edu/~gelman/arm/", "anchor_text": "available"}, {"url": "http://projects.iq.harvard.edu/files/gov2001/files/sesection_5.pdf", "anchor_text": "This lecture"}, {"url": "http://econweb.umd.edu/~sarzosa/teach/2/Disc2_Cluster_handout.pdf", "anchor_text": "this presentation"}, {"url": "http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf", "anchor_text": "Cameron & Miller, 2015"}, {"url": "https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780111304", "anchor_text": "Frison & Pocock, 1992"}, {"url": "https://www.fil.ion.ucl.ac.uk/spm/doc/papers/aph_rfx.pdf", "anchor_text": "Holmes & Friston, 1998"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/19463958/", "anchor_text": "Mumford & Nichols, 2009"}, {"url": "http://www.stat.columbia.edu/~gelman/research/published/459.pdf", "anchor_text": "Gelman, 2005"}, {"url": "http://eshinjolly.com/pymer4/", "anchor_text": "pymer4"}, {"url": "https://www.apa.org/research/action/multitask", "anchor_text": "switch cost"}, {"url": "https://cran.r-project.org/web/packages/lme4/index.html", "anchor_text": "lme4"}, {"url": "http://joss.theoj.org/papers/10.21105/joss.00862", "anchor_text": "Jolly, 2018"}, {"url": "https://www.wikiwand.com/en/Best_linear_unbiased_prediction", "anchor_text": "BLUPs"}, {"url": "http://www.stat.columbia.edu/~gelman/research/published/459.pdf", "anchor_text": "Gelman, 2005"}, {"url": "https://www.wikiwand.com/en/False_positives_and_false_negatives", "anchor_text": "false negatives"}, {"url": "https://towardsdatascience.com/gradient-descent-d3a58eb72404", "anchor_text": "find the lowest point"}, {"url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/", "anchor_text": "Barr et al, 2013"}, {"url": "http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf", "anchor_text": "Baayen et al, 2008"}, {"url": "https://www.stata.com/meeting/boston10/boston10_baum.pdf", "anchor_text": "This lecture"}, {"url": "https://www.nber.org/papers/t0327.pdf", "anchor_text": "this paper"}, {"url": "https://www.ncbi.nlm.nih.gov/pubmed/27620283", "anchor_text": "Luke, 2017"}, {"url": "https://www.reed.edu/economics/parker/s11/312/notes/Notes13.pdf", "anchor_text": "nice quick guide"}, {"url": "https://cran.r-project.org/web/packages/lmerTest/index.html", "anchor_text": "lmerTest R package"}, {"url": "http://eshinjolly.com/pymer4/usage.html", "anchor_text": "pymer4 also offers"}, {"url": "https://github.com/ejolly/pymer4/tree/dev", "anchor_text": "development branch on github"}, {"url": "https://www.wikiwand.com/en/Instrumental_variables_estimation#/Interpretation_as_two-stage_least_squares", "anchor_text": "instrumental variable estimation"}, {"url": "http://jonasobleser.com/", "anchor_text": "Jonas Oblesser"}, {"url": "http://www.senns.demon.co.uk/home.html", "anchor_text": "Stephen John Senn"}, {"url": "https://www.jepusto.com/", "anchor_text": "James E. Pustejovsky"}, {"url": "http://eshinjolly.com/2019/02/18/rep_measures/", "anchor_text": "eshinjolly.com"}, {"url": "https://medium.com/tag/data-science?source=post_page-----f58033c28329---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/statistics?source=post_page-----f58033c28329---------------statistics-----------------", "anchor_text": "Statistics"}, {"url": "https://medium.com/tag/machine-lear?source=post_page-----f58033c28329---------------machine_lear-----------------", "anchor_text": "Machine Lear"}, {"url": "https://medium.com/tag/multilevel?source=post_page-----f58033c28329---------------multilevel-----------------", "anchor_text": "Multilevel"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----f58033c28329---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&user=Eshin+Jolly&userId=cf073cc9e036&source=-----f58033c28329---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&user=Eshin+Jolly&userId=cf073cc9e036&source=-----f58033c28329---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----f58033c28329--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Ff58033c28329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----f58033c28329---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----f58033c28329--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----f58033c28329--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----f58033c28329--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----f58033c28329--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----f58033c28329--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@eshjolly?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@eshjolly?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Eshin Jolly"}, {"url": "https://medium.com/@eshjolly/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "22 Followers"}, {"url": "http://eshinjolly.com", "anchor_text": "eshinjolly.com"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf073cc9e036&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&user=Eshin+Jolly&userId=cf073cc9e036&source=post_page-cf073cc9e036--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2Fcf073cc9e036%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcomparing-common-analysis-strategies-for-repeated-measures-data-f58033c28329&user=Eshin+Jolly&userId=cf073cc9e036&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}