{"url": "https://towardsdatascience.com/advance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554", "time": 1683018439.228805, "path": "towardsdatascience.com/advance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554/", "webpage": {"metadata": {"title": "Advance BERT model via transferring knowledge from Cross-Encoders to Bi-Encoders | by Chien Vu | Towards Data Science", "h1": "Advance BERT model via transferring knowledge from Cross-Encoders to Bi-Encoders", "description": "Currently, the state-of-the-art architecture models for NLP usually reuse the BERT model which was pre-trained on large text corpora such as Wikipedia and the Toronto Books Corpus as the baseline\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/train-without-labeling-data-using-self-supervised-learning-by-relational-reasoning-b0298ad818f9", "anchor_text": "Self-Supervised Learning by Relational Reasoning", "paragraph_index": 3}, {"url": "https://sbert.net/datasets/stsbenchmark.tsv.gz", "anchor_text": "link", "paragraph_index": 15}, {"url": "https://sbert.net/datasets/quora-IR-dataset.zip", "anchor_text": "Quora Question Pairs", "paragraph_index": 24}, {"url": "https://www.linkedin.com/in/vumichien/", "anchor_text": "Linkedin", "paragraph_index": 28}], "all_paragraphs": ["Currently, the state-of-the-art architecture models for NLP usually reuse the BERT model which was pre-trained on large text corpora such as Wikipedia and the Toronto Books Corpus as the baseline [1]. By fine-tuning deep pre-trained BERT, a lot of alternative architectures were invented like DeBERT, RetriBERT, RoBERTa,\u2026 that achieved substantial improvements to the benchmarks on a variety of language understanding tasks. Among common tasks in NLP, pairwise sentence scoring has a wide number of applications in information retrieval, question answering, duplicate question detection, or clustering,... Generally, there are two typical approaches proposed: Bi-encoders and Cross-encoders.", "On the other hand, no methodology is perfect in all aspects and Bi-encoders is not an exception. The Bi-encoders method usually achieves lower performance compared with the Cross-encoders method and requires a large amount of training data. The reason is Cross-encoders can compare both inputs simultaneously, while the Bi-encoders have to independently map inputs to a meaningful vector space which requires a sufficient amount of training examples for fine-tuning.", "To solve this problem, Poly-encoders was invented [5]. Poly-encoders utilizes two separate transformers (similar to cross-encoders), but attention was applied between two inputs only at the top layer, resulting in better performance gains over Bi-encoders and large speed gains over Cross-encoders. However, Poly-encoders still have some drawbacks: they cannot be applied for tasks with symmetric similarity relations because of an asymmetrical score function and Poly-encoders representations cannot be efficiently indexed, causing issues for retrieval tasks with large corpora sizes.", "In this article, I want to introduce a new approach that can use both Cross-encoders and Bi-encoders in an effective way - data augmentation. This strategy is known as Augmented SBERT (AugSBERT) [6], which uses BERT cross-encoders to label a larger set of input pairs to augment the training data for SBERT bi-encoders. Then, SBERT bi-encoders is fine-tuned on this larger augmented training set, which yields a significant performance increase. The idea is very similar to Self-Supervised Learning by Relational Reasoning in Computer Vision. Therefore, in a simple way, we can think that it is Self-Supervised Learning in Natural Language Processing. For more details, it will be presented in the next section.", "There are three major scenarios for the Augmented SBERT approach for either pairwise-sentence regression or classification task.", "In this scenario, the straight forward data augmentation strategy is applied to prepare and extend the labeled dataset. There are three most common levels: Character, Word, Sentence.", "However, the word level is the most suitable one for the sentence pair task. Based on the performance of training Bi-Encoders, there are few suggested methodologies: Insert/substitute word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet) or substitute word by synonym (WordNet, PPDB). After creating the augmented text data, it is then combined with the original one and fit into Bi-Encoders.", "However, in the case of few labeled datasets or special cases, simple word replacement or increment strategies as shown are not helpful for data augmentation in sentence-pair tasks, even leading to worse performance compared to models without augmentation.", "In short, the straight forward data augmentation strategy involves three steps:", "In this case, because of the limited labeled datasets (gold dataset), the pre-trained Cross-encoders are used to weakly label the unlabeled data (same domain). However, randomly selecting two sentences usually leads to a dissimilar (negative) pair; while positive pairs are extremely rare. This skews the label distribution of the silver dataset heavily towards negative pairs. Therefore, the two appropriate sampling approaches are suggested:", "After that, the sampled sentence pairs will be weakly labeled by pre-trained Cross-encoders and be merged with the gold dataset. Then, Bi-encoders are trained on this extended training dataset. This model is called Augmented SBERT (AugSBERT). AugSBERT might improve the performance of existing Bi-encoders and reduce the difference with Cross-encoders.", "In summary, AugSBERT for a limited dataset involves three steps:", "This scenario happens when we want SBERT to attain high performance in different domain data (without annotation). Basically, SBERT fails to map sentences with unseen terminology to a sensible vector space. Hence, the relevant data augmentation strategy domain adaptation was proposed:", "Generally, AugSBERT benefits a lot when the source domain is rather generic and the target domain is rather specific. Vice-versa, when it goes from a specific domain to a generic target domain, only a slight performance increase is noted.", "In this experiment, I will introduce a demo on how to apply AugSBERT with different scenarios. First, we need to import some packages", "The main purpose of this scenario is extending the labeled dataset by the straight forward data augmentation strategies, therefore, we will prepare train, dev, test dataset on the Semantic Text Similarity dataset (link) and define batch size, epoch, and model name (You can specify any Huggingface/transformers pre-trained model)", "Then, we will insert words by our BERT model (you can apply another argumentation technique as I mentioned in the Technique highlight section) to create a silver dataset.", "Next, we define our Bi-encoders with mean pooling with both(gold + silver) STS benchmark dataset.", "Finally, we will evaluate our model in the test STS benchmark dataset.", "In this scenario, we will use Cross-encoders that were trained on the limited labeled dataset (gold dataset) to soft label the in-domain unlabeled dataset (silver dataset) and train Bi-encoders in both datasets (silver + gold). In this simulation, I also use again STS benchmark dataset and create new pairs of sentences by pre-trained SBERT model. First, we will define Cross-encoders and Bi-encoders.", "Step 1, we will prepare train, dev, test like before and fine-tune our Cross-encoders", "Step 2, we use our fine-tuned Cross-encoders to label unlabeled datasets.", "Step 3, we train our Bi-encoders in both gold and silver datasets", "Finally, we will evaluate our model in the test STS benchmark dataset.", "In this scenario, all the steps are very similar to scenario 2 but in a different domain. Because of the capability of our Cross-encoders, we will use a generic source dataset (STS benchmark dataset) and transfer the knowledge to a specific target dataset (Quora Question Pairs)", "Labeling Quora Question Pairs dataset (silver dataset). In this case, the task is classification so we have to convert our score to binary scores.", "Finally, evaluating on test Quora Question Pairs dataset", "AugSBERT is a simple and effective data augmentation to improve Bi-encoders for pairwise sentence scoring tasks. The idea is based on labeling new sentence pairs by using pre-trained Cross-encoders and combining them into the training set. Selecting the right sentence pairs for soft-labeling is crucial and necessary to improve the performance. The AugSBERT approach can also be used for domain adaptation, by soft-labeling data on the target domain.", "You can contact me if you want further discussion. Here is my Linkedin", "[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding.", "[2] Thomas Wolf, Victor Sanh, Julien Chaumond, and Clement Delangue. Transfertransfo: A transfer learning approach for neural network-based conversational agents.", "[3] Pierre-Emmanuel Mazare, Samuel Humeau, Martin Raison, and Antoine Bordes. Training millions of personalized dialogue agents.", "[4] Nils Reimers and Iryna Gurevych. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks.", "[5] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring.", "[6] Nandan Thakur, Nils Reimers, Johannes Daxenberge, and Iryna Gurevych. Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F3e0fc564f554&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@chienvu?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chienvu?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Chien Vu"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2928e8b6c04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&user=Chien+Vu&userId=f2928e8b6c04&source=post_page-f2928e8b6c04----3e0fc564f554---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@timmossholder?utm_source=medium&utm_medium=referral", "anchor_text": "Tim Mossholder"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://towardsdatascience.com/a-complete-guide-to-transfer-learning-from-english-to-other-languages-using-sentence-embeddings-8c427f8804a9", "anchor_text": "SBert Bi-encoders"}, {"url": "https://towardsdatascience.com/train-without-labeling-data-using-self-supervised-learning-by-relational-reasoning-b0298ad818f9", "anchor_text": "Self-Supervised Learning by Relational Reasoning"}, {"url": "https://sbert.net/datasets/stsbenchmark.tsv.gz", "anchor_text": "link"}, {"url": "https://sbert.net/datasets/quora-IR-dataset.zip", "anchor_text": "Quora Question Pairs"}, {"url": "https://www.linkedin.com/in/vumichien/", "anchor_text": "Linkedin"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----3e0fc564f554---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/transfer-learning?source=post_page-----3e0fc564f554---------------transfer_learning-----------------", "anchor_text": "Transfer Learning"}, {"url": "https://medium.com/tag/data-augmentation?source=post_page-----3e0fc564f554---------------data_augmentation-----------------", "anchor_text": "Data Augmentation"}, {"url": "https://medium.com/tag/bert?source=post_page-----3e0fc564f554---------------bert-----------------", "anchor_text": "Bert"}, {"url": "https://medium.com/tag/editors-pick?source=post_page-----3e0fc564f554---------------editors_pick-----------------", "anchor_text": "Editors Pick"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&user=Chien+Vu&userId=f2928e8b6c04&source=-----3e0fc564f554---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&user=Chien+Vu&userId=f2928e8b6c04&source=-----3e0fc564f554---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F3e0fc564f554&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----3e0fc564f554---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----3e0fc564f554--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----3e0fc564f554--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----3e0fc564f554--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chienvu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chienvu?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Chien Vu"}, {"url": "https://medium.com/@chienvu/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "141 Followers"}, {"url": "https://www.linkedin.com/in/vumichien/", "anchor_text": "https://www.linkedin.com/in/vumichien/"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff2928e8b6c04&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&user=Chien+Vu&userId=f2928e8b6c04&source=post_page-f2928e8b6c04--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1f7bb0947485&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadvance-nlp-model-via-transferring-knowledge-from-cross-encoders-to-bi-encoders-3e0fc564f554&newsletterV3=f2928e8b6c04&newsletterV3Id=1f7bb0947485&user=Chien+Vu&userId=f2928e8b6c04&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}