{"url": "https://towardsdatascience.com/natural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a", "time": 1683006292.9758358, "path": "towardsdatascience.com/natural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a/", "webpage": {"metadata": {"title": "NLP: From Watermelon Boxes to Word Embeddings | Towards Data Science", "h1": "NLP: From Watermelon Boxes to Word Embeddings", "description": "It\u2019s a pretty well-known fact that humans suck at being random. Whether or not we realize it, everything we do tends to have some pattern to it. The languages we use to communicate are no exception\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/datadriveninvestor/the-basics-of-neural-networks-304364b712dc", "anchor_text": "this article", "paragraph_index": 11}, {"url": "https://blog.goodaudience.com/writing-fairy-tales-using-ai-a64d0ecb1ddc", "anchor_text": "here", "paragraph_index": 20}], "all_paragraphs": ["It\u2019s a pretty well-known fact that humans suck at being random. Whether or not we realize it, everything we do tends to have some pattern to it.", "The languages we use to communicate are no exception to this rule. All languages follow some set pattern of rules called syntax.", "A large chunk of the deep learning industry is focused on something called Natural Language Processing (NLP). NLP is a subfield of artificial intelligence focused on training computers to process human language.", "Natural language processing started off in the 1950s when a researcher from Harvard created a machine called a \u201cwatermelon box\u201d.", "It did one simple thing\u2026. it could detect when a person said the word watermelon.", "The person who created it thought that the future of NLP was creating thousands of boxes that each detected a single word \u2026.", "This sounds pretty laughable now, but it was a pretty important invention back then.", "We\u2019ve come a long way since then and the watermelon box is now long forgotten. In fact, it was hard to find any information about them online without thousands of these square watermelons popping up:", "Now, NLP is used everywhere. Whether you\u2019re using Google Translate, Siri, or even Grammarly \u2014 There\u2019s an AI using NLP in the background to understand whatever language you\u2019re communicating in.", "So the big question is: How did neural networks get so good at understanding human language?", "Let\u2019s start with how a classic fully connected neural network would approach this problem.", "If you want to learn how neural networks work, check out this article.", "First of all, a neural network can only accept numbers as inputs. So you can\u2019t just give it a bunch of text to process. You have to convert it into a numerical value.", "The initial way this was solved was through indexing. Basically each word would be represented by its own unique number. This number is then one-hot encoded.", "This is the equivalent of giving someone who knows nothing about chemistry a single cell of a basic periodic table.", "Just as each element on the table has a unique atomic number, each word a neural network sees has a unique index.", "The position of an element on the periodic table provides a lot of information. Obviously, with just a single cell of the table, you lose out on that valuable information.", "Similarly, the problem with simple fully connected neural networks is that the AI has no context for the word.", "It doesn\u2019t know what words appeared before the one its looking at, so you can\u2019t expect it to do anything valuable with that word.", "A lot of improvements in NLP started with recurrent neural networks (RNNs). These are great at processing a series of data (like sentences) because they use memory to use context.", "You can learn how Recurrent Neural Networks work here.", "Using our chemistry analogy, it's the same as giving you the entire periodic table. You now have a better chance at understanding trends based on the row and column an element is in.", "Similarly, an RNN can use the word around the one it is analyzing to understand the context and make better predictions.", "RNNs were a major advancement that enabled better text generation and analysis.", "Since then there have been even more advances in NLP with various research papers on new training optimizers, network architectures, etc.", "But out of all of them, one of the biggest wins for NLP was the use of embedding layers like Word2Vec.", "With RNNs, an algorithm can identify the context of a word, but it doesn\u2019t truly understand each word. It just sees a random index number.", "The neural network is missing out on so much information about each word.", "Each word in any language has several properties that we subconsciously analyze when understanding that language. A word could be plural, be a noun, have a certain verb tense, etc.", "Word2Vec does exactly what it's named. It takes a word and converts it into a vector representation:", "I\u2019m going to stay loyal to my periodic table analogy and show the whole process of using embedding layers like Word2Vec to process word.", "Indexing Helium gives its unique numerical representation: its atomic number.", "Passing that number through a trained Word2Vec layer gives us the vector representation of Helium. This would be the atomic weight, electron configuration, physical state, etc.", "Here\u2019s a visualization I created to summarize the periodic table analogy of Word2Vec and RNNs impact on Natural Language Processing:", "One of the coolest parts of the vectors produced by embedding layers like Word2Vec is that you can visualize and quantify the relationships between words.", "The dimensionality of the embedding vectors depends on the embedding size (something you as a programmer decide on).", "When any word is converted into a vector, its dimensions are equal to the embedding size.", "You can then plot those vectors on a graph and visualize the relationships between various words.", "Let\u2019s take an example with an embedding size of 3. It will produce 3D vectors that we can plot on a cartesian plane:", "In practice, embedding layers are much larger than this, but you can\u2019t visualize more than 3 dimensions of space.", "You can immediately see some trends in the location of these vectors. Words that are similar have close positions in vector space. I\u2019ve color-coded the vector heads to clarify the clusters in the example:", "What\u2019s even more interesting is that there\u2019s also meaning behind the exact vector transformation between words.", "Each arrow colour is a certain vector transformation that represents the relationship between words.", "Here\u2019s a summary of the vector transformations in this example and the relationships they show:", "Word2Vec embeds words by learning to predict words from training sentences.", "The idea is that the neural network will learn to assign similar weights to words used in a similar way.", "The blank word could easily be any noun like frog, man, dog etc.", "Because of that, similar words will have similar weight values connecting them to each node in the embedding layer.", "You can measure the similarity between words using cosine distances. Related words will have small cosine distances.", "Since the words are one-hot encoded, the weight of the embedding layer turns into a lookup table where we can find the vector representation of that word.", "It's like a dictionary, you provide the index of the word you want to embed and its looks in the corresponding row in the weight matrix.", "Even within Word2Vec, there are multiple types: Continuous Bag of Words (CBOW) and Skip-gram.", "In CBOW, you take a certain window of words in a sentence and try to fill in the blank. CBOW takes context and generates a word.", "Skip-gram does the opposite \u2014 it takes a single word and guesses the words that appear around it. Skip-gram takes a word and generates its context.", "Intuitively, you\u2019d think that CBOW is easier to implement than Skip-gram since the ratio between it gives you more information and you have less to guess.", "The interesting thing is the Skip-gram is the preferred way of implementing Word2Vec. It just seems to work better.", "Beyond Word2Vec, there have been some more interesting advancements in embedding layers.", "Another type of embedding called Doc2Vec creates vector representations of entire paragraphs and as the name suggests \u2026 documents.", "Your home for data science. A Medium publication sharing concepts, ideas and codes."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4eb32e7dfd8a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@mayankja1n?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayankja1n?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Mayank Jain"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6808cd502b6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&user=Mayank+Jain&userId=6808cd502b6a&source=post_page-6808cd502b6a----4eb32e7dfd8a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/dair-ai/deep-learning-for-nlp-an-overview-of-recent-trends-d0d8f40a776d", "anchor_text": "Elvis"}, {"url": "http://www.melonmold.com/product/square-watermelon-box", "anchor_text": "melonmold.com"}, {"url": "https://www.igeeksblog.com/how-to-activate-siri-on-iphone-x/", "anchor_text": "igeeksblog.com"}, {"url": "https://medium.com/datadriveninvestor/the-basics-of-neural-networks-304364b712dc", "anchor_text": "this article"}, {"url": "https://blog.goodaudience.com/writing-fairy-tales-using-ai-a64d0ecb1ddc", "anchor_text": "here"}, {"url": "https://www.udacity.com/", "anchor_text": "Udacity"}, {"url": "https://machinelearningmastery.com/what-are-word-embeddings/", "anchor_text": "machinelearningmastery.com"}, {"url": "https://mayankjain.ca/blog-posts/17", "anchor_text": "Mayank Jain - NLP: From Watermelon Boxes to Word EmbeddingsIt's a pretty well-known fact that humans suck at being random. Whether or not we realize it, everything we do tends to\u2026mayankjain.ca"}, {"url": "https://medium.com/@mayankj2112", "anchor_text": "Medium"}, {"url": "https://www.linkedin.com/in/mayankj2112/", "anchor_text": "LinkedIn"}, {"url": "https://www.subscribepage.com/mayank", "anchor_text": "Sign up for my Monthly Update!Sign up to keep up with my monthly progress including projects, conferences, articles and more ...www.subscribepage.com"}, {"url": "https://medium.com/tag/nlp?source=post_page-----4eb32e7dfd8a---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/word2vec?source=post_page-----4eb32e7dfd8a---------------word2vec-----------------", "anchor_text": "Word2vec"}, {"url": "https://medium.com/tag/word-embeddings?source=post_page-----4eb32e7dfd8a---------------word_embeddings-----------------", "anchor_text": "Word Embeddings"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----4eb32e7dfd8a---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----4eb32e7dfd8a---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&user=Mayank+Jain&userId=6808cd502b6a&source=-----4eb32e7dfd8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&user=Mayank+Jain&userId=6808cd502b6a&source=-----4eb32e7dfd8a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F4eb32e7dfd8a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----4eb32e7dfd8a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----4eb32e7dfd8a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayankja1n?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@mayankja1n?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Mayank Jain"}, {"url": "https://medium.com/@mayankja1n/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "108 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6808cd502b6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&user=Mayank+Jain&userId=6808cd502b6a&source=post_page-6808cd502b6a--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F6808cd502b6a%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fnatural-language-processing-from-watermelon-boxes-to-word-embeddings-4eb32e7dfd8a&user=Mayank+Jain&userId=6808cd502b6a&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}