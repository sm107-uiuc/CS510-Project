{"url": "https://towardsdatascience.com/the-importance-of-being-recurrent-29c65cfa456f", "time": 1682994192.143114, "path": "towardsdatascience.com/the-importance-of-being-recurrent-29c65cfa456f/", "webpage": {"metadata": {"title": "The Importance of Being Recurrent for Modeling Hierarchical Structure | by Nathaniel Watkins | Towards Data Science", "h1": "The Importance of Being Recurrent for Modeling Hierarchical Structure", "description": "Recurrent Neural Networks , such as LSTMs, currently have performance limitations, while newer methods like FAN transformers show potential. We compare both approaches and find a clear winner."}, "outgoing_paragraph_urls": [{"url": "https://www.linkedin.com/in/ketran/", "anchor_text": "Ke Tran", "paragraph_index": 1}, {"url": "https://www.linkedin.com/in/arianna-bisazza-92754329/", "anchor_text": "Arianna Bisazza", "paragraph_index": 1}, {"url": "https://www.linkedin.com/in/christof-monz-01b4a54/", "anchor_text": "Christof Monz", "paragraph_index": 1}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "inefficient and not scalable", "paragraph_index": 3}, {"url": "https://arxiv.org/abs/1511.05552", "anchor_text": "great promise", "paragraph_index": 3}, {"url": "https://www.quora.com/What-is-FPGA-How-does-that-work", "anchor_text": "Field Programmable Gate Arrays", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Inference", "anchor_text": "logical inference", "paragraph_index": 4}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need", "paragraph_index": 5}, {"url": "https://towardsdatascience.com/introduction-to-sequence-models-rnn-bidirectional-rnn-lstm-gru-73927ec9df15", "anchor_text": "this post", "paragraph_index": 5}, {"url": "http://www.linkedin.com/in/grohith327", "anchor_text": "Rohith Gandhi", "paragraph_index": 5}, {"url": "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need", "anchor_text": "the FAN architecture is highly parallelizable", "paragraph_index": 8}, {"url": "https://arxiv.org/pdf/1806.09828.pdf", "anchor_text": "bidirectional LSTM (bi-LSTM) with an attention layer", "paragraph_index": 9}, {"url": "https://twitter.com/rctatman", "anchor_text": "Rachel Tatman", "paragraph_index": 10}, {"url": "https://arxiv.org/pdf/1506.04834.pdf", "anchor_text": "Bowman et al. (2015b)", "paragraph_index": 18}, {"url": "https://github.com/ketranm/fan_vs_rnn", "anchor_text": "https://github.com/ketranm/fan_vs_rnn", "paragraph_index": 20}, {"url": "https://nvidia.github.io/OpenSeq2Seq/html/machine-translation/convs2s.html", "anchor_text": "Convs2S", "paragraph_index": 21}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "the paper", "paragraph_index": 22}, {"url": "https://www.meetup.com/PSPPython", "anchor_text": "Puget Sound Programming Python (PuPPy) Meetup", "paragraph_index": 23}, {"url": "https://twitter.com/rctatman", "anchor_text": "Rachel Tatman", "paragraph_index": 23}, {"url": "http://www.rctatman.com/talks/atom-rnn", "anchor_text": "great talk", "paragraph_index": 23}], "all_paragraphs": ["The Importance of Being Recurrent for Modeling Hierarchical Structure", "by Ke Tran, Arianna Bisazza & Christof Monz found here:", "Recurrent Neural Networks (RNNs), such as Long Short-Term Memory networks (LSTMs), currently have performance limitations, while newer methods such as Fully Attentional Networks (FANs) show potential for replacing LSTMs without those same limitations. So the authors set out to compare the two approaches using standardized methods and found that LSTMs universally surpass FANs in prediction accuracy when applied to the hierarchy structure of language.", "For a while, it seemed that RNN\u2019s were taking the Natural Language Processing (NLP) world by storm (from about 2014\u201317). However, we\u2019ve recently started realizing the limitations of RNN\u2019s, primarily that they are \u201cinefficient and not scalable\u201d. While there is great promise in overcoming these limitations by using more specialized processing hardware, such as Field Programmable Gate Arrays, solutions are at least one hardware generation away. This means that it\u2019s worth exploring other options, such as Convolutional Neural Networks (CNN) or Transformers, for text comprehension to see if we can achieve similar or better results using another technique that is more optimal with the current status quo in hardware.", "Due to this need, the authors of this paper have elected to benchmark test two promising methods of Natural Language Understanding (NLU), comparing the results between the two using objective criteria. Specifically, they gauged how well the models seemed to understand the hierarchical nature of language by testing relationships between subject and verb as well as testing the models on logical inference tasks.", "Recurrent Neural Networks and more specifically Long Short-Term Memory networks are the gold-standard when it comes to NLP/NLU. So the authors start there as the baseline, then compare it to a Fully Attentional Network: a new model architecture from the cutting edge paper Attention is all you need. An LSTM is a sequential framework that takes inputs in one at a time, such as reading a sentence one word at a time; it varies from other RNNs by each node remembering dependencies for a longer period of time. See this post by Rohith Gandhi for an explanation of RNNs and some popular variants.", "While LSTMs have a longer term memory than other RNNs, hence the Long in LSTM, they still struggle when there is a large distance between relevant data points such as the subject and the verb in the next sentence. Furthermore, LSTMs, due to their directional nature and consistent tweaking of their cell state at each time step, tend to have trouble when the context of an earlier portion of a sentence is dependant on information in a later part of a sentence. An example from Rohith\u2019s post above:", "\u201cHe said, Teddy bears are on sale\u201d and \u201cHe said, Teddy Roosevelt was a great President\u201d. In the above two sentences, when we are looking at the word \u201cTeddy\u201d and the previous two words \u201cHe said\u201d, we might not be able to understand if the sentence refers to the President or Teddy bears", "Enter the FAN transformer, which solves these problems by looking at the entire input, such as the entire sentence, at once, instead of sequentially, and it features an attention layer that helps preserve the context between relevant data points, no matter the distance. On top of these gains, the FAN architecture is highly parallelizable, which helps it overcome or avoid the aforementioned performance limitations of RNNs.", "Currently, the state of the art in sentence-embedding models is a bidirectional LSTM (bi-LSTM) with an attention layer, which was published after this subject paper was published, but bi-LSTMs and attention layers have been well developed by the release of this paper. Bidirectional LSTMs are basically 2 LSTMs (one reading the text left to right, and the other reading right to left) that compare notes to make a collective prediction. An attention layer is similar to described above for a FAN, but in an RNN, it sits outside of the sequential portions of the model allowing much more context to be preserved between time-steps. The authors chose to use a plain LSTM, without either of these upgrades (which address all the previously mentioned downsides, except for parallelization), but that turned out not to matter much, as the LSTM still achieved better accuracy than the FAN transformer.", "While the Attention is all you need paper focused on general language to language translation performance, the authors of this paper chose to study the models\u2019 comprehension of hierarchy in language. Hierarchy is critical to truly understanding the meaning of a sentence and is a necessary step to achieving anything near human-level NLU. Here are some examples highlighting the difficulty in understanding hierarchy, even for humans, and how it changes the meaning of sentences, from a talk on this paper presented by Rachel Tatman:", "\u201cI saw the person with the binoculars\u201d", "I put the keys on the stand, on the table, by the couch, next to the desk\u2026\u201d", "One big way to ensure that a model is understanding hierarchy is to make sure that it predicts an appropriate singular/plural verb given a singular/plural subject. The chart above shows an example sentence, including the input and how the verb plurality might be used in training the model and how it might be used in testing the prediction accuracy.", "For this task, the LSTM outperformed the FAN in 3 of the tests and tied on the 3rd objective.", "Note that the \u201cdistance\u201d mentioned above is the number of words in between the subject and the verb. And the \u201cattractors\u201d are the number of nouns between the subject and verb, which might throw off the model\u2019s understanding of what word is the subject. Another example by Rachel:", "The bus always comes late. | Attractors = 0", "The bus with broken windows always comes late. | Attractors = 1", "To avoid getting too hung up on the subtleties and variations that using sample text might introduce, the authors utilized a simplified language from Bowman et al. (2015b) using just six word types, three logic operators and some symbols to perform this task. They generated the training and test dataset using a rule-based system to ensure 100% accuracy of the data. While the letters and symbols in the paper might not seem to make much sense, this example should help:", "And once again, we see the plain vanilla LSTM match or exceed the FAN on all accounts. Note that n is the number of operators on the left side of the equations.", "The paper doesn\u2019t venture to answer or even explore why these results were observed. They do a fantastic job explaining their process and detailing the hyperparameters used for repeatability, but they seem to purposefully avoid taking it further than pure empiricism. Furthermore, their code is clean, readable and documented so that you can try it all yourself: https://github.com/ketranm/fan_vs_rnn", "Some interesting opportunities to explore these results further could include tuning the transformer (such as trying a weighted transformer and/or experimenting with more hyperparameters) or adding a 3rd architecture for comparison, such as Convs2S a Convolutional Neural Network based framework.", "In theory, a Fully Attentional Network transformer seems like it should outperform a simple Long Short-Term Memory network on all accounts, but through thorough testing, this doesn\u2019t seem to be the case. While we should continue researching and tweaking FANs to explore this new option, we shouldn\u2019t discount the venerable LSTM just yet. Please read the paper for more detail.", "If this type of thing interests you, and you\u2019re in the Seattle area, I highly encourage you to join the Puget Sound Programming Python (PuPPy) Meetup for our monthly Advanced Topics on Machine Learning (AToM) discussion night, which inspired me to write this (Thanks to Rachel Tatman who recently gave a great talk on this paper). And if you\u2019re not in the area, I recommend looking for, or even starting, something similar wherever you are.", "This type of article is a new thing I\u2019m going to be doing, born out of an apparent lack of approachable content that can quickly inform anyone about the status of cutting-edge AI research, with some background to see it within context. So I look forward to hearing any feedback or questions you have on this article or the topics discussed, either in the responses here or on social media. Feel free to connect with me (just let me know that you saw this article) -->", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "A dedicated manager turned Data Scientist, enthusiastic for the innovative ways technology can impact people."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F29c65cfa456f&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@theNathanielW?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theNathanielW?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Nathaniel Watkins"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98746e78b8a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&user=Nathaniel+Watkins&userId=98746e78b8a8&source=post_page-98746e78b8a8----29c65cfa456f---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@nshuman1291?utm_source=medium&utm_medium=referral", "anchor_text": "Nathaniel Shuman"}, {"url": "https://unsplash.com?utm_source=medium&utm_medium=referral", "anchor_text": "Unsplash"}, {"url": "https://www.linkedin.com/in/ketran/", "anchor_text": "Ke Tran"}, {"url": "https://www.linkedin.com/in/arianna-bisazza-92754329/", "anchor_text": "Arianna Bisazza"}, {"url": "https://www.linkedin.com/in/christof-monz-01b4a54/", "anchor_text": "Christof Monz"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "aclweb.org/anthology/D18-1503"}, {"url": "https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0", "anchor_text": "inefficient and not scalable"}, {"url": "https://arxiv.org/abs/1511.05552", "anchor_text": "great promise"}, {"url": "https://www.quora.com/What-is-FPGA-How-does-that-work", "anchor_text": "Field Programmable Gate Arrays"}, {"url": "https://en.wikipedia.org/wiki/Inference", "anchor_text": "logical inference"}, {"url": "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf", "anchor_text": "Attention is all you need"}, {"url": "https://towardsdatascience.com/introduction-to-sequence-models-rnn-bidirectional-rnn-lstm-gru-73927ec9df15", "anchor_text": "this post"}, {"url": "http://www.linkedin.com/in/grohith327", "anchor_text": "Rohith Gandhi"}, {"url": "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need", "anchor_text": "the FAN architecture is highly parallelizable"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "subject paper"}, {"url": "https://arxiv.org/pdf/1806.09828.pdf", "anchor_text": "bidirectional LSTM (bi-LSTM) with an attention layer"}, {"url": "https://twitter.com/rctatman", "anchor_text": "Rachel Tatman"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "subject paper"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "subject paper"}, {"url": "https://arxiv.org/pdf/1506.04834.pdf", "anchor_text": "Bowman et al. (2015b)"}, {"url": "https://arxiv.org/pdf/1506.04834.pdf", "anchor_text": "Bowman et al (2015b)"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "subject paper"}, {"url": "https://github.com/ketranm/fan_vs_rnn", "anchor_text": "https://github.com/ketranm/fan_vs_rnn"}, {"url": "https://nvidia.github.io/OpenSeq2Seq/html/machine-translation/convs2s.html", "anchor_text": "Convs2S"}, {"url": "http://aclweb.org/anthology/D18-1503", "anchor_text": "the paper"}, {"url": "https://www.meetup.com/PSPPython", "anchor_text": "Puget Sound Programming Python (PuPPy) Meetup"}, {"url": "https://twitter.com/rctatman", "anchor_text": "Rachel Tatman"}, {"url": "http://www.rctatman.com/talks/atom-rnn", "anchor_text": "great talk"}, {"url": "https://twitter.com/theNathanielW", "anchor_text": "twitter.com/theNathanielW"}, {"url": "https://www.linkedin.com/in/theNathanielWatkins/", "anchor_text": "linkedin.com/in/theNathanielWatkins"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----29c65cfa456f---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/research?source=post_page-----29c65cfa456f---------------research-----------------", "anchor_text": "Research"}, {"url": "https://medium.com/tag/nlp?source=post_page-----29c65cfa456f---------------nlp-----------------", "anchor_text": "NLP"}, {"url": "https://medium.com/tag/recurrent-neural-network?source=post_page-----29c65cfa456f---------------recurrent_neural_network-----------------", "anchor_text": "Recurrent Neural Network"}, {"url": "https://medium.com/tag/naturallanguageprocessing?source=post_page-----29c65cfa456f---------------naturallanguageprocessing-----------------", "anchor_text": "Naturallanguageprocessing"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&user=Nathaniel+Watkins&userId=98746e78b8a8&source=-----29c65cfa456f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&user=Nathaniel+Watkins&userId=98746e78b8a8&source=-----29c65cfa456f---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F29c65cfa456f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----29c65cfa456f---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----29c65cfa456f--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----29c65cfa456f--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----29c65cfa456f--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theNathanielW?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@theNathanielW?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Nathaniel Watkins"}, {"url": "https://medium.com/@theNathanielW/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "23 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F98746e78b8a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&user=Nathaniel+Watkins&userId=98746e78b8a8&source=post_page-98746e78b8a8--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fdb07dded9455&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-importance-of-being-recurrent-29c65cfa456f&newsletterV3=98746e78b8a8&newsletterV3Id=db07dded9455&user=Nathaniel+Watkins&userId=98746e78b8a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}