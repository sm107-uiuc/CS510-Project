{"url": "https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d", "time": 1682997405.301933, "path": "towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d/", "webpage": {"metadata": {"title": "Debating the AI Safety Debate. Geoffrey Irving, Paul Christiano and\u2026 | by Alex Moltzau | Towards Data Science", "h1": "Debating the AI Safety Debate", "description": "As I am moving into the area of AI Safety within the field of artificial intelligence (AI) I find myself both confused and perplexed. Where do you even start? I covered the financial developments in\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@alexmoltzau/openai-or-closedai-fae7bdd0fcff?source=friends_link&sk=4b572783e9ff49add07e4aef3ded34c3", "anchor_text": "OpenAI yesterday", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1805.00899.pdf", "anchor_text": "AI safety via debate", "paragraph_index": 0}, {"url": "https://openai.com/", "anchor_text": "OpenAI", "paragraph_index": 1}, {"url": "http://research.google.com/teams/brain", "anchor_text": "Google Brain", "paragraph_index": 1}, {"url": "https://naml.us/paper/", "anchor_text": "extensive collection of papers", "paragraph_index": 1}, {"url": "https://www.fhi.ox.ac.uk/team/paul-christiano/", "anchor_text": "Future of Humanity Institute", "paragraph_index": 2}, {"url": "https://scholar.google.com/citations?user=B7oP0bIAAAAJ&hl=en", "anchor_text": "Concrete Problems in AI Safety", "paragraph_index": 2}, {"url": "https://debate-game.openai.com", "anchor_text": "https://debate-game.openai.com", "paragraph_index": 11}, {"url": "https://arxiv.org/abs/1606.06565", "anchor_text": "Concrete problems in AI safety", "paragraph_index": 29}, {"url": "https://www.theverge.com/2019/4/13/18309459/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion", "anchor_text": "another recent statement", "paragraph_index": 31}, {"url": "http://www.nora.ai", "anchor_text": "www.nora.ai", "paragraph_index": 40}, {"url": "http://twitter.com/AlexMoltzau", "anchor_text": "twitter.com/AlexMoltzau", "paragraph_index": 40}], "all_paragraphs": ["As I am moving into the area of AI Safety within the field of artificial intelligence (AI) I find myself both confused and perplexed. Where do you even start? I covered the financial developments in OpenAI yesterday, and they are one of the foremost authorities on AI Safety. As such I thought it would be interesting to look at one of their papers. The paper that I will be looking at is called AI safety via debate published October 2018. You can of course read the article yourself in arXiv, and critique my article in turn; that would be the ideal situation. This debate about AI debates is of course ongoing.", "Geoffrey Irving is a member of the AI safety team at OpenAI. He has been working at Google Brain previously. He has an extensive collection of papers ranging from simulation, match, TensorFlow.", "Paul Christiano works on AI alignment. He is a member of the safety team at OpenAI. He is also a research associate at the Future of Humanity Institute. One of his most cited papers is on Concrete Problems in AI Safety.", "Dario Amodei is the research director at OpenAI. Previously he was a senior research scientist at Google and he was a postdoctoral scholar at Stanford University School of Medicine, where he worked on applications of mass spectrometry to network models of the cellular proteome as well as to the search for cancer biomarkers.", "The paper proposes training agents via self play on a zero sum debate game.", "Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information", "This is due to the need of learning complex human goals and preferences. What challenges do we have in making AI remain safe? They list a few:", "The authors argue that a human may at times be unable to judge an answer. The behaviour could be too hard to understand or the question itself could be flawed. They ask the reader to imagine a system that both give answers and point out flaws. When a system points out flaws the process or judgement of what is flawed could be wrong.", "Their eventual goal is natural language debate, where the human judges a dialog between the agents.", "Their paper is structured in the following way:", "In TDM question shown to both agents, both agents state answers and human decides who wins. The authors claim that in a debate game it is harder to lie than refute a lie. They make the statement that shorter debates are powerful. However there are many tasks that are more complicated. They list a series of directions in which they can improve the model:", "In the experiment they prespecify one agent as honest and one as liar. They also trained another instance on the advantage of being honest. For the paper there was built a website called https://debate-game.openai.com. On this page the experiments can be tried.", "An interesting sentence that I observed in the paper was regarding honesty: \u201cTo preserve honest behavior, the human judge should be instructed to reward agents that admit ignorance and successfully argue that ignorance is justified.\u201d There is however as the paper argues a balance issue to admitting ignorance: debate is useless if optimal play is to argue we can never know anything for certain.", "\u201cThere is no adversary rewarded for fooling the human, which is good because (1) we do not know what that means a priori and (2) we are not explicitly training agents for deception.\u201d", "The authors believe existing adversarial debates between humans are a useful analogy. Legal arguments in particular include domain experts explaining details of arguments to human judges or juries with no domain knowledge.", "Debate could fail as an approach to AI alignment, and they list a few reasons why. One is: maybe humans believe whatever they want to hear? If one actor opens with a false statement that the human already believes, will any argument by the other actor convince them otherwise?", "There is the other question of whether humans will understand the debates. They raise the question of whether an actual human can play the game well enough that optimal play is honest behaviour. Will one of the agents be able to sneak in a statement that appears logical but contains a hidden flaw without the other agent pointing out the flaw?", "Humans as sufficient judges is an empirical question. If the answer is no for we can ask if the model fails at honest admission of ignorance. The authors question whether honesty is the best perspective:", "Will agents trained for debate lose performance? Direct training could be harder; adverserial is a good way to think; we may not want answers that cannot be explained. Debates could be uncompetitive with other approaches, because debate requires human input. They suggests: \u201c\u2026training models of human judges and using those for sampling, but competing with purely simulated environments may still be challenging.\u201d", "Security vulnerabilities: A sufficiently strong misaligned AI may be able to convince a human to do dangerous things via a text channel", "AI Safety Dichotomy: we are safer if the agents stay honest throughout training, but we are also safer if debate works well enough that sudden large defections are corrected.", "A sufficiently strong misaligned Alice could hack the debate environment from inside so that the game is no longer zero sum.", "This was my favourite title and it had an exclamation mark so I decided to quote it word by word in this heading. Is training for maximum convincing is safe?", "The debate model relies on agents being approximately matched in strength. Symmetry is proposed: (1) if one player moves the other does; (2) comparing two games; (3) Komi (from Go) the second player gets extra points to counter the first player advantage.", "Imperfect information game is a challenge. Additionally a difficulty with many agents is: \u201c\u2026that the human must be able to judge whether a statement in the middle of a debate is good or bad, which may be much harder than judging an overall debate.\u201d", "They summarise debate and amplification as:", "They suggest to move amplification closer to debate by training the questioner adversarially. The authors state: \u201cWe can move debate closer to amplification by training debaters on statements provided by humans, corresponding to injecting demonstrations into RL.\u201d", "Future areas of work would include: richer theoretical models; human experiments that test value judgements; ML experiments approximating the human aspect of a debate; natural language debate; and interaction between debate and other safety methods.", "As a final side note I would say that these thoughts may have led to or influenced other discussions in the field. In a more recent paper called Machine Behaviour published the 24th of April 2019 from MIT Media Lab a collection of scientists call for the study of:\u201c\u2026a broad scientific research agenda to study machine behaviour that incorporates and expands upon the discipline of computer science and includes insights from across the sciences.\u201d", "Dario Amodei, one of the authors of the paper I looked through today was referenced in the machine behaviour article with his work: Concrete problems in AI safety. The word safety is often mentioned in the context of machine behaviour, and it makes perfect sense seeing the dilemmas that are arising from honest or lying actors.", "To be honest I find it hard to debate the debate. This may seem an obvious conclusion, however the complexity involved far surpasses my abilities or comprehension. By going through this article one thing that I am sure of is that there are some wonderful people working on this topic, yet that they are approaching it with a very engineering heavy focus. They do however say very clearly that more people need to be involved in this process, and at the same time they are arguing that \u2018human time is expensive\u2019.", "It is understandable given the context of the statement. Particularly through another recent statement made by Greg Brockman, who is OpenAI\u2019s chief technology officer: \u201cOpenAI\u2019s DOTA 2 bots have trained for the equivalent of 45,000 human years.\u201d The OpenAI Five beat the world champion in a prominent video game. Albeit this is good marketing the same amount of time and practices abstracted into a warped perspective on time may give a wrong impression of artificial intelligence.", "Therein lies the challenging distinction of comparison in the human and/or machine behaviour. Moving away from processing or computing into further comparisons with the human, blurring the lines further\u2014 is it useful?", "The debate is certainly still open.", "Human preference-based reinforcement learning: Tasks too difficult for a human to perform, but a human can still judge the quality of behavior or answers", "Machine Behaviour: lies at the intersection of the fields that design and engineer AI systems and the fields that traditionally use scientific methods to study the behaviour of biological agents.", "Debate Model Security Vulnerabilities: A sufficiently strong misaligned AI may be able to convince a human to do dangerous things.", "AI Safety Dichotomy: we are safer if the agents stay honest throughout training, but we are also safer if debate works well enough that sudden large defections are corrected.", "I have been writing one article for every day for 50 days. These articles have been general in their approach exploring the intersection of social science and computer science in the field of artificial intelligence.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "AI Policy, Governance, Ethics and International Partnerships at www.nora.ai. All views are my own. twitter.com/AlexMoltzau"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd93e6641649d&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://alexmoltzau.medium.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": ""}, {"url": "https://alexmoltzau.medium.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Alex Moltzau"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa26ca41a44b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&user=Alex+Moltzau&userId=a26ca41a44b7&source=post_page-a26ca41a44b7----d93e6641649d---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://unsplash.com/@drewbutler", "anchor_text": "@drewbutler"}, {"url": "https://medium.com/@alexmoltzau/openai-or-closedai-fae7bdd0fcff?source=friends_link&sk=4b572783e9ff49add07e4aef3ded34c3", "anchor_text": "OpenAI yesterday"}, {"url": "https://arxiv.org/pdf/1805.00899.pdf", "anchor_text": "AI safety via debate"}, {"url": "https://openai.com/", "anchor_text": "OpenAI"}, {"url": "http://research.google.com/teams/brain", "anchor_text": "Google Brain"}, {"url": "https://naml.us/paper/", "anchor_text": "extensive collection of papers"}, {"url": "https://www.fhi.ox.ac.uk/team/paul-christiano/", "anchor_text": "Future of Humanity Institute"}, {"url": "https://scholar.google.com/citations?user=B7oP0bIAAAAJ&hl=en", "anchor_text": "Concrete Problems in AI Safety"}, {"url": "https://debate-game.openai.com", "anchor_text": "https://debate-game.openai.com"}, {"url": "https://arxiv.org/abs/1606.06565", "anchor_text": "Concrete problems in AI safety"}, {"url": "https://www.theverge.com/2019/4/13/18309459/openai-five-dota-2-finals-ai-bot-competition-og-e-sports-the-international-champion", "anchor_text": "another recent statement"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d93e6641649d---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&user=Alex+Moltzau&userId=a26ca41a44b7&source=-----d93e6641649d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&user=Alex+Moltzau&userId=a26ca41a44b7&source=-----d93e6641649d---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fd93e6641649d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----d93e6641649d---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----d93e6641649d--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d93e6641649d--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d93e6641649d--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----d93e6641649d--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----d93e6641649d--------------------------------", "anchor_text": ""}, {"url": "https://alexmoltzau.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://alexmoltzau.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Alex Moltzau"}, {"url": "https://alexmoltzau.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.4K Followers"}, {"url": "http://www.nora.ai", "anchor_text": "www.nora.ai"}, {"url": "http://twitter.com/AlexMoltzau", "anchor_text": "twitter.com/AlexMoltzau"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa26ca41a44b7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&user=Alex+Moltzau&userId=a26ca41a44b7&source=post_page-a26ca41a44b7--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F2efc0465f85b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdebating-the-ai-safety-debate-d93e6641649d&newsletterV3=a26ca41a44b7&newsletterV3Id=2efc0465f85b&user=Alex+Moltzau&userId=a26ca41a44b7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}