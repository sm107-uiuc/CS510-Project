{"url": "https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737", "time": 1683008597.567965, "path": "towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737/", "webpage": {"metadata": {"title": "Solving a Reinforcement Learning Problem Using Cross-Entropy Method | by Jordi TORRES.AI | Towards Data Science", "h1": "Solving a Reinforcement Learning Problem Using Cross-Entropy Method", "description": "Solving a Reinforcement Learning Problem Using Cross-Entropy Method (Agent Creation Using Deep Neural Networks)"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "previous posts", "paragraph_index": 1}, {"url": "https://www.youtube.com/watch?v=aUrX-rP_ss4", "anchor_text": "John Schulman claims works \u201cembarrassingly well\u201d on complex RL problems", "paragraph_index": 2}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "Policy-Based methods", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "In future posts", "paragraph_index": 6}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "in the Deep Learning post", "paragraph_index": 7}, {"url": "https://en.wikipedia.org/wiki/Percentile", "anchor_text": "percentile", "paragraph_index": 19}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 27}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 27}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject", "paragraph_index": 28}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "presented in this previous post", "paragraph_index": 32}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "previous post", "paragraph_index": 36}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 46}, {"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "See you in the following post", "paragraph_index": 50}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub", "paragraph_index": 51}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "can be run as a Colab google notebook using this link", "paragraph_index": 51}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech", "paragraph_index": 52}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center", "paragraph_index": 52}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series", "paragraph_index": 53}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome", "paragraph_index": 54}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai", "paragraph_index": 57}], "all_paragraphs": ["After a parenthesis of three posts introducing basics in Deep Learning and Pytorch, in this post we put the focus back to merge Reinforcement Learning and Deep Learning.", "In a previous posts we advanced that an Agent make decisions to solve complex decision-making problems under uncertainty. For this purpouse the Agent employs a policy \ud835\udf0b, as a strategy to determine the next action a based on the current state s. From this post onwards we will explore different methods to obtain a policy that allows an Agent to make decisions.", "In this post we will start with Cross-Entropy method that will help to the reader to warm-up in merging Deep Learning and Reinforcement Learning. It is an evolutionary algorithm for parameterized policy optimization that John Schulman claims works \u201cembarrassingly well\u201d on complex RL problems.", "Cross-Entropy is considered an Evolutionary Algorithm: Some individuals are sampled from a population, and only the \u201celite\u201d ones govern future generations\u2019 characteristics.", "Essentially, what the Cross-Entropy method does is take a bunch of inputs, see the outputs produced, choose the inputs that have led to the best outputs and tune the Agent till we are satisfied with the outputs we see.", "Remember that a policy, denoted by \ud835\udf0b(\ud835\udc4e|\ud835\udc60), says which action the Agent should take for every state observed. In this post, we will consider that the core of our Agent will be a neural network that produces the policy.", "We refer to the methods that solve this type of problem as Policy-Based methods that train the neural network that produces the policy. In future posts, we will go in more deeply about this type of method.", "In practice, the policy is usually represented as a probability distribution over actions (that the Agent can take at a given state), which makes it very similar to a classification problem presented before (in the Deep Learning post), with the amount of classes being equal to the amount of actions we can carry out. In our case the output of our neural network is an action vector that represents a probability distribution as shown visually in the following Figure:", "In this case we refer to it as a stochastic policy, because it returns a probability distribution over actions rather than returning a deterministic single action.", "We want a policy, a probability distribution, and we initialize it at random. Then we improve our policy by playing a few episodes and then adjusting our policy (parameter of the neural network) in a way that it is more efficient. Then repeat this process in order to our policy gradually gets better. This is the base of the Cross-Entropy method.", "Since we will consider a neural network as the heart of this first Agent, we need to find some way to obtain data that we can assimilate as a training dataset, which includes input data and their respective labels.", "The way we\u2019re going to do this is considering the problem as a supervised learning problem where observed states are considered the features (input data) and the actions constitute the labels.", "During the agent\u2019s lifetime, its experience is presented as episodes. Every episode is a sequence of observations of states that the Agent has got from the Environment, actions it has issued, and rewards for these actions. The core of the Cross-Entropy method is to throw away bad episodes and train on better ones, so how we find the better ones?. Imagine that our Agent has played several such episodes. For every episode, we can calculate the return (total reward) that the Agent has claimed. Remember that an Agent tries to accumulate as much total reward as possible by interacting with the Environment.", "Again, for simplicity we will use the Frozen-Lake example. To understand what\u2019s going on, we need to look deeper at the reward structure of the Frozen-Lake Environment. We get the reward of 1.0 only when we reach the goal, and this reward says nothing about how good each episode was. Was it quick and efficient? or did we make many rounds on the lake before we randomly stepped into the final cell? We don\u2019t know; it\u2019s just 1.0 reward and that\u2019s it.", "Let\u2019s imagine that we already have the Agent programmed and we use it to create 4 episodes, that we can visualize with the .render() method:", "Note that due to randomness in the Environment and the way that the Agent selects actions to take, the episodes have different lenght and also shown different rewards. Obviously an episode that has a reward of 1.0 is better than one that has a reward of 0.0. What about episodes that end with the same reward?", "It is clear that we can consider some episodes \u201cbetter\u201d than others, e.g. the third is shortest that the second. For this, the discount factor gamma is very helpful. We can use \ud835\udefe = 0,9. In this case, the discounted return will be equal to the reward r (1.0 or 0.0) obtained at the end of the episode at time step t by gamma to t.", "Let\u2019s illustrate these four episodes with a diagram where each cell represents the Agent\u2019s step, a transition, in the episode, and the corresponding discounted return:", "We can see that the discounted return for shorter episodes will be higher than for longer ones.", "The core of the Cross-Entropy method is simple. Basically, it generates batches of episodes, throw away bad episodes in a batch to train the neural network of the Agent on better ones. To decide which ones to throw away, we used the 70th percentile in our example, which means that we only kept the 30% that did better than 70% of the others.", "As a result, the neural network learns to repeat actions that lead to the results of neural network decisions becoming better and better as we use new batches of elite episodes. The Agent must be trained until a certain mean average reward for the batch of episodes threshold is reached.", "So, a pseudocode of the method can be described by the following steps:", "0. Initialize the Agent neural network model", "A variant of the method, which we will discuss in the next post is that we can keep \u201celite\u201d episodes for a longer time. I mean that the default version of the algorithm samples episodes from the Environment, train on the best ones, and threw them away. However, when the number of succesful episodes is small, the \u201celite\u201d episodes can be maintained longer, keeping them for several iterations to train on them.", "The Environment is the source of data from which we are going to create the dataset that will be used to train the neural network of our Agent.", "The Agent will start from a random policy, where the probability of all actions is uniform, and while training, the Agent will hopefully learn from data obtained from the Environment to optimize its policy toward reaching the optimal policy.", "The data that comes from the Environment are episode steps that should be expressed with tuples of the form <s,a,r> (state, action and reward) which are obtained in each time step as indicated in the following scheme:", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "As we have done in other posts in this series, the code presented in this post has been inspired by the code of Maxim Lapan who has written an excellent practical book on the subject.", "Let\u2019s code it. We must first import several packages:", "We will start by using the not slippery FrozenLake Environment (in the next post we will discuss more about the slippery version):", "Our state space is discrete, which means that it\u2019s just a number from zero to fifteen inclusive (our current position in the grid). The action space is also discrete, from zero to three.", "Our neural network expects a vector of numbers. To get this, we can apply the traditional onehot encoding of discrete inputs (presented in this previous post), which means that the input to our network will have 16 numbers with zero everywhere except the index that we will encode. To ease the code, we can use the ObservationWrapper class from Gym and implement our OneHotWrapper class:", "As a summary, we have in env a not slippery Frozen-Lake Environment that we will use for obtain episodes in order to obtain data to train our Agent.", "We have already advanced that our Agent is based on a neural network. Let\u2019s see how to code this neural network and how it is used to perform the selection of actions that an Agent does.", "Our model\u2019s core is a one-hidden-layer neural network with 32 neurons using a Sigmoid activation function. There is nothing special about our neural network. We start with an arbitrary number of layers and number of neurons.", "The neural network takes a single observation from the Environment as an input vector and outputs a number for every action we can perform, a probability distribution over actions. A straightforward way to proceed would be to include softmax nonlinearity after the last layer. However, remember from a previous post that we try to avoid apply softmax to increase the numerical stability of the training process. Rather than calculating softmax and then calculating Cross-Entropy loss, in this example we use the PyTorch class nn.CrossEntropyLoss, which combines both softmax and Cross-Entropy in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the neural network (also called logits).", "Other \u201chyperparameters\u201d as Loss function and the Optimizer are also set for this example using the following code:", "This abstraction makes our Agent very simple: it needs to pass an observed state that receives from the Environment to the neural network model and perform random sampling using the probability distribution to get an action to carry out:", "Let\u2019s explain in detail this code:", "Line 1: This functions requires that a first step transform the state to a tensor to ingest it to our neural network. At every iteration, we convert our current observation (Numpy array of 16 positions) to a PyTorch tensor and pass it to the model to obtain action probabilities. Remember that our neural network model needs tensors as a input data (to clarify the code we use the suffix _t to indicate that the variable is a tensor).", "Line 2: A consequence of using nn.CrossEntropyLoss we need to remember to apply softmax every time we need to get probabilities from our neural network output.", "Line 3: We need to convert the output tensor (remember that the model and softmax function return tensors) into a NumPy array. This array will have the same 2D structure as the input, with the batch dimension on axis 0, so we need to get the first batch element to obtain a 1D vector of action probabilities.", "Line 4: With the probability distribution of actions, we can use it to obtain the actual action for the current step by sampling this distribution using NumPy\u2019s function random.choice().", "In the next figure we show a screenshot of the training loop indicating the general steps of the Cross-Entropy algorithm :", "In order not to make this post too long, and specially because this method is introduced as a warm-up in the DRL coding, we leave for the next post the detailed explanation of this algorithm. And in this way we are able to go directly to test the model and become familiar with it before entering with details of the training process.", "For now I simply propose to run the code of this loop and see the results. Just to mention that we considered a good result to have a reward in 80% of the cases. Remember that entire code of this post can be found on GitHub.", "What remains now is to see if the Agent really makes good decisions. To check this, we can create a new Environment ( test_env), and check if our Agent is able to reach the Goal cell after being trained. The following code will do that:", "The code is a simple loop that interact with the Environment with the .step()method until the episode finishes (is_done). Thanks to the .render() method, we can see the behavior of the Agent more visually:", "If we try it several times we will see that it does it well enough.", "In the next post we will describe in detail the training loop (which we have skipped in this post) as well as see how we can improve the learning of the Agent taking into account a better neural network (with more neurons or different activation functions). Also we will consider the variant of the method that keeps \u201celite\u201d episodes for several iterations of the training process. See you in the following post.", "The entire code of this post can be found on GitHub and can be run as a Colab google notebook using this link.", "by UPC Barcelona Tech and Barcelona Supercomputing Center", "A relaxed introductory series that gradually and with a practical approach introduces the reader to this exciting technology that is the real enabler of the latest disruptive advances in the field of Artificial Intelligence.", "I started to write this series in May, during the period of lockdown in Barcelona. Honestly, writing these posts in my spare time helped me to #StayAtHome because of the lockdown. Thank you for reading this publication in those days; it justifies the effort I made.", "Disclaimers \u2014 These posts were written during this period of lockdown in Barcelona as a personal distraction and dissemination of scientific knowledge, in case it could be of help to someone, but without the purpose of being an academic reference document in the DRL area. If the reader needs a more rigorous document, the last post in the series offers an extensive list of academic resources and books that the reader can consult. The author is aware that this series of posts may contain some errors and suffers from a revision of the English text to improve it if the purpose were an academic document. But although the author would like to improve the content in quantity and quality, his professional commitments do not leave him free time to do so. However, the author agrees to refine all those errors that readers can report as soon as he can.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Professor at UPC Barcelona Tech & Barcelona Supercomputing Center. Research focuses on Supercomputing & Artificial Intelligence https://torres.ai @JordiTorresAI"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F23d9726a737&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://torres-ai.medium.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715----23d9726a737---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/tagged/deep-r-l-explained", "anchor_text": "DEEP REINFORCEMENT LEARNING EXPLAINED \u2014 06"}, {"url": "https://towardsdatascience.com/drl-02-formalization-of-a-reinforcement-learning-problem-108b52ebfd9a", "anchor_text": "previous posts"}, {"url": "https://www.youtube.com/watch?v=aUrX-rP_ss4", "anchor_text": "John Schulman claims works \u201cembarrassingly well\u201d on complex RL problems"}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "Policy-Based methods"}, {"url": "https://towardsdatascience.com/policy-based-methods-8ae60927a78d", "anchor_text": "In future posts"}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "in the Deep Learning post"}, {"url": "https://en.wikipedia.org/wiki/Percentile", "anchor_text": "percentile"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter04", "anchor_text": "the code of Maxim Lapan who has written an excellent practical book on the subject"}, {"url": "https://towardsdatascience.com/deep-learning-basics-1d26923cc24a", "anchor_text": "presented in this previous post"}, {"url": "https://towardsdatascience.com/deep-learning-with-pytorch-a93b09bdae96", "anchor_text": "previous post"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://towardsdatascience.com/cross-entropy-method-performance-analysis-161a5faef5fc", "anchor_text": "See you in the following post"}, {"url": "https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "entire code of this post can be found on GitHub"}, {"url": "https://colab.research.google.com/github/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_06_07_Cross_Entropy.ipynb", "anchor_text": "can be run as a Colab google notebook using this link"}, {"url": "https://www.upc.edu/en", "anchor_text": "UPC Barcelona Tech"}, {"url": "https://www.bsc.es/", "anchor_text": "Barcelona Supercomputing Center"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "series"}, {"url": "https://torres.ai/deep-reinforcement-learning-explained-series/", "anchor_text": "Deep Reinforcement Learning Explained \u2014 Jordi TORRES.AIContent of this series"}, {"url": "https://twitter.com/hashtag/StayAtHome?src=hashtag_click", "anchor_text": "#StayAtHome"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----23d9726a737---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----23d9726a737---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/deep-r-l-explained?source=post_page-----23d9726a737---------------deep_r_l_explained-----------------", "anchor_text": "Deep R L Explained"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----23d9726a737---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/towards-data-science?source=post_page-----23d9726a737---------------towards_data_science-----------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----23d9726a737---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&user=Jordi+TORRES.AI&userId=497013a3c715&source=-----23d9726a737---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F23d9726a737&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----23d9726a737---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----23d9726a737--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----23d9726a737--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----23d9726a737--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----23d9726a737--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----23d9726a737--------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://torres-ai.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Jordi TORRES.AI"}, {"url": "https://torres-ai.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://torres.ai", "anchor_text": "https://torres.ai"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F497013a3c715&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&user=Jordi+TORRES.AI&userId=497013a3c715&source=post_page-497013a3c715--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F9fb911e344f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsolving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737&newsletterV3=497013a3c715&newsletterV3Id=9fb911e344f9&user=Jordi+TORRES.AI&userId=497013a3c715&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}