{"url": "https://towardsdatascience.com/supervised-learning-b662cb97df7", "time": 1683018323.555721, "path": "towardsdatascience.com/supervised-learning-b662cb97df7/", "webpage": {"metadata": {"title": "Data Classification Algorithms\u2014 Supervised Machine Learning at its best | by G\u00fcnter R\u00f6hrich | Towards Data Science", "h1": "Data Classification Algorithms\u2014 Supervised Machine Learning at its best", "description": "This article covers several ideas behind classification methods like Support Vector Machine models, KNN, tree-based models (CART, Random Forest) and binary classification through sigmoid or logistic\u2026"}, "outgoing_paragraph_urls": [{"url": "https://towardsdatascience.com/train-test-split-c3eed34f763b", "anchor_text": "sounds familiar, right", "paragraph_index": 27}, {"url": "https://www.rdocumentation.org/packages/tree/versions/1.0-40", "anchor_text": "build, prune", "paragraph_index": 29}, {"url": "https://www.rdocumentation.org/packages/tree/versions/1.0-40/topics/cv.tree", "anchor_text": "cross validate", "paragraph_index": 29}, {"url": "http://linkedin.com/in/groehrich", "anchor_text": "linkedin.com/in/groehrich", "paragraph_index": 49}], "all_paragraphs": ["This article covers several ideas behind classification methods like Support Vector Machine models, KNN, tree-based models (CART, Random Forest) and binary classification through sigmoid or logistic regression. The purpose of this article is to guide you through the most essential ideas behind each topic and support your general understanding.", "Terminology break: There are many sources to find good examples and explanations to distinguish between learning methods, I will only recap a few aspects of them. This post is about supervised algorithms, hence algorithms for which we know a given set of possible output parameters, e.g. Class A, Class B, Class C. In other words, this type of learning maps input values to an expected output. You will often hear \u201clabeled data\u201d in this context.", "Unsupervised learning in contrast, is not aware of an expected output set \u2014 this time there are no labels. The focus lies on finding patterns in the dataset even if there is no previously defined target output. This is the clear domain of clustering, conditionality reduction or deep learning.", "Reinforcement learning is often named last, however it is an essential idea of machine learning. This type of learning aims at maximizing the cumulative reward created by your piece of software. Similar to unsupervised learning, reinforcement learning algorithms do not rely on labeled data, further they primarily use dynamic programming methods.", "When it comes to supervised learning there are several key considerations that have to be taken into account. Before tackling the idea of classification, there are a few pointers around model selection that may be relevant to help you soundly understand this topic.", "Data separation, training, validation and eventually measuring accuracy are vital in order to create and measure the efficiency of your algorithm/model. If this sounds cryptic to you, these aspects are already discussed with a fair amount of detail in the below articles \u2014 otherwise just skip them.", "Another vital aspect to understand is the bias-variance trade-off (or sometimes called \u201cdilemma\u201d \u2014 that\u2019s what it really is). There is a great article about this issue right here:", "Enough of the groundwork. The following parts of this article cover different approaches to separate data into, well, classes. Classification in an analytics sense is no different to what we understand when talking about classifying things in real life. In general, there are different ways of classification:", "Multi-class classification is an exciting field to follow, often the underlying method is based on several binary classifications.", "SVMs rely on so-called support vectors, these vectors can be imagined as lines that separate a group of data points (a convex hull) from the rest of the space. Image two areas of data points that are clearly separable through a line, this is a so called \u201chard\u201d classification task. The data points allow us to draw a straight line between the two \u201cclusters\u201d of data. The only problem we face is to find the line that creates the largest distance between the two clusters \u2014 and this is exactly what SVM is aiming at.", "Illustration 1 shows two support vectors (solid blue lines) that separate the two data point clouds (orange and grey). Our separator is the dotted line in the middle (which is interesting, as this actually isn\u2019t a support vector at all). Finding the best separator is an optimization problem, the SVM model seeks the line that maximize the gap between the two dotted lines (indicated by the arrows), and this then is our classifier.", "A side note, as the hard classification SVM model relies heavily on the margin-creation-process, it is of course quite sensitive to data points closer to the line rather than the points we see in the illustration. This is quite the inverse behavior compared to a standard regression line, where a closer point is actually less influential than a data point further away.", "The other way to use SVM is applying it on data that is not clearly separable, is called a \u201cSoft\u201d classification task. The soft SVM is based on not only the margin assumption from above, but also the amount of error it tries to minimize. In other words, soft SVM is a combination of error minimization and margin maximization.", "Illustration 2 shows the case for which a hard classifier is not working \u2014 I have just re-arranged a few data points, the initial classifier is not correct anymore. The data points are not clearly separable any longer, hence we need to come up with a model that allows errors, but tries to keep them at a minimum \u2014 the soft classifier.", "For both SVM approaches there are some important facts you must bear in mind:", "Another non-parametric approach to classify your data points is k nearest neighbors (or short KNN). This is a pretty straight forward method to classify data, it is a very \u201ctangible\u201d idea of classification when it comes to several classes.", "This method is not solving a hard optimization task (like it is done eventually in SVM), but it is often a very reliable method to classify data.", "KNN is lazy. You will also not obtain coefficients like you would get from a SVM model, hence there is basically no real training for your model. KNN needs to look at the new data point and place it in context to the \u201cold\u201d data \u2014 this is why it is commonly known as a lazy algorithm.", "As the illustration above shows, a new pink data point is added to the scatter plot. Measuring the distance from this new point to the closest 3 points around it, will indicate what class the point should be in. Usually, you would consider the mode of the values that surround the new one.", "KNN is most commonly using the Euclidean distance to find the closest neighbors of every point, however, pretty much every p value (power) could be used for calculation (depending on your use case). You may have heard of Manhattan distance, where p=1 , whereas Euclidean distance is defined as p=2.", "However, there is one remaining question, how many values (neighbors) should be considered to identify the right class? It is recommended to test a few and see how they perform in terms of their overall model accuracy.", "If you wanted to have a look at the KNN code in Python, R or Julia just follow the below link. The included GitHub Gists can be directly executed in the IDE of your choice:", "Also note, that it might be wise to do proper validation on your results otherwise you might end up with a really bad model for new data points (variance!).", "Tree-based models (Classification and Regression Tree models\u2014 CART) often work exceptionally well on pursuing regression or classification tasks. The main idea behind the tree-based approaches is that data is split into smaller junks according to one or several criteria. In tree jargon, there are branches that are connected to the leaves. The overall goal is to create branches and leaves as long as we observe a \u201csufficient drop in variance\u201d in our data. If this is not the case, we stop branching.", "If this sounds too abstract, think of a dataset containing people and their spending behavior, e.g. classify whether the person is in the target group or not (binary classification). Our intuition would probably look at the income first and separate data into a high- and low-income groups, pretty much like this:", "There might be many splits like this, maybe looking at the age of the person, maybe looking at the number of children or the number of hobbies a person has, etc. All these criteria may cause the leaf to create new branches having new leaves dividing the data into smaller junks.", "If this striving for smaller and smaller junks sounds dangerous to you, your right \u2014 having tiny junks will lead to the problem of overfitting. For this reason, every leaf should at least have a certain number of data points in it, as a rule of thumb choose 5\u201310%.", "In general, it is wise not to use all the available data to create the tree, but only a partial portion of the data\u2014 sounds familiar, right? This allows us to use the second dataset and see whether the data split we made when building the tree has really helped us to reduce the variance in our data \u2014 this is called \u201cpruning\u201d the tree.", "In theory, we are using the second data portion to verify, whether the splits hold for other data as well, otherwise we remove the branch as it does not seem to provide sufficient benefit to our model.", "In practice, the available libraries can build, prune and cross validate the tree model for you \u2014 please make sure you correctly follow the documentation and consider sound model selections standards (cross validation). The huge advantage of the tree model is, that for every leaf, we get the classifier\u2019s (or regression\u2019s) coefficients. These not only allow us to predict the outcome, but also provide insight into their overall importance to our model.", "Random forests (RF) can be summarized as a model consisting of many, many underlying tree models. Random forests consider a variety of different and randomly created, underlying trees and choose the most common response value. In other words, the random forest takes the mode out of all the responses predicted by the underlying tree models (or mean response in case of a regression random forest).", "You can always plot the tree outcome and compare results to other models, using variations in the model parameters to find a fast, but accurate model:", "Stay with me, this is essential to understand when \u2018talking random forest\u2019: Using the RF model leads to the draw back, that there is no good way to identify the coefficients\u2019 specific impact to our model (coefficient), we can only calculate the relative importance of each factor \u2014 this can be achieved through looking at the the effect of branching the factor and its total benefit to the underlying trees.", "I always wondered whether I could simply use regression to get a value between 0 and 1 and simply round (using a specified threshold) to obtain a class value. Well, this idea seemed reasonable at first, but as I could learn, a simple linear regression will not work. The reason for this is, that the values we get do not necessarily lie between 0 and 1, so how should we deal with a -42 as our response value?", "As you can see in the above illustration, an arbitrary selected value x={-1, 2} will be placed on the line somewhere in the red zone and therefore, not allow us to derive a response value that is either (at least) between or at best exactly 0 or 1. This is where the Sigmoid function comes in very handy. This function is commonly known as binary or logistic regression and provides probabilities ranging from 0 to 1.", "The huge advantage is that even an infinitely small number is mapped to \u201cclose to\u201d zero and will not be somewhere beyond our boundary. In the illustration below, you can find a sigmoid function that only shows a mapping for values -8 \u2264 x \u2264 8.", "Having shown the huge advantage of logistic regression, there is one thing you need to keep in mind: As this model is not giving you a binary response, you are required to add another step to the entire modeling process. This means, it is necessary to specify a threshold (\u201ccut-off\u201d value) to round probabilities to 0 or 1 \u2014 think of 0.519, is this really a value you would like to see assigned to 1?", "This clearly requires a so called confusion matrix. This matrix is used to identify how well a model works, hence showing you true/false positives and negatives. You could even get creative and assign different costs (weights) to the error type \u2014 this might get you a far more realistic result. E.g. the classification error of \u201cthe model says healthy, but in reality sick\u201d is very high for a deadly disease \u2014 in this case the cost of a false positive may be much higher than a false negative.", "If you\u2019re an R guy, caret library is the way to go as it offers many neat features to work with the confusion matrix.", "There is one HUGE caveat to be aware of: Always specify the positive value (positive = 1), otherwise you may see confusing results \u2014 that could be another contributor to the name of the matrix ;)", "An important side note: The sigmoid function is an extremely powerful tool to use in analytics \u2014 as we just saw in the classification idea. Recently, there has been a lot of buzz going on around neural networks and deep learning, guess what, sigmoid is essential.", "If you think of weights assigned to neurons in a neural network, the values may be far off from 0 and 1, however, eventually this is what we eventually wanted to see, \u201cis a neuron active or not\u201d \u2014 a nice classification task, isn\u2019t it? Exactly here, the sigmoid function is (or actually used to be; pointer towards rectified linear unit) a brilliant method to scale all the neurons\u2019 values onto a range of 0 and 1.", "What you need to know about the logistic regression:", "Deep learning networks (which can be both, supervised and unsupervised!) allow the classification of structured data in a variety of ways. Related methods are often suitable when dealing with many different class labels (multi-class), yet, they require a lot more coding work compared to a simpler support vector machine model. I will cover this exciting topic in a dedicated article. Meanwhile, a brilliant reference can be found here:", "This post covered a variety, but by far not all of the methods that allow the classification of data through basic machine learning algorithms. There are often many ways achieve a task, though, that does not mean there aren\u2019t completely wrong approaches either. Using a bad threshold for logistic regression, might leave you stranded with a rather poor model \u2014 so keep an eye on the details!", "Also refer to the proper methodology of sound model selection! There are a few links at the beginning of this article \u2014 choosing a good approach, but building a poor model (overfit!) will not serve your purpose of providing a good solution to an analytics problem.", "If you found this post helpful, I would appreciate a \u201cfollow\u201d \ud83e\udec0, until then:", "{ See you next time } and take care !", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist @ Mondi Group \u2022 Georgia Tech \ud83c\udf93 \u2022 linkedin.com/in/groehrich"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fb662cb97df7&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://guenterroehrich.medium.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "G\u00fcnter R\u00f6hrich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3718a9423801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=post_page-3718a9423801----b662cb97df7---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://towardsdatascience.com/cross-validation-7c0163460ea0", "anchor_text": "Proper Model Selection through Cross ValidationCross validation is state of the art. It is nothing close to a fun task, but yet vital for everyone dealing with data\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/train-test-split-c3eed34f763b", "anchor_text": "Train, Validate, Test \u2014 Why Splitting Datasets is Essential for Every Machine Learning AlgorithmIf you have already used machine learning algorithms, you likely have seen functions like train_test_split( ) or similar\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/bias-variance-dilemma-74e5f1f52b12", "anchor_text": "Bias-variance dilemma?The Bias-Variance dilemma is relevant for supervised machine learning. It\u2019s a way to diagnose an algorithm performance\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/julia-r-and-python-7cd50c2b0fe4", "anchor_text": "A compact comparison: Julia, R and Python \u2014 Data Science in 2020If you are interested in data science, you will probably have these three languages on your radar. The question is\u2026towardsdatascience.com"}, {"url": "https://towardsdatascience.com/train-test-split-c3eed34f763b", "anchor_text": "sounds familiar, right"}, {"url": "https://www.rdocumentation.org/packages/tree/versions/1.0-40", "anchor_text": "build, prune"}, {"url": "https://www.rdocumentation.org/packages/tree/versions/1.0-40/topics/cv.tree", "anchor_text": "cross validate"}, {"url": "https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc", "anchor_text": "source"}, {"url": "https://keras.io/examples/structured_data/wide_deep_cross_networks/", "anchor_text": "Keras documentation: Structured data learning with Wide, Deep, and Cross networksAuthor: Khalid Salama Date created: 2020/12/31 Last modified: 2020/12/31 Description: Using Wide & Deep and Deep &\u2026keras.io"}, {"url": "https://medium.com/tag/classification?source=post_page-----b662cb97df7---------------classification-----------------", "anchor_text": "Classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----b662cb97df7---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----b662cb97df7---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/data-science?source=post_page-----b662cb97df7---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/data-analysis?source=post_page-----b662cb97df7---------------data_analysis-----------------", "anchor_text": "Data Analysis"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=-----b662cb97df7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=-----b662cb97df7---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2Fb662cb97df7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----b662cb97df7---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----b662cb97df7--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----b662cb97df7--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----b662cb97df7--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----b662cb97df7--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----b662cb97df7--------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://guenterroehrich.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "G\u00fcnter R\u00f6hrich"}, {"url": "https://guenterroehrich.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "158 Followers"}, {"url": "http://linkedin.com/in/groehrich", "anchor_text": "linkedin.com/in/groehrich"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3718a9423801&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=post_page-3718a9423801--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F276478487b6b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsupervised-learning-b662cb97df7&newsletterV3=3718a9423801&newsletterV3Id=276478487b6b&user=G%C3%BCnter+R%C3%B6hrich&userId=3718a9423801&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}