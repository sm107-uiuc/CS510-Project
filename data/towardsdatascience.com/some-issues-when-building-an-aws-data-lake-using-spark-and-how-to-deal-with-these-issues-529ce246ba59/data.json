{"url": "https://towardsdatascience.com/some-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59", "time": 1683012053.2724512, "path": "towardsdatascience.com/some-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59/", "webpage": {"metadata": {"title": "Some issues when building an AWS data lake using Spark and how to deal with these issues | by Tran Nguyen | Towards Data Science", "h1": "Some issues when building an AWS data lake using Spark and how to deal with these issues", "description": "At first, it seemed to be quite easy to write down and run a Spark application. If you are experienced with data frame manipulation using pandas, NumPy and other packages in Python, and/or the SQL\u2026"}, "outgoing_paragraph_urls": [{"url": "https://spark.apache.org/docs/latest/", "anchor_text": "here", "paragraph_index": 3}, {"url": "https://github.com/nhntran", "anchor_text": "github.", "paragraph_index": 5}, {"url": "https://dbdiagram.io/", "anchor_text": "https://dbdiagram.io/", "paragraph_index": 11}, {"url": "https://github.com/nhntran/create-datalake-spark", "anchor_text": "my Github", "paragraph_index": 13}, {"url": "https://boto3.amazonaws.com/v1/documentation/api/latest/index.html", "anchor_text": "boto3", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/how-to-create-and-run-an-emr-cluster-using-aws-cli-3a78977dc7f0#c46c", "anchor_text": "here, Step 2: Create an IAM user.", "paragraph_index": 15}, {"url": "https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6", "anchor_text": "Here is one of the good reference", "paragraph_index": 33}, {"url": "https://stackoverflow.com/questions/48209667/using-monotonically-increasing-id-for-assigning-row-number-to-pyspark-datafram", "anchor_text": "There are 3 methods for this task", "paragraph_index": 34}, {"url": "https://kb.databricks.com/data/append-slow-with-spark-2.0.0.html", "anchor_text": "here", "paragraph_index": 40}, {"url": "https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html", "anchor_text": "\u201cOn a per-node basis, HDFS can yield 6X higher read throughput than S3\u201d.", "paragraph_index": 43}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html", "anchor_text": "s3-dist-cp", "paragraph_index": 43}, {"url": "https://github.com/nhntran/create-datalake-spark/blob/master/test_data_lake.ipynb", "anchor_text": "The data quality checks", "paragraph_index": 45}, {"url": "https://github.com/nhntran/create-datalake-spark/blob/master/sparkifydb_data_lake_demo.ipynb", "anchor_text": "some example queries and results for song play analysis", "paragraph_index": 45}, {"url": "https://github.com/nhntran/create-datalake-spark", "anchor_text": "Github", "paragraph_index": 45}, {"url": "https://en.wikipedia.org/wiki/Balance_(accounting)#:~:text=In%20banking%20and%20accounting%2C%20the,account%20during%20a%20financial%20period.", "anchor_text": "Wikipedia", "paragraph_index": 47}, {"url": "https://github.com/nhntran", "anchor_text": "my Github", "paragraph_index": 51}], "all_paragraphs": ["At first, it seemed to be quite easy to write down and run a Spark application. If you are experienced with data frame manipulation using pandas, NumPy and other packages in Python, and/or the SQL language, creating an ETL pipeline for our data using Spark is quite similar, even much easier than I thought. And compared to other databases (such as Postgres, Cassandra, AWS DWH on Redshift), creating a Data Lake database using Spark appears to be a carefree project.", "But then, when you deployed Spark application on the cloud service AWS with your full dataset, the application started to slow down and fail. Your application ran forever, you even didn\u2019t know if it was running or not when observing the AWS EMR console. You might not know where it was failed: It was difficult to debug. The Spark application behaved differently between the local mode and stand-alone mode, between the test set \u2014 a small portion of the dataset \u2014 and full dataset. The list of problems went on and on. You felt frustrated. Really, you realized that you knew nothing about Spark. Well, optimistically, then it was indeed a very good opportunity to learn more about Spark. Running into issues is the normal thing in programming anyway. But, how to solve problems quickly? Where to start?", "After struggling with creating a Data Lake database using Spark, I feel the urge to share what I have encountered and how I solved these issues. I hope it is helpful for some of you. And please, correct me if I am wrong. I am still a newbie in Spark anyway. Now, let\u2019s dive in!", "1. This article assumes that you already have some working knowledge of Spark, especially PySpark, command line environment, Jupyter notebook and AWS. For more about Spark, please read the reference here.", "2. This is your responsibility for monitoring usage charges on the AWS account you use. Remember to terminate the cluster and other related resources each time you finish working. The EMR cluster is costly.", "3. This is one of the assessing projects for the Data Engineering nanodegree on Udacity. So to respect the Udacity Honor Code, I would not include the full notebook with the workflow to explore and build the ETL pipeline for the project. Part of the Jupyter notebook version of this tutorial, together with other tutorials on Spark and many more data science tutorials could be found on my github.", "Sparkify is a startup company working on a music streaming app. Through the app, Sparkify has collected information about user activity and songs, which is stored as a directory of JSON logs (log-data - user activity) and a directory of JSON metadata files (song_data - song information). These data reside in a public S3 bucket on AWS.", "In order to improve the business growth, Sparkify wants to move their processes and data onto the data lake on the cloud.", "This project would be a workflow to explore and build an ETL (Extract \u2014 Transform \u2014 Load) pipeline that:", "Below is the sample from JSON log file and JSON song file:", "The dimension and fact tables for this database were designed as followed: Fields in bold: partition keys.", "(ERD diagram was made using https://dbdiagram.io/)", "This is my workflow for the project. An experienced data engineer might skip many of these steps, but for me, I would rather go slowly and learn more:", "The validation and demo part could be found on my Github. Other script file etl.py and my detailed sparkifydb_data_lake_etl.ipynb are not available in respect of the Udacity Honor Code.", "In order to work on the project, first, we need to know the overview of the dataset, such as the number of files, the number of lines in each file, the total size of the dataset, the structure of the file, etc. It is especially crucial if we work on the cloud, where requests could cost so much time and money.", "To do that, we can use boto3, the Amazon Web Services (AWS) SDK for Python. boto3 allows us to access AWS via an IAM user. The detail on how to create an IAM user can be found here, Step 2: Create an IAM user.", "Below is the way to set up the client for S3 on Jupyter notebook:", "The key and access key obtained from an IAM user could be saved to the file credentials.cfg at the local directory as below. Note that you may run into \u201cconfigure file parsing error\u201d if you put your key and secrete key inside \u201c \u201d or \u2018 \u2019, or if the file does not have the header such as [AWS]", "With this client for S3 created by boto3, we can access the dataset for the project and look at the file structures of log-data and song_data:", "The outputs of the exploration process are:", "The dataset is not big, ~3.6MB. However, the song_data has ~15,000 files. It is better to use a subset of song_data, such as \u2018song_data/A/A/A/\u2019 or \u2018song_data/A/\u2019 for exploring/creating/debugging the ETL pipeline first.", "My ETL pipeline worked very well on the subset of the data. However, when I run it on the whole dataset, the Spark application kept freezing without any error notice. I had to reduce/increase the sub dataset to actually see the error and fix the problem, for example changing from \u2018song_data/A/A/A\u2019 to \u2018song_data/A/\u2019 and vice versa. So what is the problem here?", "How to design a correct schema:", "Although it is the best practice in programming, we sometimes forget to do that. For a big dataset, observing the time for each task is very important for debugging and optimizing the Spark application.", "Unless you turn off INFO logging in Spark, it is very difficult, if not impossible, to know the progress of the Spark application on the terminal, which is overwhelming with INFO logging. By printing out the task name and recording the time, everything is better:", "There are at least 2 ways to import and use a function, for example:", "Either is fine. I prefer the second approach since I don\u2019t need to list all the functions on the top of my script etl.py.", "Notice that themax function is an exception since it is also a built-in max function in Python. To use max function from the pyspark.sql.functions module, you have to use F.max or using an alias, such asfrom pyspark.sql.functions import max as max_", "There could be many problems with it. I got some myself:", "This is on the \u201cINFO ContextCleaner: Cleaned accumulator xxx\u201d where I found my Spark application appeared to be freezing again and again. It\u2019s expected to be a long-running job, which took me ~115 min to write only the songs table into the s3 bucket. So if you are sure that your end-to-end process works perfectly, then be patient for 2 hours to see how it works. The process could be speeded up, check out on Tip 9 below.", "4. Checking the running time on AWS EMR console: You can see how long your Spark application ran when choosing the Application user interfaces tab on your cluster on the EMR console. The list of application can be found at the end of the page:", "My ETL pipeline on the whole dataset took ~2.1 hrs to finished on the EMR cluster (1 Master node and 2 Core nodes of type m5.xlarge).", "This issue is trivial in other databases: In Postgres, we can use SERIAL to auto-increment a column, such as songplays_id SERIAL PRIMARY KEY. In AWS Redshift, we can use IDENTITY(seed, step).", "It is not trivial to perform auto-increment for the table using Spark, at least when you try to understand it deeply and in consideration of Spark performance. Here is one of the good references to understand auto-increment in Spark.", "There are 3 methods for this task:", "Step 1: From the songplays_table dataframe, use the rdd interface to create indexes with zipWithIndex(). The result is a list of rows, each row contains 2 elements: (i) all the columns from the old data frame zipped into a \u201crow\u201d, and (ii) the auto-increment indexes:", "Step 2: Return it back to dataframe \u2014 we need to write a lambda function for it.", "Below is the time for running the Spark application on AWS EMR cluster, reading from and writing to S3:", "My EMR cluster had 1 Master node and 2 Core nodes of type m5.xlarge, as shown below:", "We definitely love to optimize the Spark application since reading and writing into S3 take a long time. Here are some optimizations that I have tried:", "You can read in detail about it here. It can be done simply by adding spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\") into the spark session.", "With this optimization, the total ETL time reduced dramatically from ~2.1hr to only 30 min.", "Use HDFS to speed up the process", "- \u201cOn a per-node basis, HDFS can yield 6X higher read throughput than S3\u201d. So we can save the analytics tables to HDFS, then copy from HDFS to S3. We could use s3-dist-cp to copy from HDFS to S3.", "This ETL pipeline is a long-running job, in which the task of writing the song table took most of the time. The songs table was partitioned by \u201cyear\u201d and \u201cartist\u201d, which could produce skew data since some early years (1961 to 199x) don\u2019t contain many songs comparing to the years 200x.", "The data quality checks to make sure if the ETL pipeline successfully added all the records to the tables, together with some example queries and results for song play analysis could be found in my notebook on Github.", "Although I have used AWS \u201cquite a lot\u201d and already reached the Free Tier usage limit with this account, whenever I came to the Billing Dashboard, the total amount due is 0.", "Don\u2019t let AWS Billing Dashboard confuse you. What it shows is the total balance, not your AWS expense. It is the balance which \u2014 according to Wikipedia \u2014 is \u201cthe difference between the sum of debit entries and the sum of credit entries entered into an account during a financial period.\u201d", "I thought when looking at the AWS Billing Dashboard, I would see the amount I had spent so far, my AWS expense. But no. Even when click on the Bill Details, everything is 0. And so I thought that I didn\u2019t use AWS that much. My promo credit was still safe.", "Only when one day, I click on the Expand All button and I were in big surprise realizing my promo credit had almost gone!!! So again, what you see on the Dashboard is the balance, not the expense. Be careful when using your EMR and EC clusters. It may cost you more money than you thought. (Well, although I admit that gaining AWS experience is so worth it).", "Thank you so much for reading this lengthy post. I do aware that people get discouraged easily with long posts, but I want to have a consolidated report for you. Good luck with your project, and I am more than happy for any discussion.", "The Jupyter notebook version of this post, together with other tutorials on Spark and many more data science tutorials could be found on my Github.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "My random thoughts through the career development process"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F529ce246ba59&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----529ce246ba59--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/@tranhnnguyenvn?source=post_page-----529ce246ba59--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Tran Nguyen"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F90ed820b9879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&user=Tran+Nguyen&userId=90ed820b9879&source=post_page-90ed820b9879----529ce246ba59---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://spark.apache.org/docs/latest/", "anchor_text": "here"}, {"url": "https://github.com/nhntran", "anchor_text": "github."}, {"url": "https://www.udacity.com/course/data-engineer-nanodegree--nd027", "anchor_text": "the Data Engineering nanodegree program on Udacity."}, {"url": "https://dbdiagram.io/", "anchor_text": "https://dbdiagram.io/"}, {"url": "https://github.com/nhntran/create-datalake-spark", "anchor_text": "my Github"}, {"url": "https://opendatascience.com/why-you-should-be-using-jupyter-notebooks/", "anchor_text": "a great environment"}, {"url": "https://boto3.amazonaws.com/v1/documentation/api/latest/index.html", "anchor_text": "boto3"}, {"url": "https://towardsdatascience.com/how-to-create-and-run-an-emr-cluster-using-aws-cli-3a78977dc7f0#c46c", "anchor_text": "here, Step 2: Create an IAM user."}, {"url": "https://stackoverflow.com/questions/42822483/extremely-slow-s3-write-times-from-emr-spark/42834182#42834182", "anchor_text": "reading and writing to S3 from EMR/Spark are extremely slow"}, {"url": "https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6", "anchor_text": "Here is one of the good reference"}, {"url": "https://stackoverflow.com/questions/48209667/using-monotonically-increasing-id-for-assigning-row-number-to-pyspark-datafram", "anchor_text": "There are 3 methods for this task"}, {"url": "https://kb.databricks.com/data/append-slow-with-spark-2.0.0.html", "anchor_text": "here"}, {"url": "https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html", "anchor_text": "\u201cOn a per-node basis, HDFS can yield 6X higher read throughput than S3\u201d."}, {"url": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html", "anchor_text": "s3-dist-cp"}, {"url": "https://github.com/nhntran/create-datalake-spark/blob/master/test_data_lake.ipynb", "anchor_text": "The data quality checks"}, {"url": "https://github.com/nhntran/create-datalake-spark/blob/master/sparkifydb_data_lake_demo.ipynb", "anchor_text": "some example queries and results for song play analysis"}, {"url": "https://github.com/nhntran/create-datalake-spark", "anchor_text": "Github"}, {"url": "https://en.wikipedia.org/wiki/Balance_(accounting)#:~:text=In%20banking%20and%20accounting%2C%20the,account%20during%20a%20financial%20period.", "anchor_text": "Wikipedia"}, {"url": "https://github.com/nhntran", "anchor_text": "my Github"}, {"url": "https://medium.com/tag/spark?source=post_page-----529ce246ba59---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/tag/data-lake?source=post_page-----529ce246ba59---------------data_lake-----------------", "anchor_text": "Data Lake"}, {"url": "https://medium.com/tag/programming?source=post_page-----529ce246ba59---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/aws?source=post_page-----529ce246ba59---------------aws-----------------", "anchor_text": "AWS"}, {"url": "https://medium.com/tag/troubleshooting?source=post_page-----529ce246ba59---------------troubleshooting-----------------", "anchor_text": "Troubleshooting"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&user=Tran+Nguyen&userId=90ed820b9879&source=-----529ce246ba59---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&user=Tran+Nguyen&userId=90ed820b9879&source=-----529ce246ba59---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F529ce246ba59&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----529ce246ba59---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----529ce246ba59--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----529ce246ba59--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----529ce246ba59--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----529ce246ba59--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----529ce246ba59--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@tranhnnguyenvn?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Tran Nguyen"}, {"url": "https://medium.com/@tranhnnguyenvn/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "105 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F90ed820b9879&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&user=Tran+Nguyen&userId=90ed820b9879&source=post_page-90ed820b9879--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff136f72e8c83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsome-issues-when-building-an-aws-data-lake-using-spark-and-how-to-deal-with-these-issues-529ce246ba59&newsletterV3=90ed820b9879&newsletterV3Id=f136f72e8c83&user=Tran+Nguyen&userId=90ed820b9879&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}