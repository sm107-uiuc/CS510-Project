{"url": "https://towardsdatascience.com/reinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a", "time": 1683002080.49909, "path": "towardsdatascience.com/reinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a/", "webpage": {"metadata": {"title": "Reinforcement learning framework and toolkits (Gym and Unity) | by Amanda Iglesias Moreno | Towards Data Science", "h1": "Reinforcement learning framework and toolkits (Gym and Unity)", "description": "Reinforcement learning provides a framework for learning policies that map states into actions with the objective of maximizing cumulative reward. In this article, we present how this framework is\u2026"}, "outgoing_paragraph_urls": [{"url": "https://gym.openai.com/", "anchor_text": "Gym", "paragraph_index": 14}, {"url": "https://gym.openai.com/envs/CartPole-v1/", "anchor_text": "CartPole-v1", "paragraph_index": 17}], "all_paragraphs": ["Reinforcement learning provides a framework for learning policies that map states into actions with the objective of maximizing cumulative reward. In this article, we present how this framework is mathematically formulated, and which algorithms can be applied to solve the RL problem. We will also address the use of (1) Gym, and (2) Unity, two toolkits for developing and comparing reinforcement learning algorithms.", "Reinforcement Learning is an area of machine learning based on the idea of learning from iteration. The learner, or decision maker, is called the agent which interacts with the environment, receiving a feedback called reward. The goal of the agent is to maximize reward through iterations with the environment. In a reinforcement learning problem, we consider that time steps are discrete. At the first time step, the agent observes the environment\u2019s state, selecting an action in response. Subsequently, the environment presents a new state to the agent and a reward, showing how appropriate its action was. This process continues at the following time steps, obtaining a sequence of states, actions, and rewards. After trial and error, the agent learns to execute appropriate actions to maximize expected cumulative reward.", "The reward is established, so that when the agent tries to maximize it, also learns a particular behaviour (e.g. play a video game or drive a car). The agent is designed not to maximize immediate reward, which is the reward provided by the environment after executing an action, but expected cumulative reward. The expected cumulative reward is the sum of rewards from the next time step onwards, and is denoted with Gt; we use the term expected since the agent cannot predict with certainty what the future reward is.", "To emphasize rewards that come sooner more than future rewards, we compute the discounted reward, representing \u03b3 the discounted rate. The discounted rate is a number between 0 and 1 and is set by the developer when defining the reinforcement learning task.", "A discounted rate \u03b3=1 represents an undiscounted reward, whereas \u03b3=0 represents the most immediate reward, meaning the larger \u03b3 is the more the agent takes into account future rewards.", "The environment for reinforcement learning is described using the Markov Decision Process. This process is defined by: (1) a set of states S, (2) a set of actions A, (3) a set of rewards R, (4) one-step dynamics of the environment p(s\u2032,r|s,a), and (5) a discount rate \u03b3. The environment responses at time step t+1, taking into account only the state and action at the prior time step. One-step dynamics and rewards describe how the environment works; therefore, they are unknown by the agent which learns how to take appropriate actions by iterating with the environment.", "A policy describes the agent\u2019s behaviour, and is a mapping from the set of states S to the set of actions A. The policy can be (1) deterministic, or (2) stochastic. A deterministic policy maps states to actions \u03c0:S\u2192A, taking a state as an input and providing an action as an output. On the contrary, a stochastic policy \u03c0:SxA\u2192[0,1] takes a state and an action and outputs the probability of taking that action at that state.", "The goal of the agent is to learn the optimal policy. A policy is considered to be optimal if the expected return is greater than or equal to the expected return of all other policies for all states. Before we deep into the concept of optimality, we present two functions that we will come across pretty often while working in reinforcement learning: (1) the state-value function, and (2) the action-value function. The state value function is the expected return when starting in state s and following policy \u03c0 for all time steps, and is denoted with a lowercase v.", "There is an equation that facilitates the calculation of the state-value function known as the Bellman Expectation Equation. According to this equation, the state-value function at any state s can be calculated using the expected value of the next state and the expected immediate reward.", "Another function of interest is the action-value function denoted with a lowercase q. This function depends not only on the environment\u2019s state, but also on the agent\u2019s action, representing the expected return, if the agent starts in state s, takes action a, and then follows the policy \u03c0 for all future time steps.", "The objective of the agent is to obtain the optimal action-value function q\u2217 by iterating with the environment, and then to use this action-value function to get the optimal policy \u03c0\u2217. We can easily achieve that by choosing the action a that provides maximum q\u2217 for each state s.", "The objective of the reinforcement task is to obtain the optimal policy which represents the optimal agent\u2019s behaviour. To do so, we can employ a wide variety of algorithms which are often classified in two groups: (1) value-based methods, and (2) policy-based methods. Value-based methods calculate the optimal policy indirectly by learning the optimal action-value function Q\u2217(s,a). Then, we can obtain the optimal policy by selecting the action that maximizes Q for each state. On the contrary, policy-based methods find the optimal policy directly, without having to compute an action-value function estimate. The following table shows the most well-known algorithms.", "The algorithm employed depends on the type of state and action space: (1) discrete, or (2) continuous. Discrete spaces present a finite set of states and actions. In a discrete space, we can represent a Q-table (action-value function estimate) as a dictionary or look-up table. The aforementioned algorithms Q-Learning and Sarsa can operate only in discrete spaces. However, most of reinforcement learning applications require continuous state and action spaces. For instance, in robotics continuous variables such as velocity, position and torque are usually employed. Continuous spaces do not present a finite set of possibilities; they can take a range of values. To work with continuous spaces, we can employ two strategies: (1) discretization, and (2) function approximation. Discretization allows us to use the algorithms Q-Learning and Sarsa with little or no modification. When the number of discrete spaces needed is very large, discretization is not feasible and function approximation becomes the only alternative. Deep neural networks become the most appealing alternative to work with continuous spaces, capturing easily not-linear relations between states and actions. Deep Q-Learning algorithm can be applied to reinforcement learning problems that present a continuous state space, but the action space has still to be discrete. To work with both continuous state and action spaces, policy-based algorithms such as REINFORCE or Proximal Policy Optimization have to be used. Another alternative could be to use Actor-Critic methods (a combination of value-based methods with policy-based methods) such as deep deterministic policy gradient.", "In future articles, we will explain how to program those algorithms to train an agent using Pytorch. However, this article is mainly focus on understanding the reinforcement learning framework and toolkits (Gym and Unity). Both libraries provide agents to train, meaning that we can apply one of the aforementions algorithms to these agents so that they learn a particular task.", "Gym is open-source library for developing reinforcement learning algorithms. The toolkit provides a wide variety of environments from Atari games to robotics. The user can easily interact with the agents, being the objective to apply an algorithm to teach them a particular task (e.g. maximize the score in a game, train a robot to walk, or balance a pole on a car).", "The first step is to install Gym on your computer. To do it, we can consult the official web page,", "or alternatively, we can read the following article published in Toward Data Science by Genevieve Hayes.", "To learn how the library works, we will explain the CartPole-v1 environment provided by Gym. Let\u2019s started \ud83d\udcaa!", "The CartPole-V1 environment consists of a pole attached by an un-actuated joint to a cart, which moves along a frictionless track. The agent can apply a force between -1 and 1 to the cart, being the objective to keep the pole balanced as long as possible. A reward of +1 is obtained every time step the pole remains upright. The episode finishes when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.", "The first step is to import the library gym and to load the CartPole-v1 environment by using the gym.make function. Once the environment is created, we need an initial observation. This first state is obtained by calling the reset function, and consists of a (4,) dimensional numpy array, containing the following information: (1) Cart Position, (2) Cart Velocity, (3) Pole Angle, and (4) Pole Velocity At Tip.", "The state space refers to the set of all possible situations our agent could encounter. Similarly, the action space is the set of all possible action our agent can take in a particular state. The type and size of both spaces can be consulted by using the env.observation_space and env.action_space methods.", "Gym spaces could be: (1) Discrete, (2) MultiDiscrete, (3) Box, and (4) Tuple. In the CartPole-v1 environment, the action space is Discrete, meaning the space contains n discrete points from 0 until n-1. We can obtain the total number of actions by typing env.action_space.n. On the contrary, the state space is a multidimensional continuous space (Box type), where each variable lies in the interval [low, high], obtaining both bounds by using env.observation_space.low and env.observation_space.high.", "We can obtain a random action by using env.action_space.sample(). However, the goal of the reinforcement learning problem is not to select an action at random, but to find the optimal action to take, meaning we use an algorithm that outputs the action that our agent should take to maximize its reward.", "After selecting the action, the environment presents a new state to the agent and a reward, showing how appropriate this action was. To obtain both the next step and the reward, we use the env.step() method. This method steps the environment by one time step and returns:", "The values and type of object returned by the step method can be observed below.", "Now it is time to put all together! :) We create an agent that takes random actions until the episode is finished, returning the total score achieved (sum of rewards) in the episode. Notice that when we train the agent the actions are going to be obtained by the policy (trained algorithms) and not at random as here.", "The cart-pole environment has a continuous state space, but the action space is discrete. To train this agent, value-based methods such as Deep Q-learning can be employed. This algorithm selects the best action for a given state; therefore, the policy obtained by using this method is deterministic. To obtain a stochastic policy, a policy-based method such as Proximal Policy Optimization can be applied.", "Unity Machine Learning Agents (ML-Agents) is an open-source Unity plugin that enables games and simulations to serve as environments for training intelligent agents.", "To learn how the library works, we will explain the Banana Colector environment. In this environment, the agent can take either yellow or blue bananas, being the objective to collect as many yellow bananas as possible, avoiding blue ones.", "First of all, we clone the repository, then we import Unity and load the Banana Colector environment.", "Unity uses the concept of brains. Each agent in the learning environment is linked to a single brain, which is responsible of making decisions for all of their linked Agents.", "We can check the brains available, and set one of them as the brain we are going to control with Python as follows.", "In the Banana Colector environment, the state space is continuous and has 37 dimensions, containing agent linear velocity and ray-based perception of the object in the agent\u2019s foward direction. The action space is discrete, having 4 dimensions: (1) moving forward, (2) moving backward, (3) turning left, and (4) turning right. A reward of +1 is provided for collecting a yellow banana, while a reward of -1 is provided for collecting a blue one.", "We can examine the state and action spaces (size and type) by using the following functions:", "Unity provides a gym-like interface to train agents. To get an initial observation, we need to call the env.reset() function as we did previously with gym. Similarly, to send an action to the environment, we use the env.step() function.", "As we did previously with the gym cart-pole example, we create an agent that takes random actions until the episode is finished, returning the total score achieved (sum of rewards) in the episode. Notice that when we train the agent the actions are going to be obtained by the policy (trained algorithm) and not at random as here.", "As we can observe, the reward obtained is -1, meaning the agent has picked more blue bananas than yellow ones. After training, the agent differentiates between both, and the total reward achieved is higher.", "As with the cart-pole environment, the state space is continuous, but the action space is discrete. Therefore, to train this agent, we can use value-based methods such as Deep Q-learning, or policy-based methods such as Proximal Policy Optimization.", "In future articles, we will explain how to train both: the cart-pole, and banana collector agents. So, stay tuned! \ud83d\ude0a", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data Scientist at Statista \u2014 Based in Hamburg \ud83d\udccd"], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F1e047889c59a&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://amandaiglesiasmoreno.medium.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": ""}, {"url": "https://amandaiglesiasmoreno.medium.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Amanda Iglesias Moreno"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bace2932c65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&user=Amanda+Iglesias+Moreno&userId=1bace2932c65&source=post_page-1bace2932c65----1e047889c59a---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://gym.openai.com/envs/#classic_control", "anchor_text": "https://gym.openai.com/envs/#classic_control"}, {"url": "https://gym.openai.com/", "anchor_text": "Gym"}, {"url": "https://gym.openai.com/docs/#installation", "anchor_text": "Gym: A toolkit for developing and comparing reinforcement learning algorithmsThe gym library is a collection of test problems - environments - that you can use to work out your reinforcement\u2026gym.openai.com"}, {"url": "https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30", "anchor_text": "How to Install OpenAI Gym in a Windows EnvironmentA step by step guide for getting OpenAI Gym up and runningtowardsdatascience.com"}, {"url": "https://gym.openai.com/envs/CartPole-v1/", "anchor_text": "CartPole-v1"}, {"url": "https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893", "anchor_text": "https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893"}, {"url": "https://gym.openai.com/envs/CartPole-v0/", "anchor_text": "https://gym.openai.com/envs/CartPole-v0/"}, {"url": "https://unity3d.com/machine-learning", "anchor_text": "https://unity3d.com/machine-learning"}, {"url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287", "anchor_text": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287"}, {"url": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e", "anchor_text": "https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"}, {"url": "https://medium.com/tag/reinforcement-learning?source=post_page-----1e047889c59a---------------reinforcement_learning-----------------", "anchor_text": "Reinforcement Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----1e047889c59a---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/programming?source=post_page-----1e047889c59a---------------programming-----------------", "anchor_text": "Programming"}, {"url": "https://medium.com/tag/python?source=post_page-----1e047889c59a---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----1e047889c59a---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&user=Amanda+Iglesias+Moreno&userId=1bace2932c65&source=-----1e047889c59a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&user=Amanda+Iglesias+Moreno&userId=1bace2932c65&source=-----1e047889c59a---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F1e047889c59a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----1e047889c59a---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----1e047889c59a--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----1e047889c59a--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----1e047889c59a--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----1e047889c59a--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----1e047889c59a--------------------------------", "anchor_text": ""}, {"url": "https://amandaiglesiasmoreno.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://amandaiglesiasmoreno.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Amanda Iglesias Moreno"}, {"url": "https://amandaiglesiasmoreno.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "2.1K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1bace2932c65&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&user=Amanda+Iglesias+Moreno&userId=1bace2932c65&source=post_page-1bace2932c65--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fbac71cd4d351&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a&newsletterV3=1bace2932c65&newsletterV3Id=bac71cd4d351&user=Amanda+Iglesias+Moreno&userId=1bace2932c65&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}