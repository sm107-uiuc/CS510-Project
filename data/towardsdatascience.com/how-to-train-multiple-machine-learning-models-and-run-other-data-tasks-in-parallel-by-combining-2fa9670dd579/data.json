{"url": "https://towardsdatascience.com/how-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579", "time": 1683001294.110344, "path": "towardsdatascience.com/how-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579/", "webpage": {"metadata": {"title": "Training multiple machine learning models and running data tasks in parallel via YARN + Spark + multithreading | by Edson Hiroshi Aoki | Towards Data Science", "h1": "Training multiple machine learning models and running data tasks in parallel via YARN + Spark + multithreading", "description": "To objective of this article is to show how a single data scientist can launch dozens or hundreds of data science-related tasks simultaneously (including machine learning model training) without\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/microsoftazure/how-do-teams-work-together-on-an-automated-machine-learning-project-11bed3a7ad86", "anchor_text": "automated machine learning", "paragraph_index": 2}, {"url": "https://medium.com/@tomaszdudek/but-what-is-this-machine-learning-engineer-actually-doing-18464d5c699", "anchor_text": "creating a \u201cthird role\u201d, named \u201cMachine Learning Engineer\u201d to bridge the gap between the two roles", "paragraph_index": 5}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "Spark on YARN documentation", "paragraph_index": 22}, {"url": "https://docs.python.org/3/library/concurrent.html", "anchor_text": "The concurrent library uses multithreading and not multiprocessing", "paragraph_index": 46}, {"url": "https://github.com/Azure/mmlspark", "anchor_text": "Microsoft Machine Learning for Spark", "paragraph_index": 58}], "all_paragraphs": ["To objective of this article is to show how a single data scientist can launch dozens or hundreds of data science-related tasks simultaneously (including machine learning model training) without using complex deployment frameworks. In fact, the tasks can be launched from a \u201cdata scientist\u201d-friendly interface, namely, a single Python script which can be run from an interactive shell such as Jupyter, Spyder or Cloudera Workbench. The tasks can be themselves parallelised in order to handle large amounts of data, such that we effectively add a second layer of parallelism.", "\u201cData science\u201d and \u201cautomation\u201d are two words that invariably go hand-in-hand with each other, as one of the keys goals of machine learning is to allow machines to perform tasks more quickly, with lower cost, and/or better quality than humans.", "Naturally, it wouldn\u2019t make sense for an organization to spend more on tech staff that are supposed to develop and maintain systems that automate work (data scientists, data engineers, DevOps engineers, software engineers and others) than on the staff that do the work manually. It\u2019s not thus surprising that a recurrent discussion is how much we can automate the work of data science teams themselves, for instance via automated machine learning.", "To achieve cost-effective data science automation, it is imperative to able to harness computational power from public or private clouds; after all, the cost of hardware is quite low compared to the cost of highly skilled technical staff. While technology to achieve so is certainly available, many organisations ended up facing the \u201cbig data software engineer vs data scientist conundrum\u201d, or more precisely, the drastic discrepancy between", "Some organisations would make \u201cdata scientists\u201d responsible for developing the analytics models in some sort of \u201ccontrolled analytics environment\u201d where one does not need to think too much about the underlying computational resources or sharing the resources with other processes, and \u201cbig data software engineers\u201d responsible for coding \u201cproduction-ready\u201d versions of the models developed by data scientists and deploy them into production. This setup resulted in obvious inefficiencies, such as:", "Different organisations dealt with this situation with different ways, either by forcing big data software engineers and data scientists learn the skills of the \u201cother role\u201d, or by creating a \u201cthird role\u201d, named \u201cMachine Learning Engineer\u201d to bridge the gap between the two roles.", "But the fact is that nowadays, there are far more resources in terms of allowing data scientists without exceptional software engineering skills to work in \u201crealistic\u201d environments, i.e. similar to production, in terms of computational complexity. Machine learning libraries such as Spark MLLib, Kubeflow, Tensorflow-GPU, and MMLSpark allow data preparation and model training to be distributed across multiple CPUs, GPUs, or a combination of both; at the same time, frameworks such as Apache Hadoop YARN and Kubernetes allow data scientists to work simultaneously using the same computational resources, by understanding only basic concepts about the underlying server infrastructure, such as number of available CPUs/GPUs and available memory.", "The intent of this article is to provide an example of how these libraries and frameworks, as well as massive (but shared) computational resources, can be leveraged together in order to automate the creation and testing of data science models.", "Frameworks like Spark and Kubeflow make easy to distribute a Big Data task, such as feature processing or machine learning model training, across GPUs and/or hundreds of CPUs without a detailed understanding of the server architecture. On the other hand, executing tasks in parallel, rather than individual parallelised tasks, is not as seamless. Of course, it\u2019s not hard for a data scientist to work with two or three PySpark sessions in Jupyter at the same time, but for the sake of automation, we might be rather interested in running dozens and hundreds of tasks simultaneously, all specified in a programmatic way with minimal human interference.", "Naturally, one may ask why bother with running tasks in parallel, instead of simply increasing the number of cores per task and make each task run in a shorter time. There are two reasons:", "It is certainly possible, using deployment tools such as Airflow, to run arbitrarily complex, dynamically defined and highly automated data analytics pipelines involving parallelised tasks. However, these tools require low-level scripting and configuration and aren\u2019t suited for quick \u201ctrial and error\u201d experiments carried on by data scientists on a daily basis, often accustomed to try and re-try ideas quickly in interactive shells such as Jupyter or Spyder. Also, taking us back to the previously mentioned \u201cbig data software engineer vs data scientist\u201d conundrum, organisations might prefer data scientists to spend their time focusing on experimenting with the data and generating business value, not on getting immersed in low-level implementation or deployment.", "In this article, I will show how we can make use of Apache Hadoop YARN to launch and monitor multiple jobs in a Hadoop cluster simultaneously, (including individually parallelised Spark jobs), directly from any Python code (including code from interactive Python shells such as Jupyter), via Python multithreading. While the example will consist of training multiple machine learning models in parallel, I will provide a generic framework that can be used to launch arbitrary data tasks such as feature engineering and model metric computation.", "Some applications for multiple model parallel training are:", "In our framework, I will call the main task, i.e. the Python code that creates the additional tasks to run in parallel, as the controller task, and the tasks being started by the controller task as the subordinate tasks. (I intentionally avoid using the expression \u201cworker\u201d to avoid confusion, as in Spark, \u201cworker\u201d is a synonym for Spark executor)", "The controller task is responsible for:", "An interesting aspect of YARN is that it allows Spark to be used both in the controller and subordinate tasks. Although neither is necessary, this allows us to handle arbitrarily large datasets without needing to worry ourselves with data engineering, as long as we have enough computational resources. Namely, the controller task can run Spark in client mode, and the subordinate tasks can run Spark in cluster mode:", "The framework is illustrated in the figure below:", "There are two things to note about the example above:", "Before I delve into parallelisation, I will first explain how to execute a subordinate task from a controller task written in Python. As mentioned before, we will do so using the spark-submit shell script contained in the Apache Spark installation, such that the subordinate task will be technically a Spark job, although it does not necessarily has executors or Spark code as I mentioned before.", "In principle, we can use spark-submit from Python by simply calling the os.system function, which allows us to execute a shell command from Python. In practice, we need to be able to debug and monitor the task; for that purpose, it is better to use the excellent subprocess library. An example:", "At the beginning of the code I set the path containing the cluster mode base Spark configuration, which is later used to change the SPARK_CONF_DIR environmental variable. This is an actually crucial step if the controller task is configured to run in Spark in client mode since the Spark configuration for cluster mode is typically different than for client mode.", "If you don\u2019t know much about how to configure Spark in cluster mode, you can start by making a copy of the existing SPARK_CONF_DIR. Inside the spark-defaults.conf file we need to have", "and certain configuration options, such as spark.yarn.rmProxy.enabled and the spark.driver.options.* options need to be disabled as there is no network-specific configuration for the driver when running Spark in cluster mode. Check the Spark on YARN documentation if you are in doubt. Of course, if the controller task is also running Spark in cluster mode, there is no need to have a separate configuration.", "Now, looking at the subsequent steps:", "Here I set up the application name, additional Spark configuration options and the command to be executed by the spark-submit script. These are straightforward to understand, but the application name is particularly important in our case \u2014 we will later understand why. We also submit a custom Python package via the spark.yarn.dist.files configuration parameter, which as I will show later, is especially handy since the subordinate task runs in the Hadoop cluster and hence has no access to the Python functions available in the local (client) environment.", "Note also that I specify two HDFS paths as arguments to the lightgbm_training.py Python script (the subordinate task\u2019s code), for a similar reason to above: since the Python script will run in the Hadoop cluster, it will not have access to any files in the client environment\u2019s file system, and hence any files to be exchanged between controller or subordinate task must be either explicitly submitted via spark.yarn.dist.files or put into a shared file system such as HDFS or AWS S3.", "After preparing the spark-submit shell command line, we are ready to execute it using the subprocess.Popen command:", "We set shell=True to make Python initiate a separate shell process to execute the command, rather than attempting to initiate spark-submit directly from the Python process. Although setting shell=False is generally preferable when using the subprocess library, doing so restricts the command line format and it\u2019s not feasible in our case.", "The stdout, stderr, bufsize and universal_newlines arguments are used to handle the output (STDOUT) and error messages (STDERR) issued by the shell command during execution time. When we are executing multiple subordinate tasks in parallel, we will probably want to ignore all execution time messages as they will be highly cluttered and impossible to interpret anyways. This is also useful to save memory for reasons we will explain later. However, before attempting to run multiple tasks in parallel, it is certainly best to first make sure that each individual task will work properly, by running a single subordinate task with output/error messages enabled.", "In the example I set stdout=subprocess.PIPE, stderr=subprocess.STDOUT, bufsize=1 and universal_newlines=True, which basically, will direct all shell command output to a First In First Out (FIFO) queue named subprocess.PIPE.", "Note that when running a Spark job in cluster mode, subprocess.PIPE will only have access to messages from the YARN Application Master, not the driver or executors. To check the driver and executor messages, you might look at the Hadoop cluster UI via your browser, or retrieve the driver and executor logs post-execution as I will show later. Additionally, if file logging is enabled in the log4j.properties file (located in the Spark configuration), the messages from the Application Master will be logged into a file rather than directed to subprocess.PIPE, so disable file logging if needed.", "Finally, to display the output/error messages in the Python script\u2019s output, I continue the code above as follows:", "The purpose of cmd_output.communicate() is to wait for the process to finish after subprocess.PIPE is empty, i.e. no more outputs from the subordinate task are written to it. It highly advisable to read the entire queue before calling cmd_output.communicate() method as done above, to prevent the queue from increasing in size and wasting memory.", "As I mentioned earlier, when we run tasks in parallel we do not want debug messages to be displayed; moreover, if a large number of tasks are sending messages to an in-memory FIFO queue at the same time, memory usage will increase messages aren\u2019t being read from the queue as fast as they are generated. A version of the code from the previous section without debugging, starting with the call to spark-submit, is as follows:", "The code starts by launching the subordinate task as before, but with debugging disabled:", "Since there are no debug messages to be displayed when the process is running, we use cmd_output.wait instead of cmd_output.communicate() to wait for the task to finish. Note that although we won\u2019t see the Application Master\u2019s messages, we can still debug the Spark job\u2019s driver and executor in runtime via the Hadoop cluster UI.", "However, we still need to be able to monitor the task from a programmatic point of view; more specifically, the controller task needs to know when the subordinate task has finished, whether it was successful, and take appropriate action in case of failure. For that purpose, we can use the application name that we set in the beginning:", "The application name can be used by YARN to retrieve the YARN application ID, which allows us to retrieve the status and other information about the subordinate task. Again, we can resort to the subprocess library to define a function that can retrieve the application ID from the application name:", "Observe that getYARNApplicationID parses the output of the yarn application -list shell command. Depending on your Hadoop version the output format may be slightly different and the parsing needs to be adjusted accordingly. If in doubt, you can test the format by running the following command in the terminal:", "The tricky aspect is that this method can only work if the application name is unique in the Hadoop cluster. Therefore, you need to make sure you are creating a unique application name, for instance by including timestamps, random strings, your user ID, etc. Optionally, you can also add other filters when attempting to parse the output of yarn application -list, for example, the user ID, the YARN queue name or the time of the day.", "Since the Spark job takes some time to be registered in YARN after it has been launched using spark-submit, I implemented the loop:", "where max_wait_time_job_start_s is the time to wait for the registration in seconds, which may need to be adjusted according to your environment.", "is straightforward; if there is no application ID, it means the Spark job has not been successfully launched and we need to throw an exception. This may also indicate that we need to increase max_wait_time_job_start_s, or change how the output of yarn application -list is parsed inside getYARNApplicationID.", "After the subordinate task has finished, checking its final status can be done as follows:", "where again, you may need to tune the parsing of yarn application -status depending on your Hadoop version. How to handle the final status is entirely up to you, but one possibility is to store the Spark job\u2019s driver and executor log in a file and raise an exception. For example:", "If not obvious, before attempting to execute subordinate tasks in parallel, make sure to test as many as tasks as possible without parallelisation, as debugging parallel tasks can be incredibly difficult.", "To perform parallelisation we will use Python\u2019s concurrent library. The concurrent library uses multithreading and not multiprocessing; i.e. the threads do run in the same processor, such that from the side of the controller task, there is no real parallel processing. However, since the threads started in the controller task are in I/O mode (unblocked) when waiting for the subordinate tasks to finish, multiple subordinate tasks can be launched asynchronously, such that they will actually run in parallel in the side of the Hadoop cluster. While we can technically use the multiprocessing library instead of the concurrent library to achieve parallelism also from the controller task\u2019s side, I would advise against it as it will substantially increase the memory consumption in the client environment for little benefit \u2014 the idea is that the \u201ctough processing\u201d is done in the Hadoop cluster.", "When we launch a Spark job, we are typically aware of the constraints of processing and memory in the cluster environment, especially in the case of a shared environment, and use configuration parameters such as spark.executor.memory and spark.executor.instances in order to control the task\u2019s processing and memory consumption. The same needs to be done in our case; we need to limit the number of subordinate tasks that execute simultaneously according to the availability of computational resources in the cluster, such that when we reach this limit, a subordinate task can only be started after another has finished.", "The concurrent package offers the futures.ThreadPoolExecutor class which allows us to start multiple threads and wait for them to finish. The class also allows us to limit the number of threads doing active processing(i.e. not blocked by I/O) via the max_workers argument. However, as I mentioned before, a thread in the controller task is treated as being blocked by I/O when the subordinate task is running, which means that max_workers won\u2019t effectively limit the number of threads. As result, all subordinate tasks will be submitted nearly simultaneously and the Hadoop cluster can become overloaded.", "This can be solved rather easily by modifying the futures.ThreadPoolExecutor class as follows:", "This new class ThreadPoolExecutorWithQueueSizeLimit works exactly like futures.ThreadPoolExecutor, but it won\u2019t allow more than maxsize threads to exist at any point of time, effectively limiting the number of subordinate tasks running simultaneously in the Hadoop cluster.", "We now need to define a function, containing the execution code of the thread, which can be passed as an argument to the class ThreadPoolExecutorWithQueueSizeLimit. Based on the previous code for executing a subordinate task from Python without debugging messages, I present the following generic thread execution function:", "As you can see, the function uses the previously defined functions getYARNApplicationID and getSparkJobFinalStatus, and the application name, the spark-submit command line and the directory to store the error logs are passed as arguments to the function.", "Note that the function raises an exception in case the yarn application ID cannot be found, or the status of the Spark job is not successful. But depending on the case, we may just want the function to return a False value, such that the controller task knows that this particular subordinate task has not been successful and needs to be executed again, without need to run again the tasks that have been already successful. In this case, we just need to replace line", "The next step is to create a generic code to start the threads and wait for their completion, as follows:", "The mandatory arguments to the function are:", "The output of the function is also a dictionary containing the return value (True or False) of each subordinate task, indexed by application name. The optional argument is dict_success_app, that can be the return value from a previous execution from the function, in case we only want to run the subordinate tasks that have not been already successful. I will show later how that can be accomplished.", "For the reader\u2019s convenience, I put together the complete code of the parallelisation framework below:", "In this example, I will show how to use the framework above to parallelise training of a multi-label classifier with hundreds of labels. Basically, we will train multiple binary classifiers in parallel, where the training of each binary model is itself parallelised via Spark. The individual binary classifiers are Gradient Boosting models trained using the Spark version of the popular LightGBM package, contained in the Microsoft Machine Learning for Spark (MMLSpark) library.", "By using the framework above, there are only two other things that the controller task needs to do:", "For the first part, we can start by looking at our previous example where we are submitting a standalone subordinate job:", "What do we need to adapt the code for multi-label classification? First, for the reasons already mentioned, the application name needs to be completely unique. Assuming that the label columns of the dataset input_data.parquet are contained in a variable lst_labels, one way to ensure likely unique applications IDs for each subordinate task would something like:", "This ensures that application names will be unique as long as the controller task is not started more once in the same millisecond (of course, if we have a shared YARN cluster other adaptions may be needed to make the application names unique, such as adding the username to the application name).", "We are yet to discuss how the subordinate task code contained in lightgbm_training.py looks like, but let\u2019s suppose it:", "In this case, the controller task needs to pass the HDFS path of the training dataset, the HDFS path to store the trained models, and the label to be used for each subordinate task, via command-line arguments to lightgbm_training.py. This can be done as shown below:", "Of course, there are many other ways to customise the subordinate tasks. We might want to use different model training hyperparameters, different datasets, different Spark configurations, or even use different Python scripts for each subordinate task. The fact that we allow the spark-submit command line to be unique for each subtask allows complete customisation.", "For the reader\u2019s convenience, I put together the controller task\u2019s code prior to and until calling executeAllThreads:", "For the second part, i.e. what the controller task should do after returning from executeAllThreads, assuming that the successful tasks have saved the trained models in the HDFS system, we can just open these files and process them as appropriate, for instance applying the models to some appropriate validation dataset, generating plots and computing performance metrics.", "If we use the parallelisation framework presented earlier as it is, there won\u2019t be \u201cunsuccessful subordinate tasks\u201d as any failure will result in an exception being raised. But if we modified executeThread to return False in case of task failure, we might store the returning dict_success_app dictionary in a JSON or Pickle file such that we can later investigate and fix the failed tasks. Finally, we can call again executeAllThreads with the optional argument dict_success_app set such that we re-run only the failed tasks.", "Let us now write the code of the subordinate task in the lightgbm_training.py script. The first step is to read the input arguments of the script, i.e. the path of the training dataset in the HDFS filesystem, the path to store the models and the name of the label column:", "Since we are using the Spark version of LightGBM, we need to create a Spark session, which we do as follows:", "Note that there is no need to set up any configuration for the Spark session, as it has been already done in the command line submitted by the controller task. Also, since we explicitly submitted a custom Python package custom_packages.tar to the Spark job, we need to use the addPyFile function to make the contents of the package usable inside our code, as the package is not included in the PYTHONPATH environment variable of the Hadoop cluster.", "The code that does the actual processing in the subordinate task is pretty straightforward. The subordinate task will read the training data, call some pre-processing function inside custom_packages.tar (say custom_data_preprocessing.datasetBalancing), perform the model training, and save the trained model with a unique name back in the HDFS file system:", "The full code of lightgbm_training.py is put together below for the reader\u2019s convenience:", "It is easy to see that the framework presented in this article can be re-used for various tasks other than multiple machine learning model training. A question is that may arise is whether it can be used for different cluster environments, for instance with Spark on Mesos rather than Spark on YARN. I believe so, but some adaptations are needed as the presented code relies heavily on the yarn command to monitor the subordinate tasks.", "By using this framework, data scientists can focus more of their time on designing the data tasks, not on manually executing them for dozens or hundreds of small variations. Another advantage is that by harnessing parallelisation, the tasks can be done in much less time, or from a different perspective, without requiring multiple data scientists to work simultaneously to complete the tasks in the same amount of time.", "Naturally, this article presents only one of many ways to improve data science automation. Organisations that realise that the time of data scientists and other skilled tech professionals is highly valuable will certainly find increasingly more ways to help these professionals focus on higher-level problems.", "Your home for data science. A Medium publication sharing concepts, ideas and codes.", "Data scientist at DBS Bank. All views are solely my own and not from my organization."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F2fa9670dd579&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Towards Data Science"}, {"url": "https://edsonaoki.medium.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Edson Hiroshi Aoki"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4db29e9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=post_page-35b4db29e9b----2fa9670dd579---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://medium.com/microsoftazure/how-do-teams-work-together-on-an-automated-machine-learning-project-11bed3a7ad86", "anchor_text": "automated machine learning"}, {"url": "https://medium.com/@tomaszdudek/but-what-is-this-machine-learning-engineer-actually-doing-18464d5c699", "anchor_text": "creating a \u201cthird role\u201d, named \u201cMachine Learning Engineer\u201d to bridge the gap between the two roles"}, {"url": "https://spark.apache.org/docs/latest/running-on-yarn.html", "anchor_text": "Spark on YARN documentation"}, {"url": "https://docs.python.org/3/library/concurrent.html", "anchor_text": "The concurrent library uses multithreading and not multiprocessing"}, {"url": "https://github.com/Azure/mmlspark", "anchor_text": "Microsoft Machine Learning for Spark"}, {"url": "https://medium.com/tag/big-data?source=post_page-----2fa9670dd579---------------big_data-----------------", "anchor_text": "Big Data"}, {"url": "https://medium.com/tag/data-science?source=post_page-----2fa9670dd579---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----2fa9670dd579---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----2fa9670dd579---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/spark?source=post_page-----2fa9670dd579---------------spark-----------------", "anchor_text": "Spark"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=-----2fa9670dd579---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=-----2fa9670dd579---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://towardsdatascience.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "More from Towards Data Science"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Ftowards-data-science%2F2fa9670dd579&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&collection=Towards+Data+Science&collectionId=7f60cf5620c9&source=post_page-----2fa9670dd579---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://towardsdatascience.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Read more from Towards Data Science"}, {"url": "https://medium.com/?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----2fa9670dd579--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----2fa9670dd579--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----2fa9670dd579--------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://edsonaoki.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Edson Hiroshi Aoki"}, {"url": "https://edsonaoki.medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "107 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F35b4db29e9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=post_page-35b4db29e9b--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8b9561c02d4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-train-multiple-machine-learning-models-and-run-other-data-tasks-in-parallel-by-combining-2fa9670dd579&newsletterV3=35b4db29e9b&newsletterV3Id=8b9561c02d4e&user=Edson+Hiroshi+Aoki&userId=35b4db29e9b&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}