{"url": "https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c", "time": 1682988279.940923, "path": "chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c/", "webpage": {"metadata": {"title": "Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot | by Stefan Kojouharov | Chatbots Life", "h1": "Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot", "description": "Over the past few months I have been collecting the best resources on NLP and how to apply NLP and Deep Learning to Chatbots. Every once in awhile, I would run across an exception piece of content\u2026"}, "outgoing_paragraph_urls": [{"url": "https://twitter.com/home?status=Really%20liked,%20%22Ultimate%20Guide%20to%20Leveraging%20NLP%20%26%20Machine%20Learning%20for%20your%20Chatbot%22%20by%20%40kojouharov%20%40ChatbotsLife%20%23ai%20%23bot", "anchor_text": "let me know here", "paragraph_index": 7}, {"url": "http://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence", "paragraph_index": 11}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embed", "paragraph_index": 21}, {"url": "http://arxiv.org/abs/1507.04808", "anchor_text": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models", "paragraph_index": 21}, {"url": "http://arxiv.org/abs/1510.08565", "anchor_text": "Attention with Intention for a Neural Network Conversation Model", "paragraph_index": 21}, {"url": "http://arxiv.org/abs/1603.06155", "anchor_text": "A Persona-Based Neural Conversation Model", "paragraph_index": 22}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU", "paragraph_index": 23}, {"url": "http://arxiv.org/abs/1603.08023", "anchor_text": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "paragraph_index": 23}, {"url": "http://googleresearch.blogspot.com/2015/11/computer-respond-to-this-email.html", "anchor_text": "tended to respond with \u201cI love you\u201d", "paragraph_index": 24}, {"url": "http://arxiv.org/abs/1510.03055", "anchor_text": "Some researchers have tried to artificially promote diversity through various objective functions", "paragraph_index": 24}, {"url": "http://www.seattletimes.com/business/baidu-research-chief-andrew-ng-fixed-on-self-taught-computers-self-driving-cars/", "anchor_text": "recent interview", "paragraph_index": 27}, {"url": "http://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3", "anchor_text": "like Microsoft\u2019s Tay did", "paragraph_index": 30}, {"url": "https://github.com/dennybritz/chatbot-retrieval/", "anchor_text": "The Code and data for this tutorial is on Github.", "paragraph_index": 31}, {"url": "http://arxiv.org/abs/1606.04870", "anchor_text": "Smart Reply", "paragraph_index": 32}, {"url": "https://twitter.com/home?status=Please%20write%20more%20articles%20like%20%22Ultimate%20Guide%20to%20Leveraging%20NLP%20%26%20Machine%20Learning%20for%20your%20Chatbot%22%20by%20%40kojouharov%20%40ChatbotsLife%20%23ai%20%23bot", "anchor_text": "let me know here.", "paragraph_index": 33}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper", "paragraph_index": 34}, {"url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "anchor_text": "github", "paragraph_index": 34}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper", "paragraph_index": 34}, {"url": "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize", "anchor_text": "tokenized", "paragraph_index": 36}, {"url": "http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball", "anchor_text": "stemmed", "paragraph_index": 36}, {"url": "http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet", "anchor_text": "lemmatized", "paragraph_index": 36}, {"url": "http://www.nltk.org/", "anchor_text": "NLTK tool", "paragraph_index": 36}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/notebooks/Data%20Exploration.ipynb", "anchor_text": "Check out the Jupyter notebook to see the data analysis", "paragraph_index": 36}, {"url": "http://arxiv.org/abs/1606.04870", "anchor_text": "Smart Reply uses clustering techniques to come up with a set of possible responses", "paragraph_index": 39}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "tf-idf", "paragraph_index": 44}, {"url": "http://scikit-learn.org/", "anchor_text": "scikit-learn", "paragraph_index": 44}, {"url": "https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html", "anchor_text": "seq2seq model", "paragraph_index": 49}, {"url": "http://arxiv.org/abs/1510.03753", "anchor_text": "reported", "paragraph_index": 49}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper", "paragraph_index": 50}, {"url": "http://www.numpy.org/", "anchor_text": "numpy", "paragraph_index": 52}, {"url": "http://pandas.pydata.org/", "anchor_text": "pandas", "paragraph_index": 52}, {"url": "http://www.tensorflow.org/", "anchor_text": "Tensorflow", "paragraph_index": 52}, {"url": "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn", "anchor_text": "TF Learn", "paragraph_index": 52}, {"url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "anchor_text": "dataset", "paragraph_index": 53}, {"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto", "anchor_text": "Example", "paragraph_index": 53}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/scripts/prepare_data.py", "anchor_text": "prepare_data.py", "paragraph_index": 55}, {"url": "https://drive.google.com/open?id=0B_bZck-ksdkpVEtVc1R6Y01HMWM", "anchor_text": "download the data files here", "paragraph_index": 55}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py", "anchor_text": "udc_inputs.py", "paragraph_index": 60}, {"url": "https://docs.python.org/2/library/functools.html#functools.partial", "anchor_text": "functools.partial", "paragraph_index": 62}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py", "anchor_text": "udc_train.py", "paragraph_index": 68}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_hparams.py", "anchor_text": "hparams.py", "paragraph_index": 69}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_model.py", "anchor_text": "create_model_fn", "paragraph_index": 70}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/models/dual_encoder.py", "anchor_text": "dual_encoder.py", "paragraph_index": 79}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py", "anchor_text": "udc_train.py", "paragraph_index": 79}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_predict.py", "anchor_text": "udc_predict.py", "paragraph_index": 84}, {"url": "https://github.com/dennybritz/chatbot-retrieval/", "anchor_text": "The Code and data for this tutorial is on Github, so check it out.", "paragraph_index": 86}, {"url": "http://m.me/SmartNotesBot", "anchor_text": "Smart Notes Bot", "paragraph_index": 90}, {"url": "http://m.me/1801531300076339", "anchor_text": "Stefan\u2019s Bot", "paragraph_index": 91}, {"url": "https://goo.gl/forms/EsadoWADHsAq3XIh2", "anchor_text": "contact me", "paragraph_index": 92}], "all_paragraphs": ["Over the past few months I have been collecting the best resources on NLP and how to apply NLP and Deep Learning to Chatbots.", "Every once in awhile, I would run across an exception piece of content and I quickly started putting together a master list. Soon I found myself sharing this list and some of the most useful articles with developers and other people in bot community.", "In process, my list became a Guide and after some urging, I have decided to share it or at least a condensed version of it -for length reasons.", "This guide is mostly based on the work done by Denny Britz who has done a phenomenal job exploring the depths of Deep Learning for Bots. Code Snippets and Github included!", "Without further ado\u2026 Let us Begin!", "Chatbots, are a hot topic and many companies are hoping to develop bots to have natural conversations indistinguishable from human ones, and many are claiming to be using NLP and Deep Learning techniques to make this possible. But with all the hype around AI it\u2019s sometimes difficult to tell fact from fiction.", "In this series I want to go over some of the Deep Learning techniques that are used to build conversational agents, starting off by explaining where we are right now, what\u2019s possible, and what will stay nearly impossible for at least a little while.", "If you find this article interesting you can let me know here.", "Retrieval-based models (easier) use a repository of predefined responses and some kind of heuristic to pick an appropriate response based on the input and context. The heuristic could be as simple as a rule-based expression match, or as complex as an ensemble of Machine Learning classifiers. These systems don\u2019t generate any new text, they just pick a response from a fixed set.", "Generative models (harder) don\u2019t rely on pre-defined responses. They generate new responses from scratch. Generative models are typically based on Machine Translation techniques, but instead of translating from one language to another, we \u201ctranslate\u201d from an input to an output (response).", "Both approaches have some obvious pros and cons. Due to the repository of handcrafted responses, retrieval-based methods don\u2019t make grammatical mistakes. However, they may be unable to handle unseen cases for which no appropriate predefined response exists. For the same reasons, these models can\u2019t refer back to contextual entity information like names mentioned earlier in the conversation. Generative models are \u201csmarter\u201d. They can refer back to entities in the input and give the impression that you\u2019re talking to a human. However, these models are hard to train, are quite likely to make grammatical mistakes (especially on longer sentences), and typically require huge amounts of training data.", "Deep Learning techniques can be used for both retrieval-based or generative models, but research seems to be moving into the generative direction. Deep Learning architectures likeSequence to Sequence are uniquely suited for generating text and researchers are hoping to make rapid progress in this area. However, we\u2019re still at the early stages of building generative models that work reasonably well. Production systems are more likely to be retrieval-based for now.", "The longer the conversation the more difficult to automate it. On one side of the spectrum areShort-Text Conversations (easier) where the goal is to create a single response to a single input. For example, you may receive a specific question from a user and reply with an appropriate answer. Then there are long conversations (harder) where you go through multiple turns and need to keep track of what has been said. Customer support conversations are typically long conversational threads with multiple questions.", "In an open domain (harder) setting the user can take the conversation anywhere. There isn\u2019t necessarily have a well-defined goal or intention. Conversations on social media sites like Twitter and Reddit are typically open domain \u2014 they can go into all kinds of directions. The infinite number of topics and the fact that a certain amount of world knowledge is required to create reasonable responses makes this a hard problem.", "\u201cOpen Domain: I can ask a question about any topic\u2026 and expect a relevant response. (Harder) Think of a long conversation around refinancing my mortgage where I could ask anything.\u201d Mark Clark", "In a closed domain (easier) setting the space of possible inputs and outputs is somewhat limited because the system is trying to achieve a very specific goal. Technical Customer Support or Shopping Assistants are examples of closed domain problems. These systems don\u2019t need to be able to talk about politics, they just need to fulfill their specific task as efficiently as possible. Sure, users can still take the conversation anywhere they want, but the system isn\u2019t required to handle all these cases \u2014 and the users don\u2019t expect it to.", "\u201cClosed Domain: You can ask a limited set of questions on specific topics. (Easier). What is the Weather in Miami?\u201d", "\u201cSquare 1 is a great first step for a chatbot because it is contained, may not require the complexity of smart machines and can deliver both business and user value.", "Square 2, questions are asked and the Chatbot has smart machine technology that generates responses. Generated responses allow the Chatbot to handle both the common questions and some unforeseen cases for which there are no predefined responses. The smart machine can handle longer conversations and appear to be more human-like. But generative response increases complexity, often by a lot.", "The way we get around this problem in the contact center today is when there is an unforeseen case for which there is no predefined responses in self-service, we pass the call to an agent.\u201d Mark Clark", "There are some obvious and not-so-obvious challenges when building conversational agents most of which are active research areas.", "To produce sensible responses systems may need to incorporate both linguistic context andphysical context. In long dialogs people keep track of what has been said and what information has been exchanged. That\u2019s an example of linguistic context. The most common approach is toembed the conversation into a vector, but doing that with long conversations is challenging. Experiments in Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models and Attention with Intention for a Neural Network Conversation Model both go into that direction. One may also need to incorporate other kinds of contextual data such as date/time, location, or information about a user.", "When generating responses the agent should ideally produce consistent answers to semantically identical inputs. For example, you want to get the same reply to \u201cHow old are you?\u201d and \u201cWhat is your age?\u201d. This may sound simple, but incorporating such fixed knowledge or \u201cpersonality\u201d into models is very much a research problem. Many systems learn to generate linguistic plausible responses, but they are not trained to generate semantically consistent ones. Usually that\u2019s because they are trained on a lot of data from multiple different users. Models like that in A Persona-Based Neural Conversation Model are making first steps into the direction of explicitly modeling a personality.", "The ideal way to evaluate a conversational agent is to measure whether or not it is fulfilling its task, e.g. solve a customer support problem, in a given conversation. But such labels are expensive to obtain because they require human judgment and evaluation. Sometimes there is no well-defined goal, as is the case with open-domain models. Common metrics such as BLEUthat are used for Machine Translation and are based on text matching aren\u2019t well suited because sensible responses can contain completely different words or phrases. In fact, in How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation researchers find that none of the commonly used metrics really correlate with human judgment.", "A common problem with generative systems is that they tend to produce generic responses like \u201cThat\u2019s great!\u201d or \u201cI don\u2019t know\u201d that work for a lot of input cases. Early versions of Google\u2019s Smart Reply tended to respond with \u201cI love you\u201d to almost anything. That\u2019s partly a result of how these systems are trained, both in terms of data and in terms of actual training objective/algorithm. Some researchers have tried to artificially promote diversity through various objective functions. However, humans typically produce responses that are specific to the input and carry an intention. Because generative systems (and particularly open-domain systems) aren\u2019t trained to have specific intentions they lack this kind of diversity.", "Given all the cutting edge research right now, where are we and how well do these systems actually work? Let\u2019s consider our taxonomy again. A retrieval-based open domain system is obviously impossible because you can never handcraft enough responses to cover all cases. A generative open-domain system is almost Artificial General Intelligence (AGI) because it needs to handle all possible scenarios. We\u2019re very far away from that as well (but a lot of research is going on in that area).", "This leaves us with problems in restricted domains where both generative and retrieval based methods are appropriate. The longer the conversations and the more important the context, the more difficult the problem becomes.", "In a recent interview, Andrew Ng, now chief scientist of Baidu, puts it well:", "Most of the value of deep learning today is in narrow domains where you can get a lot of data. Here\u2019s one example of something it cannot do: have a meaningful conversation. There are demos, and if you cherry-pick the conversation, it looks like it\u2019s having a meaningful conversation, but if you actually try it yourself, it quickly goes off the rails.", "Many companies start off by outsourcing their conversations to human workers and promise that they can \u201cautomate\u201d it once they\u2019ve collected enough data. That\u2019s likely to happen only if they are operating in a pretty narrow domain \u2014 like a chat interface to call an Uber for example. Anything that\u2019s a bit more open domain (like sales emails) is beyond what we can currently do. However, we can also use these systems to assist human workers by proposing and correcting responses. That\u2019s much more feasible.", "Grammatical mistakes in production systems are very costly and may drive away users. That\u2019s why most systems are probably best off using retrieval-based methods that are free of grammatical errors and offensive responses. If companies can somehow get their hands on huge amounts of data then generative models become feasible \u2014 but they must be assisted by other techniques to prevent them from going off the rails like Microsoft\u2019s Tay did.", "The Code and data for this tutorial is on Github.", "The vast majority of production systems today are retrieval-based, or a combination of retrieval-based and generative. Google\u2019s Smart Reply is a good example. Generative models are an active area of research, but we\u2019re not quite there yet. If you want to build a conversational agent today your best bet is most likely a retrieval-based model.", "If you want me to write more articles like this, please let me know here.", "In this post we\u2019ll work with the Ubuntu Dialog Corpus (paper, github). The Ubuntu Dialog Corpus (UDC) is one of the largest public dialog datasets available. It\u2019s based on chat logs from the Ubuntu channels on a public IRC network. The paper goes into detail on how exactly the corpus was created, so I won\u2019t repeat that here. However, it\u2019s important to understand what kind of data we\u2019re working with, so let\u2019s do some exploration first.", "The training data consists of 1,000,000 examples, 50% positive (label 1) and 50% negative (label 0). Each example consists of a context, the conversation up to this point, and an utterance, a response to the context. A positive label means that an utterance was an actual response to a context, and a negative label means that the utterance wasn\u2019t \u2014 it was picked randomly from somewhere in the corpus. Here is some sample data.", "Note that the dataset generation script has already done a bunch of preprocessing for us \u2014 it hastokenized, stemmed, and lemmatized the output using the NLTK tool. The script also replaced entities like names, locations, organizations, URLs, and system paths with special tokens. This preprocessing isn\u2019t strictly necessary, but it\u2019s likely to improve performance by a few percent. The average context is 86 words long and the average utterance is 17 words long. Check out the Jupyter notebook to see the data analysis.", "The data set comes with test and validations sets. The format of these is different from that of the training data. Each record in the test/validation set consists of a context, a ground truth utterance (the real response) and 9 incorrect utterances called distractors. The goal of the model is to assign the highest score to the true utterance, and lower scores to wrong utterances.", "The are various ways to evaluate how well our model does. A commonly used metric is recall@k. Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). If the correct one is among the picked ones we mark that test example as correct. So, a larger k means that the task becomes easier. If we set k=10 we get a recall of 100% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response.", "At this point you may be wondering how the 9 distractors were chosen. In this data set the 9 distractors were picked at random. However, in the real world you may have millions of possible responses and you don\u2019t know which one is correct. You can\u2019t possibly evaluate a million potential responses to pick the one with the highest score \u2014 that\u2019d be too expensive. Google\u2019sSmart Reply uses clustering techniques to come up with a set of possible responses to choose from first. Or, if you only have a few hundred potential responses in total you could just evaluate all of them.", "Before starting with fancy Neural Network models let\u2019s build some simple baseline models to help us understand what kind of performance we can expect. We\u2019ll use the following function to evaluate our recall@k metric:", "for predictions, label in zip(y, y_test):", "Here, y is a list of our predictions sorted by score in descending order, and y_test is the actual label. For example, a y of [0,3,1,2,5,6,4,7,8,9] Would mean that the utterance number 0 got the highest score, and utterance 9 got the lowest score. Remember that we have 10 utterances for each test example, and the first one (index 0) is always the correct one because the utterance column comes before the distractor columns in our data.", "Intuitively, a completely random predictor should get a score of 10% for recall@1, a score of 20% for recall@2, and so on. Let\u2019s see if that\u2019s the case.", "Great, seems to work. Of course we don\u2019t just want a random predictor. Another baseline that was discussed in the original paper is a tf-idf predictor. tf-idf stands for \u201cterm frequency \u2014 inverse document\u201d frequency and it measures how important a word in a document is relative to the whole corpus. Without going into too much detail (you can find many tutorials about tf-idf on the web), documents that have similar content will have similar tf-idf vectors. Intuitively, if a context and a response have similar words they are more likely to be a correct pair. At least more likely than random. Many libraries out there (such as scikit-learn) come with built-in tf-idf functions, so it\u2019s very easy to use. Let\u2019s build a tf-idf predictor and see how well it performs.", "# Convert context and utterances into tfidf vector", "# The dot product measures the similarity of the resulting vectors", "# Sort by top results and return the indices in descending order", "We can see that the tf-idf model performs significantly better than the random model. It\u2019s far from perfect though. The assumptions we made aren\u2019t that great. First of all, a response doesn\u2019t necessarily need to be similar to the context to be correct. Secondly, tf-idf ignores word order, which can be an important signal. With a Neural Network model we can do a bit better.", "The Deep Learning model we will build in this post is called a Dual Encoder LSTM network. This type of network is just one of many we could apply to this problem and it\u2019s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven\u2019t been tried yet \u2014 it\u2019s an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct. Applying other models to this problem would be an interesting project.", "The Dual Encoder LSTM we\u2019ll build looks like this (paper):", "To train the network, we also need a loss (cost) function. We\u2019ll use the binary cross-entropy loss common for classification problems. Let\u2019s call our true label for a context-response pair y. This can be either 1 (actual response) or 0 (incorrect response). Let\u2019s call our predicted probability from 4. above y\u2019. Then, the cross entropy loss is calculated as L= \u2212y * ln(y\u2019) \u2212 (1 \u2212 y) * ln(1\u2212y\u2019). The intuition behind this formula is simple. If y=1 we are left with L = -ln(y\u2019), which penalizes a prediction far away from 1, and if y=0 we are left with L= \u2212ln(1\u2212y\u2019), which penalizes a prediction far away from 0.", "For our implementation we\u2019ll use a combination of numpy, pandas, Tensorflow and TF Learn (a combination of high-level convenience functions for Tensorflow).", "The dataset originally comes in CSV format. We could work directly with CSVs, but it\u2019s better to convert our data into Tensorflow\u2019s proprietary Example format. (Quick side note: There\u2019s alsotf.SequenceExample but it doesn\u2019t seem to be supported by tf.learn yet). The main benefit of this format is that it allows us to load tensors directly from the input files and let Tensorflow handle all the shuffling, batching and queuing of inputs. As part of the preprocessing we also create a vocabulary. This means we map each word to an integer number, e.g. \u201ccat\u201d may become 2631. The TFRecord files we will generate store these integer numbers instead of the word strings. We will also save the vocabulary so that we can map back from integers to words later on.", "Each Example contains the following fields:", "The preprocessing is done by the prepare_data.py Python script, which generates 3 files:train.tfrecords, validation.tfrecords and test.tfrecords. You can run the script yourself or download the data files here.", "In order to use Tensorflow\u2019s built-in support for training and evaluation we need to create an input function \u2014 a function that returns batches of our input data. In fact, because our training and test data have different formats, we need different input functions for them. The input function should return a batch of features and labels (if available). Something along the lines of:", "# TODO Load and preprocess data here", "Because we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create_input_fn that creates an input function for the appropriate mode. It also takes a few other parameters. Here\u2019s the definition we\u2019re using:", "# TODO Load and preprocess data here", "The complete code can be found in udc_inputs.py. On a high level, the function does the following:", "We already mentioned that we want to use the recall@k metric to evaluate our model. Luckily, Tensorflow already comes with many standard evaluation metrics that we can use, including recall@k. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments:", "Above, we use functools.partial to convert a function that takes 3 arguments to one that only takes 2 arguments. Don\u2019t let the name streaming_sparse_recall_at_k confuse you. Streaming just means that the metric is accumulated over multiple batches, and sparse refers to the format of our labels.", "This brings is to an important point: What exactly is the format of our predictions during evaluation? During training, we predict the probability of the example being correct. But during evaluation our goal is to score the utterance and 9 distractors and pick the best one \u2014 we don\u2019t simply predict correct/incorrect. This means that during evaluation each example should result in a vector of 10 scores, e.g. [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11], where the scores correspond to the true response and the 9 distractors respectively. Each utterance is scored independently, so the probabilities don\u2019t need to add up to 1. Because the true response is always element 0 in array, the label for each example is 0. The example above would be counted as classified incorrectly by recall@1because the third distractor got a probability of 0.45 while the true response only got 0.34. It would be scored as correct by recall@2 however.", "Before writing the actual neural network code I like to write the boilerplate code for training and evaluating the model. That\u2019s because, as long as you adhere to the right interfaces, it\u2019s easy to swap out what kind of network you are using. Let\u2019s assume we have a model functionmodel_fn that takes as inputs our batched features, labels and mode (train or evaluation) and returns the predictions. Then we can write general-purpose code to train our model as follows:", "# We need to subclass theis manually for now. The next TF version will", "# have support ValidationMonitors with metrics built-in.", "# It\u2019s already on the master branch.", "Here we create an estimator for our model_fn, two input functions for training and evaluation data, and our evaluation metrics dictionary. We also define a monitor that evaluates our model every FLAGS.eval_every steps during training. Finally, we train the model. The training runs indefinitely, but Tensorflow automatically saves checkpoint files in MODEL_DIR, so you can stop the training at any time. A more fancy technique would be to use early stopping, which means you automatically stop training when a validation set metric stops improving (i.e. you are starting to overfit). You can see the full code in udc_train.py.", "Two things I want to mention briefly is the usage of FLAGS. This is a way to give command line parameters to the program (similar to Python\u2019s argparse). hparams is a custom object we create in hparams.py that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it.", "Now that we have set up the boilerplate code around inputs, parsing, evaluation and training it\u2019s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I\u2019ve written a create_model_fn wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions. In our case it\u2019s the Dual Encoder LSTM we described above, but we could easily swap it out for some other neural network. Let\u2019s see what that looks like:", "# Initialize embedidngs randomly or with pre-trained vectors if available", "# Embed the context and the utterance", "# We use an LSTM Cell", "# Run the utterance and context through the RNN", "# Dot product between generated response and actual response", "# Apply sigmoid to convert logits to probabilities", "# Calculate the binary cross-entropy loss", "# Mean loss across the batch of examples", "The full code is in dual_encoder.py. Given this, we can now instantiate our model function in the main routine in udc_train.py that we defined earlier.", "That\u2019s it! We can now run python udc_train.py and it should start training our networks, occasionally evaluating recall on our validation data (you can choose how often you want to evaluate using the \u2014 eval_every switch). To get a complete list of all available command line flags that we defined using tf.flags and hparams you can run python udc_train.py \u2014 help.", "After you\u2019ve trained the model you can evaluate it on the test set using python udc_test.py \u2014 model_dir=$MODEL_DIR_FROM_TRAINING, e.g. python udc_test.py \u2014 model_dir=~/github/chatbot-retrieval/runs/1467389151. This will run the recall@k evaluation metrics on the test set instead of the validation set. Note that you must call udc_test.py with the same parameters you used during training. So, if you trained with \u2014 embedding_size=128 you need to call the test script with the same.", "After training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set:", "While recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. The original paper reported 0.55, 0.72 and 0.92 for recall@1, recall@2, and recall@5 respectively, but I haven\u2019t been able to reproduce scores quite as high. Perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more.", "You can modify and run udc_predict.py to get probability scores for unseen data. For example python udc_predict.py \u2014 model_dir=./runs/1467576365/ outputs:", "You could imagine feeding in 100 potential responses to a context and then picking the one with the highest score.", "In this post we\u2019ve implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. There is still a lot of room for improvement, however. One can imagine that other neural networks do better on this task than a dual LSTM encoder. There is also a lot of room for hyperparameter optimization, or improvements to the preprocessing step. The Code and data for this tutorial is on Github, so check it out.", "I hope you have found this Condensed NLP Guide Helpful. I wanted to publish a longer version (imagine if this was 5x longer) however I don\u2019t want to scare the readers away.", "As someone who develops the front end of bots (user experience, personality, flow, etc) I find it extremely helpful to the understand the stack, know the technological pros and cons and so to be able to effectively design around NLP/NLU limitations. Ultimately a lot of the issues bots face today (eg: context) can be designed around, effectively.", "If you have any suggestions on regarding this article and how it can be improved, feel free to drop me a line.", "Creator of 10+ bots, including Smart Notes Bot. Founder of Chatbot\u2019s Life, where we help companies create great chatbots and share our insights along the way.", "Want to Talk Bots? Best way to chat directly and see my latest projects is via my Personal Bot: Stefan\u2019s Bot.", "Currently, I\u2019m consulting a number of companies on their chatbot projects. To get feedback on your Chatbot project or to Start a Chatbot Project, contact me.", "Best place to learn about Conversational AI. We share the latest News, Info, AI & NLP, Tools, Tutorials & More.", "Founder of Chatbots Life. I help Companies Make More Money using Chatbots & AI and share my Insights along the way."], "all_outgoing_urls": [{"url": "https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F531ff2dd870c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderCollection&source=---two_column_layout_nav----------------------------------", "anchor_text": "Open in app"}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_sidenav-----------", "anchor_text": "Write"}, {"url": "https://medium.com/search?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign up"}, {"url": "https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=post_page---two_column_layout_nav-----------------------global_nav-----------", "anchor_text": "Sign In"}, {"url": "https://chatbotslife.com/?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": ""}, {"url": "https://chatbotslife.com/?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Chatbots Life"}, {"url": "https://medium.com/@StefanSpeaks?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@StefanSpeaks?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Stefan Kojouharov"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F67c8653ca7e2&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&user=Stefan+Kojouharov&userId=67c8653ca7e2&source=post_page-67c8653ca7e2----531ff2dd870c---------------------follow_byline-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=--------------------------bookmark_header-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=--------------------------bookmark_header-----------", "anchor_text": "Save"}, {"url": "https://www.chatbotconference.com/", "anchor_text": ""}, {"url": "https://twitter.com/home?status=Really%20liked,%20%22Ultimate%20Guide%20to%20Leveraging%20NLP%20%26%20Machine%20Learning%20for%20your%20Chatbot%22%20by%20%40kojouharov%20%40ChatbotsLife%20%23ai%20%23bot", "anchor_text": "let me know here"}, {"url": "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463", "anchor_text": "Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big DataList of Best AI Cheat Sheetsbecominghuman.ai"}, {"url": "http://arxiv.org/abs/1409.3215", "anchor_text": "Sequence to Sequence"}, {"url": "https://chatbotslife.com/machine-learning-for-dummies-part-1-dbaca076ec07", "anchor_text": "Machine Learning for Dummies: Part 1I often get asked on how to get started with Machine Learning. Most of the time, people have troubles understanding the\u2026chatbotslife.com"}, {"url": "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463", "anchor_text": "Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big DataList of Best AI Cheat Sheetsbecominghuman.ai"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embed"}, {"url": "http://arxiv.org/abs/1507.04808", "anchor_text": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models"}, {"url": "http://arxiv.org/abs/1510.08565", "anchor_text": "Attention with Intention for a Neural Network Conversation Model"}, {"url": "http://arxiv.org/abs/1603.06155", "anchor_text": "A Persona-Based Neural Conversation Model"}, {"url": "https://en.wikipedia.org/wiki/BLEU", "anchor_text": "BLEU"}, {"url": "http://arxiv.org/abs/1603.08023", "anchor_text": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"}, {"url": "http://googleresearch.blogspot.com/2015/11/computer-respond-to-this-email.html", "anchor_text": "tended to respond with \u201cI love you\u201d"}, {"url": "http://arxiv.org/abs/1510.03055", "anchor_text": "Some researchers have tried to artificially promote diversity through various objective functions"}, {"url": "https://chatbotslife.com/finding-the-genre-of-a-song-with-deep-learning-da8f59a61194", "anchor_text": "Finding the genre of a song with Deep Learning \u2014 A.I. Odyssey part. 1A step-by-step guide to make your computer a music expert.chatbotslife.com"}, {"url": "http://www.seattletimes.com/business/baidu-research-chief-andrew-ng-fixed-on-self-taught-computers-self-driving-cars/", "anchor_text": "recent interview"}, {"url": "http://www.businessinsider.com/microsoft-deletes-racist-genocidal-tweets-from-ai-chatbot-tay-2016-3", "anchor_text": "like Microsoft\u2019s Tay did"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/", "anchor_text": "The Code and data for this tutorial is on Github."}, {"url": "http://arxiv.org/abs/1606.04870", "anchor_text": "Smart Reply"}, {"url": "https://twitter.com/home?status=Please%20write%20more%20articles%20like%20%22Ultimate%20Guide%20to%20Leveraging%20NLP%20%26%20Machine%20Learning%20for%20your%20Chatbot%22%20by%20%40kojouharov%20%40ChatbotsLife%20%23ai%20%23bot", "anchor_text": "let me know here."}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper"}, {"url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "anchor_text": "github"}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper"}, {"url": "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize", "anchor_text": "tokenized"}, {"url": "http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball", "anchor_text": "stemmed"}, {"url": "http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet", "anchor_text": "lemmatized"}, {"url": "http://www.nltk.org/", "anchor_text": "NLTK tool"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/notebooks/Data%20Exploration.ipynb", "anchor_text": "Check out the Jupyter notebook to see the data analysis"}, {"url": "http://arxiv.org/abs/1606.04870", "anchor_text": "Smart Reply uses clustering techniques to come up with a set of possible responses"}, {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "anchor_text": "tf-idf"}, {"url": "http://scikit-learn.org/", "anchor_text": "scikit-learn"}, {"url": "https://becominghuman.ai/cheat-sheets-for-ai-neural-networks-machine-learning-deep-learning-big-data-678c51b4b463", "anchor_text": "Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big DataList of Best AI Cheat Sheetsbecominghuman.ai"}, {"url": "https://www.tensorflow.org/versions/r0.9/tutorials/seq2seq/index.html", "anchor_text": "seq2seq model"}, {"url": "http://arxiv.org/abs/1510.03753", "anchor_text": "reported"}, {"url": "http://arxiv.org/abs/1506.08909", "anchor_text": "paper"}, {"url": "https://en.wikipedia.org/wiki/Word_embedding", "anchor_text": "embedded"}, {"url": "http://nlp.stanford.edu/projects/glove/", "anchor_text": "GloVe"}, {"url": "http://www.numpy.org/", "anchor_text": "numpy"}, {"url": "http://pandas.pydata.org/", "anchor_text": "pandas"}, {"url": "http://www.tensorflow.org/", "anchor_text": "Tensorflow"}, {"url": "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn", "anchor_text": "TF Learn"}, {"url": "https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "anchor_text": "dataset"}, {"url": "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto", "anchor_text": "Example"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/scripts/prepare_data.py", "anchor_text": "prepare_data.py"}, {"url": "https://drive.google.com/open?id=0B_bZck-ksdkpVEtVc1R6Y01HMWM", "anchor_text": "download the data files here"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_inputs.py", "anchor_text": "udc_inputs.py"}, {"url": "https://docs.python.org/2/library/functools.html#functools.partial", "anchor_text": "functools.partial"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py", "anchor_text": "udc_train.py"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_hparams.py", "anchor_text": "hparams.py"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_model.py", "anchor_text": "create_model_fn"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/models/dual_encoder.py", "anchor_text": "dual_encoder.py"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_train.py", "anchor_text": "udc_train.py"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/blob/master/udc_predict.py", "anchor_text": "udc_predict.py"}, {"url": "https://github.com/dennybritz/chatbot-retrieval/", "anchor_text": "The Code and data for this tutorial is on Github, so check it out."}, {"url": "https://github.com/dennybritz/chatbot-retrieval/", "anchor_text": "dennybritz/chatbot-retrievalchatbot-retrieval - Dual LSTM Encoder for Dialog Response Generationgithub.com"}, {"url": "http://blog.dennybritz.com/", "anchor_text": "http://blog.dennybritz.com/"}, {"url": "http://www.wildml.com/", "anchor_text": "http://www.wildml.com/"}, {"url": "https://www.linkedin.com/in/markwclark", "anchor_text": "https://www.linkedin.com/in/markwclark"}, {"url": "http://m.me/SmartNotesBot", "anchor_text": "Smart Notes Bot"}, {"url": "http://m.me/1801531300076339", "anchor_text": "Stefan\u2019s Bot"}, {"url": "https://goo.gl/forms/EsadoWADHsAq3XIh2", "anchor_text": "contact me"}, {"url": "https://goo.gl/forms/IluqHlfvrmgFncCk2", "anchor_text": ""}, {"url": "https://goo.gl/forms/u4aIRwi7D8om63As2", "anchor_text": ""}, {"url": "https://goo.gl/forms/K0tE5WizH4r4INvj1", "anchor_text": ""}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----531ff2dd870c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----531ff2dd870c---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----531ff2dd870c---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/chatbots?source=post_page-----531ff2dd870c---------------chatbots-----------------", "anchor_text": "Chatbots"}, {"url": "https://medium.com/tag/how-to?source=post_page-----531ff2dd870c---------------how_to-----------------", "anchor_text": "How To"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fa-chatbots-life%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&user=Stefan+Kojouharov&userId=67c8653ca7e2&source=-----531ff2dd870c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fa-chatbots-life%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&user=Stefan+Kojouharov&userId=67c8653ca7e2&source=-----531ff2dd870c---------------------clap_footer-----------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&source=--------------------------bookmark_footer-----------", "anchor_text": ""}, {"url": "https://chatbotslife.com/?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "More from Chatbots Life"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fa-chatbots-life%2F531ff2dd870c&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&collection=Chatbots+Life&collectionId=a49517e4c30b&source=post_page-----531ff2dd870c---------------------follow_footer-----------", "anchor_text": "Follow"}, {"url": "https://chatbotslife.com/?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Read more from Chatbots Life"}, {"url": "https://medium.com/?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----531ff2dd870c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----531ff2dd870c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----531ff2dd870c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@StefanSpeaks?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@StefanSpeaks?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Stefan Kojouharov"}, {"url": "https://medium.com/@StefanSpeaks/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "14.8K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F67c8653ca7e2&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&user=Stefan+Kojouharov&userId=67c8653ca7e2&source=post_page-67c8653ca7e2--two_column_layout_sidebar-----------------------follow_profile-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F694cf78528e4&operation=register&redirect=https%3A%2F%2Fchatbotslife.com%2Fultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c&newsletterV3=67c8653ca7e2&newsletterV3Id=694cf78528e4&user=Stefan+Kojouharov&userId=67c8653ca7e2&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}