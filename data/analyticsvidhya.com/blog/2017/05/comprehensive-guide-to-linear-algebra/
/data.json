{"url": "https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/\n", "time": 1683020422.9385731, "path": "analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/\n/", "webpage": {"metadata": {"title": "Linear Algebra for Data Science - A comprehensive guide for beginners", "h1": "A comprehensive beginners guide to Linear Algebra for Data Scientists", "description": "A comprehensive guide explaining linear algebra, matrices, their use to solve linear equations and their application in data science & data scientists"}, "outgoing_paragraph_urls": [{"url": "https://gist.github.com/Harshit1694/81e64ad4cce887bdec04aecea1101960", "anchor_text": "Gist", "paragraph_index": 64}, {"url": "https://gist.github.com/Harshit1694/e256625a2608cfe2d2371e22ad71f697", "anchor_text": "Gist", "paragraph_index": 64}, {"url": "https://gist.github.com/Harshit1694/455fdef53ebb93f84d4384e2a83ecc44", "anchor_text": "Gist", "paragraph_index": 64}, {"url": "https://www.youtube.com/watch?v=0-GaihnICmo&index=17&list=PLAwxTw4SYaPlH16rY8KgDwciMZPxCnCX_", "anchor_text": "link", "paragraph_index": 64}, {"url": "https://gist.github.com/Harshit1694/b10dc6c3c1888b8a1b716ab5b10a0691", "anchor_text": "Gist", "paragraph_index": 64}, {"url": "https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/dfb1bb5463c388fb167745888e3a6dd9/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/baseball.csv", "anchor_text": "link", "paragraph_index": 64}, {"url": "https://gist.github.com/Harshit1694/674bb6c71544f9c46b67c3d33f1b678e", "anchor_text": "Gist", "paragraph_index": 64}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tiger.jpg", "anchor_text": "link", "paragraph_index": 64}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tigers.png", "anchor_text": "link", "paragraph_index": 64}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "discussion portal", "paragraph_index": 64}, {"url": "https://www.analyticsvidhya.com/blog", "anchor_text": "Learn", "paragraph_index": 64}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "compete, hack", "paragraph_index": 64}, {"url": "https://www.analyticsvidhya.com/jobs/#/user/", "anchor_text": "get hired", "paragraph_index": 64}, {"url": "https://www.youtube.com/watch?v=0-GaihnICmo&index=17&list=PLAwxTw4SYaPlH16rY8KgDwciMZPxCnCX_", "anchor_text": "link", "paragraph_index": 83}, {"url": "https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/dfb1bb5463c388fb167745888e3a6dd9/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/baseball.csv", "anchor_text": "link", "paragraph_index": 132}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tiger.jpg", "anchor_text": "link", "paragraph_index": 168}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tigers.png", "anchor_text": "link", "paragraph_index": 169}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "discussion portal", "paragraph_index": 171}], "all_paragraphs": ["One of the most common questions we get on Analytics Vidhya is,", "How much maths do I need to learn to be a data scientist?", "Even though the question sounds simple, there is no simple answer to the the question. Usually, we say that you need to know basic descriptive and inferential statistics to start. That is good to start.", "But, once you have covered the basic concepts in machine learning, you will need to learn some more math. You need it to understand how these algorithms work. What are their limitations and in case they make any underlying assumptions. Now, there could be a lot of areas to study including\u00a0algebra, calculus, statistics, 3-D geometry etc.", "If you get confused (like I did) and ask experts what should you learn at this stage, most of them would suggest / agree that you go ahead with Linear Algebra.\u00a0", "But, the problem does not stop there. The next challenge is to figure out how to learn Linear Algebra. You can get lost in the detailed mathematics and derivation and learning them would not help as much! I went through that journey myself and hence decided to write this comprehensive guide.", "If you have faced this question about how to learn & what to learn in Linear Algebra \u2013 you are at the right place. Just follow this guide.", "And if you\u2019re looking to understand where linear algebra fits into the overall data science scheme, here\u2019s the perfect article:", "I would like to present 4 scenarios to showcase why learning Linear Algebra is important, if you are learning Data Science and Machine Learning.", "What do you see when you look at the image above? You most likely said flower, leaves -not too difficult. But, if I ask you to write that logic so that a computer can do the same for you \u2013 it will be a very difficult task (to say the least).", "You were able to identify the flower because the human brain has gone through million years of evolution. We do not understand what goes in the background to be able to tell whether the colour in the picture is red or black. We have somehow trained our brains to automatically perform this task.", "But making a computer do the same task is not an easy task, and is an active area of research in Machine Learning and Computer Science in general. But before we work on identifying attributes in an image, let us ponder over a particular question- How does a machine stores this image?", "You probably know that computers of today are designed to process only 0 and 1. So how can an image such as above with multiple attributes like colour be stored in a computer? This is achieved by storing the pixel intensities in a construct called Matrix.\u00a0Then, this matrix can be processed to identify colours etc.", "So any operation which you want to perform on this image would likely use Linear Algebra and matrices at the back end.", "If you are somewhat familiar with the Data Science domain, you might have heard about the world \u201cXGBOOST\u201d \u2013 an algorithm employed most frequently by winners of Data Science Competitions. It stores the numeric data in the form of Matrix to give predictions. It enables XGBOOST to process data faster and provide more accurate results. Moreover, not just XGBOOST but various other algorithms use Matrices to store and process data.", "Deep Learning- the new buzz word in town employs Matrices to store inputs such as image or speech or text to give a state-of-the-art solution to these problems. Weights learned by a Neural Network are also stored in Matrices. Below is a graphical representation of weights stored in a Matrix.", "Another active area of research in Machine Learning is dealing with text and the most common techniques employed are Bag of Words, Term Document Matrix etc. All these techniques in a very similar manner store counts(or something similar) of words in documents and store this frequency count in a Matrix form to perform tasks like Semantic analysis, Language translation, Language generation etc.", "So, now you would understand the importance of Linear Algebra in machine learning. We have seen image, text or\u00a0any data, in general, employing matrices to store and process data. This should be motivation enough to go through the material below to get you started on Linear Algebra. This is a relatively long guide, but it builds Linear Algebra\u00a0from the ground up.", "Let\u2019s start with a simple problem. Suppose\u00a0that price of 1 ball & 2 bat or 2 ball and 1 bat is 100 units. We need to find price of a ball and a bat.", "Suppose the price of a bat is Rs \u2018x\u2019 and the price of a ball is Rs \u2018y\u2019. Values of \u2018x\u2019 and \u2018y\u2019 can be anything depending on the situation i.e. \u2018x\u2019 and \u2018y\u2019 are variables.", "Let\u2019s translate this in mathematical form \u2013", "Now, to find the prices of bat and ball, we need the values of \u2018x\u2019 and \u2018y\u2019 such that it satisfies both the equations. The basic problem of linear algebra is to find these values of \u2018x\u2019 and \u2018y\u2019 i.e. the solution of a set of linear equations.", "Broadly speaking, in linear algebra data is represented in the form of linear equations. These linear equations are in turn represented in the form of matrices and vectors.", "The number of variables as well as the number of equations may vary depending upon the condition, but the representation is in form of matrices and vectors.", "It is usually helpful to visualize data problems. Let us see if that helps in this case.", "Linear equations represent flat objects. We will start with the simplest one to understand i.e. line. A line corresponding to an equation is the set of all the points which satisfy the given equation. For example,", "Now in this situation, we want both of the conditions to be satisfied i.e. the point which lies on both the lines.\u00a0 Intuitively, we want to find the intersection point of both the lines as shown in the figure below.", "Let\u2019s solve the problem by elementary algebraic operations like addition, subtraction and substitution.", "put value of y in equation (2)-", "Now, since the equation (3) is an equation in single variable x, it can be solved for x and subsequently y.", "That looks simple \u2013 let\u2019s go one step further and explore.", "Now, suppose you are given a set of three conditions with three variables each as given below and asked to find the values of all the variables. Let\u2019s solve the problem and see what happens.", "Substituting value of z in equation\u00a0(6), we get \u2013", "Now, we can solve equations (8) and (5) as a case of two variables to find the values of \u2018x\u2019 and \u2018y\u2019 in the problem of bat and ball above. Once we know\u2018x\u2019 and \u2018y\u2019, we can use\u00a0(7) \u00a0to find the value of \u2018z\u2019.", "As you might see, adding an extra variable has tremendously increased our efforts for finding the solution of the problem. Now imagine having 10 variables and 10 equations. Solving 10 equations simultaneously can prove to be tedious and time consuming. Now dive into data science. We have millions of data points. How do you solve those problems?", "We have millions of data points in a real data set. It is going to be a nightmare to reach to solutions using the approach mentioned above. And imagine if we have to do it again and again and again. It\u2019s going to take ages before we can solve this problem. And now if I tell you that it\u2019s just one part of the battle, what would you think? So, what should we do? Should we quit and let it go? Definitely NO. Then?", "Matrix is used to solve a large set of linear equations. But before we go further and take a look at matrices, let\u2019s visualise the physical meaning of our problem. Give a little bit of thought to the next topic. It directly relates to the usage of Matrices.", "A linear equation in 3 variables represents the set of all points whose coordinates satisfy the equations. Can you figure out the physical object represented by such an equation? Try to think of 2 variables at a time in any equation and then add the third one. You should figure out that it represents a three-dimensional analogue of line.", "Basically, a linear equation in three variables represents a plane. More technically, a plane is a flat geometric object which extends up to infinity.", "As in the case of a line, finding solutions to 3 variables linear equation means we want to find the intersection of those planes. Now can you imagine, in how many ways a set of three planes can intersect? Let me help you out. There are 4 possible cases \u2013", "Can you imagine the number of solutions in each case? Try doing this. Here is an aid picked from Wikipedia to help you visualise.", "So, what was the point of having you to visualise all graphs above?", "Normal humans like us and most of the super mathematicians can only visualise things in 3-Dimensions, and having to visualise things in 4 (or 10000) dimensions is difficult\u00a0impossible for mortals. So, how do mathematicians deal with higher dimensional data so efficiently? They have tricks up their sleeves and Matrices is one such trick employed by mathematicians to deal with higher dimensional data.", "Now let\u2019s proceed with our main focus i.e. Matrix.", "Matrix is a way of writing similar things together to handle and manipulate them as per our requirements easily. In Data Science, it is generally used to store information like weights in an Artificial Neural Network while training various algorithms. You will be able to understand my point by the end of this article.", "Technically, a matrix is a 2-D array of numbers (as far as Data Science is concerned). For example look at the matrix A below.", "Generally, rows are denoted by \u2018i\u2019 and column are denoted by \u2018j\u2019.\u00a0 The elements are indexed by \u2018i\u2019th row and \u2018j\u2019th column.We denote the matrix by some alphabet e.g.\u00a0 A and its elements by A(ij).", "To reach to the result, go along first row and reach to second column.", "Order of matrix \u2013 If a matrix has 3 rows and 4 columns, order of the matrix is 3*4 i.e. row*column.", "Square matrix \u2013 The matrix in which the number of rows is equal to the number of columns.", "Diagonal matrix \u2013 A matrix with all the non-diagonal elements equal to 0 is called a\u00a0diagonal matrix.", "Upper triangular matrix \u2013 Square matrix with all the elements below diagonal equal to 0.", "Lower triangular matrix \u2013 Square matrix with all the elements above the diagonal equal to 0.", "Scalar matrix \u2013 Square matrix with all the diagonal elements equal to some constant k.", "Identity matrix \u2013 Square matrix with all the diagonal elements equal to 1 and all the non-diagonal elements equal to 0.", "Column matrix \u2013 \u00a0The matrix which consists of only 1 column. Sometimes, it is used to represent a vector.", "Row matrix \u2013 \u00a0A matrix consisting only of row.", "Trace \u2013 It is the sum of all the diagonal elements of a square matrix.", "Let\u2019s play with matrices and realise the capabilities of matrix operations.", "Addition \u2013 Addition of matrices is almost similar to basic arithmetic addition. All you need is the order of all the matrices being added should be same. This point will become obvious once you will do matrix addition by yourself.", "Suppose we have 2 matrices \u2018A\u2019 and \u2018B\u2019 and the resultant matrix after the addition is \u2018C\u2019. Then", "For example, let\u2019s take two matrices and solve them.", "Observe that to get the elements of C matrix, I have added A and B element-wise i.e. 1 to 4, 3 to 5 and so on.", "Scalar Multiplication \u2013 \u00a0Multiplication of a matrix with a scalar constant is called scalar multiplication. All we have to do in a scalar multiplication is to multiply each element of the matrix with the given constant.\u00a0 Suppose we have a constant scalar \u2018c\u2019 and a matrix \u2018A\u2019.\u00a0 Then multiplying \u2018c\u2019 with \u2018A\u2019\u00a0 gives-", "c[Aij] = \u00a0[c*Aij]\nTransposition \u2013 Transposition simply means interchanging the row and column index. For example-\nAijT= Aji\nTranspose is used in vectorized implementation of linear and logistic regression.\nCode in python\n\ufeff\nCode in R\nView the code on Gist.\nOutput\n[,1] [,2] [,3]\r\n[1,] 11 12 13\r\n[2,] 14 15 16\r\n[3,] 17 18 19\nView the code on Gist.\nt(A)\r\n[,1] [,2] [,3]\r\n[1,] 11 14 17\r\n[2,] 12 15 18\r\n[3,] 13 16 19\n\u00a0\nMatrix multiplication\nMatrix multiplication is one of the most frequently used operations in linear algebra. We will learn to multiply two matrices as well as go through its important properties.\nBefore landing to algorithms, there are a few points to be kept in mind.\n\nThe multiplication of two matrices of orders i*j and j*k results into a matrix of order i*k. \u00a0Just keep the outer indices in order to get the indices of the final matrix.\nTwo matrices will be compatible for multiplication only if the number of columns of the first matrix and the number of rows of the second one are same.\nThe third point is that order of multiplication matters.\n\nDon\u2019t worry if you can\u2019t get these points. You will be able to understand by the end of this section.\nSuppose, we are given two matrices A and B to multiply. I will write the final expression first and then will explain the steps.\n\nI have picked this image from Wikipedia for your better understanding.\nIn the first illustration, we know that the order of the resulting matrix should be 3*3. So first of all, create a matrix of order 3*3. To determine (AB)ij , multiply each element of \u2018i\u2019th row of A with \u2018j\u2019th column of B one at a time and add all the terms. To help you understand element-wise multiplication, take a look at the code below.\nimport numpy as np\nA=np.arange(21,30).reshape(3,3)\nB=np.arange(31,40).reshape(3,3)\nA.dot(B)\n\n\n\n\n\n\n\n\n\n\nAB= array([[2250, 2316, 2382],\r\n       [2556, 2631, 2706],\r\n       [2862, 2946, 3030]]) B.dot(A)\n\n\n\n\n\n\n\n\n\n\nBA= array([[2310, 2406, 2502],\r\n       [2526, 2631, 2736],\r\n       [2742, 2856, 2970]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, how did we get 2250 as first element of AB matrix? \u00a02250=21*31+22*34+23*37.\u00a0Similarly, for other elements.\nCode in R\nView the code on Gist.\nA*B\r\n[,1] [,2] [,3]\r\n[1,] 220 252 286\r\n[2,] 322 360 400\r\n[3,] 442 486 532\nNotice the difference between AB and BA.\nProperties of matrix multiplication\n\nMatrix multiplication is associative provided the given matrices are compatible for multiplication i.e.\n\nABC =\u00a0 (AB)C = A(BC)\nimport numpy as np\nA=np.arange(21,30).reshape(3,3)\nB=np.arange(31,40).reshape(3,3)\nC=np.arange(41,50).reshape(3,3)\ntemp1=(A.dot(B)).dot(C)\n\n\n\n\n\n\n\n\n\n\narray([[306108, 313056, 320004],\r\n       [347742, 355635, 363528],\r\n       [389376, 398214, 407052]])\n\n\n\n\n\n\n\n\n\n\ntemp2=A.dot((B.dot(C)))\n\n\n\n\n\n\n\n\n\n\narray([[306108, 313056, 320004],\r\n       [347742, 355635, 363528],\r\n       [389376, 398214, 407052]])\n\n\n\n\n\n\n\n\n\n\n2. Matrix multiplication is not commutative i.e. AB and \u00a0BA are not equal. We have verified this result above.\nMatrix multiplication is used in linear and logistic regression when we calculate the value of output variable by parameterized vector method. As we have learned the basics of matrices, it\u2019s time to apply them.\n\u00a0\n3.3 Representing equations in matrix form\nLet me do something exciting for you.\u00a0 Take help of pen and paper and try to find the value of the matrix multiplication shown below\n\n\u00a0\nIt can be verified very easily that the expression contains our three equations. We will name our matrices as \u2018A\u2019, \u2018X\u2019 and \u2018Z\u2019.\nIt explicitly verifies that we can write our equations together in one place as\nAX\u00a0\u00a0 = Z\nNext step has to be solution methods.We will go through two methods to find the solution.\n\u00a0\n4. Solving the Problem\nNow, we will look in detail the two methods to solve matrix equations.\n\nRow Echelon Form\nInverse of a Matrix\n\n\n4.1 Row Echelon form\nNow you have visualised what an equation in 3 variables represents and had a warm up on matrix operations. Let\u2019s find the solution of the set of equations given to us to understand our first method of interest and explore it later in detail.\nI have already illustrated that solving the equations by substitution method can prove to be tedious and time taking. Our first method introduces you with a neater and more systematic method to accomplish the job in which, we manipulate our original equations systematically to find the solution.\u00a0 But what are those valid manipulations? Are there any qualifying criteria they have to fulfil? Well, yes. There are two conditions which have to be fulfilled by any manipulation to be valid.\n\nManipulation should preserve the solution i.e. solution should not be altered on imposing the manipulation.\nManipulation should be reversible.\n\nSo, what are those manipulations?\n\nWe can swap the order of equations.\nWe can multiply both sides of equations by any non-zero constant \u2018c\u2019.\nWe can multiply an equation by any non-zero constant and then add to other equation.\n\nThese points will become more clear once you go through the algorithm and practice it. The basic idea is to clear variables in successive equations and form an upper triangular matrix. Equipped with prerequisites, let\u2019s get started. But before that, it is strongly recommended to go through this link for better understanding.\n\u00a0\nI will solve our original problem as an illustration. Let\u2019s do it in steps.\n\nMake an augmented matrix from the matrix \u2018A\u2019 and \u2018Z\u2019.\n\n\nWhat I have done is I have just concatenated the two matrices. The augmented matrix simply tells that the elements in a row are coefficients of \u2018x\u2019, \u2018y\u2019 and \u2018z\u2019 and last element in the row is right-hand side of the equation.\n\nMultiply row (1) with 2 and subtract from row (2). Similarly, multiply equation 1 with 5 and subtract from row (3).\n\n\n\nIn order to make an upper triangular matrix, multiply row (2) by 2 and then subtract from row (3).\n\n\nRemember to make each leading coefficient, also called pivot equal to 1, by suitable manipulations; in this case multiplying row 2 with -1. Also, if a row consists of 0 only, it should be below each row which consists of a non-zero entry. The resulting form of Matrix is called Row Echelon form. Notice that the planes corresponding to new equations formed by manipulation are not equivalent. Doing these operations, we are just conserving the solution of equations and trying to reach to it.\n\nNow we have simplified our job, let\u2019s retrieve the modified equations. We will start from the simplest i.e. the one with the minimum number of remaining variables. If you follow the illustrated procedure, you will find that last equation comes to be the simplest one.\n\n0*x+0*y+1*z=1\nz=1\nNow retrieve equation (2) and put the value of \u2018z\u2019 in it to find \u2018y\u2019. Do the same for equation (1).\nIsn\u2019t it pretty simple and clean?\nLet\u2019s ponder over another point. Will we always be able to make an upper triangular matrix which gives a unique solution? Are there different cases possible? Recall that planes can intersect in multiple ways. Take your time to figure it out and then proceed further.\nDifferent possible cases-\n\nIt\u2019s possible that we get a unique solution as illustrated in above example. It indicates that all the three planes intersect in a point.\nWe can get a case like shown below\n\n\nNote that in last equation, 0=0 which is always true but it seems like we have got only 2 equations. One of the equations is redundant. In many cases, it\u2019s also possible that the number of redundant equations is more than one. In this case, the number of solutions is infinite.\n\nThere is another case where Echelon matrix looks as shown below\n\n\nLet\u2019s retrieve the last equation.\n0*x+0*y+0*z=4\n0=4\nIs it possible? Very clear cut intuition is NO. But, does this signify something? It\u2019s analogous to saying that it is impossible to find a solution and indeed, it is true. We can\u2019t find a solution for such a set of equations. Can you think what is happening actually in terms of planes? Go back to the section where we saw planes intersecting and find it out.\nNote that this method is efficient for a set of 5-6 equations. Although the method is quite simple, if equation set gets larger, the number of times you have to manipulate the equations becomes enormously high and the method becomes inefficient.\nRank of a matrix \u2013 Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.\nA set of vectors is linearly dependent if we can express at least one of the vectors as a linear combination of remaining vectors in the set.\n\u00a0\n4.2 Inverse of a Matrix\nFor solving a large number of equations in one go, the inverse is used. Don\u2019t panic if you are not familiar with the inverse. We will do a good amount of work on all the required concepts. Let\u2019s start with a few terms and operations.\nDeterminant of a Matrix \u2013 The concept of determinant is applicable to square matrices only. I will lead you to the generalised expression of determinant in steps. To start with, let\u2019s take a 2*2 matrix \u00a0A.\n\nFor now, just focus on 2*2 matrix. The expression of determinant of the matrix A will be:\ndet(A) =a*d-b*c\nNote that det(A) is a standard notation for determinant. Notice that all you have to do to find determinant in this case is to multiply diagonal elements together and put a positive or negative sign before them. For determining the sign, sum the indices of a particular element. If the sum is an even number, put a positive sign before the multiplication and if the sum is odd, put a negative sign.\u00a0 For example, the sum of indices of element \u2018a11\u2019 is 2. Similarly the sum of indices of element \u2018d\u2019 is 4. So we put a positive sign before the first term in the expression. \u00a0Do the same thing for the second term yourself.\nNow take a 3*3 matrix \u2018B\u2019 and find its determinant.\n\nI am writing the expression first and then will explain the procedure step by step.\n\nEach term consists of two parts basically i.e. a submatrix and a coefficient. First of all, pick a constant. Observe that coefficients are picked from the first row only. To start with, I have picked the first element of the first row. You can start wherever you want. Once you have picked the coefficient, just delete all the elements in the row and column corresponding to the chosen coefficient. Next, make a matrix of the remaining elements; each one in its original position after deleting the row and column and find the determinant of this submatrix . Repeat the same procedure for each element in the first row. Now, for determining the sign of the terms, just add the indices of the coefficient element. If it is even, put a positive sign and if odd, put a negative sign. Finally, add all the terms to find the determinant. Now, let\u2019s take a higher order matrix \u2018C\u2019 and generalise the concept.\n\n\u00a0\nTry to relate the expression to what we have done already and figure out the final expression.\nCode in python\nimport numpy as np\n #create a 4*4 matrix\n arr = np.arange(100,116).reshape(4,4)\n\n\n\n\n\n\n\n\n\n\narray([[100, 101, 102, 103],\r\n       [104, 105, 106, 107],\r\n       [108, 109, 110, 111],\r\n       [112, 113, 114, 115]])\n\n\n\n\n\n\n\n\n\n\n#find the determinant\n np.linalg.det(arr)\n\n\n\n\n\n\n\n\n\n\n-2.9582283945788078e-31\n\n\n\n\n\n\n\n\n\n\nCode in R\n\u00a0\nView the code on Gist.\n[,1] [,2] [,3]\r\n[1,] -0.16208333 -0.1125 0.17458333\r\n[2,] -0.07916667 0.1250 -0.04583333\r\n[3,] 0.20791667 -0.0125 -0.09541667\r\n\r\n#Determinant\r\n-0.0004166667\n\u00a0\nMinor of a matrix\nLet\u2019s take a square matrix A. then minor corresponding to an element A(ij) \u00a0is the determinant of the submatrix formed by deleting the \u2018i\u2019th \u00a0row and \u2018j\u2019th column of the matrix. Hope you can relate with what I have explained already in the determinant section. Let\u2019s take an example.\n\nTo find the minor corresponding to element A11, delete first row and first column to find the submatrix.\n\nNow find the determinant of this matrix as explained already. If you calculate the determinant of this matrix, you should get 4. If we denote minor by M11, then\nM11 = 4\nSimilarly, you can do for other elements.\nCofactor of a matrix\nIn the above discussion of minors, if we consider signs of minor terms, the resultant we get is called cofactor of a matrix. To assign the sign, just sum the indices of the corresponding element. If it turns out to be even, assign positive sign. Else assign negative. Let\u2019s take above illustration as an example. If we add the indices i.e. 1+1=2, so we should put a positive sign. Let\u2019s say it C11. Then\nC11 = 4\nYou should find cofactors corresponding to other elements by yourself for a good amount of practice.\nCofactor matrix\nFind the cofactor corresponding to each element. Now in the original matrix, replace the original element by the corresponding cofactor. The matrix thus found is called the cofactor matrix corresponding to the original matrix.\nFor example, let\u2019s take our matrix A. if you have found out the cofactors corresponding to each element, just put them in a matrix according to rule stated above. If you have done it right, you should get cofactor matrix\n\nAdjoint of a matrix \u2013 In our journey to find inverse, we are almost at the end. Just keep hold of the article for a couple of minutes and we will be there. So, next we will find the adjoint of a matrix.\nSuppose we have to find the adjoint of a matrix A. we will do it in two steps.\nIn step 1, find the cofactor matrix of A.\nIn step 2, just transpose the cofactor matrix.\nThe resulting matrix is the adjoint of the original matrix.\u00a0For illustration, lets find the adjoint of our matrix A. we already have cofactor matrix C. Transpose of cofactor matrix should be\n\nFinally, in the next section, we will find the inverse.\n\u00a0\n4.2.1 Finding Inverse of a matrix\nDo you remember the concept of the inverse of a number in elementary algebra? Well, if there exist two numbers such that upon their multiplication gives 1 then those two numbers are called inverse of each other. Similarly in linear algebra, if there exist two matrices such that their multiplication yields an identity matrix then the matrices are called inverse of each other. If you can not get what I explained, just go with the article. It will come intuitively to you. The best way to learning is learning by doing. So, let\u2019s jump straight to the algorithm for finding the inverse of a matrix A. Again, we will do it in two steps.\nStep 1: Find out the adjoint of the matrix A by the procedure explained in previous sections.\nStep2:\u00a0Multiply the adjoint matrix by the inverse of determinant of the matrix A. The resulting matrix is the inverse of A.\nFor example, let\u2019s take our matrix A and find it\u2019s inverse. We already have the adjoint matrix. Determinant of matrix A comes to be -2. So, its inverse will be\n\nNow suppose that the determinant comes out to be 0. What happens when we invert the determinant i.e. 0? \u00a0Does it make any sense?\u00a0 It indicates clearly that we can\u2019t find the inverse of such a matrix. Hence, this matrix is non-invertible. More technically, this type of matrix is called a singular matrix.\nKeep in mind that the resultant of multiplication of a matrix and its inverse is an identity matrix. This property is going to be used extensively in equation solving.\nInverse is used in finding parameter vector corresponding to minimum cost function in linear regression.\n\u00a0\n4.2.2 Power of matrices\nWhat happens when we multiply a number by 1? Obviously it remains the same. The same is applicable for an identity matrix i.e. if we multiply a matrix with an identity matrix of the same order, it remains same.\nLets solve our original problem with the help of matrices. Our original problem represented in matrix was as shown below\nAX = Z i.e.\n\nWhat happens when we pre multiply both the sides with inverse of coefficient matrix i.e. A. Lets find out by doing.\nA-1 A X =A-1 Z\nWe can manipulate it as,\n(A-1 A) X = A -1Z\nBut we know multiply a matrix with its inverse gives an Identity Matrix. So,\nIX =\u00a0 A -1Z\nWhere I is the identity matrix of the corresponding order.\nIf you observe keenly, we have already reached to the solution. Multiplying identity matrix to X does not change it. So the equation becomes\nX = A -1Z\nFor solving the equation, we have to just find the inverse. It can be very easily done by executing a few lines of codes. Isn\u2019t it a really powerful method?\nCode for inverse in python\nimport numpy as np\n#create an array arr1\narr1 = np.arange(5,21).reshape(4,4)\n#find the inverse\nnp.linalg.inv(arr1)\n\u00a0\n4.2.3 Application of inverse in Data Science\nInverse is used to calculate parameter vector by normal equation in linear equation. Here is an illustration. Suppose we are given a data set as shown below-\n\n\n \n\n\nTeam\nLeague\nYear\nRS\nRA\nW\nOBP\nSLG\nBA\nG\nOOBP\nOSLG\n\n\nARI\nNL\n2012\n734\n688\n81\n0.328\n0.418\n0.259\n162\n0.317\n0.415\n\n\nATL\nNL\n2012\n700\n600\n94\n0.32\n0.389\n0.247\n162\n0.306\n0.378\n\n\nBAL\nAL\n2012\n712\n705\n93\n0.311\n0.417\n0.247\n162\n0.315\n0.403\n\n\nBOS\nAL\n2012\n734\n806\n69\n0.315\n0.415\n0.26\n162\n0.331\n0.428\n\n\nCHC\nNL\n2012\n613\n759\n61\n0.302\n0.378\n0.24\n162\n0.335\n0.424\n\n\nCHW\nAL\n2012\n748\n676\n85\n0.318\n0.422\n0.255\n162\n0.319\n0.405\n\n\nCIN\nNL\n2012\n669\n588\n97\n0.315\n0.411\n0.251\n162\n0.305\n0.39\n\n\nCLE\nAL\n2012\n667\n845\n68\n0.324\n0.381\n0.251\n162\n0.336\n0.43\n\n\nCOL\nNL\n2012\n758\n890\n64\n0.33\n0.436\n0.274\n162\n0.357\n0.47\n\n\nDET\nAL\n2012\n726\n670\n88\n0.335\n0.422\n0.268\n162\n0.314\n0.402\n\n\nHOU\nNL\n2012\n583\n794\n55\n0.302\n0.371\n0.236\n162\n0.337\n0.427\n\n\nKCR\nAL\n2012\n676\n746\n72\n0.317\n0.4\n0.265\n162\n0.339\n0.423\n\n\nLAA\nAL\n2012\n767\n699\n89\n0.332\n0.433\n0.274\n162\n0.31\n0.403\n\n\nLAD\nNL\n2012\n637\n597\n86\n0.317\n0.374\n0.252\n162\n0.31\n0.364\n\n\n\n\u00a0\nIt describes the different variables of different baseball teams to predict whether it makes to playoffs or not. But for right now to make it a regression problem, suppose we are interested in predicting OOBP from the rest of the variables. So, \u2018OOBP\u2019 is our target variable. To solve this problem using linear regression, we have to find parameter vector. If you are familiar with Normal equation method, you should have the idea that to do it, we need to make use of Matrices. Lets proceed further and denote our Independent variables below as matrix \u2018X\u2019.This data is a part of a data set taken from analytics edge. Here is the link for the data set.\nso,\u00a0 X=\n\n\n\n734\n688\n81\n0.328\n0.418\n0.259\n\n\n700\n600\n94\n0.32\n0.389\n0.247\n\n\n712\n705\n93\n0.311\n0.417\n0.247\n\n\n734\n806\n69\n0.315\n0.415\n0.26\n\n\n613\n759\n61\n0.302\n0.378\n0.24\n\n\n748\n676\n85\n0.318\n0.422\n0.255\n\n\n669\n588\n97\n0.315\n0.411\n0.251\n\n\n667\n845\n68\n0.324\n0.381\n0.251\n\n\n758\n890\n64\n0.33\n0.436\n0.274\n\n\n726\n670\n88\n0.335\n0.422\n0.268\n\n\n583\n794\n55\n0.302\n0.371\n0.236\n\n\n676\n746\n72\n0.317\n0.4\n0.265\n\n\n767\n699\n89\n0.332\n0.433\n0.274\n\n\n637\n597\n86\n0.317\n0.374\n0.252\n\n\n\n\u00a0\nTo find the final parameter vector(\u03b8) assuming our initial function is parameterised by \u03b8 and X , all you have to do is to find the inverse of (XT X) which can be accomplished very easily by using code as shown below.\nFirst of all, let me make the Linear Regression formulation easier for you to comprehend.\nf \u03b8 (X)= \u03b8T X, where\u00a0\u03b8 is the parameter we wish to calculate and X is the column vector of features or independent variables.\nimport pandas as pd\nimport numpy\u00a0as np\n#you don\u2019t need to bother about the following. It just #transforms the data from original source into matrix\nDf = pd.read_csv( \"../baseball.csv\u201d)\nDf1 = df.head(14)\n# We are just taking 6 features to calculate\u00a0\u03b8.\nX = Df1[[\u2018RS\u2019, \u2018RA\u2019, \u2018W\u2019, \u2018OBP\u2019,'SLG','BA']]\nY=Df1['OOBP']\n#Converting X to matrix\nX = np.asmatrix(X)\n#taking transpose of X and assigning it to x\nx= np.transpose(X)\n#finding multiplication\nT= x.dot(X)\n#inverse of T - provided it is invertible otherwise we use pseudoinverse\ninv=np.linalg.inv(T)\n#calculating\u00a0\u03b8\ntheta=(inv.dot(X.T)).dot(Y)\nImagine if you had to solve this set of equations without using linear algebra. Let me remind you that this data set is less than even 1% of original date set. Now imagine if you had to find parameter vector without using linear algebra. It would have taken a lots of time and effort and could be even impossible to solve sometimes.\nOne major drawback of normal equation method when the number of features is large is that it is computationally very costly. The reason is that if there are \u2018n\u2019 features, the matrix (XT X) comes to be the order n*n and its solution costs time of order O( n*n*n). Generally, normal equation method is applied when a number of features is of the order of 1000 or 10,000. Data sets with a larger number of features are handled with the help another method called Gradient Descent.\nNext, we will go through another advanced concept of linear algebra called Eigenvectors.\n\u00a0\n5. Eigenvalues and Eigenvectors\nEigenvectors find a lot of applications in different domains like computer vision, physics and machine learning. If you have studied machine learning and are familiar with Principal component analysis algorithm, you must know how important the algorithm is when handling a large data set. Have you ever wondered what is going on behind that algorithm? Actually, the concept of Eigenvectors is the backbone of this algorithm. Let us explore Eigen vectors and Eigen values for a better understanding of it.\nLet\u2019s multiply a 2-dimensional vector with a 2*2 matrix and see what happens.\n\nThis operation on a vector is called linear transformation. \u00a0Notice that the directions of input and output vectors are different. Note that the column matrix denotes a vector here.\nI will illustrate my point with the help of a picture as shown below.\n\n\u00a0\nIn the above picture, there are two types of vectors coloured in red and yellow and the picture is showing the change in vectors after a linear transformation. Note that on applying a linear transformation to yellow coloured vector, its direction changes but the direction of the red coloured vector doesn\u2019t change even after applying the linear transformation. The vector coloured in red is an example of Eigenvector.\nPrecisely, for a particular matrix; vectors whose direction remains unchanged even after applying linear transformation with the matrix are called Eigenvectors for that particular matrix. Remember that the concept of Eigen values and vectors is applicable to square matrices only. Another thing to know is that I have taken a case of two-dimensional vectors but the concept of Eigenvectors is applicable to a space of any number of dimensions.\n\u00a0\n5.1 How to find Eigenvectors of a matrix?\nSuppose we have a matrix A and an Eigenvector \u2018x\u2019 corresponding to the matrix. As explained already, after multiplication with matrix the direction of \u2018x\u2019 doesn\u2019t change. Only change in magnitude is permitted. Let us write it as an equation-\nAx = cx\n(A-c)x = 0\u00a0 \u2026\u2026.(1)\nPlease note that in the term (A-c), \u2018c\u2019 denotes an identity matrix of the order equal to \u2018A\u2019 multiplied by a scalar \u2018c\u2019\nWe have two unknowns \u2018c\u2019 and \u2018x\u2019 and only one equation. Can you think of a trick to solve this equation?\nIn equation (1), if we put the vector \u2018x\u2019 as zero vector, it makes no sense. Hence, the only choice is that (A-c) is a singular matrix. And singular matrix has a property that its determinant equals to 0. We will use this property to find the value of \u2018c\u2019.\nDet(A-c) = 0\nOnce you find the determinant of the matrix (A-c) and equate to 0, you will get an equation in \u2018c\u2019 of the order depending upon the given matrix A. all you have to do is to find the solution of the equation. Suppose that we find solutions as \u2018c1\u2019 , \u2018c2\u2019 and so on. Put \u2018c1\u2019 in equation (1) and find the vector \u2018x1\u2019 corresponding to \u2018c1\u2019. The vector \u2018x1\u2019 that you just found is an Eigenvector of A. Now, repeat the same procedure with \u2018c2\u2019, \u2018c3\u2019 and so on.\nCode for finding EigenVectors in python\nimport \u00a0numpy as np\n#create an array\narr = np.arange(1,10).reshape(3,3)\n#finding the Eigenvalue and Eigenvectors of arr\nnp.linalg.eig(arr)\nCode in R for finding Eigenvalues and Eigenvectors:\nView the code on Gist.\nOutput\n147.737576 5.317459 -3.055035\n[,1] [,2] [,3]\r\n[1,] -0.3948374 0.4437557 -0.74478185\r\n[2,] -0.5497457 -0.8199420 -0.06303763\r\n[3,] -0.7361271 0.3616296 0.66432391\n5.2 Use of Eigenvectors in Data Science\nThe concept of Eigenvectors is applied in a machine learning algorithm Principal Component Analysis. Suppose you have a data with a large number of features i.e. it has a very high dimensionality. It is possible that there are redundant features in that data. Apart from this, a large number of features will cause reduced efficiency and more disk space. What PCA does is that it craps some of lesser important features. But how to determine those features? Here, Eigenvectors come to our rescue.Let\u2019s go through the algorithm of PCA. Suppose we have an \u2018n\u2019 dimensional data and we want to reduce it to \u2018k\u2019 dimensions. We will do it in steps.\nStep 1: Data is mean normalised and feature scaled.\nStep 2:\u00a0We find out the covariance matrix of our data set.\nNow we want to reduce the number of features i.e. dimensions. But cutting off features means loss of information. We want to minimise the loss of information i.e. we want to keep the maximum variance. So, we want to find out the directions in which variance is maximum. We will find these directions in the next step.\nStep 3:\u00a0We find out the Eigenvectors of the covariance matrix. You don\u2019t need to bother much about covariance matrix. It\u2019s an advanced concept of statistics. \u00a0As we have data in \u2018n\u2019 dimensions, we will find \u2018n\u2019 Eigenvectors corresponding to \u2018n\u2019 Eigenvalues.\nStep 4: We will select \u2018k\u2019 Eigenvectors corresponding to the \u2018k\u2019 largest Eigenvalues and will form a matrix in which each Eigenvector will constitute a column. We will call this matrix as U.\nNow it\u2019s the time to find the reduced data points. Suppose you want to reduce a data point \u2018a\u2019 in the data set to \u2018k\u2019 dimensions.\u00a0 To do so, you have to just transpose the matrix U and multiply it with the vector \u2018a\u2019. You will get the required vector in \u2018k\u2019 dimensions.\nOnce we are done with Eigenvectors, let\u2019s talk about another advanced and highly useful concept in Linear algebra called Singular value decomposition, popularly called as SVD. Its complete understanding needs\u00a0 a rigorous study of linear algebra.\u00a0 In fact, SVD is a complete blog in itself. We will come up with another blog completely devoted to SVD. Stay tuned for a better experience. For now, I will just give you a glimpse of how SVD helps in data science.\n\u00a0\n6. Singular Value Decomposition\nSuppose you are given a feature matrix A. As suggested by name, what we do is we decompose our matrix A in three constituent matrices for a special purpose.\u00a0 Sometimes, it is also said that svd is some sort of generalisation of Eigen value decomposition.\u00a0 I will not go into its mathematics for the reason already explained and will stick to our plan i.e. use of svd in data science.\nSvd is used to remove the redundant features in a data set. Suppose you have a data set which comprises of 1000 features. Definitely, any real data set with such a large number of features is bound to contain redundant features. if you have run ML, you should be familiar with the fact that Redundant features cause a lots of problems in running machine learning algorithms. Also, running an algorithm on the original data set will be time inefficient and will require a lot of memory. So, what should you to do handle such a problem? Do we have a choice?\u00a0 Can we omit some features? Will it lead to significant amount of information loss? Will we be able to get an efficient enough algorithm even after omitting the rows? I will answer these questions with the help of an illustration.\nLook at the pictures shown below taken from this link\n\nWe can convert this tiger into black and white and can think of it as a matrix whose elements represent the pixel intensity as relevant location. In simpler words, the matrix contains information about the intensity of pixels of the image in the form of rows and columns. But, is it necessary to have all the columns in the intensity matrix? Will we be able to represent the tiger with a lesser amount of information? The next picture will clarify my point. In this picture, different images are shown corresponding to different ranks with different resolution. For now, just assume that higher rank implies the larger amount of information about pixel intensity. The image is taken from this link\n\nIt is clear that we can reach to a pretty well image with 20 or 30 ranks instead of 100 or 200 ranks and that\u2019s what we want to do in a case of highly redundant data. What I want to convey is that to get a reasonable hypothesis, we don\u2019t have to retain all the information present in the original dataset. Even, some of the features cause a problem in reaching a solution to the best algorithm. For the example, presence of redundant features causes multi co-linearity in linear regression. Also, some features are not significant for our model. Omitting these features helps to find a better fit of algorithm along with time efficiency and lesser disk space. Singular value decomposition is used to get rid of the redundant features present in our data.\n\u00a0\n7. End notes\nIf you have made this far \u2013 give yourself a pat at the back. We have covered different aspects of Linear algebra in this article. I have tried to give sufficient amount of information as well as keep the flow such that everybody can understand the concepts and be able to do necessary calculations. Still, if you get stuck somewhere, feel free to comment below or post on discussion portal.\n\u00a0\nLearn, compete, hack and get hired!\n\nRelated\n ", "Transposition \u2013 Transposition simply means interchanging the row and column index. For example-", "Transpose is used in vectorized implementation of linear and logistic regression.", "Matrix multiplication is one of the most frequently used operations in linear algebra. We will learn to multiply two matrices as well as go through its important properties.", "Before landing to algorithms, there are a few points to be kept in mind.", "Don\u2019t worry if you can\u2019t get these points. You will be able to understand by the end of this section.", "Suppose, we are given two matrices A and B to multiply. I will write the final expression first and then will explain the steps.", "I have picked this image from Wikipedia for your better understanding.", "In the first illustration, we know that the order of the resulting matrix should be 3*3. So first of all, create a matrix of order 3*3. To determine (AB)ij , multiply each element of \u2018i\u2019th row of A with \u2018j\u2019th column of B one at a time and add all the terms. To help you understand element-wise multiplication, take a look at the code below.", "Notice the difference between AB and BA.", "2. Matrix multiplication is not commutative i.e. AB and \u00a0BA are not equal. We have verified this result above.", "Matrix multiplication is used in linear and logistic regression when we calculate the value of output variable by parameterized vector method. As we have learned the basics of matrices, it\u2019s time to apply them.", "Let me do something exciting for you.\u00a0 Take help of pen and paper and try to find the value of the matrix multiplication shown below", "It can be verified very easily that the expression contains our three equations. We will name our matrices as \u2018A\u2019, \u2018X\u2019 and \u2018Z\u2019.", "It explicitly verifies that we can write our equations together in one place as", "Next step has to be solution methods.We will go through two methods to find the solution.", "Now, we will look in detail the two methods to solve matrix equations.", "Now you have visualised what an equation in 3 variables represents and had a warm up on matrix operations. Let\u2019s find the solution of the set of equations given to us to understand our first method of interest and explore it later in detail.", "I have already illustrated that solving the equations by substitution method can prove to be tedious and time taking. Our first method introduces you with a neater and more systematic method to accomplish the job in which, we manipulate our original equations systematically to find the solution.\u00a0 But what are those valid manipulations? Are there any qualifying criteria they have to fulfil? Well, yes. There are two conditions which have to be fulfilled by any manipulation to be valid.", "These points will become more clear once you go through the algorithm and practice it. The basic idea is to clear variables in successive equations and form an upper triangular matrix. Equipped with prerequisites, let\u2019s get started. But before that, it is strongly recommended to go through this link for better understanding.", "I will solve our original problem as an illustration. Let\u2019s do it in steps.", "What I have done is I have just concatenated the two matrices. The augmented matrix simply tells that the elements in a row are coefficients of \u2018x\u2019, \u2018y\u2019 and \u2018z\u2019 and last element in the row is right-hand side of the equation.", "Remember to make each leading coefficient, also called pivot equal to 1, by suitable manipulations; in this case multiplying row 2 with -1. Also, if a row consists of 0 only, it should be below each row which consists of a non-zero entry. The resulting form of Matrix is called Row Echelon form. Notice that the planes corresponding to new equations formed by manipulation are not equivalent. Doing these operations, we are just conserving the solution of equations and trying to reach to it.", "Now retrieve equation (2) and put the value of \u2018z\u2019 in it to find \u2018y\u2019. Do the same for equation (1).", "Isn\u2019t it pretty simple and clean?", "Let\u2019s ponder over another point. Will we always be able to make an upper triangular matrix which gives a unique solution? Are there different cases possible? Recall that planes can intersect in multiple ways. Take your time to figure it out and then proceed further.", "Note that in last equation, 0=0 which is always true but it seems like we have got only 2 equations. One of the equations is redundant. In many cases, it\u2019s also possible that the number of redundant equations is more than one. In this case, the number of solutions is infinite.", "Is it possible? Very clear cut intuition is NO. But, does this signify something? It\u2019s analogous to saying that it is impossible to find a solution and indeed, it is true. We can\u2019t find a solution for such a set of equations. Can you think what is happening actually in terms of planes? Go back to the section where we saw planes intersecting and find it out.", "Note that this method is efficient for a set of 5-6 equations. Although the method is quite simple, if equation set gets larger, the number of times you have to manipulate the equations becomes enormously high and the method becomes inefficient.", "Rank of a matrix \u2013 Rank of a matrix is equal to the maximum number of linearly independent row vectors in a matrix.", "A set of vectors is linearly dependent if we can express at least one of the vectors as a linear combination of remaining vectors in the set.", "For solving a large number of equations in one go, the inverse is used. Don\u2019t panic if you are not familiar with the inverse. We will do a good amount of work on all the required concepts. Let\u2019s start with a few terms and operations.", "Determinant of a Matrix \u2013 The concept of determinant is applicable to square matrices only. I will lead you to the generalised expression of determinant in steps. To start with, let\u2019s take a 2*2 matrix \u00a0A.", "For now, just focus on 2*2 matrix. The expression of determinant of the matrix A will be:", "Note that det(A) is a standard notation for determinant. Notice that all you have to do to find determinant in this case is to multiply diagonal elements together and put a positive or negative sign before them. For determining the sign, sum the indices of a particular element. If the sum is an even number, put a positive sign before the multiplication and if the sum is odd, put a negative sign.\u00a0 For example, the sum of indices of element \u2018a11\u2019 is 2. Similarly the sum of indices of element \u2018d\u2019 is 4. So we put a positive sign before the first term in the expression. \u00a0Do the same thing for the second term yourself.", "Now take a 3*3 matrix \u2018B\u2019 and find its determinant.", "I am writing the expression first and then will explain the procedure step by step.", "Each term consists of two parts basically i.e. a submatrix and a coefficient. First of all, pick a constant. Observe that coefficients are picked from the first row only. To start with, I have picked the first element of the first row. You can start wherever you want. Once you have picked the coefficient, just delete all the elements in the row and column corresponding to the chosen coefficient. Next, make a matrix of the remaining elements; each one in its original position after deleting the row and column and find the determinant of this submatrix . Repeat the same procedure for each element in the first row. Now, for determining the sign of the terms, just add the indices of the coefficient element. If it is even, put a positive sign and if odd, put a negative sign. Finally, add all the terms to find the determinant. Now, let\u2019s take a higher order matrix \u2018C\u2019 and generalise the concept.", "Try to relate the expression to what we have done already and figure out the final expression.", "Let\u2019s take a square matrix A. then minor corresponding to an element A(ij) \u00a0is the determinant of the submatrix formed by deleting the \u2018i\u2019th \u00a0row and \u2018j\u2019th column of the matrix. Hope you can relate with what I have explained already in the determinant section. Let\u2019s take an example.", "To find the minor corresponding to element A11, delete first row and first column to find the submatrix.", "Now find the determinant of this matrix as explained already. If you calculate the determinant of this matrix, you should get 4. If we denote minor by M11, then", "Similarly, you can do for other elements.", "In the above discussion of minors, if we consider signs of minor terms, the resultant we get is called cofactor of a matrix. To assign the sign, just sum the indices of the corresponding element. If it turns out to be even, assign positive sign. Else assign negative. Let\u2019s take above illustration as an example. If we add the indices i.e. 1+1=2, so we should put a positive sign. Let\u2019s say it C11. Then", "You should find cofactors corresponding to other elements by yourself for a good amount of practice.", "Find the cofactor corresponding to each element. Now in the original matrix, replace the original element by the corresponding cofactor. The matrix thus found is called the cofactor matrix corresponding to the original matrix.", "For example, let\u2019s take our matrix A. if you have found out the cofactors corresponding to each element, just put them in a matrix according to rule stated above. If you have done it right, you should get cofactor matrix", "Adjoint of a matrix \u2013 In our journey to find inverse, we are almost at the end. Just keep hold of the article for a couple of minutes and we will be there. So, next we will find the adjoint of a matrix.", "Suppose we have to find the adjoint of a matrix A. we will do it in two steps.", "In step 1, find the cofactor matrix of A.", "In step 2, just transpose the cofactor matrix.", "The resulting matrix is the adjoint of the original matrix.\u00a0For illustration, lets find the adjoint of our matrix A. we already have cofactor matrix C. Transpose of cofactor matrix should be", "Finally, in the next section, we will find the inverse.", "Do you remember the concept of the inverse of a number in elementary algebra? Well, if there exist two numbers such that upon their multiplication gives 1 then those two numbers are called inverse of each other. Similarly in linear algebra, if there exist two matrices such that their multiplication yields an identity matrix then the matrices are called inverse of each other. If you can not get what I explained, just go with the article. It will come intuitively to you. The best way to learning is learning by doing. So, let\u2019s jump straight to the algorithm for finding the inverse of a matrix A. Again, we will do it in two steps.", "Step 1: Find out the adjoint of the matrix A by the procedure explained in previous sections.", "Step2:\u00a0Multiply the adjoint matrix by the inverse of determinant of the matrix A. The resulting matrix is the inverse of A.", "For example, let\u2019s take our matrix A and find it\u2019s inverse. We already have the adjoint matrix. Determinant of matrix A comes to be -2. So, its inverse will be", "Now suppose that the determinant comes out to be 0. What happens when we invert the determinant i.e. 0? \u00a0Does it make any sense?\u00a0 It indicates clearly that we can\u2019t find the inverse of such a matrix. Hence, this matrix is non-invertible. More technically, this type of matrix is called a singular matrix.", "Keep in mind that the resultant of multiplication of a matrix and its inverse is an identity matrix. This property is going to be used extensively in equation solving.", "Inverse is used in finding parameter vector corresponding to minimum cost function in linear regression.", "What happens when we multiply a number by 1? Obviously it remains the same. The same is applicable for an identity matrix i.e. if we multiply a matrix with an identity matrix of the same order, it remains same.", "Lets solve our original problem with the help of matrices. Our original problem represented in matrix was as shown below", "What happens when we pre multiply both the sides with inverse of coefficient matrix i.e. A. Lets find out by doing.", "But we know multiply a matrix with its inverse gives an Identity Matrix. So,", "Where I is the identity matrix of the corresponding order.", "If you observe keenly, we have already reached to the solution. Multiplying identity matrix to X does not change it. So the equation becomes", "For solving the equation, we have to just find the inverse. It can be very easily done by executing a few lines of codes. Isn\u2019t it a really powerful method?", "Inverse is used to calculate parameter vector by normal equation in linear equation. Here is an illustration. Suppose we are given a data set as shown below-", "It describes the different variables of different baseball teams to predict whether it makes to playoffs or not. But for right now to make it a regression problem, suppose we are interested in predicting OOBP from the rest of the variables. So, \u2018OOBP\u2019 is our target variable. To solve this problem using linear regression, we have to find parameter vector. If you are familiar with Normal equation method, you should have the idea that to do it, we need to make use of Matrices. Lets proceed further and denote our Independent variables below as matrix \u2018X\u2019.This data is a part of a data set taken from analytics edge. Here is the link for the data set.", "To find the final parameter vector(\u03b8) assuming our initial function is parameterised by \u03b8 and X , all you have to do is to find the inverse of (XT X) which can be accomplished very easily by using code as shown below.", "First of all, let me make the Linear Regression formulation easier for you to comprehend.", "f \u03b8 (X)= \u03b8T X, where\u00a0\u03b8 is the parameter we wish to calculate and X is the column vector of features or independent variables.", "import pandas as pd\nimport numpy\u00a0as np", "#you don\u2019t need to bother about the following. It just #transforms the data from original source into matrix", "#Converting X to matrix\nX = np.asmatrix(X)", "#taking transpose of X and assigning it to x\nx= np.transpose(X)", "#inverse of T - provided it is invertible otherwise we use pseudoinverse\ninv=np.linalg.inv(T)", "Imagine if you had to solve this set of equations without using linear algebra. Let me remind you that this data set is less than even 1% of original date set. Now imagine if you had to find parameter vector without using linear algebra. It would have taken a lots of time and effort and could be even impossible to solve sometimes.", "One major drawback of normal equation method when the number of features is large is that it is computationally very costly. The reason is that if there are \u2018n\u2019 features, the matrix (XT X) comes to be the order n*n and its solution costs time of order O( n*n*n). Generally, normal equation method is applied when a number of features is of the order of 1000 or 10,000. Data sets with a larger number of features are handled with the help another method called Gradient Descent.", "Next, we will go through another advanced concept of linear algebra called Eigenvectors.", "Eigenvectors find a lot of applications in different domains like computer vision, physics and machine learning. If you have studied machine learning and are familiar with Principal component analysis algorithm, you must know how important the algorithm is when handling a large data set. Have you ever wondered what is going on behind that algorithm? Actually, the concept of Eigenvectors is the backbone of this algorithm. Let us explore Eigen vectors and Eigen values for a better understanding of it.", "Let\u2019s multiply a 2-dimensional vector with a 2*2 matrix and see what happens.", "This operation on a vector is called linear transformation. \u00a0Notice that the directions of input and output vectors are different. Note that the column matrix denotes a vector here.", "I will illustrate my point with the help of a picture as shown below.", "In the above picture, there are two types of vectors coloured in red and yellow and the picture is showing the change in vectors after a linear transformation. Note that on applying a linear transformation to yellow coloured vector, its direction changes but the direction of the red coloured vector doesn\u2019t change even after applying the linear transformation. The vector coloured in red is an example of Eigenvector.", "Precisely, for a particular matrix; vectors whose direction remains unchanged even after applying linear transformation with the matrix are called Eigenvectors for that particular matrix. Remember that the concept of Eigen values and vectors is applicable to square matrices only. Another thing to know is that I have taken a case of two-dimensional vectors but the concept of Eigenvectors is applicable to a space of any number of dimensions.", "Suppose we have a matrix A and an Eigenvector \u2018x\u2019 corresponding to the matrix. As explained already, after multiplication with matrix the direction of \u2018x\u2019 doesn\u2019t change. Only change in magnitude is permitted. Let us write it as an equation-", "Please note that in the term (A-c), \u2018c\u2019 denotes an identity matrix of the order equal to \u2018A\u2019 multiplied by a scalar \u2018c\u2019", "We have two unknowns \u2018c\u2019 and \u2018x\u2019 and only one equation. Can you think of a trick to solve this equation?", "In equation (1), if we put the vector \u2018x\u2019 as zero vector, it makes no sense. Hence, the only choice is that (A-c) is a singular matrix. And singular matrix has a property that its determinant equals to 0. We will use this property to find the value of \u2018c\u2019.", "Once you find the determinant of the matrix (A-c) and equate to 0, you will get an equation in \u2018c\u2019 of the order depending upon the given matrix A. all you have to do is to find the solution of the equation. Suppose that we find solutions as \u2018c1\u2019 , \u2018c2\u2019 and so on. Put \u2018c1\u2019 in equation (1) and find the vector \u2018x1\u2019 corresponding to \u2018c1\u2019. The vector \u2018x1\u2019 that you just found is an Eigenvector of A. Now, repeat the same procedure with \u2018c2\u2019, \u2018c3\u2019 and so on.", "Code for finding EigenVectors in python", "#finding the Eigenvalue and Eigenvectors of arr\nnp.linalg.eig(arr)", "Code in R for finding Eigenvalues and Eigenvectors:", "The concept of Eigenvectors is applied in a machine learning algorithm Principal Component Analysis. Suppose you have a data with a large number of features i.e. it has a very high dimensionality. It is possible that there are redundant features in that data. Apart from this, a large number of features will cause reduced efficiency and more disk space. What PCA does is that it craps some of lesser important features. But how to determine those features? Here, Eigenvectors come to our rescue.Let\u2019s go through the algorithm of PCA. Suppose we have an \u2018n\u2019 dimensional data and we want to reduce it to \u2018k\u2019 dimensions. We will do it in steps.", "Step 1: Data is mean normalised and feature scaled.", "Step 2:\u00a0We find out the covariance matrix of our data set.", "Now we want to reduce the number of features i.e. dimensions. But cutting off features means loss of information. We want to minimise the loss of information i.e. we want to keep the maximum variance. So, we want to find out the directions in which variance is maximum. We will find these directions in the next step.", "Step 3:\u00a0We find out the Eigenvectors of the covariance matrix. You don\u2019t need to bother much about covariance matrix. It\u2019s an advanced concept of statistics. \u00a0As we have data in \u2018n\u2019 dimensions, we will find \u2018n\u2019 Eigenvectors corresponding to \u2018n\u2019 Eigenvalues.", "Step 4: We will select \u2018k\u2019 Eigenvectors corresponding to the \u2018k\u2019 largest Eigenvalues and will form a matrix in which each Eigenvector will constitute a column. We will call this matrix as U.", "Now it\u2019s the time to find the reduced data points. Suppose you want to reduce a data point \u2018a\u2019 in the data set to \u2018k\u2019 dimensions.\u00a0 To do so, you have to just transpose the matrix U and multiply it with the vector \u2018a\u2019. You will get the required vector in \u2018k\u2019 dimensions.", "Once we are done with Eigenvectors, let\u2019s talk about another advanced and highly useful concept in Linear algebra called Singular value decomposition, popularly called as SVD. Its complete understanding needs\u00a0 a rigorous study of linear algebra.\u00a0 In fact, SVD is a complete blog in itself. We will come up with another blog completely devoted to SVD. Stay tuned for a better experience. For now, I will just give you a glimpse of how SVD helps in data science.", "Suppose you are given a feature matrix A. As suggested by name, what we do is we decompose our matrix A in three constituent matrices for a special purpose.\u00a0 Sometimes, it is also said that svd is some sort of generalisation of Eigen value decomposition.\u00a0 I will not go into its mathematics for the reason already explained and will stick to our plan i.e. use of svd in data science.", "Svd is used to remove the redundant features in a data set. Suppose you have a data set which comprises of 1000 features. Definitely, any real data set with such a large number of features is bound to contain redundant features. if you have run ML, you should be familiar with the fact that Redundant features cause a lots of problems in running machine learning algorithms. Also, running an algorithm on the original data set will be time inefficient and will require a lot of memory. So, what should you to do handle such a problem? Do we have a choice?\u00a0 Can we omit some features? Will it lead to significant amount of information loss? Will we be able to get an efficient enough algorithm even after omitting the rows? I will answer these questions with the help of an illustration.", "Look at the pictures shown below taken from this link", "We can convert this tiger into black and white and can think of it as a matrix whose elements represent the pixel intensity as relevant location. In simpler words, the matrix contains information about the intensity of pixels of the image in the form of rows and columns. But, is it necessary to have all the columns in the intensity matrix? Will we be able to represent the tiger with a lesser amount of information? The next picture will clarify my point. In this picture, different images are shown corresponding to different ranks with different resolution. For now, just assume that higher rank implies the larger amount of information about pixel intensity. The image is taken from this link", "It is clear that we can reach to a pretty well image with 20 or 30 ranks instead of 100 or 200 ranks and that\u2019s what we want to do in a case of highly redundant data. What I want to convey is that to get a reasonable hypothesis, we don\u2019t have to retain all the information present in the original dataset. Even, some of the features cause a problem in reaching a solution to the best algorithm. For the example, presence of redundant features causes multi co-linearity in linear regression. Also, some features are not significant for our model. Omitting these features helps to find a better fit of algorithm along with time efficiency and lesser disk space. Singular value decomposition is used to get rid of the redundant features present in our data.", "If you have made this far \u2013 give yourself a pat at the back. We have covered different aspects of Linear algebra in this article. I have tried to give sufficient amount of information as well as keep the flow such that everybody can understand the concepts and be able to do necessary calculations. Still, if you get stuck somewhere, feel free to comment below or post on discussion portal.", "I am an undergraduate student majoring in Mechanical Engineering at Indian Institute of Technology, Roorkee.\nI am an aspiring data scientist and a machine learning enthusiast. I am really passionate about changing the world by using artificial intelligence.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F05%2Fcomprehensive-guide-to-linear-algebra%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=A comprehensive beginners guide to Linear Algebra for Data Scientists&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F05%2Fcomprehensive-guide-to-linear-algebra%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F05%2Fcomprehensive-guide-to-linear-algebra%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/vikas_10/", "anchor_text": "Vikas_10 Kumar"}, {"url": "https://www.analyticsvidhya.com/blog/category/business-analytics/", "anchor_text": "Business Analytics"}, {"url": "https://www.analyticsvidhya.com/blog/category/intermediate/", "anchor_text": "Intermediate"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/maths/", "anchor_text": "Maths"}, {"url": "https://www.analyticsvidhya.com/blog/category/r/", "anchor_text": "R"}, {"url": "https://www.analyticsvidhya.com/blog/category/resource/", "anchor_text": "Resource"}, {"url": "https://www.analyticsvidhya.com/blog/category/structured-data/", "anchor_text": "Structured Data"}, {"url": "https://www.analyticsvidhya.com/blog/category/technique/", "anchor_text": "Technique"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/", "anchor_text": "10 Powerful Applications of Linear Algebra in Data Science (with Multiple Resources)"}, {"url": "https://gist.github.com/Harshit1694/81e64ad4cce887bdec04aecea1101960", "anchor_text": "Gist"}, {"url": "https://gist.github.com/Harshit1694/e256625a2608cfe2d2371e22ad71f697", "anchor_text": "Gist"}, {"url": "https://gist.github.com/Harshit1694/455fdef53ebb93f84d4384e2a83ecc44", "anchor_text": "Gist"}, {"url": "https://www.youtube.com/watch?v=0-GaihnICmo&index=17&list=PLAwxTw4SYaPlH16rY8KgDwciMZPxCnCX_", "anchor_text": "link"}, {"url": "https://gist.github.com/Harshit1694/b10dc6c3c1888b8a1b716ab5b10a0691", "anchor_text": "Gist"}, {"url": "https://d37djvu3ytnwxt.cloudfront.net/assets/courseware/v1/dfb1bb5463c388fb167745888e3a6dd9/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/baseball.csv", "anchor_text": "link"}, {"url": "https://gist.github.com/Harshit1694/674bb6c71544f9c46b67c3d33f1b678e", "anchor_text": "Gist"}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tiger.jpg", "anchor_text": "link"}, {"url": "http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/images/tigers.png", "anchor_text": "link"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "discussion portal"}, {"url": "https://www.analyticsvidhya.com/blog", "anchor_text": "Learn"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "compete, hack"}, {"url": "https://www.analyticsvidhya.com/jobs/#/user/", "anchor_text": "get hired"}, {"url": "https://www.analyticsvidhya.com/blog/tag/eigenvalues/", "anchor_text": "Eigenvalues"}, {"url": "https://www.analyticsvidhya.com/blog/tag/eigenvectors/", "anchor_text": "Eigenvectors"}, {"url": "https://www.analyticsvidhya.com/blog/tag/inverse-of-matrix/", "anchor_text": "Inverse of matrix"}, {"url": "https://www.analyticsvidhya.com/blog/tag/linear-algebra/", "anchor_text": "linear algebra"}, {"url": "https://www.analyticsvidhya.com/blog/tag/live-coding/", "anchor_text": "live coding"}, {"url": "https://www.analyticsvidhya.com/blog/tag/matrices/", "anchor_text": "matrices"}, {"url": "https://www.analyticsvidhya.com/blog/tag/matrix/", "anchor_text": "Matrix"}, {"url": "https://www.analyticsvidhya.com/blog/tag/matrix-methods/", "anchor_text": "matrix methods"}, {"url": "https://www.analyticsvidhya.com/blog/tag/matrix-operations/", "anchor_text": "matrix operations"}, {"url": "https://www.analyticsvidhya.com/blog/tag/svd/", "anchor_text": "SVD"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/how-to-learn-mathematics-for-machine-learning-what-concepts-do-you-need-to-master-in-data-science/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "How to Learn Mathematics For Machine Learning? What Concepts do You Need to Master in Data Science?"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/how-much-mathematics-do-you-need-to-know-for-machine-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "How much Mathematics do you need to know for Machine Learning?"}, {"url": "https://www.analyticsvidhya.com/blog/2022/06/linear-algebra-for-data-science-with-python/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "Linear Algebra for Data Science With Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/part-15-step-by-step-guide-to-master-nlp-topic-modelling-using-nmf/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "Part 15: Step by Step Guide to Master NLP \u2013 Topic Modelling using NMF"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/5-applications-singular-value-decomposition-svd-data-science/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "Master Dimensionality Reduction with these 5 Must-Know Applications of Singular Value Decomposition (SVD) in Data Science"}, {"url": "https://www.analyticsvidhya.com/blog/2019/10/mathematics-behind-machine-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/", "anchor_text": "Mathematics behind Machine Learning \u2013 The Core Concepts you Need to Know"}, {"url": "https://www.analyticsvidhya.com/blog/author/vikas_10/", "anchor_text": "Vikas_10 Kumar"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/business-analystsenior-business-analyst-data-analytics-bfsi-gurgaon-2-5-years-of-experience/", "anchor_text": "Business Analyst/senior Business Analyst \u2013 Data Analytics \u2013 BFSI Gurgaon (2-5 Years Of Experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/business-intelligence-analyst-bangalore-3-6-years-of-experience/", "anchor_text": "Business Intelligence Analyst- Bangalore (3-6 Years of Experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}