{"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\n", "time": 1683020561.808531, "path": "analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\n/", "webpage": {"metadata": {"title": "Guide to t-SNE machine learning algorithm implemented in R & Python", "h1": "Comprehensive Guide on t-SNE algorithm with implementation in R & Python", "description": "Learn the t-SNE machine learning algorithm with implementation in R & Python. t-SNE is an advanced non-linear dimensionality reduction technique"}, "outgoing_paragraph_urls": [{"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/", "anchor_text": "read this article first", "paragraph_index": 2}, {"url": "https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/", "anchor_text": "dimensionality reduction here", "paragraph_index": 9}, {"url": "https://en.wikipedia.org/wiki/Kullback\u2013Leibler_divergence", "anchor_text": "Kullback-Leibler divergences", "paragraph_index": 33}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "Shannon entropy", "paragraph_index": 39}, {"url": "https://drive.google.com/file/d/0B6E7D59TV2zWYlJLZHdGeUYydlk/view?usp=sharing", "anchor_text": "link", "paragraph_index": 60}], "all_paragraphs": ["Imagine you get a dataset with hundreds of features (variables) and have little understanding about\u00a0the domain the data belongs to. You are expected to\u00a0identify hidden patterns in the data, explore and analyze the dataset. And not just that, you have to find out if there is a pattern in the data \u2013 is it signal or is it just noise?", "Does that thought make you uncomfortable? It made my hands sweat when I came across this situation for the first time.\u00a0Do you wonder\u00a0how to explore a multidimensional dataset? It is one of the frequently asked question by many data scientists. In this article, I will take you through a very powerful way to exactly do this.", "By now, some of you would be screaming\u00a0\u201cI\u2019ll use PCA for dimensionality reduction and visualization\u201d. Well, you are right! PCA is definitely a good choice for dimensionality reduction and visualization for datasets with a\u00a0large number of features. But, what if you could use something more advanced than PCA? (If you don\u2019t know PCA, I would strongly recommend to read this article first)", "What if you could easily search for a pattern in non-linear style? In this article, I will tell you about a new algorithm\u00a0called t-SNE (2008), which is much more effective than PCA (1933).\u00a0I will take you through the basics of t-SNE algorithm first and then will walk you through why t-SNE is a good fit for dimensionality reduction algorithms.", "You will also, get hands-on knowledge for using t-SNE in both R and Python.", "(t-SNE) t-Distributed Stochastic Neighbor Embedding\u00a0is a non-linear dimensionality reduction algorithm used for exploring high-dimensional\u00a0data. It maps multi-dimensional data to two or more dimensions suitable for human observation. With help of the t-SNE algorithms, you may have to plot fewer exploratory data analysis plots next time you work with high dimensional data.", "In order to understand how t-SNE works, let\u2019s first understand what is dimensionality reduction?", "Well, in simple terms, dimensionality reduction is the technique of representing multi-dimensional data (data with multiple features having a\u00a0correlation with each other) in 2 or 3 dimensions. ", "Some of you might question why do we need Dimensionality Reduction when we can plot the data using scatter plots, histograms\u00a0& boxplots and make sense of the pattern in data using descriptive statistics.\u00a0", "Well, even if you can understand the patterns in data and present it on simple charts, it is still difficult for anyone without statistics background to make sense of it. Also, if you have hundreds of features, you have to study thousands of charts before you can make sense of this data. (Read more about dimensionality reduction here)", "With the help of dimensionality reduction algorithm, you will be able to present the data explicitly.", "Now that you have an understanding of what is dimensionality reduction, let\u2019s look at how we\u00a0can use t-SNE algorithm for reducing dimensions.\u00a0", "Following are a few dimensionality reduction algorithms that you can check out:", "The good news is that you need to study only two of the algorithms mentioned above to effectively visualize data in lower dimensions \u2013 PCA and t-SNE.", "PCA is a linear algorithm. It will not be able to interpret complex polynomial relationship between features. On the other hand, t-SNE is based on probability distributions with random walk on neighborhood graphs to find the structure within the data. ", "A major problem with, linear dimensionality reduction algorithms is that they concentrate on placing dissimilar data points far apart in a lower dimension representation. But in order to represent high dimension data on low dimension, non-linear manifold, it is important that similar datapoints must be represented close together, which is not what linear dimensionality reduction algorithms do.", "Now, you have a brief understanding of what PCA endeavors\u00a0to do.", "Local approaches seek to map nearby points on the manifold to nearby points in the low-dimensional representation. Global approaches on the other hand attempt to preserve geometry at all scales, i.e mapping nearby points to nearby points and far away points to far away points \u00a0", "It is important to know that most of the nonlinear techniques other than t-SNE are not capable of retaining both the local and global structure of the data at the same time.", "This section is for the people interested in understanding the algorithm in depth. You can safely skip this section if you do not want to go through the math in detail.", "Let\u2019s understand why you should know about t-SNE and the algorithmic details of t-SNE. \u00a0t-SNE is an improvement on the Stochastic Neighbor Embedding (SNE) algorithm.", "Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities. The similarity of datapoint \u00a0to datapoint \u00a0is the conditional probability, \u00a0,\u00a0 would\u00a0pick\u00a0as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at  .\u00a0", "For nearby datapoints, \u00a0is relatively high, whereas for widely separated datapoints, \u00a0will be almost infinitesimal (for reasonable values of the variance of the Gaussian, ). Mathematically, the conditional probability \u00a0is given by", "where \u00a0is the variance of the Gaussian that is centered on datapoint ", "If you are not interested in the math, think about it in this way, the algorithm starts by converting the shortest distance (a straight line) between the points into probability of similarity of points. Where, the\u00a0similarity between points is: the conditional probability that  \u00a0would pick\u00a0\u00a0as its neighbor\u00a0if neighbors were picked in proportion to their probability density under a Gaussian (normal distribution) centered at .", "For the low-dimensional counterparts\u00a0 and \u00a0of the high-dimensional datapoints \u00a0and\u00a0 \u00a0it is possible to compute a similar conditional probability, which we denote by\u00a0.", "Note that, pi|i and pj|j are set to zero as we only want to model pair wise similarity.", "In simple terms step 1 and step2 calculate the conditional probability of similarity between a pair of points in ", "\u00a0For the sake of simplicity, try to understand this in detail.\u00a0", "Let us map 3D space to 2D space. What step1 and step2 are doing is calculating the probability of similarity of points in 3D space and calculating the probability of similarity of points in the corresponding 2D space. \u00a0", "Logically, the conditional probabilities \u00a0and must be equal for a\u00a0perfect representation of the similarity of the datapoints in the different dimensional spaces, i.e the difference between \u00a0and \u00a0must be zero for the perfect replication of the plot in high and low dimensions.", "By this logic SNE attempts to minimize this difference of conditional probability.", "Now here is the difference between the SNE and t-SNE algorithms.\u00a0", "To measure the minimization of sum of difference of conditional probability SNE minimizes the sum of Kullback-Leibler divergences overall data points using a gradient descent method. We must know that KL divergences are asymmetric in nature. ", "In other words, the SNE cost function focuses on retaining the local structure of the data in the map (for reasonable values of the variance of the Gaussian in the high-dimensional space,). ", "Additionally, it is very difficult (computationally inefficient) to optimize this cost function.", "So t-SNE also tries to minimize the sum of the\u00a0difference in conditional probabilities. But it does that by using the symmetric version of the SNE cost function, with simple gradients. Also, t-SNE employs a heavy-tailed distribution in the low-dimensional space to alleviate both the crowding problem (the area of the two-dimensional map that is available to accommodate moderately distant data points will not be nearly large enough compared with the area available to accommodate nearby data points) \u00a0and the optimization problems of SNE.", "If we see the equation to calculate the conditional probability, we have left out the variance from the discussion as of now. The remaining parameter to be selected is the variance of the student\u2019s t-distribution that is centered over each high-dimensional datapoint . It is not likely that there is a single value of\u00a0that is optimal for all data points in the data set because the density of the data is likely to vary. In dense regions, a smaller value of\u00a0\u00a0is usually more appropriate than in sparser regions. Any particular value of\u00a0induces a probability distribution, \u00a0, over all of the other data points. This distribution has an\u00a0", "This distribution has an\u00a0entropy which increases as\u00a0increases. t-SNE performs a binary search for the value of\u00a0\u00a0that produces a \u00a0with a fixed perplexity that is specified by\u00a0\u00a0the user. The perplexity is defined as", "where H() is the Shannon entropy of \u00a0measured in bits\u00a0", "The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.", "The minimization of the cost function is performed using gradient decent. And physically, the gradient may be interpreted as the resultant force created by a set of springs between the map point and all other map points \u00a0. All springs exert a force along the direction ( \u2013\u00a0). The spring between \u00a0and repels or attracts the map points depending on whether the distance between the two in the map is too small or too large to represent the similarities between the two high-dimensional datapoints. The force exerted by the spring between and \u00a0is proportional to its length, and also proportional to its stiffness, which is the mismatch (pj|i \u2013 qj|i + p i| j \u2212 q i| j ) between the pairwise similarities of the data points and the map points[1].-", "Now that we have understood the algorithm, it is time to analyze its performance. As you might have observed, that the algorithm computes pairwise conditional probabilities and tries to minimize the sum of the difference of the probabilities in higher and lower dimensions. This involves a lot of calculations and computations. So the algorithm is quite heavy on the system resources. ", "t-SNE has a quadratic time and space complexity in the number of data points. This makes it particularly slow and resource draining while applying it to data sets comprising of more than 10,000 observations.\u00a0", "After we have looked into the mathematical description of how does the algorithms works, to sum up,\u00a0what we have learned above. Here is a brief explanation of how t-SNE works. \u00a0", "It\u2019s quite simple actually, t-SNE a non-linear dimensionality reduction algorithm finds patterns in the data by identifying observed clusters based on similarity of data points with multiple features. But it is not a clustering algorithm it is a dimensionality reduction algorithm. This is because it maps the multi-dimensional data to a lower dimensional space, the input features are no longer identifiable. Thus you cannot make any inference based only on the output of t-SNE. So essentially it is mainly a data exploration and visualization technique.", "But t-SNE can be used in the process of classification and clustering by using its output as the input feature for other classification algorithms.", "You may ask, what are the use cases of such an algorithm. t-SNE can be used on almost all high dimensional data sets. But it is extensively applied in Image processing, NLP, genomic data and speech processing. It has been utilized for improving the analysis of brain and heart scans. Below are a few examples:", "A lot of progress has been made on FER and many algorithms like PCA have been studied for FER. But, FER still remains a challenge due to the difficulties of dimension reduction and classification. t-Stochastic Neighbor Embedding (t-SNE) is used for reducing the high-dimensional data into a relatively low-dimensional subspace and then using other algorithms like AdaBoostM2, Random Forests, Logistic Regression, NNs and others as multi-classifier for the expression classification. ", "In one such attempt\u00a0for facial recognition based on the Japanese Female Facial Expression (JAFFE) database with t-SNE and AdaBoostM2. Experimental results showed that the proposed new algorithm applied to FER gained the better performance compared with those traditional algorithms, such as PCA, LDA, LLE and SNE.[2]", "The flowchart for implementing such a combination on the data could be as follows:", "Preprocessing \u2192 normalization \u2192 t-SNE\u2192 classification algorithm ", "Mass spectrometry imaging (MSI) is a technology that simultaneously provides the spatial distribution for hundreds of biomolecules directly from tissue. Spatially mapped t-distributed stochastic neighbor embedding (t-SNE), a nonlinear visualization of the data that is able to better resolve the biomolecular intratumor heterogeneity. ", "In an unbiased manner, t-SNE can uncover tumor subpopulations that are statistically linked to patient survival in gastric cancer and metastasis status in primary tumors of breast cancer. Survival analysis performed on each t-SNE clusters will provide significantly useful results.[3]", "Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \u201ccapital city of\u201d. Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. This combination of techniques can be used to provide a bird\u2019s-eye view of different text sources, including text summaries and their source material. This enables users to explore a text source like a geographical map.[4]", "While comparing the performance of t-SNE with other algorithms, we will compare t-SNE with other algorithms based on the achieved accuracy rather than the time and resource requirements with relation to accuracy.", "t-SNE outputs provide better results than PCA and other linear dimensionality reduction models. This is because a linear method such as classical scaling is not good at modeling curved manifolds. It focuses on preserving the distances between widely separated data points rather than on preserving the distances between nearby data points.", "The Gaussian kernel employed in the high-dimensional space by t-SNE defines a soft border between the local and global structure of the data. And for pairs of data points that are close together relative to the standard deviation of the Gaussian, the importance of modeling their separations is almost independent of the magnitudes of those separations. Moreover, t-SNE determines the local neighborhood size for each datapoint separately based on the local density of the data (by forcing each conditional probability distribution to have the same perplexity)[1]. This is because the algorithm defines a soft border between the local and global structure of the data. \u00a0And unlike other non-linear dimensionality reduction algorithms, it performs better than any of them.\u00a0", "Let\u2019s implement the t-SNE algorithm on MNIST handwritten digit database. This is one of the most explored dataset for image processing.", "The \u201cRtsne\u201d package has an implementation of t-SNE in R. The \u201cRtsne\u201d package can be installed in R using the following command typed in the R console:", "MNIST data can be downloaded from the MNIST website and can be converted into a csv file with small amount of code.For this example, please download the following preprocessed MNIST data. link", "As can be seen t-SNE takes considerably longer time to execute on the same sample size of data than PCA. \u00a0", " The plots can be used for exploratory analysis. The output x & y co-ordinates and as well as cost can be used as features in classification algorithms.", "An important thing to note is that the \u201cpip install tsne\u201d produces an error. Installing \u201ctsne\u201d package is not recommended. t-SNE algorithm can be accessed from sklearn package.", " The following code is taken from the sklearn examples on the sklearn website. ", "Well for the data scientist the main problem while using t-SNE is the black box type nature of the algorithm. This impedes the process of providing inferences and insights based on the results.\u00a0Also, another problem with the algorithm is that it doesn\u2019t always provide a\u00a0similar output on successive runs.", "So then how could you use the algorithm? The best way to used the algorithm is to use it for exploratory data analysis. It will give you a very good sense of patterns hidden inside the data. It can also be used as an input parameter for other classification & clustering algorithms.", "Reduce the dataset to 2 or 3 dimensions and stack this with a non-linear stacker. Using a holdout set for stacking / blending. Then you can boost the t-SNE vectors using XGboost to get better results.", "For data science enthusiasts who are beginning to work with data science, this algorithm presents the best opportunities in terms of research and performance enhancements. There have been a few research papers attempting to improve the time complexity of the algorithm by utilizing linear functions. But an optimal solution is still required. Research papers on implementing t-SNE for a\u00a0variety of NLP problems and image processing applications is an unexplored territory and has enough scope.\u00a0", "Following are a few\u00a0common fallacies to avoid while interpreting the results of t-SNE:", "[2] Jizheng Yi et.al. Facial expression recognition Based on t-SNE and AdaBoostM2.", "IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber,Physical and Social Computing (2013)", "[3] \u00a0Walid M. Abdelmoulaa et.al. Data-driven identification of prognostic tumor subpopulations using spatially mapped t-SNE of mass spectrometry imaging data.", "[4] \u00a0Hendrik Heuer. Text comparison using word vector representations and dimensionality reduction.\u00a08th EUR. CONF. ON PYTHON IN SCIENCE (EUROSCIPY 2015)", "I hope you enjoyed reading this article. \u00a0In this article, I have tried to explore all the aspects to help you get started with t-SNE. I\u2019m sure you must be excited to explore more on t-SNE algorithm and use it at your end.", "Share your experience of working with t-SNE algorithm and if you think its better than PCA. If you have any doubts or questions, feel free to post it in the comments section.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F01%2Ft-sne-implementation-r-python%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=Comprehensive Guide on t-SNE algorithm with implementation in R & Python&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F01%2Ft-sne-implementation-r-python%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F01%2Ft-sne-implementation-r-python%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/saurabh-jaju2/", "anchor_text": "Saurabh.jaju2"}, {"url": "https://www.analyticsvidhya.com/blog/category/advanced/", "anchor_text": "Advanced"}, {"url": "https://www.analyticsvidhya.com/blog/category/algorithm/", "anchor_text": "Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/maths/", "anchor_text": "Maths"}, {"url": "https://www.analyticsvidhya.com/blog/category/python-2/", "anchor_text": "Python"}, {"url": "https://www.analyticsvidhya.com/blog/category/r/", "anchor_text": "R"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/", "anchor_text": "read this article first"}, {"url": "https://datahack.analyticsvidhya.com/contest/skillpower-machine-learning/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/", "anchor_text": "dimensionality reduction here"}, {"url": "https://en.wikipedia.org/wiki/Kullback\u2013Leibler_divergence", "anchor_text": "Kullback-Leibler divergences"}, {"url": "https://en.wikipedia.org/wiki/Entropy_(information_theory)", "anchor_text": "Shannon entropy"}, {"url": "https://drive.google.com/file/d/0B6E7D59TV2zWYlJLZHdGeUYydlk/view?usp=sharing", "anchor_text": "link"}, {"url": "https://www.analyticsvidhya.com/blog", "anchor_text": "Learn"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "compete, hack"}, {"url": "https://www.analyticsvidhya.com/jobs/#/user/", "anchor_text": "get hired"}, {"url": "https://www.analyticsvidhya.com/blog/tag/dimensionality-reduction/", "anchor_text": "Dimensionality Reduction"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning/", "anchor_text": "machine learning"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning-algorithm/", "anchor_text": "machine learning algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning-application/", "anchor_text": "machine learning application"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne/", "anchor_text": "t-SNE"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne-algorithm/", "anchor_text": "t-SNE algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne-examples/", "anchor_text": "t-SNE examples"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne-in-python/", "anchor_text": "t-SNE in Python"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne-in-r/", "anchor_text": "t-SNE in R"}, {"url": "https://www.analyticsvidhya.com/blog/tag/time-and-space-complexity/", "anchor_text": "time and space complexity"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Dimensionality Reduction Techniques Skill Test for Data Scientists (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/a-quick-introduction-to-manifold-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Introduction to Manifold Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/diminishing-the-dimensions-with-pca/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Diminishing the Dimensions with PCA!"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/dealing-with-sparse-datasets-in-machine-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Dealing with Sparse Datasets in Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2023/01/impact-of-categorical-encodings-on-anomaly-detection-methods/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Impact of Categorical Encodings on Anomaly Detection Methods"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/underrated-apriori-algorithm-based-unsupervised-machine-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Underrated Apriori Algorithm Based Unsupervised Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/author/saurabh-jaju2/", "anchor_text": "Saurabh.jaju2"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/delivery-head-to-data-science-hacker/", "anchor_text": "MyStory: How I became a Data Science Hacker from being a Delivery Head"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/sas-admin-mumbai-pune-bangalore-5-6-years-of-experience/", "anchor_text": "SAS Admin- Mumbai, Pune, Bangalore (5-6 years of experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}