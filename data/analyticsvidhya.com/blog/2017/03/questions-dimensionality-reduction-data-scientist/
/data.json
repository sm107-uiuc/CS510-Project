{"url": "https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/\n", "time": 1683020505.600248, "path": "analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/\n/", "webpage": {"metadata": {"title": "Dimensionality Reduction Techniques Skill Test for Data Scientists", "h1": "Dimensionality Reduction Techniques Skill Test for Data Scientists (Updated 2023)", "description": "40 questions to test and improve your knowledge of dimensionality reduction techniques covering topics like PCA, LDA, t-SNE, and more."}, "outgoing_paragraph_urls": [{"url": "https://datahack.analyticsvidhya.com/contest/all/", "anchor_text": "Check out more challenging competitions coming up here", "paragraph_index": 1}, {"url": "https://datahack.analyticsvidhya.com/contest/skilltest-dimensionality-reduction/lb", "anchor_text": "here", "paragraph_index": 4}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "link", "paragraph_index": 29}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "link", "paragraph_index": 34}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06064637/Image_24.jpg", "anchor_text": "", "paragraph_index": 52}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062621/Image32.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062911/Image33.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062945/Image33_B.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062131/Image_37.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://www.youtube.com/watch?v=RfrGiG1Hm3M", "anchor_text": "video", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062131/Image_37.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://books.google.co.in/books?id=MgGrCAAAQBAJ&pg=PA110&lpg=PA110&dq=LDA+produce+at+most+C-1+feature+projections&source=bl&ots=OHYbplX4IJ&sig=3wkoBzaAfZZplFSF6MXgH3wfl0I&hl=en&sa=X&ved=0ahUKEwiZxLTQ-trSAhUlS48KHVjfCHYQ6AEIUDAJ#v=onepage&q&f=false", "anchor_text": "link", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06061642/Image_Context39.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06061851/Image_40.jpg", "anchor_text": "", "paragraph_index": 67}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062945/Image33_B.jpg", "anchor_text": "", "paragraph_index": 70}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062131/Image_37.jpg", "anchor_text": "", "paragraph_index": 70}, {"url": "https://books.google.co.in/books?id=MgGrCAAAQBAJ&pg=PA110&lpg=PA110&dq=LDA+produce+at+most+C-1+feature+projections&source=bl&ots=OHYbplX4IJ&sig=3wkoBzaAfZZplFSF6MXgH3wfl0I&hl=en&sa=X&ved=0ahUKEwiZxLTQ-trSAhUlS48KHVjfCHYQ6AEIUDAJ#v=onepage&q&f=false", "anchor_text": "link", "paragraph_index": 79}], "all_paragraphs": ["Have you come across a dataset with hundreds of columns and wondered how to build a predictive model on it? Or have come across a situation where a lot of variables might be correlated? It is difficult to escape these situations while working on real-life problems. Thankfully, dimensionality reduction techniques come to our rescue here.\u00a0Dimensionality Reduction is an important technique in artificial intelligence. It is a must-have skill set for any data scientist for data analysis. To test your knowledge of dimensionality reduction techniques, we have conducted this skill test. These questions include topics like Principal Component Analysis (PCA), t-SNE, and LDA.", "Check out more challenging competitions coming up here", "A total of 582 people participated in this skill test. The questions varied from theoretical to practical. If you missed\u00a0taking the test, here is your opportunity for you to find out how many questions you could have answered correctly.", "Below is the\u00a0distribution of scores; this will help you evaluate your performance.", "You can access your performance here. More than 180 people participated in the skill test\u00a0and the highest score was 34.\u00a0Here are a few statistics about the distribution.", "Are you just getting started with Dimensionality Reduction Techniques? Do you want to learn how to use these techniques to work on real-life projects and improve the model performance? Presenting two comprehensive courses which cover all the important concepts like feature selection and dimensionality reduction-", "You have to select the 100 most important features based on the relationship between input features and the target features. Do you think this is an example of dimensionality reduction?", "Explanation: LDA is an example of an unsupervised dimensionality reduction algorithm.", "Step 1: Using the above variables, I have created two more variables, namely E = A + 3  B and F = B + 5  C + D.\nStep 2: Then, using only the variables E and F, I built a Random Forest or decision tree machine learning model.\nCould the steps performed above represent a dimensionality reduction method?", "Explanation: Yes, Because Step 1 could be used to represent the data in 2 lower dimensions.", "A. Removing columns that have too many missing values", "B. Removing columns that have high variance in data", "C. Removing columns with dissimilar data trends", "Explanation: If columns have too many missing values (say 99%), then we can remove such columns.", "Explanation: Reducing the dimension of data will take less time to train a model.", "Explanation: All of the algorithms are examples of dimensionality reduction algorithms.", "Explanation: Sometimes it is very useful to plot the data in lower dimensions. We can take the first 2 principal components and then use visualization of the data using a scatter plot.", "And then use these PCA projections as our features. Which of the following statement is correct?", "A. Higher \u2018k\u2019 means more regularization", "B. Higher \u2018k\u2019 means less regularization", "Explanation: Higher k would lead to less smoothening as we\u00a0would be able to preserve more characteristics in data, hence less regularization. By increasing regularization, we can avoid overfitting.", "A. Dataset with 1 Million entries and 300 features", "Explanation: t-SNE has quadratic time and space complexity. Thus it is a very heavy algorithm in terms of system resource utilization.", "A. It is asymmetric in nature.", "B. It is symmetric in nature.", "C. It is the same as the cost function for SNE.", "Explanation: The cost function of SNE is asymmetric in nature. Which makes it difficult to converge using gradient descent. An asymmetric cost function is one of the major differences between SNE and t-SNE.", "Imagine you are dealing with text data. To represent the words, you are using word embedding (Word2vec). In word embedding, you will end up with 1000 dimensions. Now, you want to reduce the dimensionality of this high-dimensional data such that similar words should have a similar meaning in the nearest neighbor space. In such a case,", "Explanation: t-SNE stands for t-Distributed Stochastic Neighbor Embedding, which considers the nearest neighbors for reducing the data.", "Explanation: t-SNE learns a non-parametric mapping, which means that it does not learn an explicit function that maps data from the input space to the map. For more information, refer to this link.", "A. t-SNE is linear, whereas PCA is non-linear", "B. t-SNE and PCA are both linear", "C. t-SNE and PCA are both nonlinear", "D. t-SNE is nonlinear, whereas PCA is linear", "Explanation: Option D is correct. Read the explanation from this link", "B. Smooth measure of the effective number of neighbors", "Explanation: All of the hyper-parameters in the option can be tuned.", "A. When the data is huge (in size), t-SNE may fail to produce better results.", "B. T-NSE always produces better results regardless of the size of the data", "C. PCA always performs better than t-SNE for smaller-sized data.", "Which of the following must be true for a perfect representation of xi and xj in lower dimensional space?", "Explanation: The conditional probabilities related to Bayes\u2019 theorem for the similarity of two points must be equal because the similarity between the points must remain unchanged in both higher and lower dimensions for them to be perfect representations.", "A. LDA aims to maximize the distance between classes and minimize the within-class distance.", "B. LDA aims to minimize both distances between classes and the distance within the class.", "C. LDA aims to minimize the distance between classes and maximize the distance within the class.", "D. LDA aims to maximize both distances between classes and the distance within the class.", "A. If the discriminatory information is not in the mean but in the variance of the data", "B. If the discriminatory information is in the mean but not in the variance of the data", "C. If the discriminatory information is in the mean and variance of the data", "Explanation: All of the options are correct", "Explanation: When all eigenvectors are the same in such case you won\u2019t be able to select the principal components because, in that case, all principal components are equal.", "Explanation: When you get the features in lower dimensions, then you will lose some information of data most of the time, and you won\u2019t be able to interpret the lower dimension data.", "\nSelect the angle which will capture maximum variability along a single axis.", "Explanation: Option B has the largest possible variance in data.", "Explanation: PCA is a deterministic algorithm that doesn\u2019t have parameters to initialize and doesn\u2019t have a local minima problem like most machine learning algorithms.", "The below snapshot shows the scatter plot of two features (X1 and X2) with the class information (Red, Blue). You can also see the direction of PCA and LDA.", "A. Building a classification algorithm with PCA (A principal component in the direction of PCA)", "B. Building a classification algorithm with LDA", "Explanation: If our goal is to classify these points, PCA projection does only more harm than good\u2014the majority of blue and red points would land overlapped on the first principal component. hence PCA would confuse the classifier.", "A. When data has zero median", "B. When data has zero mean", "C. Both are always the same", "Explanation: When the data has a zero mean vector, otherwise, you have to center the data first before taking SVD.", "Explanation: The first principal component is v = [ \u221a 2 /2, \u221a 2/ 2 ] T (you shouldn\u2019t really need to solve any SVD or eigenproblem to see this). Note that we should apply normalization to the principal component to have unit length. (The negation v = [\u2212 \u221a 2/ 2, \u2212 \u221a 2/ 2 ] T is also correct.)", "What are their coordinates in the 1-d subspace?", "For the projected data, you just obtained projections ( (\u2212 \u221a 2 ), (0), (\u221a 2) ). We then represent them in the original 2-d space and consider them as the reconstruction of the original data points.", "Explanation: The reconstruction error is 0 since all three points are perfectly located in the direction of the first principal component. Or, you can actually calculate the reconstruction: z1 \u00b7v.", "x\u02c61 = \u2212 \u221a 2\u00b7[ \u221a 2/ 2 , \u221a 2/2 ] T = [\u22121, \u22121]T\nx\u02c62 = 0*[0, 0]T = [0,0] x\u02c63 = \u221a 2* [1, 1]T = [1,1]\nwhich are exactly x1, x2, x3.\nQ32. In LDA, the idea is to find the line that best separates the two classes. In the given image, which of the following is a good projection?\n\n\nA. LD1\nB. LD2\nC. Both\nD. None of these\nSolution: (A)\nExplanation: LD1 Is a good projection because it best separates the class.\nQuestion Context: 33\nPCA is a good technique to try because it is simple to understand and is commonly used to reduce the dimensionality of the data. Obtain the eigenvalues \u03bb1 \u2265 \u03bb2 \u2265 \u2022 \u2022 \u2022 \u2265 \u03bbN and plot.\n\n\nTo see how f(M) increases with M and takes the maximum value 1 at M = D. We have two graphs given below:\n\n\u00a0\nQ33. Which of the above graph shows better performance of PCA? Where M is the first M principal component, and D is the total number of features?\nA. Left\nB. Right\nC. Any of A and B\nD. None of these\nSolution: (A)\nExplanation: PCA is good if f(M) asymptotes rapidly to 1. This happens if the first eigenvalues are big and the remainder is small. PCA is bad if all the eigenvalues are roughly equal. See examples of both cases in the figure.\nQ34. Which of the following option is true?\nA. LDA explicitly attempts to model the difference between the classes of data. On the other hand, PCA does not consider any difference in class.\nB. Both attempt to model the difference between the classes of data.\nC. PCA explicitly attempts to model the difference between the classes of data. LDA, on the other hand, does not consider any difference in class.\nD. Both don\u2019t attempt to model the difference between the classes of data.\nSolution: (A)\nExplanation: Options are self-explanatory.\nQ35. Which of the following can be the first 2 principal components after applying PCA?\n\n(0.5, 0.5, 0.5, 0.5) and (0.71, 0.71, 0, 0)\n(0.5, 0.5, 0.5, 0.5) and (0, 0, -0.71, -0.71)\n(0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5)\n(0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5)\n\nA. 1 and 2\nB. 1 and 3\nC. 2 and 4\nD. 3 and 4\nSolution: (D)\nExplanation: The two loading vectors are not orthogonal for the first two choices.\nQ36. Which of the following gives the difference(s) between the logistic regression and LDA?\n\nIf the classes are well separated, the parameter estimates for logistic regression can be unstable.\nIf the sample size is small and the distribution of features is normal for each class. In such cases, linear discriminant analysis is more stable than logistic regression.\n\nA. 1\nB. 2\nC. 1 and 2\nD. None of these\nSolution: (C)\nExplanation: Refer to this video\nQ37. Which of the following offset do we consider in PCA?\n\n\nA. Vertical offset\nB. Perpendicular offset\nC. Both\nD. None of these\nSolution: (B)\nExplanation: We always consider residuals as vertical offsets. Perpendicular offsets are useful in the case of PCA.\nQ38. Imagine you are dealing with 10 class classification problem, and you want to know at most how many discriminant vectors can be produced by LDA. What is the correct answer?\nA. 20\nB. 9\nC. 21\nD. 11\nE. 10\nSolution: (B)\nExplanation: LDA produces, at most c \u2212 1 discriminant vector. You may refer to this link for more information.\nQuestion Context: 39\nThe given dataset consists of images of the \u201cHoover Tower\u201d and some other towers. Now, you want to use PCA (Eigenface) and the nearest neighbor method to build a classifier that predicts whether a new image depicts a \u201cHoover tower\u201d or not. The figure gives a sample of your input training dataset images.\n\n\nQ39. In order to get reasonable performance from the \u201cEigenface\u201d algorithm, what pre-processing steps will be required on these images?\n\nAlign the towers in the same position in the image.\nScale or crop all images to the same size.\n\nA. 1\nB. 2\nC. 1 and 2\nD. None of these\nSolution: (C)\nExplanation: Both statements are correct.\nQ40. What is the optimum number of principal components in the below figure?\n\n\nA. 7\nB. 30\nC. 40\nD. Can\u2019t Say\nSolution: (B)\nExplanation: We can see in the above figure that the number of components = 30 is giving highest variance with the lowest number of components. Hence option \u2018B\u2019 is the right answer.\nBonus Content: Top 3 Dimensionality Reduction Interview Questions\nQ1. Can we use deep learning for dimensionality reduction?\nA. Yes, we can use a type of neural network called autoencoder with an activation function for dimensionality reduction.\nQ2. What are the three main methods of reducing dimensionality?\nA. Principle Component Analysis, Linear Discriminant Analysis, and T-distributed Stochastic Neighbor Embedding are three examples of dimensionality reduction.\nQ3. What is the application of dimensional reduction regarding big data?\nA. It can be used in data mining of big data so that we can easily use various learning techniques on the resultant data.\nConclusion\nI hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge of\u00a0dimensionality reduction. Don\u2019t forget to check out our other blogs on data science interview questions covering topics such as time series, SQL, k-means clustering, linear regression, KNN, and more.\n\n\nRelated\n ", "Explanation: LD1 Is a good projection because it best separates the class.", "PCA is a good technique to try because it is simple to understand and is commonly used to reduce the dimensionality of the data. Obtain the eigenvalues \u03bb1 \u2265 \u03bb2 \u2265 \u2022 \u2022 \u2022 \u2265 \u03bbN and plot.", "To see how f(M) increases with M and takes the maximum value 1 at M = D. We have two graphs given below:\n\n\u00a0", "C. Any of A and B", "Explanation: PCA is good if f(M) asymptotes rapidly to 1. This happens if the first eigenvalues are big and the remainder is small. PCA is bad if all the eigenvalues are roughly equal. See examples of both cases in the figure.", "A. LDA explicitly attempts to model the difference between the classes of data. On the other hand, PCA does not consider any difference in class.", "B. Both attempt to model the difference between the classes of data.", "C. PCA explicitly attempts to model the difference between the classes of data. LDA, on the other hand, does not consider any difference in class.", "D. Both don\u2019t attempt to model the difference between the classes of data.", "Explanation: The two loading vectors are not orthogonal for the first two choices.", "Explanation: We always consider residuals as vertical offsets. Perpendicular offsets are useful in the case of PCA.", "Explanation: LDA produces, at most c \u2212 1 discriminant vector. You may refer to this link for more information.", "The given dataset consists of images of the \u201cHoover Tower\u201d and some other towers. Now, you want to use PCA (Eigenface) and the nearest neighbor method to build a classifier that predicts whether a new image depicts a \u201cHoover tower\u201d or not. The figure gives a sample of your input training dataset images.", "Explanation: We can see in the above figure that the number of components = 30 is giving highest variance with the lowest number of components. Hence option \u2018B\u2019 is the right answer.", "A. Yes, we can use a type of neural network called autoencoder with an activation function for dimensionality reduction.", "A. Principle Component Analysis, Linear Discriminant Analysis, and T-distributed Stochastic Neighbor Embedding are three examples of dimensionality reduction.", "A. It can be used in data mining of big data so that we can easily use various learning techniques on the resultant data.", "I hope you enjoyed taking the test and found the solutions helpful. The test focused on conceptual as well as practical knowledge of\u00a0dimensionality reduction. Don\u2019t forget to check out our other blogs on data science interview questions covering topics such as time series, SQL, k-means clustering, linear regression, KNN, and more.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F03%2Fquestions-dimensionality-reduction-data-scientist%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=Dimensionality Reduction Techniques Skill Test for Data Scientists (Updated 2023)&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F03%2Fquestions-dimensionality-reduction-data-scientist%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F03%2Fquestions-dimensionality-reduction-data-scientist%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/facebook_user_4/", "anchor_text": "1201904"}, {"url": "https://www.analyticsvidhya.com/blog/category/intermediate/", "anchor_text": "Intermediate"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questions/", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/skilltest/", "anchor_text": "Skilltest"}, {"url": "https://datahack.analyticsvidhya.com/contest/all/", "anchor_text": "Check out more challenging competitions coming up here"}, {"url": "https://datahack.analyticsvidhya.com/contest/skilltest-dimensionality-reduction/lb", "anchor_text": ""}, {"url": "https://datahack.analyticsvidhya.com/contest/skilltest-dimensionality-reduction/lb", "anchor_text": "here"}, {"url": "https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/", "anchor_text": "Beginners Guide To Learn Dimension Reduction Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/", "anchor_text": "Practical Guide to Principal Component Analysis (PCA) in R & Python"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "Comprehensive Guide on t-SNE algorithm with implementation in R & Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/svm-and-pca-tutorial-for-beginners/", "anchor_text": "Support Vector Machine (SVM) and Principal Component Analysis Tutorial for Beginners"}, {"url": "https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=blog&utm_medium=40DimensionalityReductionQuestionsArticle", "anchor_text": "Applied Machine Learning \u2013 Beginner to Professional"}, {"url": "https://courses.analyticsvidhya.com/bundles/certified-ai-ml-blackbelt-plus?utm_source=blog&utm_medium=40DimensionalityReductionQuestionsArticle", "anchor_text": "Certified AI & ML Blackbelt+ Program"}, {"url": "https://lvdmaaten.github.io/tsne/", "anchor_text": "link"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/", "anchor_text": "link"}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06064954/Image_18.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06064637/Image_24.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06064252/Image_cont_26.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/17140918/Image_291.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062621/Image32.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062911/Image33.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062945/Image33_B.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062131/Image_37.jpg", "anchor_text": ""}, {"url": "https://www.youtube.com/watch?v=RfrGiG1Hm3M", "anchor_text": "video"}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06062131/Image_37.jpg", "anchor_text": ""}, {"url": "https://books.google.co.in/books?id=MgGrCAAAQBAJ&pg=PA110&lpg=PA110&dq=LDA+produce+at+most+C-1+feature+projections&source=bl&ots=OHYbplX4IJ&sig=3wkoBzaAfZZplFSF6MXgH3wfl0I&hl=en&sa=X&ved=0ahUKEwiZxLTQ-trSAhUlS48KHVjfCHYQ6AEIUDAJ#v=onepage&q&f=false", "anchor_text": "link"}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06061642/Image_Context39.jpg", "anchor_text": ""}, {"url": "https://cdn.analyticsvidhya.com/wp-content/uploads/2017/03/06061851/Image_40.jpg", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/tag/dimensionality/", "anchor_text": "dimensionality"}, {"url": "https://www.analyticsvidhya.com/blog/tag/lda/", "anchor_text": "LDA"}, {"url": "https://www.analyticsvidhya.com/blog/tag/pca/", "anchor_text": "PCA"}, {"url": "https://www.analyticsvidhya.com/blog/tag/principal-components-analysis/", "anchor_text": "Principal Components Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/tag/reducing-dimensionality/", "anchor_text": "Reducing Dimensionality"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne/", "anchor_text": "t-SNE"}, {"url": "https://www.analyticsvidhya.com/blog/tag/t-sne-in-python/", "anchor_text": "t-SNE in Python"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/ace-your-data-science-skills-with-datahour-sessions/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "Ace Your Data Science Skills with DataHour Sessions"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/diminishing-the-dimensions-with-pca/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "Diminishing the Dimensions with PCA!"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "Comprehensive Guide on t-SNE algorithm with implementation in R & Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-dimensionality-reduction-pca/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "20 Questions to Test Your Skills On Dimensionality Reduction (PCA)"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/dimensionality-reduction-a-descry-for-data-scientist/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "Dimensionality Reduction a Descry for Data Scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2022/09/principal-component-analysis-interview-questions/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/", "anchor_text": "Principal Component Analysis Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/author/facebook_user_4/", "anchor_text": "1201904"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/team-lead-data-quality-gurgaon-india-3-years-of-experience/", "anchor_text": "Team Lead, Data Quality- Gurgaon, India (3+ Years Of Experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/senior-analyst-dashboard-and-analytics-hyderabad-1-4-years-of-experience/", "anchor_text": "Senior Analyst \u2013 Dashboard and Analytics \u2013 Hyderabad (1- 4+ Years Of Experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}