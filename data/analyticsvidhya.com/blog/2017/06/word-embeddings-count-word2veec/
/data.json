{"url": "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n", "time": 1683020408.8475788, "path": "analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n/", "webpage": {"metadata": {"title": "Understanding Word Embeddings: From Word2Vec to Count Vectors", "h1": "An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec", "description": "Word embeddings are techniques used in natural language processing. This includes tools & techiniques like word2vec, TD-IDF, count vectors, etc."}, "outgoing_paragraph_urls": [{"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "this awesome article", "paragraph_index": 17}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "Understanding and Coding Neural Networks from scratch", "paragraph_index": 17}, {"url": "http://bit.ly/wevi-online", "anchor_text": "This", "paragraph_index": 17}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit", "anchor_text": "this", "paragraph_index": 17}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 17}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem: Identify the Sentiments", "paragraph_index": 17}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 17}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem : Twitter Sentiment Analysis", "paragraph_index": 17}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course", "paragraph_index": 17}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "this awesome article", "paragraph_index": 28}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "Understanding and Coding Neural Networks from scratch", "paragraph_index": 28}, {"url": "http://bit.ly/wevi-online", "anchor_text": "This", "paragraph_index": 28}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit", "anchor_text": "this", "paragraph_index": 28}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 28}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem: Identify the Sentiments", "paragraph_index": 28}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 28}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem : Twitter Sentiment Analysis", "paragraph_index": 28}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course", "paragraph_index": 28}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "this awesome article", "paragraph_index": 64}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "Understanding and Coding Neural Networks from scratch", "paragraph_index": 78}, {"url": "http://bit.ly/wevi-online", "anchor_text": "This", "paragraph_index": 89}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit", "anchor_text": "this", "paragraph_index": 89}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 89}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem: Identify the Sentiments", "paragraph_index": 89}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 89}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem : Twitter Sentiment Analysis", "paragraph_index": 89}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course", "paragraph_index": 89}, {"url": "http://bit.ly/wevi-online", "anchor_text": "This", "paragraph_index": 91}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit", "anchor_text": "this", "paragraph_index": 97}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 102}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem: Identify the Sentiments", "paragraph_index": 102}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "", "paragraph_index": 102}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem : Twitter Sentiment Analysis", "paragraph_index": 102}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course", "paragraph_index": 102}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course", "paragraph_index": 111}], "all_paragraphs": ["Before we start, have a look at the below examples.", "So what do the above examples have in common?", "You possible guessed it right \u2013 TEXT processing. All the above three scenarios deal with humongous amount of text to perform different range of tasks like clustering in the google search example, classification in the second and Machine Translation in the third.", "Humans can deal with text format quite intuitively but provided we have millions of documents being generated in a single day, we cannot have humans performing the above the three tasks. It is neither scalable nor effective.", "So, how do we make computers of today perform clustering, classification etc on a text data since we know that they are generally inefficient at handling and processing strings or texts for any fruitful outputs?", "Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that \u201cApple\u201d in \u201cApple is a tasty fruit\u201d is a fruit that can be eaten and not a company?", "The answer to the above questions lie in creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in.", "And all of these are implemented by using Word Embeddings or numerical representations of texts so that computers may handle them.", "Below, we will see formally what are Word Embeddings and their different types and how we can actually implement them to perform the tasks like returning efficient Google search results.", "The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.", "Formally, given a training sample of tweets and labels, where label \u20181\u2019 denotes the tweet is racist/sexist and label \u20180\u2019 denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.", "Are you a beginner looking for a place to start your journey in Natural Language Processing? Presenting two comprehensive Certified Programs, covering the concepts of Natural Language Processing(NLP), curated just for you!", "In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. But before we dive into the details of Word Embeddings, the following question should be asked \u2013 Why do we need Word Embeddings?", "As it turns out, many Machine Learning algorithms and almost all Deep Learning Architectures are incapable of processing strings\u00a0or plain text\u00a0in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications. Some real world applications of text applications are \u2013 sentiment analysis of reviews by Amazon etc., document or news classification or clustering by Google\u00a0etc.", "Let us now define Word Embeddings formally. A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.", "Take a look at this example \u2013 sentence=\u201d Word Embeddings are Word converted into numbers\u00a0\u201c", "A word\u00a0in this sentence may be \u201cEmbeddings\u201d or \u201cnumbers \u201d etc.", "A dictionary\u00a0may be the list of all unique words in the sentence.\u00a0So, a dictionary may look like \u2013 [\u2018Word\u2019,\u2019Embeddings\u2019,\u2019are\u2019,\u2019Converted\u2019,\u2019into\u2019,\u2019numbers\u2019]\nA vector representation of a word\u00a0may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of \u201cnumbers\u201d\u00a0in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].\nThis is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest.\nDifferent Types of Word Embeddings\nThe different types of word embeddings can be broadly classified into two categories-\n\nFrequency-based Embedding\nPrediction-based Embedding\n\nLet us try to understand each of these methods in detail.\nFrequency-based Embedding\nThere are generally three types of vectors that we encounter under this category.\n\nCount Vector\nTF-IDF Vector\nCo-Occurrence Vector\n\nLet us look into each of these\u00a0vectorization methods in detail.\nCount Vector\nConsider a Corpus C of D documents {d1,d2\u2026..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).\nLet us understand this using a simple example.\nD1: He is a lazy boy. She is also lazy.\nD2: Neeraj is a lazy person.\nThe dictionary created may be a list of unique tokens(words) in the corpus =[\u2018He\u2019,\u2019She\u2019,\u2019lazy\u2019,\u2019boy\u2019,\u2019Neeraj\u2019,\u2019person\u2019]\nHere, D=2, N=6\nThe count matrix M of size 2 X 6 will be represented as \u2013\n\u00a0HeShelazyboyNeerajpersonD1112100D2001011\nNow, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for \u2018lazy\u2019 in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as \u2013 D2 contains \u2018lazy\u2019: once, \u2018Neeraj\u2019: once and \u2018person\u2019 once.\nNow there may be quite a few variations while preparing the above matrix M. The variations will be generally in-\n\nThe way dictionary is prepared.Why? Because in real world applications we might have a corpus which contains millions of documents. And with millions of document, we can extract hundreds of millions of unique words. So basically, the matrix that will be prepared like above will be a very sparse one and inefficient for any computation. So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary.\nThe way count is taken for each word.We may either take the frequency (number of times a word has appeared in the document) or the presence(has the word appeared in the document?) to be the entry in the count matrix M. But generally, frequency method is preferred over the latter.\n\nBelow is a representational image of the matrix M for easy understanding.\n\n\nTF-IDF Vectorization\nThis is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.\nCommon words like \u2018is\u2019, \u2018the\u2019, \u2018a\u2019 etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word \u201cMessi\u201d in comparison to other documents. But common words like \u201cthe\u201d etc. are also going to be present in higher frequency in almost every document.\nIdeally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\nTF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.\nSo, how exactly does TF-IDF work?\nConsider the below sample table which gives the count of terms(tokens/words) in two documents.\n\n\nNow, let us define a few terms related to TF-IDF.\nTF = (Number of times term t appears in a document)/(Number of terms in the document)\nSo, TF(This,Document1) = 1/8\nTF(This, Document2)=1/5\nIt denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word \u2018Messi\u2019 in large number.\nIDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\nwhere N is the number of documents and n is the number of documents a term t has appeared in.\nSo, IDF(This) = log(2/2) = 0.\nSo, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\nLet us compute IDF for the word \u2018Messi\u2019.\nIDF(Messi) = log(2/1) = 0.301.\nNow, let us compare the TF-IDF for a common word \u2018This\u2019 and a word \u2018Messi\u2019 which seems to be of relevance to Document 1.\nTF-IDF(This,Document1) = (1/8) * (0) = 0\nTF-IDF(This, Document2) = (1/5) * (0) = 0\nTF-IDF(Messi, Document1) =\u00a0(4/8)*0.301 = 0.15\nAs, you can see for Document1 , TF-IDF method heavily penalises the word \u2018This\u2019 but assigns greater weight to \u2018Messi\u2019. So, this may be understood as \u2018Messi\u2019 is an important word for Document1 from the context of the entire corpus.\nCo-Occurrence Matrix With a Fixed Context Window\nThe big idea \u2013 Similar words tend to occur together and will have similar context for example \u2013 Apple is a fruit. Mango is a fruit.Apple and mango tend to have a similar context i.e fruit.\nBefore I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified \u2013 Co-Occurrence and Context Window.\nCo-occurrence \u2013 For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.\nContext Window \u2013 Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,\nQuickBrownFoxJumpOverTheLazyDog\nThe green words are a 2 (around) context window for the word \u2018Fox\u2019 and for calculating the co-occurrence only these words will be counted. Let us see context window for the word \u2018Over\u2019.\nQuickBrownFoxJumpOverTheLazyDog\nNow, let us take an example corpus to calculate a co-occurrence matrix.\nCorpus = He is not lazy. He is intelligent. He is smart.\n\u00a0HeisnotlazyintelligentsmartHe042121is401221not210100lazy121000intelligent220000smart110000\nLet us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.\nRed box- It is the number of times \u2018He\u2019 and \u2018is\u2019 have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.\nHeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\nWhile the word \u2018lazy\u2019 has never appeared with \u2018intelligent\u2019 in the context window and therefore has been assigned 0 in the blue box.\nVariations of Co-occurrence Matrix\nLet\u2019s say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-\n\nA co-occurrence matrix of size V X V. Now, for even a decent corpus V gets very large and difficult to handle. So generally, this architecture is never preferred in practice.\nA co-occurrence matrix of size V X N where N is a subset of V and can be obtained by removing irrelevant words like stopwords etc. for example. This is still very large and presents computational difficulties.\n\nBut, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.\nLet me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.\nAnd, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.\nSo, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.\n\n\nAdvantages of Co-occurrence Matrix\n\nIt preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.\nIt uses SVD at its core, which produces more accurate word vector representations than existing methods.\nIt uses factorization which is a well-defined problem and can be efficiently solved.\nIt has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.\n\nDisadvantages of Co-Occurrence Matrix\n\nIt requires huge memory to store the co-occurrence matrix.But, this problem can be circumvented by factorizing the matrix out of the system for example in Hadoop clusters etc. and can be saved.\n\nPrediction-based Embedding\nPre-requisite: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through this awesome article by Sunil to gain a very good understanding of how NN works.\nSo far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.\nWord2vec is not a single algorithm but a combination of two techniques \u2013 CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working.\nCBOW (Continuous Bag of words)\nThe way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.\nSuppose, we have a corpus C = \u201cHey, this is sample corpus using only one context word.\u201d and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.\n\n\nThe target for a single datapoint say Datapoint 4 is shown as below\nHeythisissamplecorpususingonlyonecontextword0001000000\nThis matrix shown in the above image is sent into a shallow neural network with three layers: an\u00a0input layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.\nLet us first see a diagrammatic representation of the CBOW model.\n\n\nThe matrix representation of the above image for a single data point is below.\n\n\nThe flow is as follows:\n\nThe input layer and the target, both are one- hot encoded of size [1 X V]. Here V=10 in the above example.\nThere are two sets of weights. one is between the input and the hidden layer and second between hidden and output layer.Input-Hidden layer matrix size =[V X N] , hidden-Output layer matrix \u00a0size =[N X V] : Where N is the number of dimensions we choose to represent our word in. It is arbitary\u00a0and a hyper-parameter for a Neural Network. Also, N is the number of neurons in the hidden layer. Here, N=4.\nThere is a no activation function between any layers.( More specifically, I am referring to linear activation)\nThe input is multiplied by the input-hidden weights and called hidden activation. It is simply the corresponding row in the input-hidden matrix copied.\nThe hidden input gets multiplied by hidden- output weights and output is calculated.\nError between output and target is calculated and propagated back to re-adjust the weights.\nThe weight \u00a0between the hidden layer and the output layer is taken as the word vector representation of the word.\n\nWe saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.\n\n\nBelow is a matrix representation of the above architecture for an easy understanding.\n\n\nThe image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.\nSo, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.\nThe steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.\nIn both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article \u2013 Understanding and Coding Neural Networks from scratch.\nThe differences between MLP and CBOW are \u00a0mentioned below for clarification:\n\nThe objective function in MLP is a MSE(mean square error) whereas in CBOW it is negative log likelihood of a word given a set of context i.e -log(p(wo/wi)), where p(wo/wi) is given as\n\n\n\nwo : output wordwi: context words\n2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has \u00a0sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP.\nAdvantages of CBOW:\n\nBeing probabilistic is nature, it is supposed to perform superior to deterministic methods(generally).\nIt is low on memory. It does not need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.\n\nDisadvantages of CBOW:\n\nCBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.\nTraining a CBOW from scratch can take forever if not properly optimized.\n\nSkip \u2013 Gram model\nSkip \u2013 gram follows the same topology as of CBOW. It just flips CBOW\u2019s architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=\u201dHey, this is sample corpus using only one context word.\u201d Let us construct the training data.\n\n\nThe input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be \u201ctwo\u201d one hot encoded target variables and \u201ctwo\u201d corresponding outputs as can be seen by the blue section in the image.\nTwo separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.\nThe weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.\nThe skip-gram architecture is shown below.\n\n\nFor a better understanding, matrix style structure with calculation has been shown below.\n\n\nLet us break down the above image.\nInput layer \u00a0size \u2013 [1 X V], Input hidden weight matrix size \u2013 [V X N], Number of neurons in hidden layer \u2013 N, Hidden-Output weight matrix size \u2013 [N X V], Output layer size \u2013 C [1 X V]\nIn the above example, C is the number of context words=2, V= 10, N=4\n\nThe row in red is the hidden activation corresponding to the input one-hot encoded vector. It is basically the corresponding row of input-hidden matrix copied.\nThe yellow matrix is the weight between the hidden layer and the output layer.\nThe blue matrix is obtained by the matrix multiplication of hidden activation and the hidden output weights. There will be two rows calculated for two target(context) words.\nEach row of the blue matrix is converted into its softmax probabilities individually as shown in the green box.\nThe grey matrix contains the one hot encoded vectors of the two context words(target).\nError is calculated by substracting the first row of the grey matrix(target) from the first row of the green matrix(output) element-wise. This is repeated for the next row. Therefore, for n\u00a0target context words, we will have n error vectors.\nElement-wise sum is taken over all the error vectors to obtain a final error vector.\nThis error vector is propagated back to update the weights.\n\nAdvantages of Skip-Gram Model\n\nSkip-gram model can capture two semantics for a single word. i.e it will have two vector representations of Apple. One for the company and other for the fruit.\nSkip-gram with negative sub-sampling outperforms every other method generally.\n\nThis\u00a0is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.\nWord Embeddings Use Cases\nSince word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-\n\nFinding the degree of similarity between two words.model.similarity('woman','man')0.73723527\nFinding odd one out.model.doesnt_match('breakfast cereal dinner lunch';.split())'cereal'\nAmazing things like woman+king-man =queenmodel.most_similar(positive=['woman','king'],negative=['man'],topn=1)queen: 0.508\nProbability of a text under the modelmodel.score(['The fox jumped over the lazy dog'.split()])0.21\n\nBelow is one interesting visualisation of word2vec.\n\n\nThe above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.\n\n\n5. It can be used to perform Machine Translation.\nThe above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.\nUsing Pre-trained Word Vectors\nWe are going to use google\u2019s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.\nfrom gensim.models import Word2Vec\n#loading the downloaded modelmodel = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n#the model is loaded. It can be used to perform all of the tasks mentioned above.\n# getting word vectors of a worddog = model['dog']\n#performing king queen magicprint(model.most_similar(positive=['woman', 'king'], negative=['man']))\n#picking odd one outprint(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n#printing similarity indexprint(model.similarity('woman', 'man'))\nTraining Your Own Word Vectors\nWe will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.\nword2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I won\u2019t be covering the pre-preprocessing part here. So let\u2019s take an example list of list to train our word2vec model.\nsentence=[[\u2018Neeraj\u2019,\u2019Boy\u2019],[\u2018Sarwan\u2019,\u2019is\u2019],[\u2018good\u2019,\u2019boy\u2019]]\n#training word2vec on 3 sentencesmodel = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\nLet us try to understand the parameters of this model.\nsentence \u2013 list of list of our corpusmin_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.size=300 \u2013 the number of dimensions in which we wish to represent our word. This is the size of the word vector.workers=4 \u2013 used for parallelization\n#using the model\n#The new trained model can be used similar to the pre-trained ones.\n#printing similarity indexprint(model.similarity('woman', 'man'))\nPractice Projects\nNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:\nPractice Problem: Identify the SentimentsIdentify the sentiment of tweetsPractice Problem : Twitter Sentiment AnalysisTo detect hate speech in tweets\nConclusion\nWord Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.\nFrequently Asked Questions\nQ1. What are word embeddings? A. Word embeddings are texts converted into numbers in order to feed the data into machine learning models.  Q2. What are the types of word embeddings? A. Word embeddings are broadly classified into 2: frequency-based embedding and prediction-based embedding.  Q3. What are the types of vectors under frequency-based embedding? A. Frequency-based embedding deals with 3 types of vectors: count vector, TF-IDF vector, and co-occurrence vector.  \nNote: We also have a video course on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!\n\nRelated\n ", "A vector representation of a word\u00a0may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of \u201cnumbers\u201d\u00a0in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].", "This is just a very simple method to represent a word in the vector form. Let us look at different types of Word Embeddings or Word Vectors and their advantages and disadvantages over the rest.", "The different types of word embeddings can be broadly classified into two categories-", "Let us try to understand each of these methods in detail.", "There are generally three types of vectors that we encounter under this category.", "Let us look into each of these\u00a0vectorization methods in detail.", "Consider a Corpus C of D documents {d1,d2\u2026..dD} and N unique tokens extracted out of the corpus C. The N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. Each row in the matrix M contains the frequency of tokens in document D(i).", "Let us understand this using a simple example.", "D1: He is a lazy boy. She is also lazy.", "D2: Neeraj is a lazy person.", "The dictionary created may be a list of unique tokens(words) in the corpus =[\u2018He\u2019,\u2019She\u2019,\u2019lazy\u2019,\u2019boy\u2019,\u2019Neeraj\u2019,\u2019person\u2019]\nHere, D=2, N=6\nThe count matrix M of size 2 X 6 will be represented as \u2013\n\u00a0HeShelazyboyNeerajpersonD1112100D2001011\nNow, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for \u2018lazy\u2019 in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as \u2013 D2 contains \u2018lazy\u2019: once, \u2018Neeraj\u2019: once and \u2018person\u2019 once.\nNow there may be quite a few variations while preparing the above matrix M. The variations will be generally in-\n\nThe way dictionary is prepared.Why? Because in real world applications we might have a corpus which contains millions of documents. And with millions of document, we can extract hundreds of millions of unique words. So basically, the matrix that will be prepared like above will be a very sparse one and inefficient for any computation. So an alternative to using every unique word as a dictionary element would be to pick say top 10,000 words based on frequency and then prepare a dictionary.\nThe way count is taken for each word.We may either take the frequency (number of times a word has appeared in the document) or the presence(has the word appeared in the document?) to be the entry in the count matrix M. But generally, frequency method is preferred over the latter.\n\nBelow is a representational image of the matrix M for easy understanding.\n\n\nTF-IDF Vectorization\nThis is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.\nCommon words like \u2018is\u2019, \u2018the\u2019, \u2018a\u2019 etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word \u201cMessi\u201d in comparison to other documents. But common words like \u201cthe\u201d etc. are also going to be present in higher frequency in almost every document.\nIdeally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.\nTF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.\nSo, how exactly does TF-IDF work?\nConsider the below sample table which gives the count of terms(tokens/words) in two documents.\n\n\nNow, let us define a few terms related to TF-IDF.\nTF = (Number of times term t appears in a document)/(Number of terms in the document)\nSo, TF(This,Document1) = 1/8\nTF(This, Document2)=1/5\nIt denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word \u2018Messi\u2019 in large number.\nIDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\nwhere N is the number of documents and n is the number of documents a term t has appeared in.\nSo, IDF(This) = log(2/2) = 0.\nSo, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\nLet us compute IDF for the word \u2018Messi\u2019.\nIDF(Messi) = log(2/1) = 0.301.\nNow, let us compare the TF-IDF for a common word \u2018This\u2019 and a word \u2018Messi\u2019 which seems to be of relevance to Document 1.\nTF-IDF(This,Document1) = (1/8) * (0) = 0\nTF-IDF(This, Document2) = (1/5) * (0) = 0\nTF-IDF(Messi, Document1) =\u00a0(4/8)*0.301 = 0.15\nAs, you can see for Document1 , TF-IDF method heavily penalises the word \u2018This\u2019 but assigns greater weight to \u2018Messi\u2019. So, this may be understood as \u2018Messi\u2019 is an important word for Document1 from the context of the entire corpus.\nCo-Occurrence Matrix With a Fixed Context Window\nThe big idea \u2013 Similar words tend to occur together and will have similar context for example \u2013 Apple is a fruit. Mango is a fruit.Apple and mango tend to have a similar context i.e fruit.\nBefore I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified \u2013 Co-Occurrence and Context Window.\nCo-occurrence \u2013 For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.\nContext Window \u2013 Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,\nQuickBrownFoxJumpOverTheLazyDog\nThe green words are a 2 (around) context window for the word \u2018Fox\u2019 and for calculating the co-occurrence only these words will be counted. Let us see context window for the word \u2018Over\u2019.\nQuickBrownFoxJumpOverTheLazyDog\nNow, let us take an example corpus to calculate a co-occurrence matrix.\nCorpus = He is not lazy. He is intelligent. He is smart.\n\u00a0HeisnotlazyintelligentsmartHe042121is401221not210100lazy121000intelligent220000smart110000\nLet us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.\nRed box- It is the number of times \u2018He\u2019 and \u2018is\u2019 have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.\nHeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HeisnotlazyHeisintelligentHeissmart\nWhile the word \u2018lazy\u2019 has never appeared with \u2018intelligent\u2019 in the context window and therefore has been assigned 0 in the blue box.\nVariations of Co-occurrence Matrix\nLet\u2019s say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-\n\nA co-occurrence matrix of size V X V. Now, for even a decent corpus V gets very large and difficult to handle. So generally, this architecture is never preferred in practice.\nA co-occurrence matrix of size V X N where N is a subset of V and can be obtained by removing irrelevant words like stopwords etc. for example. This is still very large and presents computational difficulties.\n\nBut, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.\nLet me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.\nAnd, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.\nSo, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.\n\n\nAdvantages of Co-occurrence Matrix\n\nIt preserves the semantic relationship between words. i.e man and woman tend to be closer than man and apple.\nIt uses SVD at its core, which produces more accurate word vector representations than existing methods.\nIt uses factorization which is a well-defined problem and can be efficiently solved.\nIt has to be computed once and can be used anytime once computed. In this sense, it is faster in comparison to others.\n\nDisadvantages of Co-Occurrence Matrix\n\nIt requires huge memory to store the co-occurrence matrix.But, this problem can be circumvented by factorizing the matrix out of the system for example in Hadoop clusters etc. and can be saved.\n\nPrediction-based Embedding\nPre-requisite: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through this awesome article by Sunil to gain a very good understanding of how NN works.\nSo far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.\nWord2vec is not a single algorithm but a combination of two techniques \u2013 CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working.\nCBOW (Continuous Bag of words)\nThe way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.\nSuppose, we have a corpus C = \u201cHey, this is sample corpus using only one context word.\u201d and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.\n\n\nThe target for a single datapoint say Datapoint 4 is shown as below\nHeythisissamplecorpususingonlyonecontextword0001000000\nThis matrix shown in the above image is sent into a shallow neural network with three layers: an\u00a0input layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.\nLet us first see a diagrammatic representation of the CBOW model.\n\n\nThe matrix representation of the above image for a single data point is below.\n\n\nThe flow is as follows:\n\nThe input layer and the target, both are one- hot encoded of size [1 X V]. Here V=10 in the above example.\nThere are two sets of weights. one is between the input and the hidden layer and second between hidden and output layer.Input-Hidden layer matrix size =[V X N] , hidden-Output layer matrix \u00a0size =[N X V] : Where N is the number of dimensions we choose to represent our word in. It is arbitary\u00a0and a hyper-parameter for a Neural Network. Also, N is the number of neurons in the hidden layer. Here, N=4.\nThere is a no activation function between any layers.( More specifically, I am referring to linear activation)\nThe input is multiplied by the input-hidden weights and called hidden activation. It is simply the corresponding row in the input-hidden matrix copied.\nThe hidden input gets multiplied by hidden- output weights and output is calculated.\nError between output and target is calculated and propagated back to re-adjust the weights.\nThe weight \u00a0between the hidden layer and the output layer is taken as the word vector representation of the word.\n\nWe saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.\n\n\nBelow is a matrix representation of the above architecture for an easy understanding.\n\n\nThe image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.\nSo, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.\nThe steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.\nIn both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article \u2013 Understanding and Coding Neural Networks from scratch.\nThe differences between MLP and CBOW are \u00a0mentioned below for clarification:\n\nThe objective function in MLP is a MSE(mean square error) whereas in CBOW it is negative log likelihood of a word given a set of context i.e -log(p(wo/wi)), where p(wo/wi) is given as\n\n\n\nwo : output wordwi: context words\n2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has \u00a0sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP.\nAdvantages of CBOW:\n\nBeing probabilistic is nature, it is supposed to perform superior to deterministic methods(generally).\nIt is low on memory. It does not need to have huge RAM requirements like that of co-occurrence matrix where it needs to store three huge matrices.\n\nDisadvantages of CBOW:\n\nCBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.\nTraining a CBOW from scratch can take forever if not properly optimized.\n\nSkip \u2013 Gram model\nSkip \u2013 gram follows the same topology as of CBOW. It just flips CBOW\u2019s architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=\u201dHey, this is sample corpus using only one context word.\u201d Let us construct the training data.\n\n\nThe input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be \u201ctwo\u201d one hot encoded target variables and \u201ctwo\u201d corresponding outputs as can be seen by the blue section in the image.\nTwo separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.\nThe weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.\nThe skip-gram architecture is shown below.\n\n\nFor a better understanding, matrix style structure with calculation has been shown below.\n\n\nLet us break down the above image.\nInput layer \u00a0size \u2013 [1 X V], Input hidden weight matrix size \u2013 [V X N], Number of neurons in hidden layer \u2013 N, Hidden-Output weight matrix size \u2013 [N X V], Output layer size \u2013 C [1 X V]\nIn the above example, C is the number of context words=2, V= 10, N=4\n\nThe row in red is the hidden activation corresponding to the input one-hot encoded vector. It is basically the corresponding row of input-hidden matrix copied.\nThe yellow matrix is the weight between the hidden layer and the output layer.\nThe blue matrix is obtained by the matrix multiplication of hidden activation and the hidden output weights. There will be two rows calculated for two target(context) words.\nEach row of the blue matrix is converted into its softmax probabilities individually as shown in the green box.\nThe grey matrix contains the one hot encoded vectors of the two context words(target).\nError is calculated by substracting the first row of the grey matrix(target) from the first row of the green matrix(output) element-wise. This is repeated for the next row. Therefore, for n\u00a0target context words, we will have n error vectors.\nElement-wise sum is taken over all the error vectors to obtain a final error vector.\nThis error vector is propagated back to update the weights.\n\nAdvantages of Skip-Gram Model\n\nSkip-gram model can capture two semantics for a single word. i.e it will have two vector representations of Apple. One for the company and other for the fruit.\nSkip-gram with negative sub-sampling outperforms every other method generally.\n\nThis\u00a0is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.\nWord Embeddings Use Cases\nSince word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-\n\nFinding the degree of similarity between two words.model.similarity('woman','man')0.73723527\nFinding odd one out.model.doesnt_match('breakfast cereal dinner lunch';.split())'cereal'\nAmazing things like woman+king-man =queenmodel.most_similar(positive=['woman','king'],negative=['man'],topn=1)queen: 0.508\nProbability of a text under the modelmodel.score(['The fox jumped over the lazy dog'.split()])0.21\n\nBelow is one interesting visualisation of word2vec.\n\n\nThe above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.\n\n\n5. It can be used to perform Machine Translation.\nThe above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.\nUsing Pre-trained Word Vectors\nWe are going to use google\u2019s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.\nfrom gensim.models import Word2Vec\n#loading the downloaded modelmodel = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n#the model is loaded. It can be used to perform all of the tasks mentioned above.\n# getting word vectors of a worddog = model['dog']\n#performing king queen magicprint(model.most_similar(positive=['woman', 'king'], negative=['man']))\n#picking odd one outprint(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n#printing similarity indexprint(model.similarity('woman', 'man'))\nTraining Your Own Word Vectors\nWe will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.\nword2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I won\u2019t be covering the pre-preprocessing part here. So let\u2019s take an example list of list to train our word2vec model.\nsentence=[[\u2018Neeraj\u2019,\u2019Boy\u2019],[\u2018Sarwan\u2019,\u2019is\u2019],[\u2018good\u2019,\u2019boy\u2019]]\n#training word2vec on 3 sentencesmodel = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\nLet us try to understand the parameters of this model.\nsentence \u2013 list of list of our corpusmin_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.size=300 \u2013 the number of dimensions in which we wish to represent our word. This is the size of the word vector.workers=4 \u2013 used for parallelization\n#using the model\n#The new trained model can be used similar to the pre-trained ones.\n#printing similarity indexprint(model.similarity('woman', 'man'))\nPractice Projects\nNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:\nPractice Problem: Identify the SentimentsIdentify the sentiment of tweetsPractice Problem : Twitter Sentiment AnalysisTo detect hate speech in tweets\nConclusion\nWord Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.\nFrequently Asked Questions\nQ1. What are word embeddings? A. Word embeddings are texts converted into numbers in order to feed the data into machine learning models.  Q2. What are the types of word embeddings? A. Word embeddings are broadly classified into 2: frequency-based embedding and prediction-based embedding.  Q3. What are the types of vectors under frequency-based embedding? A. Frequency-based embedding deals with 3 types of vectors: count vector, TF-IDF vector, and co-occurrence vector.  \nNote: We also have a video course on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!\n\nRelated\n ", "The count matrix M of size 2 X 6 will be represented as \u2013", "Now, a column can also be understood as word vector for the corresponding word in the matrix M. For example, the word vector for \u2018lazy\u2019 in the above matrix is [2,1] and so on.Here, the rows correspond to the documents in the corpus and the columns correspond to the tokens in the dictionary. The second row in the above matrix may be read as \u2013 D2 contains \u2018lazy\u2019: once, \u2018Neeraj\u2019: once and \u2018person\u2019 once.", "Now there may be quite a few variations while preparing the above matrix M. The variations will be generally in-", "Below is a representational image of the matrix M for easy understanding.", "This is another method which is based on the frequency method but it is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. So, what is the rationale behind this? Let us try to understand.", "Common words like \u2018is\u2019, \u2018the\u2019, \u2018a\u2019 etc. tend to appear quite frequently in comparison to the words which are important to a document. For example, a document A on Lionel Messi is going to contain more occurences of the word \u201cMessi\u201d in comparison to other documents. But common words like \u201cthe\u201d etc. are also going to be present in higher frequency in almost every document.", "Ideally, what we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents.", "TF-IDF works by penalising these common words by assigning them lower weights while giving importance to words like Messi in a particular document.", "So, how exactly does TF-IDF work?", "Consider the below sample table which gives the count of terms(tokens/words) in two documents.", "Now, let us define a few terms related to TF-IDF.", "TF = (Number of times term t appears in a document)/(Number of terms in the document)", "It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. eg: A document about Messi should contain the word \u2018Messi\u2019 in large number.", "IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.", "where N is the number of documents and n is the number of documents a term t has appeared in.", "So, how do we explain the reasoning behind IDF? Ideally, if a word has appeared in all the document, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.", "Let us compute IDF for the word \u2018Messi\u2019.", "Now, let us compare the TF-IDF for a common word \u2018This\u2019 and a word \u2018Messi\u2019 which seems to be of relevance to Document 1.", "As, you can see for Document1 , TF-IDF method heavily penalises the word \u2018This\u2019 but assigns greater weight to \u2018Messi\u2019. So, this may be understood as \u2018Messi\u2019 is an important word for Document1 from the context of the entire corpus.", "Co-Occurrence Matrix With a Fixed Context Window", "The big idea \u2013 Similar words tend to occur together and will have similar context for example \u2013 Apple is a fruit. Mango is a fruit.Apple and mango tend to have a similar context i.e fruit.", "Before I dive into the details of how a co-occurrence matrix is constructed, there are two concepts that need to be clarified \u2013 Co-Occurrence and Context Window.", "Co-occurrence \u2013 For a given corpus, the co-occurrence of a pair of words say w1 and w2 is the number of times they have appeared together in a Context Window.", "Context Window \u2013 Context window is specified by a number and the direction. So what does a context window of 2 (around) means? Let us see an example below,", "The green words are a 2 (around) context window for the word \u2018Fox\u2019 and for calculating the co-occurrence only these words will be counted. Let us see context window for the word \u2018Over\u2019.", "Now, let us take an example corpus to calculate a co-occurrence matrix.", "Corpus = He is not lazy. He is intelligent. He is smart.", "Let us understand this co-occurrence matrix by seeing two examples in the table above. Red and the blue box.", "Red box- It is the number of times \u2018He\u2019 and \u2018is\u2019 have appeared in the context window 2 and it can be seen that the count turns out to be 4. The below table will help you visualise the count.", "While the word \u2018lazy\u2019 has never appeared with \u2018intelligent\u2019 in the context window and therefore has been assigned 0 in the blue box.", "Let\u2019s say there are V unique words in the corpus. So Vocabulary size = V. The columns of the Co-occurrence matrix form the context words. The different variations of Co-Occurrence Matrix are-", "But, remember this co-occurrence matrix is not the word vector representation that is generally used. Instead, this Co-occurrence matrix is decomposed using techniques like PCA, SVD etc. into factors and combination of these factors forms the word vector representation.", "Let me illustrate this more clearly. For example, you perform PCA on the above matrix of size VXV. You will obtain V principal components. You can choose k components out of these V components. So, the new matrix will be of the form V X k.", "And, a single word, instead of being represented in V dimensions will be represented in k dimensions while still capturing almost the same semantic meaning. k is generally of the order of hundreds.", "So, what PCA does at the back is decompose Co-Occurrence matrix into three matrices, U,S and V where U and V are both orthogonal matrices. What is of importance is that dot product of U and S gives the word vector representation and V gives the word context representation.", "Pre-requisite: This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated. If you are new to Neural Network, I would suggest you go through this awesome article by Sunil to gain a very good understanding of how NN works.", "So far, we have seen deterministic methods to determine word vectors. But these methods proved to be limited in their word representations until Mitolov etc. el introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.", "Word2vec is not a single algorithm but a combination of two techniques \u2013 CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations. Let us discuss both these methods separately and gain intuition into their working.", "The way CBOW work is that it tends to predict the probability of a word given a context. A context may be a single word or a group of words. But for simplicity, I will take a single context word and try to predict a single target word.", "Suppose, we have a corpus C = \u201cHey, this is sample corpus using only one context word.\u201d and we have defined a context window of 1. This corpus may be converted into a training set for a CBOW model as follow. The input is shown below. The matrix on the right in the below image contains the one-hot encoded from of the input on the left.", "The target for a single datapoint say Datapoint 4 is shown as below", "This matrix shown in the above image is sent into a shallow neural network with three layers: an\u00a0input layer, a hidden layer and an output layer. The output layer is a softmax layer which is used to sum the probabilities obtained in the output layer to 1. Now let us see how the forward propagation will work to calculate the hidden layer activation.", "Let us first see a diagrammatic representation of the CBOW model.", "The matrix representation of the above image for a single data point is below.", "We saw the above steps for a single context word. Now, what about if we have multiple context words? The image below describes the architecture for multiple context words.", "Below is a matrix representation of the above architecture for an easy understanding.", "The image above takes 3 context words and predicts the probability of a target word. The input can be assumed as taking three one-hot encoded vectors in the input layer as shown above in red, blue and green.", "So, the input layer will have 3 [1 X V] Vectors in the input as shown above and 1 [1 X V] in the output layer. Rest of the architecture is same as for a 1-context CBOW.", "The steps remain the same, only the calculation of hidden activation changes. Instead of just copying the corresponding rows of the input-hidden weight matrix to the hidden layer, an average is taken over all the corresponding rows of the matrix. We can understand this with the above figure. The average vector calculated becomes the hidden activation. So, if we have three context words for a single target word, we will have three initial hidden activations which are then averaged element-wise to obtain the final activation.", "In both a single context word and multiple context word, I have shown the images till the calculation of the hidden activations since this is the part where CBOW differs from a simple MLP network. The steps after the calculation of hidden layer are same as that of the MLP as mentioned in this article \u2013 Understanding and Coding Neural Networks from scratch.", "The differences between MLP and CBOW are \u00a0mentioned below for clarification:", "wo : output wordwi: context words", "2. The gradient of error with respect to hidden-output weights and input-hidden weights are different since MLP has \u00a0sigmoid activations(generally) but CBOW has linear activations. The method however to calculate the gradient is same as an MLP.", "Skip \u2013 gram follows the same topology as of CBOW. It just flips CBOW\u2019s architecture on its head. The aim of skip-gram is to predict the context given a word. Let us take the same corpus that we built our CBOW model on. C=\u201dHey, this is sample corpus using only one context word.\u201d Let us construct the training data.", "The input vector for skip-gram is going to be similar to a 1-context CBOW model. Also, the calculations up to hidden layer activations are going to be the same. The difference will be in the target variable. Since we have defined a context window of 1 on both the sides, there will be \u201ctwo\u201d one hot encoded target variables and \u201ctwo\u201d corresponding outputs as can be seen by the blue section in the image.", "Two separate errors are calculated with respect to the two target variables and the two error vectors obtained are added element-wise to obtain a final error vector which is propagated back to update the weights.", "The weights between the input and the hidden layer are taken as the word vector representation after training. The loss function or the objective is of the same type as of the CBOW model.", "The skip-gram architecture is shown below.", "For a better understanding, matrix style structure with calculation has been shown below.", "Let us break down the above image.", "Input layer \u00a0size \u2013 [1 X V], Input hidden weight matrix size \u2013 [V X N], Number of neurons in hidden layer \u2013 N, Hidden-Output weight matrix size \u2013 [N X V], Output layer size \u2013 C [1 X V]\nIn the above example, C is the number of context words=2, V= 10, N=4\n\nThe row in red is the hidden activation corresponding to the input one-hot encoded vector. It is basically the corresponding row of input-hidden matrix copied.\nThe yellow matrix is the weight between the hidden layer and the output layer.\nThe blue matrix is obtained by the matrix multiplication of hidden activation and the hidden output weights. There will be two rows calculated for two target(context) words.\nEach row of the blue matrix is converted into its softmax probabilities individually as shown in the green box.\nThe grey matrix contains the one hot encoded vectors of the two context words(target).\nError is calculated by substracting the first row of the grey matrix(target) from the first row of the green matrix(output) element-wise. This is repeated for the next row. Therefore, for n\u00a0target context words, we will have n error vectors.\nElement-wise sum is taken over all the error vectors to obtain a final error vector.\nThis error vector is propagated back to update the weights.\n\nAdvantages of Skip-Gram Model\n\nSkip-gram model can capture two semantics for a single word. i.e it will have two vector representations of Apple. One for the company and other for the fruit.\nSkip-gram with negative sub-sampling outperforms every other method generally.\n\nThis\u00a0is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.\nWord Embeddings Use Cases\nSince word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-\n\nFinding the degree of similarity between two words.model.similarity('woman','man')0.73723527\nFinding odd one out.model.doesnt_match('breakfast cereal dinner lunch';.split())'cereal'\nAmazing things like woman+king-man =queenmodel.most_similar(positive=['woman','king'],negative=['man'],topn=1)queen: 0.508\nProbability of a text under the modelmodel.score(['The fox jumped over the lazy dog'.split()])0.21\n\nBelow is one interesting visualisation of word2vec.\n\n\nThe above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.\n\n\n5. It can be used to perform Machine Translation.\nThe above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.\nUsing Pre-trained Word Vectors\nWe are going to use google\u2019s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.\nfrom gensim.models import Word2Vec\n#loading the downloaded modelmodel = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)\n#the model is loaded. It can be used to perform all of the tasks mentioned above.\n# getting word vectors of a worddog = model['dog']\n#performing king queen magicprint(model.most_similar(positive=['woman', 'king'], negative=['man']))\n#picking odd one outprint(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n#printing similarity indexprint(model.similarity('woman', 'man'))\nTraining Your Own Word Vectors\nWe will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.\nword2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I won\u2019t be covering the pre-preprocessing part here. So let\u2019s take an example list of list to train our word2vec model.\nsentence=[[\u2018Neeraj\u2019,\u2019Boy\u2019],[\u2018Sarwan\u2019,\u2019is\u2019],[\u2018good\u2019,\u2019boy\u2019]]\n#training word2vec on 3 sentencesmodel = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\nLet us try to understand the parameters of this model.\nsentence \u2013 list of list of our corpusmin_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.size=300 \u2013 the number of dimensions in which we wish to represent our word. This is the size of the word vector.workers=4 \u2013 used for parallelization\n#using the model\n#The new trained model can be used similar to the pre-trained ones.\n#printing similarity indexprint(model.similarity('woman', 'man'))\nPractice Projects\nNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:\nPractice Problem: Identify the SentimentsIdentify the sentiment of tweetsPractice Problem : Twitter Sentiment AnalysisTo detect hate speech in tweets\nConclusion\nWord Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.\nFrequently Asked Questions\nQ1. What are word embeddings? A. Word embeddings are texts converted into numbers in order to feed the data into machine learning models.  Q2. What are the types of word embeddings? A. Word embeddings are broadly classified into 2: frequency-based embedding and prediction-based embedding.  Q3. What are the types of vectors under frequency-based embedding? A. Frequency-based embedding deals with 3 types of vectors: count vector, TF-IDF vector, and co-occurrence vector.  \nNote: We also have a video course on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!\n\nRelated\n ", "In the above example, C is the number of context words=2, V= 10, N=4", "This\u00a0is an excellent interactive tool to visualise CBOW and skip gram in action. I would suggest you to really go through this link for a better understanding.", "Since word embeddings or word Vectors are numerical representations of contextual similarities between words, they can be manipulated and made to perform amazing tasks like-", "Below is one interesting visualisation of word2vec.", "The above image is a t-SNE representation of word vectors in 2 dimension and you can see that two contexts of apple have been captured. One is a fruit and the other company.", "5. It can be used to perform Machine Translation.", "The above graph is a bilingual embedding with chinese in green and english in yellow. If we know the words having similar meanings in chinese and english, the above bilingual embedding can be used to translate one language into the other.", "We are going to use google\u2019s pre-trained model. It contains word vectors for a vocabulary of 3 million words trained on around 100 billion words from the google news dataset. The downlaod link for the model is this. Beware it is a 1.5 GB download.", "#the model is loaded. It can be used to perform all of the tasks mentioned above.", "# getting word vectors of a worddog = model['dog']", "We will be training our own word2vec on a custom corpus. For training the model we will be using gensim and the steps are illustrated as below.", "word2Vec requires that a format of list of list for training where every document is contained in a list and every list contains list of tokens of that documents. I won\u2019t be covering the pre-preprocessing part here. So let\u2019s take an example list of list to train our word2vec model.", "sentence=[[\u2018Neeraj\u2019,\u2019Boy\u2019],[\u2018Sarwan\u2019,\u2019is\u2019],[\u2018good\u2019,\u2019boy\u2019]]\n#training word2vec on 3 sentencesmodel = gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)\nLet us try to understand the parameters of this model.\nsentence \u2013 list of list of our corpusmin_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.size=300 \u2013 the number of dimensions in which we wish to represent our word. This is the size of the word vector.workers=4 \u2013 used for parallelization\n#using the model\n#The new trained model can be used similar to the pre-trained ones.\n#printing similarity indexprint(model.similarity('woman', 'man'))\nPractice Projects\nNow, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:\nPractice Problem: Identify the SentimentsIdentify the sentiment of tweetsPractice Problem : Twitter Sentiment AnalysisTo detect hate speech in tweets\nConclusion\nWord Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.\nFrequently Asked Questions\nQ1. What are word embeddings? A. Word embeddings are texts converted into numbers in order to feed the data into machine learning models.  Q2. What are the types of word embeddings? A. Word embeddings are broadly classified into 2: frequency-based embedding and prediction-based embedding.  Q3. What are the types of vectors under frequency-based embedding? A. Frequency-based embedding deals with 3 types of vectors: count vector, TF-IDF vector, and co-occurrence vector.  \nNote: We also have a video course on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!\n\nRelated\n ", "Let us try to understand the parameters of this model.", "sentence \u2013 list of list of our corpusmin_count=1 -the threshold value for the words. Word with frequency greater than this only are going to be included into the model.size=300 \u2013 the number of dimensions in which we wish to represent our word. This is the size of the word vector.workers=4 \u2013 used for parallelization", "#using the model\n#The new trained model can be used similar to the pre-trained ones.", "Now, its time to take the plunge and actually play with some other real datasets. So are you ready to take on the challenge? Accelerate your NLP journey with the following Practice Problems:", "Word Embeddings is an active research area trying to figure out better word representations than the existing ones. But, with time they have grown large in number and more complex. This article was aimed at simplying some of the workings of these embedding models without carrying the mathematical overhead. If you feel think that I was able to clear some of your confusion, comment below. Any changes or suggestions would be welcomed.", "A. Word embeddings are texts converted into numbers in order to feed the data into machine learning models.", "A. Word embeddings are broadly classified into 2: frequency-based embedding and prediction-based embedding.", "A. Frequency-based embedding deals with 3 types of vectors: count vector, TF-IDF vector, and co-occurrence vector.", "Note: We also have a video course on Natural Language Processing covering many NLP topics including bag of words, TF-IDF, and word embeddings. Do check it out!", "I am a perpetual, quick learner and keen to explore the realm of Data analytics and science. I am deeply excited about the times we live in and the rate at which data is being generated and being transformed as an asset. I am well versed with a few tools for dealing with data and also in the process of learning some other tools and knowledge required to exploit data.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F06%2Fword-embeddings-count-word2veec%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=An Intuitive Understanding of  Word Embeddings: From Count Vectors to Word2Vec&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F06%2Fword-embeddings-count-word2veec%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2017%2F06%2Fword-embeddings-count-word2veec%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/nss/", "anchor_text": "NSS"}, {"url": "https://www.analyticsvidhya.com/blog/category/advanced/", "anchor_text": "Advanced"}, {"url": "https://www.analyticsvidhya.com/blog/category/algorithm/", "anchor_text": "Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/python-2/", "anchor_text": "Python"}, {"url": "https://www.analyticsvidhya.com/blog/category/technique/", "anchor_text": "Technique"}, {"url": "https://www.analyticsvidhya.com/blog/category/text/", "anchor_text": "Text"}, {"url": "https://www.analyticsvidhya.com/blog/category/unstructured-data/", "anchor_text": "Unstructured Data"}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=av_blog&utm_medium=practice_blog_word_embeddings", "anchor_text": "Practice Now"}, {"url": "https://courses.analyticsvidhya.com/bundles/certified-natural-language-processing-master-s-program?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "Certified Natual Language Processing Master\u2019s Program"}, {"url": "https://courses.analyticsvidhya.com/bundles/certified-ai-ml-blackbelt-plus?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "Certified AI & ML Blackbelt+ Program"}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "this awesome article"}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/", "anchor_text": "Understanding and Coding Neural Networks from scratch"}, {"url": "http://bit.ly/wevi-online", "anchor_text": "This"}, {"url": "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit", "anchor_text": "this"}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": ""}, {"url": "https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem: Identify the Sentiments"}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": ""}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=word-embeddings-count-word2veec&utm_medium=blog", "anchor_text": "Practice Problem : Twitter Sentiment Analysis"}, {"url": "https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&utm_medium=word-embeddings-count-word2veec", "anchor_text": "video course"}, {"url": "https://www.analyticsvidhya.com/blog/tag/artificial-neural-network/", "anchor_text": "Artificial Neural Network"}, {"url": "https://www.analyticsvidhya.com/blog/tag/deep-learning/", "anchor_text": "deep learning"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning/", "anchor_text": "machine learning"}, {"url": "https://www.analyticsvidhya.com/blog/tag/natural-language-processing/", "anchor_text": "Natural language processing"}, {"url": "https://www.analyticsvidhya.com/blog/tag/nlp/", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/tag/text-processing/", "anchor_text": "Text Processing"}, {"url": "https://www.analyticsvidhya.com/blog/tag/word2vec/", "anchor_text": "Word2Vec"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=ReadingList&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "What is NLP?"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/top-10-applications-of-natural-language-processing-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Applications of NLP"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Understanding Text Pre-processing"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Tokenization in NLP"}, {"url": "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Byte Pair Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2022/09/tokenizer-free-language-modeling-with-pixels/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Tokenizer Free Language Modeling with Pixels"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/how-to-remove-stopwords-text-normalization-nltk-spacy-gensim-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Stopword Removal"}, {"url": "https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Stemming vs Lemmatization"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/how-to-build-word-cloud-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Text Mining"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Spacy Tutorials"}, {"url": "https://www.analyticsvidhya.com/blog/2022/02/topic-identification-with-gensim-library-using-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Gensim Tutorials"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/regex-cheatsheet-for-natural-language-processing-tasks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "What are Regular Expressions?"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/4-applications-of-regular-expressions-that-every-data-scientist-should-know-with-python-code/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Regular Expressions"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/fuzzy-string-matching-a-hands-on-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "String Similarity"}, {"url": "https://www.analyticsvidhya.com/blog/2021/11/autocorrect-feature-using-nlp-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Spelling Correction"}, {"url": "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to Topic Modeling"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/part-2-topic-modeling-and-latent-dirichlet-allocation-lda-using-gensim-and-sklearn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Latent Dirichlet Allocation (LDA)"}, {"url": "https://www.analyticsvidhya.com/blog/2022/08/supervised-topic-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement Topic Modeling"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/a-guide-to-feature-engineering-in-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to Feature Engineering for Text Data"}, {"url": "https://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement Text Feature Engineering Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/one-hot-encoding-vs-label-encoding-using-scikit-learn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to One Hot Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/how-to-perform-one-hot-encoding-for-multi-categorical-variables/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement One Hot Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Limitations of One Hot Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/part-5-step-by-step-guide-to-master-nlp-text-vectorization-approaches/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Count Vectorizer and TF-IDF"}, {"url": "https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Solving Text classification using TF-IDF"}, {"url": "https://www.analyticsvidhya.com/blog/2020/06/nlp-project-information-extraction/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Information Retrieval System Explained in Simple terms!"}, {"url": "https://www.analyticsvidhya.com/blog/2015/04/pagerank-explained-simple/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "How does Google Rank Search Results?"}, {"url": "https://www.analyticsvidhya.com/blog/2019/10/how-to-build-knowledge-graph-text-using-spacy/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Knowledge Graph"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Understanding Word2Vec"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Understanding Skip Gram and Continous Bag Of Words"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/practical-guide-to-word-embedding-system/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Word2Vec Implementation in Gensim"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Visualizing Word2Vec"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/word-sense-disambiguation-importance-in-natural-language-processing/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Word Senses and Word Sense Ambiguity"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/part-11-step-by-step-guide-to-master-nlp-syntactic-analysis/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Why Are We Interested in Syntatic Strucure?"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "What is a Dependency Grammar?"}, {"url": "https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Neural Dependency Parsing"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to Language Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "N-Gram Language Models"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Neural Language models"}, {"url": "https://www.analyticsvidhya.com/blog/2019/01/sequence-models-deeplearning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Why Sequence models?"}, {"url": "https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Usecases of Sequence models"}, {"url": "https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to RNN"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/tutorial-on-rnn-lstm-gru-with-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement RNN"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/a-visual-guide-to-recurrent-neural-networks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Shortcomings of RNN"}, {"url": "https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "What is Long Short Term Memory (LSTM)"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/lstm-for-text-classification/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implementing LSTM"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/detecting-fake-news-with-natural-language-processing/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Build Your Own Fake News Classification Model"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/introduction-to-gated-recurrent-unit-gru/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "What is Gated Recurrent Unit (GRU)?"}, {"url": "https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implementing GRU"}, {"url": "https://www.analyticsvidhya.com/blog/2019/01/neural-machine-translation-keras/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introduction to Machine Translation"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/3-important-nlp-libraries-indian-languages-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Multilingualism in NLP"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Drawbacks of Seq2Seq model"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Mathematical Calculation of Attention"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/an-explanatory-guide-to-bert-tokenizer/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Understand Positional Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Introducing Transformers Model"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/implementation-of-attention-mechanism-for-caption-generation-on-transformers-using-tensorflow/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Key Query Value Attention in Tranformer Encoder"}, {"url": "https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Pretrained Language Models in NLP"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/building-a-machine-learning-model-for-title-generation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Generative Pre-training (GPT) for Natural Language Understanding(NLU)"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Finetuning GPT-2"}, {"url": "https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Understanding BERT"}, {"url": "https://www.analyticsvidhya.com/blog/2021/12/fine-tune-bert-model-for-sentiment-analysis-in-google-colab/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Finetune Masked language Modeling in BERT"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement Text Classification using BERT"}, {"url": "https://www.analyticsvidhya.com/blog/2022/06/fine-tune-bert-model-for-named-entity-recognition-in-google-colab/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Finetuning BERT for NER"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/albert-model-for-self-supervised-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Extensions of BERT: Roberta, Spanbert, ALBER"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/mobilebert/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "MobileBERT"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/hands-on-experience-with-gpt3/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "GPT-3"}, {"url": "https://www.analyticsvidhya.com/blog/2022/05/prompt-engineering-in-gpt-3/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Prompt Engineering in GPT-3"}, {"url": "https://www.analyticsvidhya.com/blog/2022/11/an-introduction-to-bigbird/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Bigbird"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/6-pretrained-models-text-classification/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "T5 and large language models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/11/end-to-end-question-answering-system-using-nlp-and-squad-dataset/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement Question Answering on SQUAD"}, {"url": "https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Text Summarization"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "A Beginner\u00e2\u20ac\u2122s Introduction to NER (Named Entity Recognition)"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/new-anaphora-and-co-reference-resolution-technique-for-biographies/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Coreference Resolution"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/visualizing-sounds-librosa/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Visualizing Sounds Using Librosa Machine Learning Library!"}, {"url": "https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Audio Processing"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/analysis-of-zero-crossing-rates-of-different-music-genre-tracks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Audio Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/how-to-perform-automatic-music-generation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Audio Classification using Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/introduction-to-automatic-speech-recognition-and-natural-language-processing/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Automatic Speech Recognition"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/learn-build-first-speech-to-text-model-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Implement Automatic Speech Recognition"}, {"url": "https://www.analyticsvidhya.com/blog/2022/09/can-voice-conversion-improve-asr-in-low-resource-settings/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Underlying Engineering Behind Alexa\u00e2\u20ac\u2122s Contextual ASR"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/speech-separation-by-facebook-ai-research/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Audio Separation"}, {"url": "https://www.analyticsvidhya.com/blog/2021/12/creating-chatbot-building-using-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Building Chatbots"}, {"url": "https://www.analyticsvidhya.com/blog/2019/04/learn-build-chatbot-rasa-nlp-ipl/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Building Chatbots using Rasa"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/automate-nlp-tasks-using-evalml-library/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Automate NLP Tasks using EvalML Library"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/roadmap-to-master-nlp-in-2022/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Roadmap to Master NLP in 2022"}, {"url": "https://www.analyticsvidhya.com/blog/2022/05/a-complete-guide-on-feature-extraction-techniques/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "A Complete Guide on Feature Extraction Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/feature-extraction-and-embeddings-in-nlp-a-beginners-guide-to-understand-natural-language-processing/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Feature Extraction and Embeddings in NLP: A Beginners guide to understand Natural Language Processing"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "An Essential Guide to Pretrained Word Embeddings for NLP Practitioners"}, {"url": "https://www.analyticsvidhya.com/blog/2022/06/an-end-to-end-guide-on-nlp-pipeline/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "An End to End Guide on NLP Pipeline"}, {"url": "https://www.analyticsvidhya.com/blog/2022/03/learn-basics-of-natural-language-processing-nlp-using-gensim-part-1/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/", "anchor_text": "Learn Basics of Natural Language Processing (NLP) using Gensim: Part 1"}, {"url": "https://www.analyticsvidhya.com/blog/author/nss/", "anchor_text": "NSS"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/data-science-evangelist-gurgaon-2-to-3-years-of-experience/", "anchor_text": "Data Science Evangelist- Gurgaon (2 to 3 years of experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/senior-data-analyst-chennai-2-4-years-of-experience/", "anchor_text": "Senior Data Analyst-Chennai (2-4 Years of Experience)"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}