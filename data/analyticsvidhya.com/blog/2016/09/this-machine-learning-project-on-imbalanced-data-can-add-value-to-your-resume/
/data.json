{"url": "https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/\n", "time": 1683020767.6486201, "path": "analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/\n/", "webpage": {"metadata": {"title": "This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume", "h1": "This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume", "description": "This is a machine learning project for freshers and starters in data science. This would help them build a better resume and get hired"}, "outgoing_paragraph_urls": [{"url": "http://discuss.analyticsvidhya.com", "anchor_text": "community support", "paragraph_index": 0}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/", "anchor_text": "read this article", "paragraph_index": 8}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/", "anchor_text": "here", "paragraph_index": 12}, {"url": "http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/", "anchor_text": "Data set information", "paragraph_index": 20}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/09/train.zip", "anchor_text": "Download Train Data", "paragraph_index": 22}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/09/test.zip", "anchor_text": "Download Test Data", "paragraph_index": 22}, {"url": "http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names", "anchor_text": "data set page", "paragraph_index": 28}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/", "anchor_text": "recommended article", "paragraph_index": 37}, {"url": "https://www.analyticsvidhya.com/jobs/", "anchor_text": "currently hiring jobs", "paragraph_index": 37}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/", "anchor_text": "recommended article", "paragraph_index": 60}], "all_paragraphs": ["It takes sheer courage and hard work to become a successful self-taught data scientist or to make a mid career transition. But, with growing\u00a0community support, more and more people are now encouraged to make these bold career move. Do you dream to build a career in data science? Trust me, you are not alone!", "Like it or not, a bitter truth about data science industry is that self-learning / certification / coursework is not sufficient to get you a job. People with little or no experience in real data science projects usually get filtered out in early stages by recruiters. This leaves\u00a0self taught learners or people transitioning in a difficult place \u2013 \u201cYou need some experience on your CV to get your first data science job!\u201d \u2013 How do you solve for it?", "If you are facing this challenge, don\u2019t let it stop you. Don\u2019t lose hope. Here is the way out \u2013\u00a0there are several open data set repositories.\u00a0With them\u00a0to our disposal, we can\u00a0use them tactically to build projects and make our resumes worthier. After all, recruiters are looking for proof of your knowledge.", "In this article, I\u2019ll get you started with this process. I\u2019ll tell you how to use open data sets to create meaningful projects and improve your knowledge in this process. I\u2019ve created this ML project (in R), which you can showcase on your resume. But,\u00a0make sure that you work & develop the\u00a0project from where I leave. Having a resume which stands out by doing such projects, will increase your chances of getting hired magnificently.", "Note:\u00a0This ML project will be most helpful to people (using R) who are actively looking for their first or next job in data science. You must have knowledge of ML algorithms. If you are already an experienced (>3 years) data scientist,\u00a0you might have already worked on this / similar projects.", "The data used in this project is imbalanced. In real life, some extremely\u00a0critical situations result in imbalanced data sets. For example \u2013 fraud detection, cancer detection, manufacturing defects, online ads conversion etc. Thus, having prior experience of working on such data might rule the situation in your favor (worth a try!).", "Furthermore, some characteristics of this project which makes it a worthy project include:", "If not job, this project will give you enough\u00a0confidence & knowledge that you can build more ML projects on your own. Once you have completed this project, put in on your GitHub repo and showcase it to the world. Leave the link in the comments below to showcase your motivation \u2013 you never know when a recruiter drops by!", "Note: If you are new to imbalanced classification problems. I recommend you to read this article.", "Given various features, the aim is to build a predictive model to determine the income level for people in US. The income levels are binned at below 50K and above 50K.", "From the problem statement, it\u2019s evident that\u00a0this is a binary classification problem.", "Generating hypothesis is the most crucial step in building models. Yet, most analysts tend to overlook this step. In simple words, this technique enlightens our way by indicating which direction (set of variables) to choose.", "This step should be practiced before looking at the data. This is done to think broadly and not be constrained by what is available. In this step, we\u2019ll create a laundry list of factors which we think could influence the prediction metrics. Read more about hypothesis generation here.", "Let\u2019s think of some hypothesis which\u00a0can influence the outcome. Here is a set of hypothesis to get you started:", "H\u00f2 : There is no significant impact of the variables (below) on the dependent variable.", "Ha : There exists a significant impact of the variables (below) on the dependent variable.", "Remind you, this is not an exhaustive list. I\u2019d suggest you not to limit your thoughts with the ones above, your aim should be to make your project as comprehensive & presentable as possible.", "Also, every time you think of an hypothesis, try and think of what would be the relationship like and why would that hypothesis stand true. For example, when I say Education \u2013 what I really mean is this: \u201cI think that with higher education, people would have higher chances of better employment and hence their income would have higher chances of being more than 50K.\u201d", "Similarly, spend some time thinking about your set of hypothesis, how would they be impacting income and hence what is the best way to capture the mathematical relationship.", "P.S. Do this before you move forward in the article", "For this project, we\u2019ve taken the data set from UCI Machine Learning Repository: Data set information.\u00a0The first step is to look at the data and identify which of our hypothesis are available in the data.", "If you\u2019ve used this data repository in past, you would know that downloading and modeling data isn\u2019t as easy as it might look. If you download the data from link given above, you\u2019d find that column headers are missing. Therefore, for your convenience, I\u2019ve provided the link for workable version of test and train data:", "Download Train Data\nDownload Test Data", "As mentioned above, we\u2019ll use R for this project. Now, we\u2019ll load the data into R and look at it closely.", "We see that train data has 199523 rows & 41 columns. Test data has 99762 rows and 41 columns. Generally, test data comes with one less column than train. It means that this data set has test prediction values also. This will help us in evaluating our model.", "The denominations of these target levels aren\u2019t same. This disparity will cause trouble in model evaluation. Being a binary classification problem, we can encode these variables as 0 and 1.", "Let\u2019s look at the severity of imbalanced classes in our data:", "We see that the majority class has a proportion of\u00a094%. In other words, with a decent ML algorithm, our model would get 94% model accuracy. In absolute figures, it looks incredible. But, our performance would depend on, how good can we predict the minority classes. More on this in coming sections!", "As seen in str() above, the columns in the both data set aren\u2019t as per column classes given on data set page. Let\u2019s update the column classes accordingly. data.table package offers fast and simple way to make changes in multiple columns at once.", "Now, let\u2019s separate categorical variables & numerical variables. This will help us in further analysis.", "Removing train and test files would allow us to use our memory for other computational purposes which was earlier held up by these data sets.", "Let\u2019s begin with numerical data now. The best way to understand these variables is using Histogram.", "#load libraries\n> library(ggplot2)\n> library(plotly)", "For ease of understanding, we\u2019ve created a histogram overlapped with density curve. This curve will helps us decipher the distribution pattern more clearly. ggplotly() package will make our resultant plots interactive, thereby saving us\u00a0lot of time. Let\u2019s look at some variables:", "As we can see, the data set consists of\u00a0people aged from 0 to 90 with frequency of people declining with age. Now, if we think of the problem we are trying to solve, do you think population below age 20 could earn >50K under normal circumstances? I don\u2019t think so. Therefore, we can bin this variable into age groups.", "This is a nasty right skewed graph. In skewed distribution, normalizing is always an option. But, we need to look into this variable deeper as this insight isn\u2019t significant enough for decision making. One option could be, to check for unique values. If they are less, we can tabulate the distribution (done in upcoming sections).", "Furthermore, in classification problems, we should also plot numerical variables with dependent variable. This would help us determine the clusters (if exists) of classes 0 and 1. For this, we need to add the target variable in num_train data:", "#add target variable\n> num_train[,income_level := cat_train$income_level]\n#create a scatter plot\n> ggplot(data=num_train,aes(x = age, y=wage_per_hour))+geom_point(aes(colour=income_level))+scale_y_continuous(\"wage per hour\", breaks = seq(0,10000,1000))\n\nAs we can see, most of the people having income_level 1, seem to fall in the age of 25-65 earning wage of $1000 to $4000 per hour. This plot further strengthens our assumption that age < 20 would have income_level 0, hence we will\u00a0bin this variable.\nIdentifying hidden trends is easier said than done. We need to look at a variable(s) from different angles to spot the hidden trends.\u00a0Don\u2019t stop here. I suggest you to plot all variables and understand their distribution. This would give us enough idea for doing\u00a0feature engineering.\nSimilarly, we can visualize our categorical variables as well. For categories, rather than a bland bar chart, a dodged bar chart provides more information. In dodged bar chart, we plot the categorical variables & dependent variable adjacent to each other.\n#dodged bar chart\n> all_bar <- function(i){\n\u00a0ggplot(cat_train,aes(x=i,fill=income_level))+geom_bar(position = \"dodge\", \u00a0color=\"black\")+scale_fill_brewer(palette = \"Pastel1\")+theme(axis.text.x =element_text(angle \u00a0= 60,hjust = 1,size=10))\n}\n#variable class_of_worker\n> all_bar(cat_train$class_of_worker)\n\nThough, no specific information is provided about Not in universe category.\u00a0Let\u2019s assume that, this response is given by people who got frustrated (due to any reason) while filling their census data. This variable looks imbalanced i.e. only two category levels seem to dominate. In such situation, a good practice is to combine levels having less than 5% frequency of the total category frequency.\n#variable education\n> all_bar(cat_train$education)\n\nEvidently, all children have income_level\u00a00. Also, we can infer than Bachelors degree\u00a0holders have the largest proportion of people have income_level\u00a01.\u00a0Similarly, you can plot other categorical variables also.\nAlternative way of checking categories is using 2 way tables. Yes, you can create proportionate tables to check the effect of dependent variable per categories as shown:\n> prop.table(table(cat_train$marital_status,cat_train$income_level),1)\n> prop.table(table(cat_train$class_of_worker,cat_train$income_level),1)\n\u00a0\n3. Data Cleaning\nLet\u2019s check for missing values in numeric variables.\n#check missing values in numerical data\n> table(is.na(num_train))\n> table(is.na(num_test))\nWe see that numeric variables has no missing values. Good for us! While working on numeric variables, a good practice is to\u00a0check for correlation in numeric variables. caret package offers a convenient way to filter out variables with high correlation. Let\u2019s see:\n> library(caret)\n#set threshold as 0.7\n> ax <-findCorrelation(x = cor(num_train), cutoff = 0.7)\n> num_train <- num_train[,-ax,with=FALSE]\u00a0\n> num_test[,weeks_worked_in_year := NULL]\nThe variable weeks_worked_in_year\u00a0gets removed. For hygiene purpose, we\u2019ve removed that variable from test data too. It\u2019s not necessary though!\nNow, let\u2019s check for missing values in categorical data. We\u2019ll use base sapply() to find out percentage of missing values per column.\n#check missing values per columns\n > mvtr <- sapply(cat_train, function(x){sum(is.na(x))/length(x)})*100\n > mvte <- sapply(cat_test, function(x){sum(is.na(x)/length(x))}*100)\n > mvtr\n > mvte\nWe find that\u00a0some of the variables have ~50% missing values. High proportion of missing value can be attributed to difficulty in data collection. For now, we\u2019ll remove these category levels. A simple subset() function does the trick.\n#select columns with missing value less than 5%\n> cat_train <- subset(cat_train, select = mvtr < 5 )\n> cat_test <- subset(cat_test, select = mvte < 5)\nFor the rest of missing values, a nicer approach would be to label them as \u2018Unavailable\u2019. Imputing missing values on large data sets can be painstaking. data.table\u2019s set() function makes this computation insanely fast.\n#set NA as Unavailable - train data\n#convert to characters\n> cat_train <- cat_train[,names(cat_train) := lapply(.SD, as.character),.SDcols = names(cat_train)]\n> for (i in seq_along(cat_train)) set(cat_train, i=which(is.na(cat_train[[i]])), j=i, value=\"Unavailable\")\n#convert back to factors\n> cat_train <- cat_train[, names(cat_train) := lapply(.SD,factor), .SDcols = names(cat_train)]\n#set NA as Unavailable - test data\n> cat_test <- cat_test[, (names(cat_test)) := lapply(.SD, as.character), .SDcols = names(cat_test)]\n> for (i in seq_along(cat_test)) set(cat_test, i=which(is.na(cat_test[[i]])), j=i, value=\"Unavailable\")\n#convert back to factors\n> cat_test <- cat_test[, (names(cat_test)) := lapply(.SD, factor), .SDcols = names(cat_test)]\n\u00a0\n4. Data Manipulation\nWe are approaching towards machine learning stage. But, machine learning algorithms return better accuracy\u00a0when the data set has clear signals to offer. Specially, in case of imbalanced classification, we should try our best to shape the data such that we can derive\u00a0maximum information\u00a0about minority class.\nIn previous analysis, we saw that categorical variables have several levels with low frequencies. Such levels don\u2019t help as chances are they wouldn\u2019t be available in test set. We\u2019ll do this hygiene check anyways, in coming steps. To combine levels, a simple for loop does the trick. After combining, the new category level will named as \u2018Other\u2019.\n#combine factor levels with less than 5% values\n#train\n> for(i in names(cat_train)){\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p <- 5/100\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ld <- names(which(prop.table(table(cat_train[[i]])) < p))\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 levels(cat_train[[i]])[levels(cat_train[[i]]) %in% ld] <- \"Other\"\n}\n#test\n> for(i in names(cat_test)){\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 p <- 5/100\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ld <- names(which(prop.table(table(cat_test[[i]])) < p))\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 levels(cat_test[[i]])[levels(cat_test[[i]]) %in% ld] <- \"Other\"\n}\nTime for hygiene check. Let\u2019s check if there exists a mismatch between categorical levels in train and test data. Either you can write a function for accomplish this. We\u2019ll rather use a hack derived from mlr package.\n#check columns with unequal levels \nlibrary(mlr)\n> summarizeColumns(cat_train)[,\"nlevs\"]\n> summarizeColumns(cat_test)[,\"nlevs\"]\nThe parameter \u201cnlevs\u201d returns the unique number of level from the given set of variables.\nBefore proceeding to the modeling stage, let\u2019s look at numeric variables and reflect on possible ways for binning. Since a histogram wasn\u2019t enough for us to make decision, let\u2019s create simple tables representing counts of unique values in these variables as shown:\n> num_train[,.N,age][order(age)]\n>\u00a0num_train[,.N,wage_per_hour][order(-N)]\nSimilarly, you should check other variables also. After this activity, we are clear that more than 70-80% of the observations are 0 in these variables. Let\u2019s bin these variables accordingly. I used a decision tree to determine the range of resultant bins. However, it will be interested to see how 0-25, 26-65, 66-90 works (discerned from plots above). \u00a0You should try it sometime later!\n#bin age variable 0-30 31-60 61 - 90\n> num_train[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c(\"young\",\"adult\",\"old\"))]\n> num_train[,age := factor(age)]\n> num_test[,age:= cut(x = age,breaks = c(0,30,60,90),include.lowest = TRUE,labels = c(\"young\",\"adult\",\"old\"))]\n> num_test[,age := factor(age)]\n#Bin numeric variables with Zero and MoreThanZero\n> num_train[,wage_per_hour := ifelse(wage_per_hour == 0,\"Zero\",\"MoreThanZero\")][,wage_per_hour := as.factor(wage_per_hour)]\n> num_train[,capital_gains := ifelse(capital_gains == 0,\"Zero\",\"MoreThanZero\")][,capital_gains := as.factor(capital_gains)]\n> num_train[,capital_losses := ifelse(capital_losses == 0,\"Zero\",\"MoreThanZero\")][,capital_losses := as.factor(capital_losses)]\n> num_train[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0,\"Zero\",\"MoreThanZero\")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]\n> num_test[,wage_per_hour := ifelse(wage_per_hour == 0,\"Zero\",\"MoreThanZero\")][,wage_per_hour := as.factor(wage_per_hour)]\n> num_test[,capital_gains := ifelse(capital_gains == 0,\"Zero\",\"MoreThanZero\")][,capital_gains := as.factor(capital_gains)]\n> num_test[,capital_losses := ifelse(capital_losses == 0,\"Zero\",\"MoreThanZero\")][,capital_losses := as.factor(capital_losses)]\n> num_test[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0,\"Zero\",\"MoreThanZero\")][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]\nNow, we can\u00a0remove the dependent variable from num_train, we added for\u00a0visualization purpose earlier.\n> num_train[,income_level := NULL]\n\u00a0\n5. Machine Learning\nMaking predictions on this data should atleast give us ~94% accuracy. However, while working on imbalanced problems, accuracy is considered to be a poor evaluation metrics because:\n\nAccuracy is calculated by ratio of correct classifications / incorrect classifications.\nThis metric would largely tell us how accurate our predictions are on the majority class (since it comprises 94% of values). But, we need to know if we are predicting minority class correctly. We\u2019re doomed here.\n\nIn such situations, we should use elements of confusion matrix.\u00a0\nFollowing are the metrics we\u2019ll use to evaluate our predictive accuracy:\n\nSensitivity = True Positive Rate (TP/TP+FN) \u2013 It says, \u2018out of all the positive (majority class) values, how many have been predicted correctly\u2019.\nSpecificity = True Negative Rate (TN/TN +FP) \u2013 It says, \u2018out of all the negative (minority class) values, how many have been predicted correctly\u2019.\nPrecision = (TP/TP+FP)\nRecall = Sensitivity\nF score = 2 * (Precision * Recall)/ (Precision + Recall) \u2013 It is the harmonic mean of precision and recall. It is used to compare several models side-by-side. Higher the better.\n\nIn quest of better accuracy, we\u2019ll use various techniques used on imbalanced classification. For modeling purpose, we\u2019ll use the fantastic mlr package, which I use(over) these days. I hope you\u2019ve read the recommended article\u00a0mentioned above because if you are unfamiliar with these techniques, chances are you would lose the way moving forward.\n#combine data and make test & train files\n> d_train <- cbind(num_train,cat_train)\n> d_test <- cbind(num_test,cat_test)\n#remove unwanted files\n> rm(num_train,num_test,cat_train,cat_test) #save memory\n#load library for machine learning\n> library(mlr)\n#create task\n> train.task <- makeClassifTask(data = d_train,target = \"income_level\")\n> test.task <- makeClassifTask(data=d_test,target = \"income_level\")\n#remove zero variance features\n> train.task <- removeConstantFeatures(train.task)\n> test.task <- removeConstantFeatures(test.task)\n#get variable importance chart\n> var_imp <- generateFilterValuesData(train.task, method = c(\"information.gain\"))\n> plotFilterValues(var_imp,feat.type.cols = TRUE)\n\nIn simple words, you can understand that the variable major_occupation_code would provide highest information to the model followed by other variables in descending order. This chart is deduced using a tree algorithm, where at every split, the information is calculated using reduction in entropy (homogeneity). Let\u2019s keep this knowledge safe, we might use it in coming steps.\nNow, we\u2019ll try to make our data balanced using various techniques such as over sampling, undersampling and SMOTE. In SMOTE, the algorithm looks at n nearest neighbors, measures the distance between them and introduces a new observation at the center of n observations. While proceeding, we must keep in mind that these techniques have their own drawbacks such as:\n\nundersampling leads to loss of information\noversampling leads to overestimation of minority class\n\nBeing your first project(hopefully), we should try all techniques and experience how it affects.\n#undersampling\u00a0\n> train.under <- undersample(train.task,rate = 0.1) #keep only 10% of majority class\n> table(getTaskTargets(train.under))\n#oversampling\n> train.over <- oversample(train.task,rate=15) #make minority class 15 times\n> table(getTaskTargets(train.over))\n#SMOTE\n> train.smote <- smote(train.task,rate = 15,nn = 5)\nLooks like, my machine gave up at these SMOTE parameters. It\u2019s been over 50 minutes and this code hasn\u2019t executed. Look at the havoc this is creating in my poor machine:\n\nWhile working on such data sets, it\u2019s important for you to learn the ways to hop such obstacles. Let\u2019s modify the parameters and run it again:\n> system.time(\n\u00a0 \u00a0 train.smote <- smote(train.task,rate = 10,nn = 3)\u00a0\n\u00a0 \u00a0)\nWarning messages:\n# 1: In is.factor(x) :\n# Reached total allocation of 8084Mb: see help(memory.size)\n# 2: In is.factor(x) :\n# Reached total allocation of 8084Mb: see help(memory.size)\n# user system elapsed \n# 81.95 21.86 184.56\n> table(getTaskTargets(train.smote))\nIt did run with some warning messages. We can ignore them for now. \u00a0Let\u2019s now look at the available algorithms we can use to solve this problem.\n#lets see which algorithms\u00a0are available\n> listLearners(\"classif\",\"twoclass\")[c(\"class\",\"package\")]\nWe\u2019ll start with naive Bayes, an algorithms based on bayes theorem.\u00a0In case of high dimensional data like text-mining, naive Bayes tends to do wonders in accuracy. It works on categorical data. In case of numeric variables,\u00a0a normal distribution is considered for these variables and a mean and standard deviation\u00a0is calculated. Then, using some standard z-table calculations probabilities can be estimated for each of your continuous variables to make the naive Bayes classifier.\nWe\u2019ll use naive Bayes on all 4 data sets (imbalanced, oversample, undersample and SMOTE) and compare the prediction accuracy using cross validation.\n#naive Bayes\n> naive_learner <- makeLearner(\"classif.naiveBayes\",predict.type = \"response\")\n> naive_learner$par.vals <- list(laplace = 1)\n#10fold CV - stratified\n> folds <- makeResampleDesc(\"CV\",iters=10,stratify = TRUE)\n#cross validation function\n> fun_cv <- function(a){\n\u00a0 \u00a0 \u00a0crv_val <- resample(naive_learner,a,folds,measures = list(acc,tpr,tnr,fpr,fp,fn))\n\u00a0 \u00a0 \u00a0crv_val$aggr\n}\n> fun_cv (train.task) \n# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean \n# 0.7337249 \u00a0 \u00a0 \u00a0 0.8954134 \u00a0 \u00a0 0.7230270 \u00a0 \u00a00.2769730\n> fun_cv(train.under) \n# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean \n# 0.7637315 \u00a0 \u00a0 \u00a00.9126978 \u00a0 \u00a0 0.6651696 \u00a0 \u00a0 0.3348304\n> fun_cv(train.over)\n# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean \n# \u00a0 0.7861459 \u00a0 \u00a0 0.9145749 \u00a0 \u00a0 0.6586852 \u00a0 \u00a00.3413148\n> fun_cv(train.smote)\n# acc.test.mean tpr.test.mean tnr.test.mean fpr.test.mean \n# \u00a0 0.8562135 \u00a0 \u00a0 0.9168955 \u00a0 \u00a00.8160638 \u00a0 \u00a0 0.1839362\nThis package names cross validated results are test.mean. After comparing, we see that train.smote gives the highest true positive rate and true negative rate. Hence, we learn that SMOTE technique outperforms the other two sampling methods.\nNow, let\u2019s build our model SMOTE data and check our final prediction accuracy.\n#train and predict\n> nB_model <- train(naive_learner, train.smote)\n> nB_predict <- predict(nB_model,test.task)\n#evaluate\n> nB_prediction <- nB_predict$data$response\n> dCM <- confusionMatrix(d_test$income_level,nB_prediction)\n# Accuracy : 0.8174\n# Sensitivity : 0.9862\n# Specificity : 0.2299 \n#calculate F measure\n> precision <- dCM$byClass['Pos Pred Value']\n> recall <- dCM$byClass['Sensitivity']\n> f_measure <- 2*((precision*recall)/(precision+recall))\n> f_measure\u00a0\nThe function confusionMatrix is taken from library(caret). This naive Bayes model predicts 98% of the majority class correctly, but disappoints at\u00a0minority class prediction (~23%). Let us not get hopeless\u00a0and try more techniques to improve our accuracy. Remember, the more you hustle, better you get!\nLet\u2019s use xgboost algorithm and try to improve our model. We\u2019ll do 5 fold cross validation and 5 round random search for parameter tuning. Finally, we\u2019ll build the model using the best tuned parameters.\n#xgboost\n>\u00a0set.seed(2002)\n> xgb_learner <- makeLearner(\"classif.xgboost\",predict.type = \"response\")\n> xgb_learner$par.vals <- list(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective = \"binary:logistic\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 eval_metric = \"error\",\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 nrounds = 150,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 print.every.n = 50\n)\n#define hyperparameters for tuning\n> xg_ps <- makeParamSet(\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeIntegerParam(\"max_depth\",lower=3,upper=10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeNumericParam(\"lambda\",lower=0.05,upper=0.5),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeNumericParam(\"eta\", lower = 0.01, upper = 0.5),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeNumericParam(\"subsample\", lower = 0.50, upper = 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeNumericParam(\"min_child_weight\",lower=2,upper=10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeNumericParam(\"colsample_bytree\",lower = 0.50,upper = 0.80)\n)\n#define search function\n> rancontrol <- makeTuneControlRandom(maxit = 5L) #do 5 iterations\n\n#5 fold cross validation\n> set_cv <- makeResampleDesc(\"CV\",iters = 5L,stratify = TRUE)\n\n#tune parameters\n> xgb_tune <- tuneParams(learner = xgb_learner, task = train.task, resampling = set_cv, measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = xg_ps, control = rancontrol)\n# Tune result:\n# Op. pars: max_depth=3; lambda=0.221; eta=0.161; subsample=0.698; min_child_weight=7.67; colsample_bytree=0.642\n# acc.test.mean=0.948,tpr.test.mean=0.989,tnr.test.mean=0.324,fpr.test.mean=0.676\nNow, we can use these parameter for modeling using xgb_tune$x\u00a0which contains the best tuned parameters.\n#set optimal parameters\n> xgb_new <- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)\n#train model\n> xgmodel <- train(xgb_new, train.task)\n#test model\n> predict.xg <- predict(xgmodel, test.task)\n#make prediction\n> xg_prediction <- predict.xg$data$response\n#make confusion matrix\n> xg_confused <- confusionMatrix(d_test$income_level,xg_prediction)\nAccuracy : 0.948\nSensitivity : 0.9574\nSpecificity : 0.6585\n\n> precision <- xg_confused$byClass['Pos Pred Value']\n> recall <- xg_confused$byClass['Sensitivity']\n> f_measure <- 2*((precision*recall)/(precision+recall))\n> f_measure\n#0.9726374\u00a0\nAs we can see, xgboost has outperformed naive Bayes model\u2019s accuracy (as expected!). Can we further improve ?\nUntil now, we\u2019ve used all the variables in the data. Shall we try using the important ones? Consider it your homework. Let me provide you hint to do this:\n#top 20 features\n> filtered.data <- filterFeatures(train.task,method = \"information.gain\",abs = 20)\n#train\n> xgb_boost <- train(xgb_new,filtered.data)\nAfter this, follow the same steps as above for predictions and evaluation. Tell me\u00a0your understanding in comments below.\nUntil now, our model has been making label predictions. The threshold used for making these predictions in 0.5 as seen by:\n> predict.xg$threshold\n[[1]] 0.5\nDue to imbalanced nature of the data, the threshold of 0.5 will always favor the majority class since the probability of a class 1 is quite low. Now, we\u2019ll try a new technique:\n\nInstead of labels, we\u2019ll predict probabilities\nPlot and study the AUC curve\nAdjust the threshold for better prediction\n\nWe\u2019ll continue using xgboost for this stunt. To do this, we need to change the predict.type parameter while defining learner.\n#xgboost AUC \n> xgb_prob <- setPredictType(learner = xgb_new,predict.type = \"prob\")\n#train model\n> xgmodel_prob <- train(xgb_prob,train.task)\n#predict\n> predict.xgprob <- predict(xgmodel_prob,test.task)\nNow, let\u2019s look at the probability table thus created:\n#predicted probabilities\n> predict.xgprob$data[1:10,]\nSince, we have obtained the class probabilities, let\u2019s create an AUC curve and determine the basis to modify prediction threshold.\n> df <- generateThreshVsPerfData(predict.xgprob,measures = list(fpr,tpr))\n> plotROCCurves(df)\n\nAUC is a measure of true positive rate and false positive rate. We aim to reach as close to top left corner as possible. Therefore, we should aim to reduce the threshold so that the false positive rate can be reduced.\n#set threshold as 0.4\n> pred2 <- setThreshold(predict.xgprob,0.4)\n> confusionMatrix(d_test$income_level,pred2$data$response)\n# Sensitivity : 0.9512 \n# Specificity : 0.7228\nWith 0.4 threshold, our model returned better predictions than our previous xgboost model at 0.5 threshold. Thus, you can see that setting threshold using AUC curve actually affect our model performance. Let\u2019s give one more try.\n> pred3 <- setThreshold(predict.xgprob,0.30)\n> confusionMatrix(d_test$income_level,pred3$data$response)\n#Accuracy : 0.944 \n# Sensitivity : 0.9458 \n# Specificity : 0.7771\nThis model has outperformed all our models i.e. in other words, this is the best model because 77% of the minority classes have been predicted correctly.\nSimilarly, you can try and test other threshold values to check if your model improves. In this xgboost model, there is a lot you can do such as:\n\nIncrease the number of rounds\nDo 10 fold CV\nIncrease repetitions in random search\nBuild models on other 3 data sets and see which one is better\n\nApart from the methods listed above, you can also assign class weights such that the algorithm pays more attention while classifying the class with higher weight. I leave this part as homework to you. Run the code below and update me if you model surpassed our previous xgboost prediction. Use SVM in homework. An important tip: The code below might take longer than expected to run, therefore close all other applications.\n#use SVM\n> getParamSet(\"classif.svm\")\n> svm_learner <- makeLearner(\"classif.svm\",predict.type = \"response\")\n> svm_learner$par.vals<- list(class.weights = c(\"0\"=1,\"1\"=10),kernel=\"radial\")\n> svm_param <- makeParamSet(\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeIntegerParam(\"cost\",lower = 10^-1,upper = 10^2),\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 makeIntegerParam(\"gamma\",lower= 0.5,upper = 2)\n)\n#random search\n> set_search <- makeTuneControlRandom(maxit = 5L) #5 times\n#cross validation #10L seem to take forever\n> set_cv <- makeResampleDesc(\"CV\",iters=5L,stratify = TRUE)\n#tune Params\n> svm_tune <- tuneParams(learner = svm_learner,task = train.task,measures = list(acc,tpr,tnr,fpr,fp,fn), par.set = svm_param,control = set_search,resampling = set_cv)\n)\n#set hyperparameters\n>svm_new <- setHyperPars(learner = svm_learner, par.vals = svm_tune$x)\n#train model\n>svm_model <- train(svm_new,train.task)\n#test model\n>predict_svm <- predict(svm_model,test.task)\n> confusionMatrix(d_test$income_level,predict_svm$data$response)\n\u00a0\nEnd Notes\nI hope this project will helped you understand the importance of data exploration, visualization, manipulation in machine learning. To showcase this project on your resume, create your github account, upload this project along with important findings. I have not deliberately provided my code so that you write it at your end.\nMake sure you understand this project and develop on it further. Recruiter can figure out who\u2019s honest and not. And trust me, such projects will different your profile from other candidates.\nIf you want me to create\u00a0and share more such machine learning projects, write your suggestion on problem types below. I\u2019d love work on something which you think is challenging and would like to conquer\u00a0it.\nLooking for a job in analytics? Check out currently hiring jobs\u00a0in machine learning and data science\n\nRelated\n ", "As we can see, most of the people having income_level 1, seem to fall in the age of 25-65 earning wage of $1000 to $4000 per hour. This plot further strengthens our assumption that age < 20 would have income_level 0, hence we will\u00a0bin this variable.", "Identifying hidden trends is easier said than done. We need to look at a variable(s) from different angles to spot the hidden trends.\u00a0Don\u2019t stop here. I suggest you to plot all variables and understand their distribution. This would give us enough idea for doing\u00a0feature engineering.", "Similarly, we can visualize our categorical variables as well. For categories, rather than a bland bar chart, a dodged bar chart provides more information. In dodged bar chart, we plot the categorical variables & dependent variable adjacent to each other.", "Though, no specific information is provided about Not in universe category.\u00a0Let\u2019s assume that, this response is given by people who got frustrated (due to any reason) while filling their census data. This variable looks imbalanced i.e. only two category levels seem to dominate. In such situation, a good practice is to combine levels having less than 5% frequency of the total category frequency.", "Evidently, all children have income_level\u00a00. Also, we can infer than Bachelors degree\u00a0holders have the largest proportion of people have income_level\u00a01.\u00a0Similarly, you can plot other categorical variables also.", "Alternative way of checking categories is using 2 way tables. Yes, you can create proportionate tables to check the effect of dependent variable per categories as shown:", "Let\u2019s check for missing values in numeric variables.", "We see that numeric variables has no missing values. Good for us! While working on numeric variables, a good practice is to\u00a0check for correlation in numeric variables. caret package offers a convenient way to filter out variables with high correlation. Let\u2019s see:", "The variable weeks_worked_in_year\u00a0gets removed. For hygiene purpose, we\u2019ve removed that variable from test data too. It\u2019s not necessary though!", "Now, let\u2019s check for missing values in categorical data. We\u2019ll use base sapply() to find out percentage of missing values per column.", "We find that\u00a0some of the variables have ~50% missing values. High proportion of missing value can be attributed to difficulty in data collection. For now, we\u2019ll remove these category levels. A simple subset() function does the trick.", "For the rest of missing values, a nicer approach would be to label them as \u2018Unavailable\u2019. Imputing missing values on large data sets can be painstaking. data.table\u2019s set() function makes this computation insanely fast.", "We are approaching towards machine learning stage. But, machine learning algorithms return better accuracy\u00a0when the data set has clear signals to offer. Specially, in case of imbalanced classification, we should try our best to shape the data such that we can derive\u00a0maximum information\u00a0about minority class.", "In previous analysis, we saw that categorical variables have several levels with low frequencies. Such levels don\u2019t help as chances are they wouldn\u2019t be available in test set. We\u2019ll do this hygiene check anyways, in coming steps. To combine levels, a simple for loop does the trick. After combining, the new category level will named as \u2018Other\u2019.", "Time for hygiene check. Let\u2019s check if there exists a mismatch between categorical levels in train and test data. Either you can write a function for accomplish this. We\u2019ll rather use a hack derived from mlr package.", "The parameter \u201cnlevs\u201d returns the unique number of level from the given set of variables.", "Before proceeding to the modeling stage, let\u2019s look at numeric variables and reflect on possible ways for binning. Since a histogram wasn\u2019t enough for us to make decision, let\u2019s create simple tables representing counts of unique values in these variables as shown:", "Similarly, you should check other variables also. After this activity, we are clear that more than 70-80% of the observations are 0 in these variables. Let\u2019s bin these variables accordingly. I used a decision tree to determine the range of resultant bins. However, it will be interested to see how 0-25, 26-65, 66-90 works (discerned from plots above). \u00a0You should try it sometime later!", "Now, we can\u00a0remove the dependent variable from num_train, we added for\u00a0visualization purpose earlier.", "Making predictions on this data should atleast give us ~94% accuracy. However, while working on imbalanced problems, accuracy is considered to be a poor evaluation metrics because:", "In such situations, we should use elements of confusion matrix.\u00a0", "Following are the metrics we\u2019ll use to evaluate our predictive accuracy:", "In quest of better accuracy, we\u2019ll use various techniques used on imbalanced classification. For modeling purpose, we\u2019ll use the fantastic mlr package, which I use(over) these days. I hope you\u2019ve read the recommended article\u00a0mentioned above because if you are unfamiliar with these techniques, chances are you would lose the way moving forward.", "#load library for machine learning\n> library(mlr)", "#remove zero variance features\n> train.task <- removeConstantFeatures(train.task)\n> test.task <- removeConstantFeatures(test.task)", "#get variable importance chart\n> var_imp <- generateFilterValuesData(train.task, method = c(\"information.gain\"))\n> plotFilterValues(var_imp,feat.type.cols = TRUE)", "In simple words, you can understand that the variable major_occupation_code would provide highest information to the model followed by other variables in descending order. This chart is deduced using a tree algorithm, where at every split, the information is calculated using reduction in entropy (homogeneity). Let\u2019s keep this knowledge safe, we might use it in coming steps.", "Now, we\u2019ll try to make our data balanced using various techniques such as over sampling, undersampling and SMOTE. In SMOTE, the algorithm looks at n nearest neighbors, measures the distance between them and introduces a new observation at the center of n observations. While proceeding, we must keep in mind that these techniques have their own drawbacks such as:", "Being your first project(hopefully), we should try all techniques and experience how it affects.", "Looks like, my machine gave up at these SMOTE parameters. It\u2019s been over 50 minutes and this code hasn\u2019t executed. Look at the havoc this is creating in my poor machine:", "While working on such data sets, it\u2019s important for you to learn the ways to hop such obstacles. Let\u2019s modify the parameters and run it again:", "It did run with some warning messages. We can ignore them for now. \u00a0Let\u2019s now look at the available algorithms we can use to solve this problem.", "We\u2019ll start with naive Bayes, an algorithms based on bayes theorem.\u00a0In case of high dimensional data like text-mining, naive Bayes tends to do wonders in accuracy. It works on categorical data. In case of numeric variables,\u00a0a normal distribution is considered for these variables and a mean and standard deviation\u00a0is calculated. Then, using some standard z-table calculations probabilities can be estimated for each of your continuous variables to make the naive Bayes classifier.", "We\u2019ll use naive Bayes on all 4 data sets (imbalanced, oversample, undersample and SMOTE) and compare the prediction accuracy using cross validation.", "This package names cross validated results are test.mean. After comparing, we see that train.smote gives the highest true positive rate and true negative rate. Hence, we learn that SMOTE technique outperforms the other two sampling methods.", "Now, let\u2019s build our model SMOTE data and check our final prediction accuracy.", "#calculate F measure\n> precision <- dCM$byClass['Pos Pred Value']\n> recall <- dCM$byClass['Sensitivity']", "The function confusionMatrix is taken from library(caret). This naive Bayes model predicts 98% of the majority class correctly, but disappoints at\u00a0minority class prediction (~23%). Let us not get hopeless\u00a0and try more techniques to improve our accuracy. Remember, the more you hustle, better you get!", "Let\u2019s use xgboost algorithm and try to improve our model. We\u2019ll do 5 fold cross validation and 5 round random search for parameter tuning. Finally, we\u2019ll build the model using the best tuned parameters.", "#define search function\n> rancontrol <- makeTuneControlRandom(maxit = 5L) #do 5 iterations\n", "Now, we can use these parameter for modeling using xgb_tune$x\u00a0which contains the best tuned parameters.", "#set optimal parameters\n> xgb_new <- setHyperPars(learner = xgb_learner, par.vals = xgb_tune$x)", "#test model\n> predict.xg <- predict(xgmodel, test.task)", "#make prediction\n> xg_prediction <- predict.xg$data$response", "As we can see, xgboost has outperformed naive Bayes model\u2019s accuracy (as expected!). Can we further improve ?", "Until now, we\u2019ve used all the variables in the data. Shall we try using the important ones? Consider it your homework. Let me provide you hint to do this:", "After this, follow the same steps as above for predictions and evaluation. Tell me\u00a0your understanding in comments below.", "Until now, our model has been making label predictions. The threshold used for making these predictions in 0.5 as seen by:", "Due to imbalanced nature of the data, the threshold of 0.5 will always favor the majority class since the probability of a class 1 is quite low. Now, we\u2019ll try a new technique:", "We\u2019ll continue using xgboost for this stunt. To do this, we need to change the predict.type parameter while defining learner.", "Now, let\u2019s look at the probability table thus created:", "Since, we have obtained the class probabilities, let\u2019s create an AUC curve and determine the basis to modify prediction threshold.", "AUC is a measure of true positive rate and false positive rate. We aim to reach as close to top left corner as possible. Therefore, we should aim to reduce the threshold so that the false positive rate can be reduced.", "With 0.4 threshold, our model returned better predictions than our previous xgboost model at 0.5 threshold. Thus, you can see that setting threshold using AUC curve actually affect our model performance. Let\u2019s give one more try.", "This model has outperformed all our models i.e. in other words, this is the best model because 77% of the minority classes have been predicted correctly.", "Similarly, you can try and test other threshold values to check if your model improves. In this xgboost model, there is a lot you can do such as:", "Apart from the methods listed above, you can also assign class weights such that the algorithm pays more attention while classifying the class with higher weight. I leave this part as homework to you. Run the code below and update me if you model surpassed our previous xgboost prediction. Use SVM in homework. An important tip: The code below might take longer than expected to run, therefore close all other applications.", "#random search\n> set_search <- makeTuneControlRandom(maxit = 5L) #5 times", "#cross validation #10L seem to take forever\n> set_cv <- makeResampleDesc(\"CV\",iters=5L,stratify = TRUE)", "#set hyperparameters\n>svm_new <- setHyperPars(learner = svm_learner, par.vals = svm_tune$x)", "I hope this project will helped you understand the importance of data exploration, visualization, manipulation in machine learning. To showcase this project on your resume, create your github account, upload this project along with important findings. I have not deliberately provided my code so that you write it at your end.", "Make sure you understand this project and develop on it further. Recruiter can figure out who\u2019s honest and not. And trust me, such projects will different your profile from other candidates.", "If you want me to create\u00a0and share more such machine learning projects, write your suggestion on problem types below. I\u2019d love work on something which you think is challenging and would like to conquer\u00a0it.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F09%2Fthis-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=This Machine Learning Project on Imbalanced Data Can Add Value to Your Resume&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F09%2Fthis-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F09%2Fthis-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/avcontentteam/", "anchor_text": "avcontentteam"}, {"url": "https://www.analyticsvidhya.com/blog/category/classification/", "anchor_text": "Classification"}, {"url": "https://www.analyticsvidhya.com/blog/category/intermediate/", "anchor_text": "Intermediate"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/project/", "anchor_text": "Project"}, {"url": "https://www.analyticsvidhya.com/blog/category/r/", "anchor_text": "R"}, {"url": "https://www.analyticsvidhya.com/blog/category/structured-data/", "anchor_text": "Structured Data"}, {"url": "https://www.analyticsvidhya.com/blog/category/supervised/", "anchor_text": "Supervised"}, {"url": "http://discuss.analyticsvidhya.com", "anchor_text": "community support"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/", "anchor_text": "read this article"}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/", "anchor_text": "here"}, {"url": "http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/", "anchor_text": "Data set information"}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/09/train.zip", "anchor_text": "Download Train Data"}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/09/test.zip", "anchor_text": "Download Test Data"}, {"url": "http://archive.ics.uci.edu/ml/machine-learning-databases/census-income-mld/census-income.names", "anchor_text": "data set page"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/", "anchor_text": "recommended article"}, {"url": "https://www.analyticsvidhya.com/jobs/", "anchor_text": "currently hiring jobs"}, {"url": "https://www.analyticsvidhya.com/blog/tag/auc-curve/", "anchor_text": "AUC curve"}, {"url": "https://www.analyticsvidhya.com/blog/tag/categorical-variables/", "anchor_text": "categorical variables"}, {"url": "https://www.analyticsvidhya.com/blog/tag/class-weight-svm/", "anchor_text": "class weight SVM"}, {"url": "https://www.analyticsvidhya.com/blog/tag/continuous-variables/", "anchor_text": "Continuous Variables"}, {"url": "https://www.analyticsvidhya.com/blog/tag/correlation/", "anchor_text": "Correlation"}, {"url": "https://www.analyticsvidhya.com/blog/tag/data-exploration/", "anchor_text": "data exploration"}, {"url": "https://www.analyticsvidhya.com/blog/tag/data-scientist/", "anchor_text": "data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/tag/dodge-bar-chart/", "anchor_text": "dodge bar chart"}, {"url": "https://www.analyticsvidhya.com/blog/tag/imbalanced-classification/", "anchor_text": "imbalanced classification"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning-project/", "anchor_text": "machine learning project"}, {"url": "https://www.analyticsvidhya.com/blog/tag/missing-values/", "anchor_text": "Missing Values"}, {"url": "https://www.analyticsvidhya.com/blog/tag/naive-bayes/", "anchor_text": "Naive Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/tag/oversample/", "anchor_text": "oversample"}, {"url": "https://www.analyticsvidhya.com/blog/tag/resume/", "anchor_text": "resume"}, {"url": "https://www.analyticsvidhya.com/blog/tag/scatter-plot/", "anchor_text": "Scatter Plot"}, {"url": "https://www.analyticsvidhya.com/blog/tag/smote/", "anchor_text": "SMOTE"}, {"url": "https://www.analyticsvidhya.com/blog/tag/undersampling/", "anchor_text": "undersampling"}, {"url": "https://www.analyticsvidhya.com/blog/tag/xgboost-auc/", "anchor_text": "xgboost AUC"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=ReadingList&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Machine Learning Basics for a Newbie"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/10-things-know-before-first-data-science-project/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "6 Steps of Machine learning Lifecycle"}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/build-predictive-model-10-minutes-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Predictive Modeling"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/introduction-to-exploratory-data-analysis-eda/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Exploratory Data Analysis & Data Insights"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/how-to-learn-mathematics-for-machine-learning-what-concepts-do-you-need-to-master-in-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Descriptive Statistics"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/comprehensive-practical-guide-inferential-statistics-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Inferential Statistics"}, {"url": "https://www.analyticsvidhya.com/blog/2014/07/statistics/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "How to Understand Population Distributions?"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/how-to-extract-tabular-data-from-doc-files-using-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Reading Data Files into Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-data-types-in-statistics-for-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Different Variable Datatypes"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/statistics-for-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Probability for Data Science"}, {"url": "https://www.analyticsvidhya.com/blog/2017/04/40-questions-on-probability-for-all-aspiring-data-scientists/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Basic Concepts of Probability"}, {"url": "https://www.analyticsvidhya.com/blog/2017/02/basic-probability-data-science-with-examples/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Axioms of Probability"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/conditional-probability-bayes-theorem/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Conditional Probability"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/the-measure-of-central-tendencies-in-statistics-a-beginners-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Central Tendencies for Continuous Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/dispersion-of-data-range-iqr-variance-standard-deviation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Spread of Data"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/univariate-analysis-visualization-with-illustrations-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "KDE plots for Continuous Variable"}, {"url": "https://www.analyticsvidhya.com/blog/2015/11/8-ways-deal-continuous-variables-predictive-modeling/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Overview of Distribution for Continuous variables"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/statistics-data-science-normal-distribution/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Normal Distribution"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/how-to-transform-features-into-normal-gaussian-distribution/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Skewed Distribution"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Skeweness and Kurtosis"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/probability-types-of-probability-distribution-functions/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Distribution for Continuous Variable"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/3-central-tendency-measures-mean-mode-median/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Central Tendencies for Categorical Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/discrete-probability-distributions/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding Discrete Distributions"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/exploratory-data-analysiseda-from-scratch-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Performing EDA on Categorical Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/dealing-with-missing-values-in-python-a-complete-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Dealing with Missing Values"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/detecting-and-treating-outliers-treating-the-odd-one-out/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding Outliers"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/how-to-treat-outliers-in-a-data-set/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Identifying Outliers in Data"}, {"url": "https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Outlier Detection in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2022/08/dealing-with-outliers-using-the-z-score-method/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Outliers Detection Using IQR, Z-score, LOF and DBSCAN"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/introductory-statistics-for-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Sample and Population"}, {"url": "https://www.analyticsvidhya.com/blog/2019/05/statistics-101-introduction-central-limit-theorem/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Central Limit Theorem"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/intermediate-statistical-concepts-for-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Confidence Interval and Margin of Error"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/top-python-libraries-to-automate-exploratory-data-analysis-in-2021/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Bivariate Analysis Introduction"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/different-type-of-correlation-metrics-used-by-data-scientist/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Covariance"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/beginners-guide-to-pearsons-correlation-coefficient/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Pearson Correlation"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/comparison-of-pearson-and-spearman-correlation-coefficients/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Spearman's Correlation & Kendall's Tau"}, {"url": "https://www.analyticsvidhya.com/blog/2015/06/establish-causality-events/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Correlation versus Causation"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/the-clever-ingredient-that-decide-the-rise-and-the-fall-of-your-machine-learning-model-exploratory-data-analysis/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Tabular and Graphical methods for Bivariate Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2022/03/exploratory-data-analysis-eda-credit-card-fraud-detection-case-study/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Performing Bivariate Analysis on Continuous-Continuous Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2015/05/data-visualization-resource/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Tabular and Graphical methods for Continuous-Categorical Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/hypothesis-testing-in-machine-learning-everything-you-need-to-know/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Hypothesis Testing"}, {"url": "https://www.analyticsvidhya.com/blog/2019/09/everything-know-about-p-value-from-scratch-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "P-value"}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/hypothesis-testing-explained/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Two sample Z-test"}, {"url": "https://www.analyticsvidhya.com/blog/2020/06/statistics-analytics-hypothesis-testing-z-test-t-test/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "T-test"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/feature-selection-using-statistical-tests/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "T-test vs Z-test"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/eda-exploratory-data-analysis-with-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Performing Bivariate Analysis on Continuous-Catagorical variables"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/what-is-chi-square-test-how-it-works/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Chi-Squares Test"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/exploratory-data-analysis-using-data-visualization-techniques/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Bivariate Analysis on Categorical Categorical Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/exploratory-data-analysis-the-go-to-technique-to-explore-your-data/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Multivariate Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "A Comprehensive Guide to Data Exploration"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/network-analysis-ipl-data/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "The Data Science behind IPL"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/5-regression-algorithms-you-should-know-introductory-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Supervised Learning vs Unsupervised Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Reinforcement Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/deep-understanding-of-discriminative-and-generative-models-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Generative and Descriminative Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/hypothesis-testing-parametric-and-non-parametric-tests-in-statistics/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Parametric and Non Parametric model"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/build-your-first-machine-learning-pipeline-using-scikit-learn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Machine Learning Pipeline"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/tutorial-to-data-preparation-for-training-machine-learning-model/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Preparing Dataset"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/build-your-first-linear-regression-machine-learning-model/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Build a Benchmark Model: Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/wine-quality-prediction-using-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Build a Benchmark Model: Classification"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Evaluation Metrics for Machine Learning Everyone should know"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Confusion Matrix"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/classification-problem-relation-between-sensitivity-specificity-and-accuracy/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Accuracy"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/precision-recall-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Precision and Recall"}, {"url": "https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "AUC-ROC"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Log Loss"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "R2 and Adjusted R2"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/handling-missing-data-with-simpleimputer/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Dealing with Missing Values"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/defining-analysing-and-implementing-imputation-techniques/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Replacing Missing Values"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/knnimputer-a-robust-way-to-impute-missing-values-using-scikit-learn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Imputing Missing Values in Data"}, {"url": "https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Working with Categorical Variables"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/zooming-out-a-look-at-outlier-and-how-to-deal-with-them-indata-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Working with Outliers"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/data-preprocessing-in-data-mining-a-hands-on-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Preprocessing Data for Model Building"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/cost-function-is-no-rocket-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding Cost Function"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding Gradient Descent"}, {"url": "https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Math Behind Gradient Descent"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/what-is-multicollinearity/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Assumptions of Linear Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement Linear Regression from Scratch"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/multiple-linear-regression-using-python-and-scikit-learn/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Train Linear Regression in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/predicting-using-linear-regression-in-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Linear Regression in R"}, {"url": "https://www.analyticsvidhya.com/blog/2013/12/residual-plots-regression-model/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Diagnosing Residual Plots in Linear Regression Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/everything-you-need-to-know-about-linear-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Generalized Linear Models"}, {"url": "https://www.analyticsvidhya.com/blog/2017/08/skilltest-logistic-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Logistic Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/conceptual-understanding-of-logistic-regression-for-data-science-beginners/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Odds Ratio"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/beginners-take-how-logistic-regression-is-related-to-linear-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Logistic Regression from Scratch"}, {"url": "https://www.analyticsvidhya.com/blog/2015/01/scikit-learn-python-machine-learning-tool/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Scikit-learn in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/logistic-regression-an-introductory-note/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Train Logistic Regression in python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-logistic-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Multiclass using Logistic Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2016/02/multinomial-ordinal-logistic-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "How to use Multinomial and Ordinal Logistic Regression in R ?"}, {"url": "https://www.analyticsvidhya.com/blog/2017/07/30-questions-to-test-a-data-scientist-on-linear-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Challenges with Linear Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Regularisation"}, {"url": "https://www.analyticsvidhya.com/blog/2021/11/study-of-regularization-techniques-of-linear-model-and-its-roles/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Regularisation"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Ridge Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/lasso-and-ridge-regularization-a-rescuer-from-overfitting/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Lasso Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to K Nearest Neighbours"}, {"url": "https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Determining the Right Value of K in KNN"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/simple-understanding-and-implementation-of-knn-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement KNN from Scratch"}, {"url": "https://www.analyticsvidhya.com/blog/2018/08/k-nearest-neighbor-introduction-regression-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement KNN in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/bias-and-variance-tradeoff-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Bias Variance Tradeoff"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/underfitting-overfitting-best-fitting-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Overfitting and Underfitting"}, {"url": "https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Visualizing Overfitting and Underfitting"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/how-to-choose-an-appropriate-ml-algorithm-data-science-projects/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Selecting the Right Model"}, {"url": "https://www.analyticsvidhya.com/blog/2018/05/improve-model-performance-cross-validation-in-python-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "What is Validation?"}, {"url": "https://www.analyticsvidhya.com/blog/2022/02/k-fold-cross-validation-technique-and-its-essentials/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Hold-Out Validation"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/introduction-to-k-fold-cross-validation-in-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding K Fold Cross Validation"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Feature Selection"}, {"url": "https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Feature Selection Algorithms"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-missing-value-ratio-and-its-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Missing Value Ratio"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-low-variance-filter-and-its-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Low Variance Filter"}, {"url": "https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "High Correlation Filter"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Backward Feature Elimination"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/discovering-the-shades-of-feature-selection-methods/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Forward Feature Selection"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/forward-feature-selection-and-its-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement Feature Selection in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement Feature Selection in R"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Decision Tree"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-gini-impurity/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Purity in Decision Tree"}, {"url": "https://www.analyticsvidhya.com/blog/2022/04/complete-flow-of-decision-tree-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Terminologies Related to Decision Tree"}, {"url": "https://www.analyticsvidhya.com/blog/2020/06/4-ways-split-decision-tree/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "How to Select Best Split Point in Decision Tree?"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/how-to-select-best-split-in-decision-trees-using-chi-square/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Chi-Squares"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/25-questions-to-test-your-skills-on-decision-trees/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Information Gain"}, {"url": "https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Reduction in Variance"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Optimizing Performance of Decision Tree"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-decision-tree-classification-using-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Train Decision Tree using Scikit Learn"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/cost-complexity-pruning-decision-trees/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Pruning of Decision Trees"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/step-by-step-process-of-feature-engineering-for-machine-learning-algorithms-in-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Feature Engineering"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/types-of-feature-transformation-and-scaling/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Feature Transformation"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/feature-engineering-feature-improvements-scaling/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Feature Scaling"}, {"url": "https://www.analyticsvidhya.com/blog/2018/08/guide-automated-feature-engineering-featuretools-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Feature Engineering"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/complete-guide-on-encode-numerical-features-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Frequency Encoding"}, {"url": "https://www.analyticsvidhya.com/blog/2020/06/feature-engineering-guide-data-science-hackathons/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Automated Feature Engineering: Feature Tools"}, {"url": "https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Naive Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/naive-bayes-algorithm-a-complete-guide-for-data-science-enthusiasts/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Conditional Probability and Bayes Theorem"}, {"url": "https://www.analyticsvidhya.com/blog/2019/07/introduction-online-rating-systems-bayesian-adjusted-rating/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Bayesian Adjustment Rating: The Incredible Concept Behind Online Ratings!"}, {"url": "https://www.analyticsvidhya.com/blog/2022/03/building-naive-bayes-classifier-from-scratch-to-perform-sentiment-analysis/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Working of Naive Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/a-guide-to-the-naive-bayes-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Math behind Naive Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/frequently-asked-interview-questions-on-naive-bayes-classifier/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Types of Naive Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/introduction-to-naive-bayes-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementation of Na\u00c3\u00afve Bayes"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/demystifying-the-difference-between-multi-class-and-multi-label-classification-problem-statements-in-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding how to solve Multiclass and Multilabled Classification Problem"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Evaluation Metrics: Multi Class Classification"}, {"url": "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Ensemble Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/ensemble-stacking-for-machine-learning-and-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Basic Ensemble Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/exploring-ensemble-learning-in-machine-learning-world/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Basic Ensemble Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2015/08/optimal-weights-ensemble-learner-neural-network/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Finding Optimal Weights of Ensemble Learner using Neural Network"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/ensemble-modeling-for-neural-networks-using-large-datasets-simplified/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Why Ensemble Models Work well?"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/how-to-use-stacking-to-choose-the-best-possible-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Stacking"}, {"url": "https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Stacking"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/improve-predictive-model-score-stacking-regressor/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Variants of Stacking"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/advanced-ensemble-learning-technique-stacking-and-its-variants/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Variants of Stacking"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/basic-ensemble-technique-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Blending"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/what-is-bootstrap-sampling-in-statistics-and-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Bootstrap Sampling"}, {"url": "https://www.analyticsvidhya.com/blog/2019/09/data-scientists-guide-8-types-of-sampling-techniques/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Random Sampling"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Hyper-parameters of Random Forest"}, {"url": "https://www.analyticsvidhya.com/blog/2018/10/interpret-random-forest-model-machine-learning-programmers/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Random Forest"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/out-of-bag-oob-score-in-the-random-forest-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Out-of-Bag (OOB) Score in the Random Forest"}, {"url": "https://www.analyticsvidhya.com/blog/2022/05/ipl-team-win-prediction-project-using-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "IPL Team Win Prediction Project Using Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Boosting"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/boosting-in-machine-learning-definition-functions-types-and-features/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Gradient Boosting Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/4-boosting-algorithms-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Math behind GBM"}, {"url": "https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing GBM in python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/distinguish-between-tree-based-machine-learning-algorithms/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Regularized Greedy Forests"}, {"url": "https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Extreme Gradient Boosting"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing XGBM in python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/5-hyperparameter-optimization-techniques-you-must-know-for-data-science-hackathons/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Tuning Hyperparameters of XGBoost in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2016/01/xgboost-algorithm-easy-steps/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implement XGBM in R/H2O"}, {"url": "https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Adaptive Boosting"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/introduction-to-adaboost-algorithm-with-python-implementation/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Adaptive Boosing"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "LightGBM"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/complete-guide-on-how-to-use-lightgbm-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing LightGBM in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Catboost"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/how-to-use-catboost-for-mental-fatigue-score-prediction/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Catboost in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/evaluating-machine-learning-models-hyperparameter-tuning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Different Hyperparameter Tuning methods"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/an-effective-approach-to-hyper-parameter-tuning-a-beginners-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Different Hyperparameter Tuning methods"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/tune-hyperparameters-with-gridsearchcv/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "GridsearchCV"}, {"url": "https://www.analyticsvidhya.com/blog/2022/11/hyperparameter-tuning-using-randomized-search/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "RandomizedsearchCV"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Bayesian Optimization for Hyperparameter Tuning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/bayesian-optimization-bayes_opt-or-hyperopt/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Hyperopt"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/support-vector-regression-tutorial-for-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding SVM Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/support-vector-machinessvm-a-complete-guide-for-beginners/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "SVM Kernels In-depth Intuition and Practical Implementation"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/support-vector-machine-better-understanding/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "SVM Kernel Tricks"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/support-vector-machines/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Kernels and Hyperparameters in SVM"}, {"url": "https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing SVM from Scratch in Python and R"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/diminishing-the-dimensions-with-pca/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Principal Component Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/an-end-to-end-comprehensive-guide-for-pca/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Steps to Perform Principal Compound Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/simplifying-maths-behind-pca/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Computation of Covariance Matrix"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/pca-and-its-underlying-mathematical-principles/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Finding Eigenvectors and Eigenvalues"}, {"url": "https://www.analyticsvidhya.com/blog/2016/03/pca-practical-guide-principal-component-analysis-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing PCA in python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/visualizing-pca-in-r-programming-with-factoshiny/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Visualizing PCA"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/a-brief-introduction-to-linear-discriminant-analysis/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "A Brief Introduction to Linear Discriminant Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/dimensionality-reduction-using-factor-analysis-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Factor Analysis"}, {"url": "https://www.analyticsvidhya.com/blog/2020/11/introduction-to-clustering-in-python-for-beginners-in-data-science/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2022/11/hierarchical-clustering-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Applications of Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Evaluation Metrics for Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding K-Means"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/k-means-clustering-simplified-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementation of K-Means in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/beginners-guide-to-clustering-in-r-program/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementation of K-Means in R"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Choosing Right Value for K"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/a-definitive-guide-for-predicting-customer-lifetime-value-clv/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Profiling Market Segments using K-Means Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/single-link-hierarchical-clustering-clearly-explained/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Hierarchical Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementation of Hierarchial Clustering"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/how-dbscan-clustering-works/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "DBSCAN"}, {"url": "https://www.analyticsvidhya.com/blog/2017/02/test-data-scientist-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Defining Similarity between clusters"}, {"url": "https://www.analyticsvidhya.com/blog/2019/10/gaussian-mixture-models-clustering/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Build Better and Accurate Clusters with Gaussian Mixture Models"}, {"url": "https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understand Basics of Recommendation Engine with Case Study"}, {"url": "https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "8 Proven Ways for improving the \u00e2\u20ac\u0153Accuracy\u00e2\u20ac_x009d_ of a Machine Learning Model"}, {"url": "https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Dask"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/cuml-blazing-fast-machine-learning-model-training-with-nvidias-rapids/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Working with CuML"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/beginners-guide-to-machine-learning-explainability/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Machine Learning Interpretability"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/building-trust-in-machine-learning-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Framework and Interpretable Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/explain-how-your-model-works-using-explainable-ai/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "model Agnostic Methods for Interpretability"}, {"url": "https://www.analyticsvidhya.com/blog/2019/08/decoding-black-box-step-by-step-guide-interpretable-machine-learning-models-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementing Interpretable Model"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/shapley-value-machine-learning-interpretability-game-theory/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Understanding SHAP"}, {"url": "https://www.analyticsvidhya.com/blog/2022/09/out-of-core-ml-an-efficient-technique-to-handle-large-data/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Out-of-Core ML"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/6-python-libraries-interpret-machine-learning-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Interpretable Machine Learning Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/ml-interpretability-using-lime-in-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Model Agnostic Methods for Interpretability"}, {"url": "https://www.analyticsvidhya.com/blog/2019/12/game-theory-101-decision-making-normal-form-games/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Game Theory & Shapley Values"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/does-the-popularity-of-automl-means-the-end-of-data-science-jobs/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to AutoML"}, {"url": "https://www.analyticsvidhya.com/blog/2017/07/mlbox-library-automated-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Implementation of MLBox"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/anomaly-detection-using-isolation-forest-a-complete-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to PyCaret"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/automate-machine-learning-using-tpot%e2%80%8a-%e2%80%8aexplore-thousands-of-possible-pipelines-and-find-the-best/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "TPOT"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/beginners-guide-to-automl-with-an-easy-autogluon-example/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Auto-Sklearn"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/breast-cancer-prediction-using-evalml/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "EvalML"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/quick-hacks-to-save-machine-learning-model-using-pickle-and-joblib/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Pickle and Joblib"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/integrating-machine-learning-into-web-applications-with-flask/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Model Deployment"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/build-web-app-instantly-for-machine-learning-using-streamlit/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploying Machine Learning Model using Streamlit"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/a-hands-on-guide-to-containerized-your-machine-learning-workflow-with-docker/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploying ML Models in Docker"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/developing-data-web-streamlit-app/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploy Using Streamlit"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/deploy-your-ml-dl-streamlit-application-on-heroku/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploy on Heroku"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/easily-deploy-your-machine-learning-model-into-a-web-app-netlify/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploy Using Netlify"}, {"url": "https://www.analyticsvidhya.com/blog/2022/02/building-ml-model-in-aws-sagemaker/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Amazon Sagemaker"}, {"url": "https://www.analyticsvidhya.com/blog/2022/01/huggingface-transformer-model-using-amazon-sagemaker/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Setting up Amazon SageMaker"}, {"url": "https://www.analyticsvidhya.com/blog/2020/11/deployment-of-ml-models-in-cloud-aws-sagemaker%e2%80%8ain-built-algorithms/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Using SageMaker Endpoint to Generate Inference"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/how-to-deploy-machine-learning-models-in-azure-cloud-with-the-help-of-python-and-flask/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploy on Microsoft Azure Cloud"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/easy-introduction-to-flask-framework-for-beginners/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Introduction to Flask for Model"}, {"url": "https://www.analyticsvidhya.com/blog/2020/04/how-to-deploy-machine-learning-model-flask/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Deploying ML model using Flask"}, {"url": "https://www.analyticsvidhya.com/blog/2015/12/18-mobile-apps-data-scientist-data-analysts/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Model Deployment in Android"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/introduction-apple-core-ml-3-deep-learning-models-iphone/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Model Deployment in Iphone"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2023/02/partial-auc-scores-a-better-metric-for-binary-classification/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Partial AUC Scores: A Better Metric for Binary Classification"}, {"url": "https://www.analyticsvidhya.com/blog/2023/01/boost-your-career-best-data-science-courses-in-2023/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "The Ultimate Guide to Choosing the Best Data Science Course to Boost Your Career in 2023"}, {"url": "https://www.analyticsvidhya.com/blog/2023/01/practicing-machine-learning-with-imbalanced-dataset/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Practicing Machine Learning with Imbalanced Dataset"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/data-scientist-resume-guide/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Data Scientist Resume Guide"}, {"url": "https://www.analyticsvidhya.com/blog/2023/01/step-by-step-guide-to-become-a-data-scientist-in-2023/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "Step-by-Step Guide to Become a Data Scientist in 2023"}, {"url": "https://www.analyticsvidhya.com/blog/2023/03/how-can-a-statistician-become-a-data-scientist/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/", "anchor_text": "How can a Statistician Become a Data Scientist?"}, {"url": "https://www.analyticsvidhya.com/blog/author/avcontentteam/", "anchor_text": "avcontentteam"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/", "anchor_text": "Comprehensive Introduction to Apache Spark, RDDs & Dataframes (using PySpark)"}, {"url": "https://www.analyticsvidhya.com/blog/2016/09/senior-database-administrator-bengaluru-7-8-years-of-experience/", "anchor_text": "Senior Database Administrator \u2013 Bengaluru ( 7-8 Years of Experience )"}, {"url": "https://www.analyticsvidhya.com/cdn-cgi/l/email-protection", "anchor_text": "[email protected]"}, {"url": "https://www.analyticsvidhya.com/blog/2016/09/this-machine-learning-project-on-imbalanced-data-can-add-value-to-your-resume/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}