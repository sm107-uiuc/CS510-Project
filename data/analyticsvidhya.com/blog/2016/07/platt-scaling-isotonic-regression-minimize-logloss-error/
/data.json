{"url": "https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/\n", "time": 1683020834.629017, "path": "analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/\n/", "webpage": {"metadata": {"title": "Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R", "h1": "Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R", "description": "This tutorial explains using isotonic regression and platt scaling to calibrate predicted probabilities to improve logloss error in data set"}, "outgoing_paragraph_urls": [{"url": "http://datahack.analyticsvidhya.com/contest/all", "anchor_text": "data science / machine learning competitions", "paragraph_index": 0}, {"url": "https://www.drivendata.org/competitions/2/page/7/", "anchor_text": "Predict Blood Donations", "paragraph_index": 3}, {"url": "https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/", "anchor_text": "SVM", "paragraph_index": 11}, {"url": "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/", "anchor_text": "boosted trees", "paragraph_index": 11}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/", "anchor_text": "Naive Bayes", "paragraph_index": 11}, {"url": "http://www.datascienceassn.org/sites/default/files/Predicting%20good%20probabilities%20with%20supervised%20learning.pdf", "anchor_text": "Niculescu-Mizil et al", "paragraph_index": 14}, {"url": "https://www.drivendata.org/competitions/2/", "anchor_text": "Predict Blood Donations", "paragraph_index": 19}, {"url": "https://github.com/NeerajSarwan/Calibrating-Probabilities-in-Supervised-Learning.git", "anchor_text": "Github", "paragraph_index": 45}], "all_paragraphs": ["This article is best suited for people who actively (or are aspiring to) participate in data science / machine learning competitions and try hard to improve their model\u2019s accuracy.", "Many a times, we face\u00a0problems (data sets) whose evaluation metric is LogLoss. It\u2019s mostly seen in problems where probabilities needs to be\u00a0predicted. Recently, I came across a similar problem.\u00a0You know, it\u2019s being said, \u2018when you know no boundaries, you do things which were seen improbable\u2019. So, in the quest of my accuracy improvement, I discovered \u00a02 powerful methods to improve accuracy of predicted probabilities.", "In this article, you\u2019ll learn about these methods and how to implement them in R to improve your rankings and score.\u00a0Also, I\u2019ll show you how I achieved a uplift of 87 ranks on competition leaderboard using one of these methods.", "Note: I\u2019ve participated in\u00a0Predict Blood Donations\u00a0competition which is currently active and uses Log Loss as its evaluation metric.", "This Evaluation Metric is employed in the classification tasks where rather than predicting the actual classes, one needs to predict the probabilities corresponding to that classes. So, in a way this metric measures the quality of predictions rather than the accuracy. The formula for Log Loss goes as-", "Where\u00a0\u00a0is the actual class value (i.e 0 or 1 in case of binary classification) of a particular observation(row) and\u00a0\u00a0is the predicted probability of that class.", "Now, lets take an example of a single row where the actual class is 0 and you predict the probability with absolute certainty i.e 1. Log Loss in this case turns out to be infinity. So, it is easy to see that Log Loss metric penalizes absurd\u00a0misclassification with a high certainty such that the rest of predictions become irrelevant.", "So in a way, \u00a0a better model is one which gives average probability to the observation about which it is unsure of. The closer the predicted probability to the actual class value, the better it is.", "Here I provide you the relevant code in R which can be used to calculate the Log Loss during the cross-validation stage while model building:", "# Log Loss as defined on the kaggle forum", "I\u2019m sure many of you would have this question, \u2018\u00a0why and under what circumstances will I need to calibrate the probabilities?\u2019.", "Over time and due to the efforts of a scholarly researchers and\u00a0data scientists, experiments have shown that maximum margin methods such as SVM, boosted trees etc push the real posterior probability away from 0 and 1 while methods such as Naive Bayes tend to push the probabilities towards 0 and 1. And in cases where predicting the accurate probabilities is more important, this poses a serious problem.", "It has also been noticed empirically that boosted trees, random forests and SVMs performs best after calibration. Let\u2019s see what works best!", "Here, we discuss two methods of calibrating the posterior probabilities \u2013 Platt Scaling and Isotonic Regression with the help of a real\u00a0data set. I\u2019ll show you\u00a0how I achieved a boosting of 87 ranks just by applying Platt Scaling on my model.", "First of all, I would like to introduce reliability plots which can be used to visualize our calibration. As described in Niculescu-Mizil et al:", "On real problems where the true conditional probabilities are not known, model calibration can be visualized with reliability diagrams (DeGroot & Fienberg, 1982). First, the prediction space is discretized into ten bins. Cases with predicted value between 0 and 0.1 fall in the first bin, between 0.1 and 0.2 in the second bin, etc.\n For each bin, the mean predicted value is plotted against the true fraction of positive cases. If the model is well calibrated the points will fall near the diagonal line.", "Now, let\u2019s learn to create reliability diagrams in R. The number of bins have been converted into an user-entered parameter.", "I would like you to pay attention to the very first line of this code. This line creates a simple plot with a grey line inclined at an angle of 45\u00b0 . This line will form our benchmark solution. Any line closer to this diagonal line in comparison to the other lines will be a better solution on Log Loss metric.", "You should probably save this function somewhere and keep it handy since we will be using it to graphically view the performance of our model on Log Loss metric.", "Practice time! It is better if we understand all this using a real dataset. The dataset that I will be using is the Predict Blood Donations\u00a0dataset.", "Since our motive is to understand the usage and impact of Platt Scaling, we won\u2019t be dealing with pre-processing and feature engineering. I have also removed the ID\u00a0and the Total Volume Donated\u00a0variable since ID is of no use for the prediction purpose and Total Volume Donated is perfectly correlated to Number of Donations.", "Platt scaling is a way of transforming classification output into probability distribution. For example: If you\u2019ve got the dependent variable as 0 & 1 in train data set, using this method you can convert it into probability.", "Let\u2019s now understand\u00a0how Platt Scaling is applied in real Predictive Modeling problems (in order):", "I hope that was easy to visualize and understand. Now let\u2019s start with the actual coding process for a practical\u00a0understanding.", "# reading the train dataset\ntrain <-read.csv(\"train.csv\")", "# removing the X column since it is irrelevant for our training and total colume column since it is perfectly correlated to number of donations\ntrain<-train[-c(1,4)]", " # training a logistic regression model on the cross validation dataset\nmodel_log<-glm(y~x,data = dataframe,family = binomial)", "As you can see the blue line is more closer to the grey line indicating that Platt Scaling actually improved (or reduced) our Log Loss error metric. The most important point to be noted here is that other metrics like accuracy, AUC etc are not influenced to an appreciable extent using the Platt Scaling.", "Now, we will use the above method to make predictions on the data set. Let\u2019s find out how much improvement did we achieve:", "# The below line makes the test data set similar to train data set by removing the X and the Total Volume Donated column.\ntest<-test[-c(1,4)]", "# Predicting on the test dataset without Platt Scaling\nresult_test<-as.data.frame(predict(model_rf,newdata = test,type = \"prob\"))", "The result_test scored 1.6932\u00a0on the Public Leaderboard without any feature engineering and Platt Scaling,\u00a0whereas result_test_platt scored 0.4895\u00a0without feature engineering but platt scaling.", "So, the method we used is simple, but its results are astonishing. You can see, I got a massive improvement in my error score. This must\u00a0have given\u00a0you enough\u00a0idea about the prowess of Platt Scaling.", "Next, let\u2019s discuss another interesting method which can be used to improve the performance on a Log Loss metric- Isotonic Regression.", "Isotonic Regression is similar to Platt Scaling. It\u2019s a non-parametric regression technique. Non-parametric means that it doesn\u2019t make any assumptions such as of linearity among variables, constant error variance etc.", "The only difference lies in the function being fit. The function we\u00a0fit in isotonic regression continuously increases/decreases. The below code builds upon the Platt Scaling code mentioned above, to build\u00a0the reliability plot before followed by\u00a0isotonic regression. The method employs the use of isoreg function in the stats package:", "fit.isoreg function fits a linear line to the function since the isoreg function fits a step wise function to the data.\u00a0Let\u2019s visualise the iso.model function fitted through the isoreg function.", "# This line of code creates an isotonic regression model on the cross validation dataset\niso.model <- isoreg(result_cv_unique, cv_actual_unique)", "Here x0 is the predicted value on the cross validation data set using the model_rf (random forest) and x$y\u00a0consist of actual dependent values on the cross validation data set. The red line in the first diagram shows a isotonic function fitted to the cross validation data set. Figure 2 is the cumulative version of the the Figure 1.", "The fit.isoreg function mentioned above smooths (makes more flexible) the stepping function for better prediction.\u00a0Now, lets actually compute the reliability plot data for the isotonic regression and then calculate the LogLoss.", "# Predicting the cross validation dataset after the isotonic regression\nresult_cv_isotonic <- fit.isoreg(iso.model, result_cv$`1`)", "It might seem like isotonic function didn\u2019t work because of the restriction of number of bins. But, you can change the number of bins to have a better view. Let\u2019s see if isotonic function actually worked by making the calibrated prediction on test data set and uploading the results.", "The Log Loss for the result_test on the Public Leaderboard was 1.6932 without any feature engineering and isotonic regression\u00a0and after isotonic regression on the Public Leaderboard was 0.5050.", "This article is\u00a0code intensive and focused mainly in providing the readers about the technique that kagglers often employ to improve their scores on Log Loss metric.", "These methods work because of the underlying assumptions of the algorithms. This is by no means an exhaustive list of the methods. I\u2019d\u00a0encourage\u00a0you\u00a0to experiment with these methods since they can be heavily modified according to the problem at hand.", "Also, please feel free to suggest any more methods that are currently being employed or you can think of. For your\u00a0convenience, I have explained the above two examples with a particular seed value so that you\u00a0can reproduce exact results. I have also uploaded the code file in R on Github.", "Did you like reading this article ? \u00a0Do share your experience / suggestions in the comments section below. I\u2019d love to know your intuitive solutions to learn more ways of solving LogLoss problems.", "I am a perpetual, quick learner and keen to explore the realm of Data analytics and science. I am deeply excited about the times we live in and the rate at which data is being generated and being transformed as an asset. I am well versed with a few tools for dealing with data and also in the process of learning some other tools and knowledge required to exploit data.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F07%2Fplatt-scaling-isotonic-regression-minimize-logloss-error%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=Using Platt Scaling and Isotonic Regression to Minimize LogLoss Error in R&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F07%2Fplatt-scaling-isotonic-regression-minimize-logloss-error%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F07%2Fplatt-scaling-isotonic-regression-minimize-logloss-error%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/nss/", "anchor_text": "NSS"}, {"url": "https://www.analyticsvidhya.com/blog/category/advanced/", "anchor_text": "Advanced"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/r/", "anchor_text": "R"}, {"url": "https://www.analyticsvidhya.com/blog/category/regression/", "anchor_text": "Regression"}, {"url": "https://www.analyticsvidhya.com/blog/category/structured-data/", "anchor_text": "Structured Data"}, {"url": "https://www.analyticsvidhya.com/blog/category/supervised/", "anchor_text": "Supervised"}, {"url": "https://www.analyticsvidhya.com/blog/category/technique/", "anchor_text": "Technique"}, {"url": "http://datahack.analyticsvidhya.com/contest/all", "anchor_text": "data science / machine learning competitions"}, {"url": "https://www.drivendata.org/competitions/2/page/7/", "anchor_text": "Predict Blood Donations"}, {"url": "https://www.analyticsvidhya.com/blog/2015/10/understaing-support-vector-machine-example-code/", "anchor_text": "SVM"}, {"url": "https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/", "anchor_text": "boosted trees"}, {"url": "https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/", "anchor_text": "Naive Bayes"}, {"url": "http://www.datascienceassn.org/sites/default/files/Predicting%20good%20probabilities%20with%20supervised%20learning.pdf", "anchor_text": "Niculescu-Mizil et al"}, {"url": "https://www.drivendata.org/competitions/2/", "anchor_text": "Predict Blood Donations"}, {"url": "https://github.com/NeerajSarwan/Calibrating-Probabilities-in-Supervised-Learning.git", "anchor_text": "Github"}, {"url": "http://datahack.analyticsvidhya.com/contest/all", "anchor_text": "Live Competitions"}, {"url": "https://www.analyticsvidhya.com/blog/tag/competitions/", "anchor_text": "Competitions"}, {"url": "https://www.analyticsvidhya.com/blog/tag/evaluation-metric/", "anchor_text": "evaluation metric"}, {"url": "https://www.analyticsvidhya.com/blog/tag/isotonic-regression/", "anchor_text": "isotonic regression"}, {"url": "https://www.analyticsvidhya.com/blog/tag/logloss-metric/", "anchor_text": "logloss metric"}, {"url": "https://www.analyticsvidhya.com/blog/tag/machine-learning/", "anchor_text": "machine learning"}, {"url": "https://www.analyticsvidhya.com/blog/tag/non-parametric-approach/", "anchor_text": "non-parametric approach"}, {"url": "https://www.analyticsvidhya.com/blog/tag/platt-scaling/", "anchor_text": "platt scaling"}, {"url": "https://www.analyticsvidhya.com/blog/tag/probability/", "anchor_text": "probability"}, {"url": "https://www.analyticsvidhya.com/blog/tag/regression/", "anchor_text": "regression"}, {"url": "https://www.analyticsvidhya.com/blog/tag/reliability-plots/", "anchor_text": "reliability plots"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/calibration-of-machine-learning-models/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "Calibration of Machine Learning Models"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/know-the-best-evaluation-metrics-for-your-regression-model/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "Know The Best Evaluation Metrics for Your Regression Model !"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/automated-machine-learning-for-supervised-learning-part-1/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "Automated Machine Learning for Supervised Learning (Part 1)"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-your-first-machine-learning-model-linear-regression/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "All you need to know about your first Machine Learning model \u2013 Linear Regression"}, {"url": "https://www.analyticsvidhya.com/blog/2020/10/quick-guide-to-evaluation-metrics-for-supervised-and-unsupervised-machine-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "Quick Guide to Evaluation Metrics for Supervised and Unsupervised Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2021/02/isotonic-regression-and-the-pava-algorithm/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/", "anchor_text": "Isotonic Regression and the PAVA algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/author/nss/", "anchor_text": "NSS"}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/rahul105/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/sion/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/chirag676/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/barney6/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/arnab1408/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/prateekmaj21/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/creators-club/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/author/shanthababu/", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2016/07/capstone-project/", "anchor_text": "Tapping Twitter Sentiments: A Complete Case-Study on 2015 Chennai Floods"}, {"url": "https://www.analyticsvidhya.com/blog/2016/07/data-hackathons-analytics-vidhya/", "anchor_text": "Strategic Thinking Competition by Analytics Vidhya, 16th July 2016"}, {"url": "https://www.analyticsvidhya.com/blog/2016/07/platt-scaling-isotonic-regression-minimize-logloss-error/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}