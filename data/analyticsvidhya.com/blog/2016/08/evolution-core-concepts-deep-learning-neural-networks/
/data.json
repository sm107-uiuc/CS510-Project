{"url": "https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/\n", "time": 1683020795.207361, "path": "analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/\n/", "webpage": {"metadata": {"title": "Evolution and Concepts Of Neural Networks | Deep Learning", "h1": "The Evolution and Core Concepts of Deep Learning & Neural Networks", "description": "Brief history of neural networks in the area of Deep Learning. This article contains evolution and concepts of neural networks and deep learning."}, "outgoing_paragraph_urls": [{"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.5.png", "anchor_text": "", "paragraph_index": 15}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.1.png", "anchor_text": "", "paragraph_index": 16}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.2.png", "anchor_text": "", "paragraph_index": 16}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.3.png", "anchor_text": "", "paragraph_index": 16}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.png", "anchor_text": "", "paragraph_index": 23}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.6.png", "anchor_text": "", "paragraph_index": 23}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.7.png", "anchor_text": "", "paragraph_index": 23}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.1.png", "anchor_text": "", "paragraph_index": 23}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq7.png", "anchor_text": "", "paragraph_index": 30}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-4.png", "anchor_text": "", "paragraph_index": 30}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-6.png", "anchor_text": "", "paragraph_index": 31}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.2.png", "anchor_text": "", "paragraph_index": 36}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.3.png", "anchor_text": "", "paragraph_index": 36}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.2.png", "anchor_text": "", "paragraph_index": 42}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.3.png", "anchor_text": "", "paragraph_index": 42}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.4.png", "anchor_text": "", "paragraph_index": 43}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.5.png", "anchor_text": "", "paragraph_index": 43}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.7.png", "anchor_text": "", "paragraph_index": 45}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.8.png", "anchor_text": "", "paragraph_index": 45}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.png", "anchor_text": "", "paragraph_index": 83}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.1.png", "anchor_text": "", "paragraph_index": 84}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.3.png", "anchor_text": "", "paragraph_index": 87}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.4.png", "anchor_text": "", "paragraph_index": 87}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.5.png", "anchor_text": "", "paragraph_index": 88}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.7.png", "anchor_text": "", "paragraph_index": 89}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.14-1.png", "anchor_text": "", "paragraph_index": 90}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.15.png", "anchor_text": "", "paragraph_index": 90}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-14.png", "anchor_text": "", "paragraph_index": 105}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/", "anchor_text": "Identify the Digits", "paragraph_index": 108}], "all_paragraphs": ["With the evolution of neural networks, various tasks which were considered unimaginable\u00a0can be done conveniently now. Tasks such as image recognition, speech recognition, finding deeper relations in a data set have become much easier. A sincere thanks to the eminent researchers in this field whose\u00a0discoveries and findings have helped us leverage the true power of neural networks.", "If you are truly interested in pursuing machine learning as a subject, a thorough understand of deep learning networks is crucial for you. Most ML algorithms tend of lose accuracy when given a data set with several variables, whereas a deep learning model does wonders in such situations.. Therefore, it\u2019s important for us to understand how does it work!", "In this article, I\u2019ve explained the core concepts used in deep learning i.e. what sort of backend calculations result in enhanced model accuracy. Along side, I\u2019ve also shared various modeling tips and a sneak peek into the history of neural networks.", "Neural networks are the building blocks of today\u2019s technological breakthrough in the field\u00a0of Deep Learning.\u00a0 A neural network can be seen as simple processing unit that is\u00a0massively parallel, capable to store knowledge and apply this knowledge to make predictions.", "A neural network mimics the brain in a way the network acquires knowledge from its environment through a learning process. Then, intervention connection strengths known as synaptic weights are used to store the acquired knowledge. In the learning process, the synaptic weights of the network are modified in an orderly fashion to attain the desired objective. In 1950, the neuro-psychologist Karl Lashley\u2019s thesis was published in which he described the brain as a distributed system.", "Another reason that the neural network is compared with the human brain is that, they operate like non-linear parallel information-processing systems which\u00a0rapidly perform computations such as pattern recognition and perception.\u00a0As a result, these networks perform very well in areas like speech, audio and image recognition where the inputs / signals are inherently nonlinear.", "McCulloch and Pitts were pioneers of neural networks who wrote a research article on the model with two inputs and single output in 1943. The following were the features of that model:", "\u00a0A neuron would only be activated if:", "There is a certain threshold level computed by summing up the input values for which the output is either zero or one.", "In Hebb\u2019s 1949 book \u2018The Organization of Behaviour\u2019, the idea that the connectivity of brain is continuously changing in response to changes in tasks was proposed for the first time. This rule implies that the connection between two neurons is active at the same time. This soon became the source of inspiration for the development of computational models of learning and adaptive systems.", "Artificial neural networks have the ability to learn from\u00a0supplied data, known as adaptive learning, while the ability of a neural network to create its own organization or representation of information is known as self-organisation.", "After 15 years, the perceptron developed by Rosenblatt in 1958 emerged as the next model of neuron. Perceptron is the simplest neural network that linearly separates the data into two classes. Later, he randomly interconnected the perceptron and used a trial and error method to change the weights for the learning.", "After 1969, the research came to a dead end in this area for the next 15 years after the mathematicians Marvin Minsky and Seymour Parpert published a mathematical analysis of the perceptron. They\u00a0found that the perceptron was not capable of representing many important problems, like the exclusive-or function (XOR). Secondly, there was an issue that the computers did not have enough processing power to effectively handle large neural networks.", "In 1986, the development of the back-propagation algorithm was reported by Rumelhart, Hinton, and Williams \u00a0that can solve problems like XOR, beginning a second generation of neural networks. In that same year, the celebrated two-volume book, Parallel Distributed Processing: Explorations in the Microstructures of Cognition, edited by Rumelhart and McClelland, was published. That book has been a major influence in the use of back-propagation, which has emerged as the most popular learning algorithm for the training of multilayer perceptrons.", "The simplest type of perceptron has a single layer of weights connecting the inputs and output. \u00a0In this way, it can be considered the simplest kind of feed-forward network. In a feed forward network, the information always moves in one direction; it never goes backwards.", "Figure 1\u00a0shows a single-layer perceptron for easier conceptual grounding and clarification into multilayer perceptron (explained ahead). Single layer perceptron \u00a0represents the m weights that are seen as a set of synapses or connecting links between one layer and another layer within the network. This parameter indicates how important each feature \u00a0\u00a0is. Below is the adder function of features of the input multiplied by their respective synaptic connection:", "The bias \u00a0, \u00a0acts as an affine transformation to the output of the adder function \u00a0 giving \u00a0\u00a0, the induced local field as:", "Moving onwards, multi-layer perceptron, also known as feed-forward neural networks, consists of a sequence of layers each fully connected to the next one.", "A multilayer perceptron (MLP) has one or more hidden layers along with the input and output layers, each layer contains several neurons that interconnect with each other by weight links. The number of neurons in the input layer will be the number of attributes in the dataset, neurons in the output layer will be the number of classes given in the dataset.", "Figure 2 shows a multilayer perceptron where we have three layers at least and each layer is connected to the last one. To make the architecture deep, we need to introduce multiple hidden layers.", "Initialization of the parameters, weights and biases plays an important role in determining the final model. There is a lot of literature on initialization strategy.", "A good random initialization strategy can avoid getting stuck at local minima. Local minima problem is when the network gets stuck in the error surface and does not go down while training even when\u00a0there is capacity left for learning.", "Doing\u00a0experiment by using various initialization strategies is out of the scope of this research work.", "The initialization strategy should be selected according to the activation function used. For tanh the initialization interval should be \u00a0where \u00a0is the number of units in the (i-1)-th layer, and is the number of units in the ith layer. Similarly for the sigmoid activation function the initialization interval should be . These initialization strategies ensure that information propagated upwards and backwards in the network at the early stage of training.", "The activation function defines the output of a neuron in terms of the induced local field v as:", "where \u03c6(.)\u00a0is the activation function. There are various types of activation functions, the following are the commonly used ones:", "Figure 2\u00a0indicates that either the neuron is fully active or not. However, this function is not differentiable which is quite vital when using the back-propagation algorithm (explained later).", "The sigmoid function is a logistic function bounded by 0 and 1, as with the threshold function, but this activation function is continuous and differentiable.", "where \u03b1 is the slope parameter of the above function. Moreover, it is nonlinear in nature that helps to increase the performance making sure that small changes in the weights and bias causes small changes in the output of the neuron.", "\u00a0This function enables activation functions to range from -1 to +1.", "ReLUs are the smooth approximation to the the sum of many logistic units and produce sparse activity vectors.\u00a0Below is the equation of the function:", "In figure 3, \u00a0\u00a0 is the smooth approximation to the rectifier).", "In 2013, Goodfellow\u00a0found out that the Maxout network using a new activation function is a natural companion to dropout.", "Maxout units facilitate optimization by dropout and improve the accuracy of dropout\u2019s fast approximate model averaging technique.\u00a0 A single maxout unit can be interpreted as making a piece wise linear approximation to an arbitrary convex function.", "Maxout networks learn not just the relationship between hidden units, but also the activation function of each hidden unit. Below is the graphical depiction of how this works:", "Figure 4 shows the Maxout network with 5 visible units, 3 hidden units and 2 pieces for each hidden unit.", "where \u00a0is the mean vector of size of the input obtained by accessing the matrix W\u00a0\u2208 \u00a0 at the second coordinate i and third coordinate j . The number of intermediate units (\u00a0k )\u00a0is called the number of pieces used by the Maxout nets.", "The back-propagation algorithm can be used to train feed forward neural networks or multilayer perceptrons. It is a method to minimize the cost function by changing weights and biases in the network. To learn and make better predictions, a number of epochs (training cycles) are executed where the error determined by the cost function is backward propagated by gradient descent until a sufficiently small error is achieved.", "Let\u2019s say in 100-sized mini-batch, 100 training examples are shown to the learning algorithm and weights are updated accordingly. After all mini-batches are presented sequentially,\u00a0the\u00a0average of accuracy levels and training cost levels are calculated for each epoch.", "Stochastic gradient descent is used in the real-time on-line processing, where the parameters are updated while presenting only one training example, and so average of accuracy levels and training costs are taken for the entire training dataset at each epoch.", "In this method all the training examples are shown to the learning algorithm and the weights are updated.", "There are various cost functions. Below are some examples:", "\u00a0 where \u00a0is the predicted output\u00a0\u00a0is the actual output", "where the f\u00a0 function is the model\u2019s predicted probability for the input \u00a0\u00a0label to be , W\u00a0are its parameters, and n \u00a0is the training-batch size.", "NLL is the cost function used in all the experiments of the report.", "where \u00a0is the value of the output is, \u00a0is the value of the feature input, \u03b8\u00a0is the parameters and D is the training set.", "Learning rate controls the change in the weight from one iteration to another. As a general rule, smaller learning rates are considered as stable but cause slower learning. On the other hand higher learning rates can be unstable causing oscillations and numerical errors but speed up the learning.", "Momentum provides inertia to escape local minima; the idea is to simply add a certain fraction of the previous weight update to the current one, helping to avoid becoming stuck in local minima.", "Softmax is a neural transfer function that is generalized form of logistic function implemented in the output layer that turns the vectors \u00a0into the probabilities that add up and constraint to 1.", "For classification, a softmax function may be incorporated in the output layer that will give the probability of each occurring class. Activation function is used to compute the predicted output of each neuron in each layer by using inputs, weights and bias.", "The back propagation method trains the multilayer neural network by modifying its synaptic connection weights between the layers to improve model performance based on the error correction learning function which needs to be continuous and differentiable. The following parameters have been evaluated in the experiments:", "Before 2006, various failed attempts at training deep supervised feed forward neural networks were made that resulted in over-fitting of the performance on the unseen data\u00a0i.e.\u00a0training error reduces\u00a0while validation error increases.", "A deep network usually means an artificial neural network that has more than one hidden layer.\u00a0Training the deep hidden layers required more computational power. Having a greater depth seemed to be better because intuitively neurons can make the use of the work done by the neuron in the layer below resulting in distributed representation of the data.", "Bengio suggests that the neurons in the hidden layers are seen as feature detectors learned by the neuron in the below layer. This result in better generalization that is a subset of neurons learns from data in a specific region of the input space.", "Moreover deeper architectures can be more efficient as fewer computational units are needed to represent the same functions, achieving greater efficiency. The core idea behind the distributed representation is the sharing of statistical strengths where different components of the architecture are re-used for different purposes.", "Deep neural architectures are composed of multiple layers utilizing non-linear operations, such as in neural nets with many hidden layers. There are often various factors of variation in the dataset, like aspects of the data separately and often independently may vary.", "Deep Learning algorithms can capture these factors that explain the statistical variations in the data, and how they interact to generate the kind of data we observe. Lower level abstractions are more directly tied to particular observations; on the other hand higher level ones are more abstract because their connection to perceived data is more remote.", "The focus of deep architecture learning is to automatically discover such abstractions, from low level features to the higher level concepts. It is desirable for the learning algorithms to enable this discovery without manually defining necessary abstractions.", "Training samples in the dataset must be at least as numerous as the variations in the test set otherwise the learning algorithm cannot generalize. Deep Learning methods aim to learn feature hierarchies, composing lower level features into higher level abstractions.", "Deep neural nets with a huge number of parameters are very powerful machine learning systems.\u00a0However, over-fitting is a serious problem in deep networks. Over-fitting is when the validation error starts to go up while the training error declines. Dropout\u00a0is one of the regularization techniques for addressing this problem which is discussed later.", "Today one of the most important factors for the increased success of Deep Learning techniques is advancement in the computing power. Graphical Processing Units (GPU) and cloud computing are crucial for applying Deep Learning to many problems.", "Cloud computing allows clustering of computers and on demand processing that helps to reduce the computation time by paralleling the training of the neural network. GPU\u2019s, on the other hand, are special purpose chips for high performance mathematical calculations, speeding up the computation of matrices.", "In 2006-07, three papers\u00a0revolutionized the deep learning discipline. The key principles in their work were that each layer can be pre-trained by unsupervised learning, done one layer at a time. Finally supervised training by back-propagation of the error is used to fine-tune all the layers, effectively giving better initialization by unsupervised learning than by random initialization.", "One of the unsupervised algorithms is Restricted Boltzmann Machines (RBM) that is used to pre-train deep belief network.\u00a0The RBM is a simplified version of the Boltzmann Machine, inspired by statistical mechanics, which models energy based probabilities for the underlying distributions of the given data sets from which conditional distributions can be derived.", "Boltzmann Machines are bidirectionally connected networks of stochastic processing units of visible units and hidden units. The raw data corresponds to the \u2018visible\u2019 neurons and samples to observed states and the feature detectors correspond to \u2018hidden\u2019 neurons. In a Boltzmann Machine, visible neurons provide the input to the network and the environment in which it operates. During training visible neurons are clamped (set to a defined value) determined by the training data.\u00a0Hidden neurons on the other hand operate freely.", "However, Boltzmann Machine are difficult to train because of its connectivity. An RBM has restricted connectivity to make learning easier; there are no connections between hidden units in a single layer forming a bipartite graph, depicted in figure 2. The advantage of this is that the hidden units are updated independently and in parallel given the visible state.", "These networks are governed by an energy function that determines the probability of the hidden/visible states. Each possible joint configuration of the visible and hidden units has a Hopfield energy determined by the weights and biases. The energies of the joint configurations are optimized by Gibbs sampling that learns the parameters by minimizing the lowest energy function of the RBM.", "In figure 5, left layer represents the visible layer and right layer represents the hidden layer.", "In Deep Belief Network (DBN), RBM is trained by input data with important features of the input data captured by stochastic neurons in the hidden layer. In the second layer the activations of the trained features are treated as input data. The learning process in the second RBM layer can be viewed as learning feature of features. Every time a new layer of features is added to the deep belief network, a variational lower bound on the log-probability of the original training data is improved.", "Figure 6 shows RBM converts its data distribution into a posterior distribution over its hidden units.", "The weights of the RBM are randomly initialized causing the difference in the distribution of p(x) and q(x). During learning, weights are iteratively adjusted to minimize the error between p(x) and q(x). In figure 2 q(x) is the approximate of the original data and p(x) is the original data.", "The rule for adjusting the synaptic weight from neuron one and another is independent of whether both the neurons are visible or hidden or one of each.\u00a0The updated parameters by the layers of RBM are used as initialization in DBN\u2019s that fine-tunes all the layers by supervised training of backpropagation.", "For the IDS data of KDD Cup 1999, it is appropriate to use multimodal (Bernoulli-Gaussian) RBM as KDD Cup 1999 consists of mixed data types, specifically continuous and categorical. In multimodal RBM there are two different channel input layers used in the RBM, one is Gaussian input unit used for continuous features and the other one is Bernoulli input unit layer where binary features are used. Using multimodal RBM is beyond the scope of this research work.", "Recent developments have introduced powerful regularizers to deep networks to reduce over-fitting. In machine learning, regularization is additional information usually introduced in the form of a penalty to penalize complexity of the model that leads to over-fitting.", "Dropout is a regularization technique for deep neural networks introduced by Hinton\u00a0which consists of preventing co-adaptation of feature detectors by randomly turning o\ufb00 a portion of neurons at every training iteration but using the entire network (with weights scaled down) at test time.", "Dropout reduces over-\ufb01tting by being equivalent to training an exponential number of models that share weights. There exists an exponential number of di\ufb00erent dropout con\ufb01gurations for a given training iteration, so a di\ufb00erent model is almost certainly trained every time. At test time, the average of all models is used, which acts as a powerful ensemble method.", "In figure 7, dropout randomly drops the connections between the neural network layer", "In figure 8, at training time the connections are dropped with probability, while at the test time weights are scaled to \u03c1w", "Averaging many models usually has been the key for many winner of the machine learning competitions. Many different types of model are used and then combined to make predictions at test time.", "Random forest is a very powerful bagging algorithm which is created by averaging many decision trees giving them different training sample sets with replacement. It is well known that the decision trees are easy to fit to data and fast at test time so averaging different individual trees by giving them different training sets is affordable.", "However, using the same approach with deep neural networks will prove to be very computationally expensive. It is already costly to train individual deep neural networks and training multiple deep neural networks and then averaging seems to be impractical. Moreover a single network that is efficient at test time is needed rather than having lots of large neural nets.", "Dropout is an efficient way to average many large neural nets. Each time while training the model hidden units can be omitted with some probability as in figure 8, which is usually \u03c1= 0.5, when the training example is presented. As a result a \u2018mean network\u2019 model that has all the outgoing weights halved is used at test time as in figure 4. The mean network is equivalent to taking the geometric mean of the probability distributions over labels predicted by all \u00a0possible networks with a single hidden layer of \u00a0units and \u2018softmax\u2019 output layer.", "As per the is mathematical proof of how dropout can be seen as an ensemble method.", "is the prediction of the \u201censemble\u201d using the geometric mean.", "is the prediction of a single sub model.", "d is the binary vector that tells which inputs to include into the softmax classifier.", "Suppose there are \u00a0different units. There will be 2^N possible assignments to d, and so;", "\u00a0 \u00a0where y is the single and \u00a0\u00a0is the vector of the classes index.", "The sum of the probabilities of the output by a single sub-model is used to normalize\u00a0", ",\u00a0as per the definition of softmax", "So, the predicted probability must be proportional to this. To re-normalize the above expression, it is divided by \u00a0which means the predicted probability distribution is\u00a0", "Another way to view dropout is that, it is able to prevent co-adaption among the feature detectors. Co-adaption of the feature detector means that if a hidden unit knows which other hidden units are present, it can co-adapt with them on the training data. However, on the test dataset complex co-adaptions are likely to fail to generalize.", "Dropout can also be used in the input layer at a lower rate, typically 20% probability. The concept here is the same as de-noising auto encoders developed.\u00a0In this method some of the inputs are omitted. This hurts the training accuracy but improves generalization acting in a similar way as adding noise to the dataset while training.", "In 2013 a variant of dropout is introduced called Drop connect.\u00a0Instead of dropping hidden units with certain probability weights are randomly dropped with certain probability. It has been shown that a Drop connect network seemed to perform better than dropout on the MNIST data set.", "A class imbalance problem arises when one of the classes (minority class) is heavily under-represented in comparison to the other classes (majority class). This problem has real world significance where it is costly to misclassify minority classes such as detecting anomalous activities like fraud or intrusion. There are various techniques to deal with the class imbalance problem such as explained below:", "One widely used approach to address the class imbalance problem is resampling of the data set. The sampling method involves pre-processing and balances the training data set by adjusting the prior distribution for minority and majority classes. SMOTE is an over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples rather than by over-sampling with replacement.", "It has been suggested that oversampling the minority class by replacement does not improve the results significantly rather it tends to over-fit the classification of the minority class. Instead the SMOTE algorithm operates in \u2018feature space\u2019 rather than \u2018data space\u2019. It creates synthetic samples by oversampling the minority class which tends to generalize better.", "The idea is inspired by creating extra training data by operating on real data so that there is more data that helps to generalize prediction.", "In this algorithm firstly \u00a0nearest neighbours are computed for the minority class. Then, synthetic samples of the minority class are computed in the following manner: a random number of nearest neighbours is chosen and distance between that neighbour and the original minority class data point is taken.", "This distance is multiplied by a random number between 0 and 1 and adds the result to the feature vector of the original minority class data as an additional sample, thus creating synthetic minority class samples.", "Cost sensitivity learning seems to be quite an effective way to address the class imbalance for classification problems. Three cost sensitive methods have been described that are specific to neural networks.", "Incorporate the prior probabilities of the class in the output layer of the neural network while testing unseen examples", "Adjusted learning rates based on the costs. Higher learning rates should be assigned to examples with high misclassifications costs making a larger impact on the weight changes for those examples", "Modifying the mean square error function. As a result, the learning done by backpropagation will minimize misclassification costs. The new error function is:", "with the cost factor being K[i,j].", "This new error function results in a new delta rule used in the updating of the weights of the network:", "where the first equation represents the error function for output neurons and the second equation represents the error function for hidden neurons.", "If you are not comfortable with math, the mathematical functions explained above might seem intimidating to you. Therefore, you are advised to undergo online courses on algebra and integrals.", "In this article, we discussed the core concepts of deep learning such as gradient descent, backpropagation algorithm, cost function etc and their respective role in building a\u00a0robust deep learning model. This article is a result of our research work done on deep learning. Hope you found this article helpful.\u00a0And to gain expertise in working in neural network try out the\u00a0deep learning practice problem \u2013\u00a0Identify the Digits.", "Have you done any research \u00a0of a similar topics ? Let us know your suggestions / opinions on\u00a0building powerful deep learning models.", " Notify me of follow-up comments by email.", " Notify me of new posts by email.", "Make Money While Sleeping: Side Hustles to Generate Passive Income..", "Google Bard Learnt Bengali on Its Own: Sundar Pichai", "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your..", "Understand Random Forest Algorithms With Examples (Updated 2023)", " A verification link has been sent to your email id ", " If you have not recieved the link please goto\nSign Up  page again\n", "This email id is not registered with us. Please enter your registered email id."], "all_outgoing_urls": [{"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Home"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": ""}, {"url": "https://courses.analyticsvidhya.com/courses/Machine-Learning-Certification-Course-for-Beginners?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Machine Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/getting-started-with-neural-networks?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "Deep Learning"}, {"url": "https://courses.analyticsvidhya.com/courses/Intro-to-NLP?utm_source=blog_navbar&utm_medium=start_here_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/guide/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Guides"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/?utm_source=blog_navbar&utm_medium=machine_learning_button", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/?utm_source=blog_navbar&utm_medium=deep_learning_button", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/nlp/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "NLP"}, {"url": "https://www.analyticsvidhya.com/blog/category/computer-vision/?utm_source=blog_navbar&utm_medium=article_button", "anchor_text": "Computer Vision"}, {"url": "https://www.analyticsvidhya.com/blog/category/data-visualization/?utm_source=blog_navbar&utm_medium=_button", "anchor_text": "Data Visualization"}, {"url": "https://www.analyticsvidhya.com/blog/category/interview-questsions/?utm_source=blog_navbar&utm_medium=career_button", "anchor_text": "Interview Questions"}, {"url": "https://www.analyticsvidhya.com/blog/category/infographics/?utm-source=blog-navbar", "anchor_text": "Infographics"}, {"url": "https://jobsnew.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "Jobs"}, {"url": "https://www.analyticsvidhya.com/blog/category/podcast/?utm-source=blog-navbar", "anchor_text": "Podcasts"}, {"url": "https://courses.analyticsvidhya.com/courses/ebook-machine-learning-simplified?utm_source=bolg-navbar&utm_medium=homepage&utm_campaign=ebook", "anchor_text": "E-Books"}, {"url": "https://www.analyticsvidhya.com/corporate/?utm-source=blog-navbar", "anchor_text": "For Companies"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2019/?utm-source=blog-navbar", "anchor_text": "Datahack Summit"}, {"url": "https://dsat.analyticsvidhya.com/?utm-source=blog-navbar", "anchor_text": "DSAT"}, {"url": "https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/?utm-source=blog-navbar", "anchor_text": "Glossary"}, {"url": "https://www.analyticsvidhya.com/blog-archive/?utm-source=blog-navbar", "anchor_text": "Archive"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_navbar&utm_medium=blackbelt_button", "anchor_text": "Certified AI & ML BlackBelt Plus"}, {"url": "https://bootcamp.analyticsvidhya.com/?utm_source=blog_navbar&utm_medium=bootcamp_button", "anchor_text": "Data Science Immersive Bootcamp"}, {"url": "https://courses.analyticsvidhya.com/collections?utm_source=blog_navbar&utm_medium=all_courses_button", "anchor_text": "All Courses"}, {"url": "https://datahack.analyticsvidhya.com/blogathon/?utm_source=blog&utm_medium=nav_bar", "anchor_text": "Blogathon"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=Blogs&utm_medium=Nav_Bar", "anchor_text": "Conference"}, {"url": "https://lekhak.analyticsvidhya.com/write/", "anchor_text": "Write an Article"}, {"url": "https://www.analyticsvidhya.com/creators-club/?utm-medium=blog-navbar&utm_source=creator_club_button", "anchor_text": "Creators Club"}, {"url": "https://id.analyticsvidhya.com/accounts/profile/", "anchor_text": "Manage your AV Account"}, {"url": "https://datahack.analyticsvidhya.com/user/?utm-source=blog-navbar", "anchor_text": "My Hackathons"}, {"url": "https://profile.analyticsvidhya.com/accounts/bookmarks/", "anchor_text": "My Bookmarks"}, {"url": "https://courses.analyticsvidhya.com/enrollments/?utm-source=blog-navbar", "anchor_text": "My Courses"}, {"url": "https://jobsnew.analyticsvidhya.com/jobs/myactive/?utm-source=blog-navbar", "anchor_text": "My Applied Jobs"}, {"url": "http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F08%2Fevolution-core-concepts-deep-learning-neural-networks%2F", "anchor_text": "Facebook"}, {"url": "http://twitter.com/share?text=The Evolution and Core Concepts of Deep Learning & Neural Networks&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F08%2Fevolution-core-concepts-deep-learning-neural-networks%2F", "anchor_text": "Twitter"}, {"url": "http://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fwww.analyticsvidhya.com%2Fblog%2F2016%2F08%2Fevolution-core-concepts-deep-learning-neural-networks%2F", "anchor_text": "Linkedin"}, {"url": "https://www.analyticsvidhya.com/blog/author/guest-blog/", "anchor_text": "guest_blog"}, {"url": "https://www.analyticsvidhya.com/blog/category/algorithm/", "anchor_text": "Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/category/beginner/", "anchor_text": "Beginner"}, {"url": "https://www.analyticsvidhya.com/blog/category/deep-learning/", "anchor_text": "Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/category/machine-learning/", "anchor_text": "Machine Learning"}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/SLP.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.5.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/Eq-10.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.3.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-10.4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/MLP-3.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.6.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.7.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq4.5.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq7.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq-6.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-5.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq8.3.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.3.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.5.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.6.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.7.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq9.8.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq10.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq11.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq12.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-6.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-7.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-8.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-9.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.2.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.3.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.4.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.5.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq15.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.7.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.8.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.10.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.11.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.12.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.13.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.14-1.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/eq13.15.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-12.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-13.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-10.png", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/wp-content/uploads/2016/07/image-14.png", "anchor_text": ""}, {"url": "https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/", "anchor_text": "Identify the Digits"}, {"url": "https://www.analyticsvidhya.com/cdn-cgi/l/email-protection", "anchor_text": "[email protected]"}, {"url": "https://www.analyticsvidhya.com/cdn-cgi/l/email-protection", "anchor_text": "[email protected]"}, {"url": "https://www.analyticsvidhya.com/about-me/write/", "anchor_text": "posting your blog"}, {"url": "https://www.analyticsvidhya.com/blog/tag/backpropagation-algorithm/", "anchor_text": "backpropagation algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/tag/big-data/", "anchor_text": "Big data"}, {"url": "https://www.analyticsvidhya.com/blog/tag/boltzmann-machine/", "anchor_text": "boltzmann machine"}, {"url": "https://www.analyticsvidhya.com/blog/tag/core-analytics/", "anchor_text": "Core analytics"}, {"url": "https://www.analyticsvidhya.com/blog/tag/cost-function/", "anchor_text": "cost function"}, {"url": "https://www.analyticsvidhya.com/blog/tag/deep-belief-networks/", "anchor_text": "deep belief networks"}, {"url": "https://www.analyticsvidhya.com/blog/tag/deep-learning/", "anchor_text": "deep learning"}, {"url": "https://www.analyticsvidhya.com/blog/tag/gradient-descent/", "anchor_text": "gradient descent"}, {"url": "https://www.analyticsvidhya.com/blog/tag/imbalanced-classification/", "anchor_text": "imbalanced classification"}, {"url": "https://www.analyticsvidhya.com/blog/tag/learning-rate/", "anchor_text": "learning rate"}, {"url": "https://www.analyticsvidhya.com/blog/tag/momentum/", "anchor_text": "momentum"}, {"url": "https://www.analyticsvidhya.com/blog/tag/multilayer-perceptron/", "anchor_text": "multilayer perceptron"}, {"url": "https://www.analyticsvidhya.com/blog/tag/neural-networks/", "anchor_text": "neural networks"}, {"url": "https://www.analyticsvidhya.com/blog/tag/single-layer-perceptron/", "anchor_text": "single layer perceptron"}, {"url": "https://www.analyticsvidhya.com/blog/tag/smote/", "anchor_text": "SMOTE"}, {"url": "https://www.analyticsvidhya.com/blog/tag/softmax/", "anchor_text": "softmax"}, {"url": "https://www.analyticsvidhya.com/datahack-summit-2023/?utm_source=blog_india&utm_medium=side_banner&utm_campaign=27-Apr-2023||&utm_content=generativeAI", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=blog_outside_india&utm_medium=side_banner&utm_campaign=24-Mar-2023||&utm_content=project#ReinforceProject", "anchor_text": ""}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=ReadingList&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/a-comprehensive-tutorial-on-deep-learning-part-2/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "What is Deep Learning?"}, {"url": "https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "DL vs ML vs AI"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Why Deep Learning is So Popular?"}, {"url": "https://www.analyticsvidhya.com/blog/2017/02/6-deep-learning-applications-beginner-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Applications of Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2020/12/mlp-multilayer-perceptron-simple-overview/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Multi Layer Perceptron"}, {"url": "https://www.analyticsvidhya.com/blog/2017/09/creating-visualizing-neural-network-in-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Visualizing the Neural Network"}, {"url": "https://www.analyticsvidhya.com/blog/2017/07/debugging-neural-network-with-tensorboard/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Understanding Decision Boundary"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/estimation-of-neurons-and-forward-propagation-in-neural-net/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Forward and Backward Propagation Intuition"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/understanding-gradient-descent-algorithm-and-the-maths-behind-it/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Gradient Descent Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/understanding-gradient-descent-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Variants of Gradient Descent Algorithm"}, {"url": "https://www.analyticsvidhya.com/blog/2022/06/understanding-loss-function-in-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Loss Function"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/guide-for-loss-function-in-tensorflow/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Binary and Categorical Cross Entropy"}, {"url": "https://www.analyticsvidhya.com/blog/2022/03/a-basic-introduction-to-activation-function-in-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Why Do We Need Activation Functions?"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/activation-functions-and-their-derivatives-a-quick-complete-guide/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Linear Activation Function"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/neural-networks-and-activation-function/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Sigmoid and Tanh Function"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/introduction-to-softmax-for-neural-network/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Softmax"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "How to Select Right Activation Function?"}, {"url": "https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Perceptron"}, {"url": "https://www.analyticsvidhya.com/blog/2022/10/multi-layer-perceptrons-notations-and-trainable-parameters/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Weights in Perceptron"}, {"url": "https://www.analyticsvidhya.com/blog/2014/10/ann-work-simplified/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Artificial Neural Network"}, {"url": "https://www.analyticsvidhya.com/blog/2020/07/neural-networks-from-scratch-in-python-and-r/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Understanding Forward Propagation Mathematically"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/how-does-backward-propagation-work-in-neural-networks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Understand Backward Propagation Mathematically"}, {"url": "https://www.analyticsvidhya.com/blog/2016/04/neural-networks-python-theano/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Neural Networks in Python"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Problems with Gradient Descent"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/variants-of-gradient-descent-algorithm/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Gradient Descent with Momentum"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-to-gradient-based-optimizers/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Adagrad and Adadelta"}, {"url": "https://www.analyticsvidhya.com/blog/2021/08/quick-start-with-tensorflow-callbacks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Learning Rate Schedulers"}, {"url": "https://www.analyticsvidhya.com/blog/2019/03/deep-learning-frameworks-comparison/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Overview of Deep Learning Frameworks"}, {"url": "https://www.analyticsvidhya.com/blog/2021/10/implementing-artificial-neural-networkclassification-in-python-from-scratch/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Neural Networks using Keras"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/understanding-sequential-vs-functional-api-in-keras/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Functional API in Keras"}, {"url": "https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Neural Networks using Keras"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Hyperparameter Tuning of MLP in Keras"}, {"url": "https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Understanding Early stopping"}, {"url": "https://www.analyticsvidhya.com/blog/2019/11/4-tricks-improve-deep-learning-model-performance/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Understanding Dropout"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Vanishing and Exploding Gradients"}, {"url": "https://www.analyticsvidhya.com/blog/2021/05/how-to-initialize-weights-in-neural-networks/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Weights Initialization Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Weight Initializing Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Batch Normalization"}, {"url": "https://www.analyticsvidhya.com/blog/2020/08/image-augmentation-on-the-fly-using-keras-imagedatagenerator/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Image Augmentation Techniques"}, {"url": "https://www.analyticsvidhya.com/blog/2021/12/step-by-step-guide-to-build-image-caption-generator-using-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Image Generator and Fit Generator"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/improving-your-deep-learning-model-using-model-checkpointing-implementation-part-2/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Model Checkpointing"}, {"url": "https://www.analyticsvidhya.com/blog/2021/03/improving-your-deep-learning-model-using-model-checkpointing-part-1/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Model Checkpointing"}, {"url": "https://www.analyticsvidhya.com/blog/2016/10/investigation-on-handling-structured-imbalanced-datasets-with-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Dealing with Class Imbalance"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/ensemble-deep-learning-an-ensemble-of-deep-learning-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Ensemble Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to GPU and TPU"}, {"url": "https://www.analyticsvidhya.com/blog/2020/09/tensorflow-object-detection-1-0-2-0-train-export-optimize-tensorrt-infer-jetson-nano/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "TensorRT"}, {"url": "https://www.analyticsvidhya.com/blog/2021/04/understanding-supervised-and-unsupervised-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Unsupervised Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2018/05/essentials-of-deep-learning-trudging-into-unsupervised-deep-learning/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "How to Solve Unsupervised Learning Problems?"}, {"url": "https://www.analyticsvidhya.com/blog/2021/01/auto-encoders-for-computer-vision-an-endless-world-of-possibilities/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Autoencoders"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/dimensionality-reduction-using-autoencoders-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implementing Autoencoders"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/a-beginners-guide-to-codeless-deep-learning-mnist-digit-classification/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "A Beginners Guide to Codeless Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2020/03/tensorflow-serving-deploy-deep-learning-models/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "TensorFlow Serving"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/creating-android-ml-app-kivymd/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Build Deep Learning Models for Android"}, {"url": "https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to PyTorch and Tensors"}, {"url": "https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Mathematical and Matrix Operations in PyTorch"}, {"url": "https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Important PyTorch Modules"}, {"url": "https://www.analyticsvidhya.com/blog/2021/09/convolutional-neural-network-pytorch-implementation-on-cifar10-dataset/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Implement CNN in PyTorch"}, {"url": "https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Transfer Learning in PyTorch"}, {"url": "https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Working with Text Data in PyTorch"}, {"url": "https://www.analyticsvidhya.com/blog/2021/07/understanding-rnn-step-by-step-with-pytorch/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Building a RNN model in PyTorch"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/complete-guide-on-how-to-use-autoencoders-in-python/?utm_source=reading_list&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Autoencoders in PyTorch"}, {"url": "https://blackbelt.analyticsvidhya.com/plus?utm_source=RelatedArticles&utm_medium=blog", "anchor_text": "Become a full stack data scientist"}, {"url": "https://www.analyticsvidhya.com/blog/2023/02/introduction-to-neural-network-build-your-own-network/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Introduction to Neural Network: Build your own Network"}, {"url": "https://www.analyticsvidhya.com/blog/2023/02/neuro-symbolic-ai-enhancing-common-sense-in-ai/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Neuro Symbolic AI: Enhancing Common Sense in AI"}, {"url": "https://www.analyticsvidhya.com/blog/2023/01/why-is-sigmoid-function-important-in-artificial-neural-networks/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Why is Sigmoid Function Important in Artificial Neural Networks?"}, {"url": "https://www.analyticsvidhya.com/blog/2023/03/machine-learning-libraries-in-2023/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "A Comprehensive Guide to Top Machine Learning Libraries in 2023"}, {"url": "https://www.analyticsvidhya.com/blog/2022/11/advance-guide-on-interview-questions-of-deep-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "Advance Guide on Interview Questions of Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/?utm_source=related_WP&utm_medium=https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/", "anchor_text": "CNN vs. RNN vs. ANN \u2013 Analyzing 3 Types of Neural Networks in Deep Learning"}, {"url": "https://www.analyticsvidhya.com/blog/author/guest-blog/", "anchor_text": "guest_blog"}, {"url": "https://www.analyticsvidhya.com/blog/creators/?utm-medium=blog-footer&utm_source=top-authors", "anchor_text": "view more"}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/blog/2016/08/tutorial-data-science-command-line-scikit-learn/", "anchor_text": "Tutorial \u2013 Data Science at Command Line with R & Python (Scikit Learn)"}, {"url": "https://www.analyticsvidhya.com/blog/2016/08/innovation-in-analytics-education-great-lakes-using-mentored-learning-for-online-courses/", "anchor_text": "Innovation in Analytics Education: Great Lakes using mentored learning for Online Courses"}, {"url": "https://www.analyticsvidhya.com/blog/2016/08/evolution-core-concepts-deep-learning-neural-networks/#respond", "anchor_text": "Cancel reply"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/how-to-make-money-with-chatgpt/", "anchor_text": "Make Money While Sleeping: Side Hustles to Generate Passive Income.."}, {"url": "https://www.analyticsvidhya.com/blog/author/aayush1/", "anchor_text": "Aayush Tyagi -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/google-bard-learnt-bengali-on-its-own-sundar-pichai/", "anchor_text": "Google Bard Learnt Bengali on Its Own: Sundar Pichai"}, {"url": "https://www.analyticsvidhya.com/blog/author/yana_khare/", "anchor_text": "Yana Khare -"}, {"url": "https://www.analyticsvidhya.com/blog/2023/04/freedomgpt-personal-bold-and-uncensored-chatbot-running-locally-on-your-pc/", "anchor_text": "FreedomGPT: Personal, Bold and Uncensored Chatbot Running Locally on Your.."}, {"url": "https://www.analyticsvidhya.com/blog/author/sabreena/", "anchor_text": "K.sabreena -"}, {"url": "https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/", "anchor_text": "Understand Random Forest Algorithms With Examples (Updated 2023)"}, {"url": "https://www.analyticsvidhya.com/blog/author/sruthi94/", "anchor_text": "Sruthi E R -"}, {"url": "https://www.analyticsvidhya.com/", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.analyticsvidhya.android", "anchor_text": ""}, {"url": "https://apps.apple.com/us/app/analytics-vidhya/id1470025572", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/about/", "anchor_text": "About Us"}, {"url": "https://www.analyticsvidhya.com/team/", "anchor_text": "Our Team"}, {"url": "https://www.analyticsvidhya.com/careers/", "anchor_text": "Careers"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Contact us"}, {"url": "https://www.analyticsvidhya.com/blog/", "anchor_text": "Blog"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hackathon"}, {"url": "https://discuss.analyticsvidhya.com/", "anchor_text": "Discussions"}, {"url": "https://jobsnew.analyticsvidhya.com/", "anchor_text": "Apply Jobs"}, {"url": "https://www.analyticsvidhya.com/corporate/", "anchor_text": "Post Jobs"}, {"url": "https://courses.analyticsvidhya.com/", "anchor_text": "Trainings"}, {"url": "https://datahack.analyticsvidhya.com/", "anchor_text": "Hiring Hackathons"}, {"url": "https://www.analyticsvidhya.com/contact/", "anchor_text": "Advertising"}, {"url": "https://www.facebook.com/AnalyticsVidhya/", "anchor_text": ""}, {"url": "https://www.linkedin.com/company/analytics-vidhya/", "anchor_text": ""}, {"url": "https://www.youtube.com/channel/UCH6gDteHtH4hg3o2343iObA", "anchor_text": ""}, {"url": "https://twitter.com/analyticsvidhya", "anchor_text": ""}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}, {"url": "https://www.analyticsvidhya.com/refund-policy/", "anchor_text": "Refund Policy"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/terms", "anchor_text": "I accept the Terms and Conditions"}, {"url": "https://www.analyticsvidhya.com/privacy-policy/", "anchor_text": "Privacy Policy"}, {"url": "https://www.analyticsvidhya.com/terms/", "anchor_text": "Terms of Use"}]}, "scrape_status": {"code": "1"}}