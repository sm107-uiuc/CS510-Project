{"url": "https://medium.com/@jdwittenauer/machine-learning-exercises-in-python-part-1-60db0df846a4", "time": 1683019472.080396, "path": "medium.com/@jdwittenauer/machine-learning-exercises-in-python-part-1-60db0df846a4/", "webpage": {"metadata": {"title": "Machine Learning Exercises In Python, Part 1 | by John Wittenauer | Medium", "h1": "Machine Learning Exercises In Python, Part 1", "description": "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here. Part 1 \u2014\u2026"}, "outgoing_paragraph_urls": [{"url": "http://www.johnwittenauer.net", "anchor_text": "Curious Insight", "paragraph_index": 0}, {"url": "https://www.coursera.org/course/ml", "anchor_text": "machine learning", "paragraph_index": 1}, {"url": "https://github.com/jdwittenauer/ipython-notebooks", "anchor_text": "here", "paragraph_index": 1}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/", "anchor_text": "Part 1 \u2014 Simple Linear Regression", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/", "anchor_text": "Part 2 \u2014 Multivariate Linear Regression", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/", "anchor_text": "Part 3 \u2014 Logistic Regression", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/", "anchor_text": "Part 4 \u2014 Multivariate Logistic Regression", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/", "anchor_text": "Part 5 \u2014 Neural Networks", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/", "anchor_text": "Part 6 \u2014 Support Vector Machines", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/", "anchor_text": "Part 7 \u2014 K-Means Clustering & PCA", "paragraph_index": 2}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/", "anchor_text": "Part 8 \u2014 Anomaly Detection & Recommendation", "paragraph_index": 2}, {"url": "https://www.coursera.org/course/ml", "anchor_text": "Machine Learning", "paragraph_index": 3}, {"url": "https://github.com/jdwittenauer/ipython-notebooks", "anchor_text": "my IPython repo on Github", "paragraph_index": 4}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "here", "paragraph_index": 9}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "here", "paragraph_index": 11}, {"url": "http://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "this article", "paragraph_index": 24}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "Curious Insight", "paragraph_index": 34}, {"url": "https://twitter.com/jdwittenauer", "anchor_text": "twitter", "paragraph_index": 35}], "all_paragraphs": ["This content originally appeared on Curious Insight", "This post is part of a series covering the exercises from Andrew Ng\u2019s machine learning class on Coursera. The original code, exercise text, and data files for this post are available here.", "Part 1 \u2014 Simple Linear RegressionPart 2 \u2014 Multivariate Linear RegressionPart 3 \u2014 Logistic RegressionPart 4 \u2014 Multivariate Logistic RegressionPart 5 \u2014 Neural NetworksPart 6 \u2014 Support Vector MachinesPart 7 \u2014 K-Means Clustering & PCAPart 8 \u2014 Anomaly Detection & Recommendation", "One of the pivotal moments in my professional development this year came when I discovered Coursera. I\u2019d heard of the \u201cMOOC\u201d phenomenon but had not had the time to dive in and take a class. Earlier this year I finally pulled the trigger and signed up for Andrew Ng\u2019s Machine Learning class. I completed the whole thing from start to finish, including all of the programming exercises. The experience opened my eyes to the power of this type of education platform, and I\u2019ve been hooked ever since.", "This blog post will be the first in a series covering the programming exercises from Andrew\u2019s class. One aspect of the course that I didn\u2019t particularly care for was the use of Octave for assignments. Although Octave/Matlab is a fine platform, most real-world \u201cdata science\u201d is done in either R or Python (certainly there are other languages and tools being used, but these two are unquestionably at the top of the list). Since I\u2019m trying to develop my Python skills, I decided to start working through the exercises from scratch in Python. The full source code is available at my IPython repo on Github. You\u2019ll also find the data used in these exercises and the original exercise PDFs in sub-folders off the root directory if you\u2019re interested.", "While I can explain some of the concepts involved in this exercise along the way, it\u2019s impossible for me to convey all the information you might need to fully comprehend it. If you\u2019re really interested in machine learning but haven\u2019t been exposed to it yet, I encourage you to check out the class (it\u2019s completely free and there\u2019s no commitment whatsoever). With that, let\u2019s get started!", "In the first part of exercise 1, we\u2019re tasked with implementing simple linear regression to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities. You\u2019d like to figure out what the expected profit of a new food truck might be given only the population of the city that it would be placed in.", "Let\u2019s start by examining the data which is in a file called \u201cex1data1.txt\u201d in the \u201cdata\u201d directory of my repository above. First we need to import a few libraries.", "Now let\u2019s get things rolling. We can use pandas to load the data into a data frame and display the first few rows using the \u201chead\u201d function.", "(Note: Medium can\u2019t render tables \u2014 the full example is here)", "Another useful function that pandas provides out-of-the-box is the \u201cdescribe\u201d function, which calculates some basic statistics on a data set. This is helpful to get a \u201cfeel\u201d for the data during the exploratory analysis stage of a project.", "(Note: Medium can\u2019t render tables \u2014 the full example is here)", "Examining stats about your data can be helpful, but sometimes you need to find ways to visualize it too. Fortunately this data set only has one dependent variable, so we can toss it in a scatter plot to get a better idea of what it looks like. We can use the \u201cplot\u201d function provided by pandas for this, which is really just a wrapper for matplotlib.", "It really helps to actually look at what\u2019s going on, doesn\u2019t it? We can clearly see that there\u2019s a cluster of values around cities with smaller populations, and a somewhat linear trend of increasing profit as the size of the city increases. Now let\u2019s get to the fun part \u2014 implementing a linear regression algorithm in python from scratch!", "If you\u2019re not familiar with linear regression, it\u2019s an approach to modeling the relationship between a dependent variable and one or more independent variables (if there\u2019s one independent variable then it\u2019s called simple linear regression, and if there\u2019s more than one independent variable then it\u2019s called multiple linear regression). There are lots of different types and variances of linear regression that are outside the scope of this discussion so I won\u2019t go into that here, but to put it simply \u2014 we\u2019re trying to create a *linear model* of the data X, using some number of parameters theta, that describes the variance of the data such that given a new data point that\u2019s not in X, we could accurately predict what the outcome y would be without actually knowing what y is.", "In this implementation we\u2019re going to use an optimization technique called gradient descent to find the parameters theta. If you\u2019re familiar with linear algebra, you may be aware that there\u2019s another way to find the optimal parameters for a linear model called the \u201cnormal equation\u201d which basically solves the problem at once using a series of matrix calculations. However, the issue with this approach is that it doesn\u2019t scale very well for large data sets. In contrast, we can use variants of gradient descent and other optimization methods to scale to data sets of unlimited size, so for machine learning problems this approach is more practical.", "Okay, that\u2019s enough theory. Let\u2019s write some code. The first thing we need is a cost function. The cost function evaluates the quality of our model by calculating the error between our model\u2019s prediction for a data point, using the model parameters, and the actual data point. For example, if the population for a given city is 4 and we predicted that it was 7, our error is (7\u20134)^2 = 3^2 = 9 (assuming an L2 or \u201cleast squares\u201d loss function). We do this for each data point in X and sum the result to get the cost. Here\u2019s the function:", "Notice that there are no loops. We\u2019re taking advantage of numpy\u2019s linear algrebra capabilities to compute the result as a series of matrix operations. This is far more computationally efficient than an unoptimizted \u201cfor\u201d loop.", "In order to make this cost function work seamlessly with the pandas data frame we created above, we need to do some manipulating. First, we need to insert a column of 1s at the beginning of the data frame in order to make the matrix operations work correctly (I won\u2019t go into detail on why this is needed, but it\u2019s in the exercise text if you\u2019re interested \u2014 basically it accounts for the intercept term in the linear equation). Second, we need to separate our data into independent variables X and our dependent variable y.", "Finally, we\u2019re going to convert our data frames to numpy matrices and instantiate a parameter matirx.", "One useful trick to remember when debugging matrix operations is to look at the shape of the matrices you\u2019re dealing with. It\u2019s also helpful to remember when walking through the steps in your head that matrix multiplications look like (i x j) * (j x k) = (i x k), where i, j, and k are the shapes of the relative dimensions of the matrix.", "Okay, so now we can try out our cost function. Remember the parameters were initialized to 0 so the solution isn\u2019t optimal yet, but we can see if it works.", "So far so good. Now we need to define a function to perform gradient descent on the parameters *theta* using the update rules defined in the exercise text. Here\u2019s the function for gradient descent:", "The idea with gradient descent is that for each iteration, we compute the gradient of the error term in order to figure out the appropriate direction to move our parameter vector. In other words, we\u2019re calculating the changes to make to our parameters in order to reduce the error, thus bringing our solution closer to the optimal solution (i.e best fit).", "This is a fairly complex topic and I could easily devote a whole blog post just to discussing gradient descent. If you\u2019re interested in learning more, I would recommend starting with this article and branching out from there.", "Once again we\u2019re relying on numpy and linear algebra for our solution. You may notice that my implementation is not 100% optimal. In particular, there\u2019s a way to get rid of that inner loop and update all of the parameters at once. I\u2019ll leave it up to the reader to figure it out for now (I\u2019ll cover it in a later post).", "Now that we\u2019ve got a way to evaluate solutions, and a way to find a good solution, it\u2019s time to apply this to our data set.", "Note that we\u2019ve initialized a few new variables here. If you look closely at the gradient descent function, it has parameters called alpha and iters. Alpha is the learning rate \u2014 it\u2019s a factor in the update rule for the parameters that helps determine how quickly the algorithm will converge to the optimal solution. Iters is just the number of iterations. There is no hard and fast rule for how to initialize these parameters and typically some trial-and-error is involved.", "We now have a parameter vector descibing what we believe is the optimal linear model for our data set. One quick way to evaluate just how good our regression model is might be to look at the total error of our new solution on the data set:", "That\u2019s certainly a lot better than 32, but it\u2019s not a very intuitive way to look at it. Fortunately we have some other techniques at our disposal.", "We\u2019re now going to use matplotlib to visualize our solution. Remember the scatter plot from before? Let\u2019s overlay a line representing our model on top of a scatter plot of the data to see how well it fits. We can use numpy\u2019s \u201clinspace\u201d function to create an evenly-spaced series of points within the range of our data, and then \u201cevaluate\u201d those points using our model to see what the expected profit would be. We can then turn it into a line graph and plot it.", "Not bad! Our solution looks like and optimal linear model of the data set. Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.", "Notice that the cost always decreases \u2014 this is an example of what\u2019s called a convex optimization problem. If you were to plot the entire solution space for the problem (i.e. plot the cost as a function of the model parameters for every possible value of the parameters) you would see that it looks like a \u201cbowl\u201d shape with a \u201cbasin\u201d representing the optimal solution.", "That\u2019s all for now! In part 2 we\u2019ll finish off the first exercise by extending this example to more than 1 variable. I\u2019ll also show how the above solution can be reached by using a popular machine learning library called scikit-learn.", "To comment on this article, check out the original post at Curious Insight", "Follow me on twitter to get new post updates", "Data scientist, engineer, author, investor, entrepreneur", "Data scientist, engineer, author, investor, entrepreneur"], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdwittenauer?source=post_page-----60db0df846a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdwittenauer?source=post_page-----60db0df846a4--------------------------------", "anchor_text": "John Wittenauer"}, {"url": "http://www.johnwittenauer.net", "anchor_text": "Curious Insight"}, {"url": "https://www.coursera.org/course/ml", "anchor_text": "machine learning"}, {"url": "https://github.com/jdwittenauer/ipython-notebooks", "anchor_text": "here"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/", "anchor_text": "Part 1 \u2014 Simple Linear Regression"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-2/", "anchor_text": "Part 2 \u2014 Multivariate Linear Regression"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-3/", "anchor_text": "Part 3 \u2014 Logistic Regression"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/", "anchor_text": "Part 4 \u2014 Multivariate Logistic Regression"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-5/", "anchor_text": "Part 5 \u2014 Neural Networks"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-6/", "anchor_text": "Part 6 \u2014 Support Vector Machines"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/", "anchor_text": "Part 7 \u2014 K-Means Clustering & PCA"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-8/", "anchor_text": "Part 8 \u2014 Anomaly Detection & Recommendation"}, {"url": "https://www.coursera.org/course/ml", "anchor_text": "Machine Learning"}, {"url": "https://github.com/jdwittenauer/ipython-notebooks", "anchor_text": "my IPython repo on Github"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "here"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "here"}, {"url": "http://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "this article"}, {"url": "http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1", "anchor_text": "Curious Insight"}, {"url": "https://twitter.com/jdwittenauer", "anchor_text": "twitter"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----60db0df846a4---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/python?source=post_page-----60db0df846a4---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/@jdwittenauer?source=post_page-----60db0df846a4--------------------------------", "anchor_text": "More from John Wittenauer"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a990d50597b&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40jdwittenauer%2Fmachine-learning-exercises-in-python-part-1-60db0df846a4&newsletterV3=8e0612f4b153&newsletterV3Id=8a990d50597b&user=John+Wittenauer&userId=8e0612f4b153&source=-----60db0df846a4---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----60db0df846a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----60db0df846a4--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----60db0df846a4--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----60db0df846a4--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----60db0df846a4--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----60db0df846a4--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----60db0df846a4--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@jdwittenauer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@jdwittenauer?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "John Wittenauer"}, {"url": "https://medium.com/@jdwittenauer/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "240 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F8a990d50597b&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40jdwittenauer%2Fmachine-learning-exercises-in-python-part-1-60db0df846a4&newsletterV3=8e0612f4b153&newsletterV3Id=8a990d50597b&user=John+Wittenauer&userId=8e0612f4b153&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}