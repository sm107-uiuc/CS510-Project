{"url": "https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1", "time": 1682988328.7456708, "path": "medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1/", "webpage": {"metadata": {"title": "Everything You Need to Know About Artificial Neural Networks | by Josh | Technology, Invention, App, and More | Medium", "h1": "Everything You Need to Know About Artificial Neural Networks", "description": "The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we\u2019re learning more about how to improve their systems\u2026"}, "outgoing_paragraph_urls": [{"url": "https://github.com/ryankiros/neural-storyteller", "anchor_text": "programs that can tell stories about pictures", "paragraph_index": 0}, {"url": "https://www.youtube.com/watch?v=3yCAZWdqX_Y", "anchor_text": "cars that are driving themselves", "paragraph_index": 0}, {"url": "http://deepdreamgenerator.com", "anchor_text": "programs that create art", "paragraph_index": 0}, {"url": "http://www.bloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artificial-intelligence", "anchor_text": "this", "paragraph_index": 0}, {"url": "http://josh.ai", "anchor_text": "Josh.ai", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "for example the sigmoid function", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Artificial_neuron", "anchor_text": "McCulloch-Pitts neurons", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Multilayer_perceptron", "anchor_text": "multilayer perceptrons", "paragraph_index": 3}, {"url": "http://www.thetalkingmachines.com/blog/2015/2/26/the-history-of-machine-learning-from-the-inside-out", "anchor_text": "this", "paragraph_index": 4}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "backpropagation", "paragraph_index": 5}, {"url": "http://wizardofodds.com/games/blackjack/card-counting/high-low/", "anchor_text": "low cards are out of the deck", "paragraph_index": 12}, {"url": "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/", "anchor_text": "this tutorial", "paragraph_index": 12}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "this blog post", "paragraph_index": 12}, {"url": "https://github.com/facebook/MemNN", "anchor_text": "Memory Networks", "paragraph_index": 13}, {"url": "http://arxiv.org/pdf/1503.08895v5.pdf", "anchor_text": "this", "paragraph_index": 14}, {"url": "http://colah.github.io/posts/2014-07-Conv-Nets-Modular/", "anchor_text": "here", "paragraph_index": 15}, {"url": "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/", "anchor_text": "this", "paragraph_index": 15}, {"url": "https://www.youtube.com/watch?v=qv6UVOQ0F44", "anchor_text": "this", "paragraph_index": 16}, {"url": "https://www.youtube.com/watch?v=V1eYniJ0Rnk", "anchor_text": "this", "paragraph_index": 16}, {"url": "http://josh.ai", "anchor_text": "Josh.ai", "paragraph_index": 18}, {"url": "http://josh.ai", "anchor_text": "Josh.ai", "paragraph_index": 19}, {"url": "https://www.josh.ai", "anchor_text": "https://josh.ai", "paragraph_index": 19}, {"url": "http://facebook.com/joshdotai", "anchor_text": "http://facebook.com/joshdotai", "paragraph_index": 20}, {"url": "http://twitter.com/joshdotai", "anchor_text": "http://twitter.com/joshdotai", "paragraph_index": 21}], "all_paragraphs": ["The year 2015 was a monumental year in the field of artificial intelligence. Not only are computers learning more and learning faster, but we\u2019re learning more about how to improve their systems. Everything is starting to align, and because of it we\u2019re seeing strides we\u2019ve never thought possible until now. We have programs that can tell stories about pictures. We have cars that are driving themselves. We even have programs that create art. If you want to read more about advancements in 2015, read this article. Here at Josh.ai, with AI technology becoming the core of just about everything we do, we think it\u2019s important to understand some of the common terminology and to get a rough idea of how it all works.", "A lot of the advances in artificial intelligence are new statistical models, but the overwhelming majority of the advances are in a technology called artificial neural networks (ANN). If you\u2019ve read anything about them before, you\u2019ll have read that these ANNs are a very rough model of how the human brain is structured. Take note that there is a difference between artificial neural networks and neural networks. Though most people drop the artificial for the sake of brevity, the word artificial was prepended to the phrase so that people in computational neurobiology could still use the term neural network to refer to their work. Below is a diagram of actual neurons and synapses in the brain compared to artificial ones.", "Fear not if the diagram doesn\u2019t come through very clearly. What\u2019s important to understand here is that in our ANNs we have these units of calculation called neurons. These artificial neurons are connected by synapses which are really just weighted values. What this means is that given a number, a neuron will perform some sort of calculation (for example the sigmoid function), and then the result of this calculation will be multiplied by a weight as it \u201ctravels.\u201d The weighted result can sometimes be the output of your neural network, or as I\u2019ll talk about soon, you can have more neurons configured in layers, which is the basic concept to an idea that we call deep learning.", "Artificial neural networks are not a new concept. In fact, we didn\u2019t even always call them neural networks and they certainly don\u2019t look the same now as they did at their inception. Back during the 1960s we had what was called a perceptron. Perceptrons were made of McCulloch-Pitts neurons. We even had biased perceptrons, and ultimately people started creating multilayer perceptrons, which is synonymous with the general artificial neural network we hear about now.", "But wait, if we\u2019ve had neural networks since the 1960s, why are they just now getting huge? It\u2019s a long story, and I encourage you to listen to this podcast episode to listen to the \u201cfathers\u201d of modern ANNs talk about their perspective of the topic. To quickly summarize, there\u2019s a hand full of factors that kept ANNs from becoming more popular. We didn\u2019t have the computer processing power and we didn\u2019t have the data to train them. Using them was frowned upon due to them having a seemingly arbitrary ability to perform well. Each one of these factors is changing. Our computers are getting faster and more powerful, and with the internet, we have all kinds of data being shared for use.", "You see, I mentioned above that the neurons and synapses perform calculations. The question on your mind should be: \u201cHow do they learn what calculations to perform?\u201d Was I right? The answer is that we need to essentially ask them a large amount of questions, and provide them with answers. This is a field called supervised learning. With enough examples of question-answer pairs, the calculations and values stored at each neuron and synapse are slowly adjusted. Usually this is through a process called backpropagation.", "Imagine you\u2019re walking down a sidewalk and you see a lamp post. You\u2019ve never seen a lamp post before, so you walk right into it and say \u201couch.\u201d The next time you see a lamp post you scoot a few inches to the side and keep walking. This time your shoulder hits the lamp post and again you say \u201couch.\u201d The third time you see a lamp post, you move all the way over to ensure you don\u2019t hit the lamp post. Except now something terrible has happened \u2014 now you\u2019ve walked directly into the path of a mailbox, and you\u2019ve never seen a mailbox before. You walk into it and the whole process happens again. Obviously, this is an oversimplification, but it is effectively what backpropogation does. An artificial neural network is given a multitude of examples and then it tries to get the same answer as the example given. When it is wrong, an error is calculated and the values at each neuron and synapse are propagated backwards through the ANN for the next time. This process takes a LOT of examples. For real world applications, the number of examples can be in the millions.", "Now that we have an understanding of artificial neural networks and somewhat of an understanding in how they work, there\u2019s another question that should be on your mind. How do we know how many neurons we need to use? And why did you bold the word layers earlier? Layers are just sets of neurons. We have an input layer which is the data we provide to the ANN. We have the hidden layers, which is where the magic happens. Lastly, we have the output layer, which is where the finished computations of the network are placed for us to use.", "Layers themselves are just sets of neurons. In the early days of multilayer perceptrons, we originally thought that having just one input layer, one hidden layer, and one output layer was sufficient. It makes sense, right? Given some numbers, you just need one set of computations, and then you get an output. If your ANN wasn\u2019t calculating the correct value, you just added more neurons to the single hidden layer. Eventually, we learned that in doing this we were really just creating a linear mapping from each input to the output. In other words, we learned that a certain input would always map to a certain output. We had no flexibility and really could only handle inputs we\u2019d seen before. This was by no means what we wanted.", "Now introduce deep learning, which is when we have more than one hidden layer. This is one of the reasons we have better ANNs now, because we need hundreds of nodes with tens if not more layers. This leads to a massive amount of variables that we need to keep track of at a time. Advances in parallel programming also allow us to run even larger ANNs in batches. Our artificial neural networks are now getting so large that we can no longer run a single epoch, which is an iteration through the entire network, at once. We need to do everything in batches which are just subsets of the entire network, and once we complete an entire epoch, then we apply the backpropagation.", "Along with now using deep learning, it\u2019s important to know that there are a multitude of different architectures of artificial neural networks. The typical ANN is setup in a way where each neuron is connected to every other neuron in the next layer. These are specifically called feed forward artificial neural networks (even though ANNs are generally all feed forward). We\u2019ve learned that by connecting neurons to other neurons in certain patterns, we can get even better results in specific scenarios.", "Recurrent Neural Networks (RNN) were created to address the flaw in artificial neural networks that didn\u2019t make decisions based on previous knowledge. A typical ANN had learned to make decisions based on context in training, but once it was making decisions for use, the decisions were made independent of each other.", "When would we want something like this? Well, think about playing a game of Blackjack. If you were given a 4 and a 5 to start, you know that 2 low cards are out of the deck. Information like this could help you determine whether or not you should hit. RNNs are very useful in natural language processing since prior words or characters are useful in understanding the context of another word. There are plenty of different implementations, but the intention is always the same. We want to retain information. We can achieve this through having bi-directional RNNs, or we can implement a recurrent hidden layer that gets modified with each feedforward. If you want to learn more about RNNs, check out either this tutorial where you implement an RNN in Python or this blog post where uses for an RNN are more thoroughly explained.", "An honorable mention goes to Memory Networks. The concept is that we need to retain more information than what an RNN or LSTM keeps if we want to understand something like a movie or book where a lot of events might occur that build on each other.", "Sam walks into the kitchen.Sam picks up an apple.Sam walks into the bedroom.Sam drops the apple.Q: Where is the apple.A: BedroomSample taken from this paper.", "Convolutional Neural Networks (CNN), sometimes called LeNets (named after Yann LeCun), are artificial neural networks where the connections between layers appear to be somewhat arbitrary. However, the reason for the synapses to be setup the way they are is to help reduce the number of parameters that need to be optimized. This is done by noting a certain symmetry in how the neurons are connected, and so you can essentially \u201cre-use\u201d neurons to have identical copies without necessarily needing the same number of synapses. CNNs are commonly used in working with images thanks to their ability to recognize patterns in surrounding pixels. There\u2019s redundant information contained when you look at each individual pixel compared to its surrounding pixels, and you can actually compress some of this information thanks to their symmetrical properties. Sounds like the perfect situation for a CNN if you ask me. Christopher Olah has a great blog post about understanding CNNs as well as other types of ANNs which you can find here. Another great resource for understanding CNNs is this blog post.", "The last ANN type that I\u2019m going to talk about is the type called Reinforcement Learning. Reinforcement Learning is a generic term used for the behavior that computers exhibit when trying to maximize a certain reward, which means that it in itself isn\u2019t an artificial neural network architecture. However, you can apply reinforcement learning or genetic algorithms to build an artificial neural network architecture that you might not have thought to use before. A great example and explanation can be found in this video, where YouTube user SethBling creates a reinforcement learning system that builds an artificial neural network architecture that plays a Mario game entirely on its own. Another successful example of reinforcement learning can be seen in this video where the company DeepMind was able to teach a program to master various Atari games.", "Now you should have a basic understanding of what\u2019s going on with the state of the art work in artificial intelligence. Neural networks are powering just about everything we do, including language translation, animal recognition, picture captioning, text summarization and just about anything else you can think of. You\u2019re sure to hear more about them in the future so it\u2019s good that you understand them now!", "This post was written by Aaron at Josh.ai. Previously, Aaron worked at Northrop Grumman before joining the Josh team where he works on natural language programming (NLP) and artificial intelligence (AI). Aaron is a skilled YoYo expert, loves video games and music, has been programming since middle school and recently turned 21.", "Josh.ai is an AI agent for your home. If you\u2019re interested in following Josh and getting early access to the beta, enter your email at https://josh.ai.", "Like Josh on Facebook \u2014 http://facebook.com/joshdotai", "Follow Josh on Twitter \u2014 http://twitter.com/joshdotai", "Technology trends and New Invention? Follow this collection to update the latest trend! [UPDATE] As a collection editor, I don\u2019t have any permission to add your articles in the wild. Please submit your article and I will approve. Also, follow this collection, please."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/technology-invention-and-more?source=post_page-----57fac18245a1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/technology-invention-and-more?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Technology, Invention, App, and More"}, {"url": "https://medium.com/@joshdotai?source=post_page-----57fac18245a1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joshdotai?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Josh"}, {"url": "http://www.hippowallpapers.com/the-network-wallpapers", "anchor_text": "credit"}, {"url": "https://github.com/ryankiros/neural-storyteller", "anchor_text": "programs that can tell stories about pictures"}, {"url": "https://www.youtube.com/watch?v=3yCAZWdqX_Y", "anchor_text": "cars that are driving themselves"}, {"url": "http://deepdreamgenerator.com", "anchor_text": "programs that create art"}, {"url": "http://www.bloomberg.com/news/articles/2015-12-08/why-2015-was-a-breakthrough-year-in-artificial-intelligence", "anchor_text": "this"}, {"url": "http://josh.ai", "anchor_text": "Josh.ai"}, {"url": "http://www.intechopen.com/source/html/39067/media/image1.png", "anchor_text": "http://www.intechopen.com/source/html/39067/media/image1.png"}, {"url": "https://en.wikipedia.org/wiki/Sigmoid_function", "anchor_text": "for example the sigmoid function"}, {"url": "https://en.wikipedia.org/wiki/Perceptron", "anchor_text": "perceptron"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neuron", "anchor_text": "McCulloch-Pitts neurons"}, {"url": "https://en.wikipedia.org/wiki/Multilayer_perceptron", "anchor_text": "multilayer perceptrons"}, {"url": "http://neuroph.sourceforge.net/tutorials/MultiLayerPerceptron.html", "anchor_text": "credit"}, {"url": "http://www.thetalkingmachines.com/blog/2015/2/26/the-history-of-machine-learning-from-the-inside-out", "anchor_text": "this"}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "backpropagation"}, {"url": "http://www.codeproject.com/Articles/175777/Financial-predictor-via-neural-network", "anchor_text": "credit"}, {"url": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png", "anchor_text": "https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png"}, {"url": "http://physiol.gu.se/maberg/images.html", "anchor_text": "credit"}, {"url": "http://wizardofodds.com/games/blackjack/card-counting/high-low/", "anchor_text": "low cards are out of the deck"}, {"url": "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/", "anchor_text": "this tutorial"}, {"url": "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "anchor_text": "this blog post"}, {"url": "https://github.com/facebook/MemNN", "anchor_text": "Memory Networks"}, {"url": "http://arxiv.org/pdf/1503.08895v5.pdf", "anchor_text": "this"}, {"url": "http://colah.github.io/posts/2014-07-Conv-Nets-Modular/", "anchor_text": "here"}, {"url": "http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/", "anchor_text": "this"}, {"url": "https://www.youtube.com/watch?v=qv6UVOQ0F44", "anchor_text": "this"}, {"url": "https://www.youtube.com/watch?v=V1eYniJ0Rnk", "anchor_text": "this"}, {"url": "http://robohub.org/artificial-general-intelligence-that-plays-atari-video-games-how-did-deepmind-do-it/", "anchor_text": "credit"}, {"url": "http://josh.ai", "anchor_text": "Josh.ai"}, {"url": "http://josh.ai", "anchor_text": "Josh.ai"}, {"url": "https://www.josh.ai", "anchor_text": "https://josh.ai"}, {"url": "http://facebook.com/joshdotai", "anchor_text": "http://facebook.com/joshdotai"}, {"url": "http://twitter.com/joshdotai", "anchor_text": "http://twitter.com/joshdotai"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----57fac18245a1---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----57fac18245a1---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----57fac18245a1---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/technology-invention-and-more?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "More from Technology, Invention, App, and More"}, {"url": "https://medium.com/technology-invention-and-more?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Read more from Technology, Invention, App, and More"}, {"url": "https://medium.com/?source=post_page-----57fac18245a1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----57fac18245a1--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----57fac18245a1--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----57fac18245a1--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----57fac18245a1--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@joshdotai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@joshdotai?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Josh"}, {"url": "https://medium.com/@joshdotai/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "42K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fa3fa8600a099&operation=register&redirect=https%3A%2F%2Fmedium.com%2Ftechnology-invention-and-more%2Feverything-you-need-to-know-about-artificial-neural-networks-57fac18245a1&newsletterV3=66b5ae01967f&newsletterV3Id=a3fa8600a099&user=Josh&userId=66b5ae01967f&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}