{"url": "https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab", "time": 1682988282.32807, "path": "medium.com/machine-learning-for-humans/supervised-learning-740383a2feab/", "webpage": {"metadata": {"title": "Machine Learning for Humans, Part 2.1: Supervised Learning | by Vishal Maini | Machine Learning for Humans | Medium", "h1": "Machine Learning for Humans, Part 2.1: Supervised Learning", "description": "The two tasks of supervised learning: regression and classification. Linear regression, loss functions, and gradient descent."}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d", "anchor_text": "Part 2.2", "paragraph_index": 9}, {"url": "http://scikit-learn.org/stable/", "anchor_text": "scikit-learn", "paragraph_index": 30}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow", "paragraph_index": 30}, {"url": "https://en.wikipedia.org/wiki/Partial_derivative", "anchor_text": "partial derivatives", "paragraph_index": 38}, {"url": "https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d", "anchor_text": "Part 2.2: Supervised Learning II", "paragraph_index": 51}, {"url": "http://www-bcf.usc.edu/~gareth/ISL/index.html", "anchor_text": "An Introduction to Statistical Learning", "paragraph_index": 52}, {"url": "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/", "anchor_text": "this tutorial", "paragraph_index": 53}, {"url": "http://eli.thegreenplace.net/2016/understanding-gradient-descent/", "anchor_text": "here", "paragraph_index": 53}], "all_paragraphs": ["How much money will we make by spending more dollars on digital advertising? Will this loan applicant pay back the loan or not? What\u2019s going to happen to the stock market tomorrow?", "In supervised learning problems, we start with a data set containing training examples with associated correct labels. For example, when learning to classify handwritten digits, a supervised learning algorithm takes thousands of pictures of handwritten digits along with labels containing the correct number each image represents. The algorithm will then learn the relationship between the images and their associated numbers, and apply that learned relationship to classify completely new images (without labels) that the machine hasn\u2019t seen before. This is how you\u2019re able to deposit a check by taking a picture with your phone!", "To illustrate how supervised learning works, let\u2019s examine the problem of predicting annual income based on the number of years of higher education someone has completed. Expressed more formally, we\u2019d like to build a model that approximates the relationship f between the number of years of higher education X and corresponding annual income Y.", "One method for predicting income would be to create a rigid rules-based model for how income and education are related. For example: \u201cI\u2019d estimate that for every additional year of higher education, annual income increases by $5,000.\u201d", "You could come up with a more complex model by including some rules about degree type, years of work experience, school tiers, etc. For example: \u201cIf they completed a Bachelor\u2019s degree or higher, give the income estimate a 1.5x multiplier.\u201d", "But this kind of explicit rules-based programming doesn\u2019t work well with complex data. Imagine trying to design an image classification algorithm made of if-then statements describing the combinations of pixel brightnesses that should be labeled \u201ccat\u201d or \u201cnot cat\u201d.", "Supervised machine learning solves this problem by getting the computer to do the work for you. By identifying patterns in the data, the machine is able to form heuristics. The primary difference between this and human learning is that machine learning runs on computer hardware and is best understood through the lens of computer science and statistics, whereas human pattern-matching happens in a biological brain (while accomplishing the same goals).", "In supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running labeled training data through a learning algorithm. This learned function can be used to estimate the income of people whose income Y is unknown, as long as we have years of education X as inputs. In other words, we can apply our model to the unlabeled test data to estimate Y.", "The goal of supervised learning is to predict Y as accurately as possible when given new examples where X is known and Y is unknown. In what follows we\u2019ll explore several of the most common approaches to doing so.", "The rest of this section will focus on regression. In Part 2.2 we\u2019ll dive deeper into classification methods.", "Regression predicts a continuous target variable Y. It allows you to estimate a value, such as housing prices or human lifespan, based on input data X.", "Here, target variable means the unknown variable we care about predicting, and continuous means there aren\u2019t gaps (discontinuities) in the value that Y can take on. A person\u2019s weight and height are continuous values. Discrete variables, on the other hand, can only take on a finite number of values \u2014 for example, the number of kids somebody has is a discrete variable.", "Predicting income is a classic regression problem. Your input data X includes all relevant information about individuals in the data set that can be used to predict income, such as years of education, years of work experience, job title, or zip code. These attributes are called features, which can be numerical (e.g. years of work experience) or categorical (e.g. job title or field of study).", "You\u2019ll want as many training observations as possible relating these features to the target output Y, so that your model can learn the relationship f between X and Y.", "The data is split into a training data set and a test data set. The training set has labels, so your model can learn from these labeled examples. The test set does not have labels, i.e. you don\u2019t yet know the value you\u2019re trying to predict. It\u2019s important that your model can generalize to situations it hasn\u2019t encountered before so that it can perform well on the test data.", "In our trivially simple 2D example, this could take the form of a .csv file where each row contains a person\u2019s education level and income. Add more columns with more features and you\u2019ll have a more complex, but possibly more accurate, model.", "How do we build models that make accurate, useful predictions in the real world? We do so by using supervised learning algorithms.", "Now let\u2019s get to the fun part: getting to know the algorithms. We\u2019ll explore some of the ways to approach regression and classification and illustrate key machine learning concepts throughout.", "\u201cDraw the line. Yes, this counts as machine learning.\u201d", "First, we\u2019ll focus on solving the income prediction problem with linear regression, since linear models don\u2019t work well with image recognition tasks (this is the domain of deep learning, which we\u2019ll explore later).", "We have our data set X, and corresponding target values Y. The goal of ordinary least squares (OLS) regression is to learn a linear model that we can use to predict a new y given a previously unseen x with as little error as possible. We want to guess how much income someone earns based on how many years of education they received.", "Linear regression is a parametric method, which means it makes an assumption about the form of the function relating X and Y (we\u2019ll cover examples of non-parametric methods later). Our model will be a function that predicts \u0177 given a specific x:", "\u03b20 is the y-intercept and \u03b21 is the slope of our line, i.e. how much income increases (or decreases) with one additional year of education.", "Our goal is to learn the model parameters (in this case, \u03b20 and \u03b21) that minimize error in the model\u2019s predictions.", "1. Define a cost function, or loss function, that measures how inaccurate our model\u2019s predictions are.", "2. Find the parameters that minimize loss, i.e. make our model as accurate as possible.", "Graphically, in two dimensions, this results in a line of best fit. In three dimensions, we would draw a plane, and so on with higher-dimensional hyperplanes.", "Mathematically, we look at the difference between each real data point (y) and our model\u2019s prediction (\u0177). Square these differences to avoid negative numbers and penalize larger differences, and then add them up and take the average. This is a measure of how well our data fits the line.", "For a simple problem like this, we can compute a closed form solution using calculus to find the optimal beta parameters that minimize our loss function. But as a cost function grows in complexity, finding a closed form solution with calculus is no longer feasible. This is the motivation for an iterative approach called gradient descent, which allows us to minimize a complex loss function.", "\u201cPut on a blindfold, take a step downhill. You\u2019ve found the bottom when you have nowhere to go but up.\u201d", "Gradient descent will come up over and over again, especially in neural networks. Machine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it\u2019s worth understanding the details.", "The goal of gradient descent is to find the minimum of our model\u2019s loss function by iteratively getting a better and better approximation of it.", "Imagine yourself walking through a valley with a blindfold on. Your goal is to find the bottom of the valley. How would you do it?", "A reasonable approach would be to touch the ground around you and move in whichever direction the ground is sloping down most steeply. Take a step and repeat the same process continually until the ground is flat. Then you know you\u2019ve reached the bottom of a valley; if you move in any direction from where you are, you\u2019ll end up at the same elevation or further uphill.", "Going back to mathematics, the ground becomes our loss function, and the elevation at the bottom of the valley is the minimum of that function.", "Let\u2019s take a look at the loss function we saw in regression:", "We see that this is really a function of two variables: \u03b20 and \u03b21. All the rest of the variables are determined, since X, Y, and n are given during training. We want to try to minimize this function.", "The function is f(\u03b20,\u03b21)=z. To begin gradient descent, you make some guess of the parameters \u03b20 and \u03b21 that minimize the function.", "Next, you find the partial derivatives of the loss function with respect to each beta parameter: [dz/d\u03b20, dz/d\u03b21]. A partial derivative indicates how much total loss is increased or decreased if you increase \u03b20 or \u03b21 by a very small amount.", "Put another way, how much would increasing your estimate of annual income assuming zero higher education (\u03b20) increase the loss (i.e. inaccuracy) of your model? You want to go in the opposite direction so that you end up walking downhill and minimizing loss.", "Similarly, if you increase your estimate of how much each incremental year of education affects income (\u03b21), how much does this increase loss (z)? If the partial derivative dz/\u03b21 is a negative number, then increasing \u03b21 is good because it will reduce total loss. If it\u2019s a positive number, you want to decrease \u03b21. If it\u2019s zero, don\u2019t change \u03b21 because it means you\u2019ve reached an optimum.", "Keep doing that until you reach the bottom, i.e. the algorithm converged and loss has been minimized. There are lots of tricks and exceptional cases beyond the scope of this series, but generally, this is how you find the optimal parameters for your parametric model.", "Overfitting: \u201cSherlock, your explanation of what just happened is too specific to the situation.\u201d Regularization: \u201cDon\u2019t overcomplicate things, Sherlock. I\u2019ll punch you for every extra word.\u201d Hyperparameter (\u03bb): \u201cHere\u2019s the strength with which I will punch you for every extra word.\u201d", "A common problem in machine learning is overfitting: learning a function that perfectly explains the training data that the model learned from, but doesn\u2019t generalize well to unseen test data. Overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren\u2019t representative of patterns in the real world. This becomes especially problematic as you make your model increasingly complex. Underfitting is a related issue where your model is not complex enough to capture the underlying trend in the data.", "Remember that the only thing we care about is how the model performs on test data. You want to predict which emails will be marked as spam before they\u2019re marked, not just build a model that is 100% accurate at reclassifying the emails it used to build itself in the first place. Hindsight is 20/20 \u2014 the real question is whether the lessons learned will help in the future.", "The model on the right has zero loss for the training data because it perfectly fits every data point. But the lesson doesn\u2019t generalize. It would do a horrible job at explaining a new data point that isn\u2019t yet on the line.", "1. Use more training data. The more you have, the harder it is to overfit the data by learning too much from any single training example.", "2. Use regularization. Add in a penalty in the loss function for building a model that assigns too much explanatory power to any one feature or allows too many features to be taken into account.", "The first piece of the sum above is our normal cost function. The second piece is a regularization term that adds a penalty for large beta coefficients that give too much explanatory power to any specific feature. With these two elements in place, the cost function now balances between two priorities: explaining the training data and preventing that explanation from becoming overly specific.", "The lambda coefficient of the regularization term in the cost function is a hyperparameter: a general setting of your model that can be increased or decreased (i.e. tuned) in order to improve performance. A higher lambda value will more harshly penalize large beta coefficients that could lead to potential overfitting. To decide the best value of lambda, you\u2019d use a method called cross-validation which involves holding out a portion of the training data during training, and then seeing how well your model explains the held-out portion. We\u2019ll go over this in more depth", "Here\u2019s what we covered in this section:", "In the next section \u2014 Part 2.2: Supervised Learning II \u2014 we\u2019ll talk about two foundational methods of classification: logistic regression and support vector machines.", "For a more thorough treatment of linear regression, read chapters 1\u20133 of An Introduction to Statistical Learning. The book is available for free online and is an excellent resource for understanding machine learning concepts with accompanying exercises.", "To actually implement gradient descent in Python, check out this tutorial. And here is a more mathematically rigorous description of the same concepts.", "In practice, you\u2019ll rarely need to implement gradient descent from scratch, but understanding how it works behind the scenes will allow you to use it more effectively and understand why things break when they do.", "More from Machine Learning for Humans \ud83e\udd16\ud83d\udc76", "Demystifying artificial intelligence & machine learning. Discussions on safe and intentional application of AI for positive social impact.", "Strategy & communications @DeepMindAI. Previously @Upstart, @Yale, @TrueVenturesTEC. Views expressed here are my own."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/machine-learning-for-humans?source=post_page-----740383a2feab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/machine-learning-for-humans?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Machine Learning for Humans"}, {"url": "https://medium.com/@v_maini?source=post_page-----740383a2feab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@v_maini?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Vishal Maini"}, {"url": "https://www.dropbox.com/s/e38nil1dnl7481q/machine_learning.pdf?dl=0", "anchor_text": "Download here"}, {"url": "http://paypal.me/ml4h", "anchor_text": "paypal.me/ml4h"}, {"url": "https://en.wikipedia.org/wiki/Paul_Erd%C5%91s", "anchor_text": "Paul Erd\u0151s"}, {"url": "https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d", "anchor_text": "Part 2.2"}, {"url": "http://www.deeplearningbook.org/contents/linear_algebra.html", "anchor_text": "linear algebra review"}, {"url": "http://scikit-learn.org/stable/", "anchor_text": "scikit-learn"}, {"url": "https://www.tensorflow.org/", "anchor_text": "TensorFlow"}, {"url": "https://en.wikipedia.org/wiki/Partial_derivative", "anchor_text": "partial derivatives"}, {"url": "https://medium.com/@v_maini/machine-learning-for-humans-part-1-why-machine-learning-matters-965dfadf19d4", "anchor_text": "ML course"}, {"url": "https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d", "anchor_text": "Part 2.2: Supervised Learning II"}, {"url": "http://www-bcf.usc.edu/~gareth/ISL/index.html", "anchor_text": "An Introduction to Statistical Learning"}, {"url": "http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html", "anchor_text": "Boston Housing dataset"}, {"url": "https://medium.com/@haydar_ai/learning-data-science-day-9-linear-regression-on-boston-housing-dataset-cd62a80775ef", "anchor_text": "Python"}, {"url": "https://datascienceplus.com/linear-regression-from-scratch-in-r/", "anchor_text": "R"}, {"url": "http://www.kaggle.com", "anchor_text": "Kaggle"}, {"url": "https://www.kaggle.com/c/house-prices-advanced-regression-techniques", "anchor_text": "housing price prediction"}, {"url": "https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/", "anchor_text": "this tutorial"}, {"url": "http://eli.thegreenplace.net/2016/understanding-gradient-descent/", "anchor_text": "here"}, {"url": "https://twitter.com/v_maini", "anchor_text": "Vishal"}, {"url": "https://twitter.com/seriousssam", "anchor_text": "Samer"}, {"url": "https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12", "anchor_text": "Part 1: Why Machine Learning Matters"}, {"url": "https://medium.com/@v_maini/supervised-learning-2-5c1c23f3560d", "anchor_text": "Part 2.2: Supervised Learning II"}, {"url": "https://medium.com/@v_maini/supervised-learning-3-b1551b9c4930", "anchor_text": "Part 2.3: Supervised Learning III"}, {"url": "https://medium.com/@v_maini/unsupervised-learning-f45587588294", "anchor_text": "Part 3: Unsupervised Learning"}, {"url": "https://medium.com/@v_maini/neural-networks-deep-learning-cdad8aeae49b", "anchor_text": "Part 4: Neural Networks & Deep Learning"}, {"url": "https://medium.com/@v_maini/reinforcement-learning-6eacf258b265", "anchor_text": "Part 5: Reinforcement Learning"}, {"url": "https://medium.com/@v_maini/how-to-learn-machine-learning-24d53bb64aa1", "anchor_text": "Appendix: The Best Machine Learning Resources"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----740383a2feab---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----740383a2feab---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/supervised-learning?source=post_page-----740383a2feab---------------supervised_learning-----------------", "anchor_text": "Supervised Learning"}, {"url": "https://medium.com/tag/tech?source=post_page-----740383a2feab---------------tech-----------------", "anchor_text": "Tech"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----740383a2feab---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/machine-learning-for-humans?source=post_page-----740383a2feab--------------------------------", "anchor_text": "More from Machine Learning for Humans"}, {"url": "https://medium.com/machine-learning-for-humans?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Read more from Machine Learning for Humans"}, {"url": "https://medium.com/?source=post_page-----740383a2feab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----740383a2feab--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----740383a2feab--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----740383a2feab--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----740383a2feab--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@v_maini?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@v_maini?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Vishal Maini"}, {"url": "https://medium.com/@v_maini/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "13K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F547a12a87a6e&operation=register&redirect=https%3A%2F%2Fmedium.com%2Fmachine-learning-for-humans%2Fsupervised-learning-740383a2feab&newsletterV3=19a4c39d50a8&newsletterV3Id=547a12a87a6e&user=Vishal+Maini&userId=19a4c39d50a8&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}