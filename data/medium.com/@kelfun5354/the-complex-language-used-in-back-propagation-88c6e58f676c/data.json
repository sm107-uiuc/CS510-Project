{"url": "https://medium.com/@kelfun5354/the-complex-language-used-in-back-propagation-88c6e58f676c", "time": 1683019569.1792362, "path": "medium.com/@kelfun5354/the-complex-language-used-in-back-propagation-88c6e58f676c/", "webpage": {"metadata": {"title": "Simplifying the complex language used in Back Propagation | by Kelvin Li | Medium", "h1": "Simplifying the complex language used in Back Propagation", "description": "I\u2019ve looked all over the internet for explanations of what exactly back propagation is and everyone either uses complicated mathematical language or complex codes to try to explain what back\u2026"}, "outgoing_paragraph_urls": [{"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "artificial neural networks", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Gradient", "anchor_text": "gradient", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network#Components_of_an_artificial_neural_network", "anchor_text": "weights", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent", "paragraph_index": 3}, {"url": "https://en.wikipedia.org/wiki/Loss_function", "anchor_text": "loss function", "paragraph_index": 3}, {"url": "http://www.purplemath.com/modules/slope.htm", "anchor_text": "slope", "paragraph_index": 4}], "all_paragraphs": ["I\u2019ve looked all over the internet for explanations of what exactly back propagation is and everyone either uses complicated mathematical language or complex codes to try to explain what back propagation is. If someone who doesn\u2019t know either wants to know what it is then how will they really grasp what it is?", "In this post, I would like to unveil the secrets of the universe with everyone and hopefully I\u2019ll do a good job at it.", "According to Wikipedia, Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network.", "Backpropagation is commonly used by the gradient descent optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function", "If you have taken a basic elementary algebra class, you may have heard of the idea of a slope. Some people might think the idea of a slope is very insignificant but it is actually the game-changing concept that caused all the technological advancement within the last century.", "To know the slope of something means that you know the rate of something changing over a period of time. Knowing this gives us power to manipulate things to our advantage.", "Now you can think of a gradient as the slope of something in a higher dimension. I won\u2019t go into details but that is the general gist of what a gradient is.", "Weights are the values that we want to use to adjust the outputs of our functions in each neuron. So say we have an output of 2 and we want to change the 2 into a 1, then we would multiply the 2 by a .5 to get the desired result. This means that .5 will be the weight in this case. In a way we are weighing down the output to what we want it to be.", "A neuron is simply just a function. A Neural Network(a bunch of neurons) is simply a bunch of functions. Each neuron also has an activation function that spits out a value for the next neuron to calculate.", "Think of these functions as how much of a yes or a no an input is. An example would be picture recognizing. When you feed the neural network a picture, the node will spit out a number between 0 and 1. Where 0 is being very NO and 1 being very YES. This process continues between every node until the very end. Which ever node has the highest number, between 0 and 1, would be the decision the machine makes.", "A loss function is just some function that we use to determine how correct the predicted output is from the real output. For example, we input a picture of a cat into the machine but the machine predicts that it\u2019s a dinosaur. Clearly the machine is not doing a very good job. So we need some way to know how correct the machine is compared to the real data. Which is where the loss function comes in.", "Now that we have all the necessary understandings, we can go into the real sauce.", "Now what I am about to explain to you is going to either confuse the crap out of you or make you feel enlightened.", "Let\u2019s pretend you are trying to build a door lock opening mechanism. This mechanism involves you pressing a button, which triggers a ball rolling down a platform and knocks over a switch that unlocks the door.", "Now lets think about this. There are a few components that we have to keep in mind. The 1st component being you pressing the button, the 2nd component is the ball rolling down a platform, and the 3rd component being the switch being knocked over.", "There is actually a lot of physics going on around here but let\u2019s just focus on the ball rolling down the platform.", "Now when you create this mechanism, you want the door to ideally open in 3 seconds. But you don\u2019t have any tools to measure the time and length, so all you can do is to create a platform through intuition.", "You build your first platform and let the ball roll and realized that it took 9 seconds for the door to open after pressing the button.", "So you go back to the platform and make the platform steeper.", "You performed the same trial and error over and over again until you got the ideal opening time.", "Well true. But the idea is basically the same. In a Neural Net, we have weights assigned to each neuron. These weights will get multiplied by a certain input and modified through some activation function. The result of these activation function might not always be what we want.", "What backpropagation would do is that it will do some calculus (will be covered in another post) to determine the direction of increase/decrease, aka the gradient,(cut less of the platform or cut more of the platform) to achieve the best weights (ideal time the door opens). It then updates these weights every time it has created new weights and runs the neural net again(every trial you cut a piece of the platform to test).", "Eventually we will achieve the best possible weights that satisfies our desired accuracy.", "In my next post, I will discuss more in depth about the math that is involved with backpropagation."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelfun5354?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelfun5354?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "Kelvin Li"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network", "anchor_text": "artificial neural networks"}, {"url": "https://en.wikipedia.org/wiki/Gradient", "anchor_text": "gradient"}, {"url": "https://en.wikipedia.org/wiki/Artificial_neural_network#Components_of_an_artificial_neural_network", "anchor_text": "weights"}, {"url": "https://en.wikipedia.org/wiki/Gradient_descent", "anchor_text": "gradient descent"}, {"url": "https://en.wikipedia.org/wiki/Loss_function", "anchor_text": "loss function"}, {"url": "http://www.purplemath.com/modules/slope.htm", "anchor_text": "slope"}, {"url": "https://en.wikipedia.org/wiki/Backpropagation", "anchor_text": "https://en.wikipedia.org/wiki/Backpropagation"}, {"url": "https://en.wikipedia.org/wiki/Loss_functions_for_classification", "anchor_text": "https://en.wikipedia.org/wiki/Loss_functions_for_classification"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----88c6e58f676c---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/ai?source=post_page-----88c6e58f676c---------------ai-----------------", "anchor_text": "AI"}, {"url": "https://medium.com/tag/backpropagation?source=post_page-----88c6e58f676c---------------backpropagation-----------------", "anchor_text": "Backpropagation"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----88c6e58f676c---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/@kelfun5354?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "More from Kelvin Li"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F31bf086858e4%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40kelfun5354%2Fthe-complex-language-used-in-back-propagation-88c6e58f676c&user=Kelvin+Li&userId=31bf086858e4&source=-----88c6e58f676c---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----88c6e58f676c--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----88c6e58f676c--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----88c6e58f676c--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@kelfun5354?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@kelfun5354?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Kelvin Li"}, {"url": "https://medium.com/@kelfun5354/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "36 Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fusers%2F31bf086858e4%2Flazily-enable-writer-subscription&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40kelfun5354%2Fthe-complex-language-used-in-back-propagation-88c6e58f676c&user=Kelvin+Li&userId=31bf086858e4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}