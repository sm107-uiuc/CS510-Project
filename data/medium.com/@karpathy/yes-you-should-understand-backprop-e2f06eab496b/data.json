{"url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b", "time": 1683019478.128464, "path": "medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b/", "webpage": {"metadata": {"title": "Yes you should understand backprop | by Andrej Karpathy | Medium", "h1": "Yes you should understand backprop", "description": "When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The\u2026"}, "outgoing_paragraph_urls": [{"url": "http://cs231n.stanford.edu/", "anchor_text": "CS231n", "paragraph_index": 0}, {"url": "https://en.wikipedia.org/wiki/Leaky_abstraction", "anchor_text": "leaky abstraction", "paragraph_index": 3}, {"url": "https://youtu.be/gYpoJMlgyXA?t=14m14s", "anchor_text": "CS231n lecture video", "paragraph_index": 8}, {"url": "https://youtu.be/gYpoJMlgyXA?t=20m54s", "anchor_text": "CS231n lecture video", "paragraph_index": 11}, {"url": "https://www.youtube.com/watch?v=yCC09vCHzF8", "anchor_text": "CS231n lecture video", "paragraph_index": 15}, {"url": "https://github.com/devsisters/DQN-tensorflow/issues/16", "anchor_text": "issue", "paragraph_index": 21}, {"url": "https://www.youtube.com/watch?v=i94OvYb6noo", "anchor_text": "CS231n lecture on backprop", "paragraph_index": 23}, {"url": "https://cs231n.github.io/", "anchor_text": "CS231n assignments", "paragraph_index": 23}], "all_paragraphs": ["When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:", "\u201cWhy do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?\u201d", "This is seemingly a perfectly sensible appeal - if you\u2019re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of \u201cit\u2019s worth knowing what\u2019s under the hood as an intellectual curiosity\u201d, or perhaps \u201cyou might want to improve on the core algorithm later\u201d, but there is a much stronger and practical argument, which I wanted to devote a whole post to:", "> The problem with Backpropagation is that it is a leaky abstraction.", "In other words, it is easy to fall into the trap of abstracting away the learning process \u2014 believing that you can simply stack arbitrary layers together and backprop will \u201cmagically make them work\u201d on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.", "We\u2019re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can \u201csaturate\u201d and entirely stop learning \u2014 your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):", "If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (\u201cvanish\u201d), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.", "Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you\u2019re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.", "TLDR: if you\u2019re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn\u2019t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.", "Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:", "If you stare at this for a while you\u2019ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn\u2019t \u201cfire\u201d), then its weights will get zero gradient. This can lead to what is called the \u201cdead ReLU\u201d problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron\u2019s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It\u2019s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.", "TLDR: If you understand backpropagation and your network has ReLUs, you\u2019re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.", "Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I\u2019ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):", "This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you\u2019ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.", "What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b\u2026)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.", "TLDR: If you understand backpropagation and you\u2019re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.", "Lets look at one more \u2014 the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector \u2014 turns out this trivial operation is not supported in TF). Anyway, I searched \u201cdqn tensorflow\u201d, clicked the first link, and found the core code. Here is an excerpt:", "If you\u2019re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \\gamma \\argmax_a Q(s\u2019,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.", "The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.", "The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:", "It\u2019s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can\u2019t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.", "I submitted an issue on the DQN repo and this was promptly fixed.", "Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because \u201cTensorFlow automagically makes my networks learn\u201d, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.", "The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.", "That\u2019s it for now! I hope you\u2019ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I\u2019m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)", "I like to train deep neural nets on large datasets.", "I like to train deep neural nets on large datasets."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "Andrej Karpathy"}, {"url": "http://cs231n.stanford.edu/", "anchor_text": "CS231n"}, {"url": "https://en.wikipedia.org/wiki/Leaky_abstraction", "anchor_text": "leaky abstraction"}, {"url": "https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html", "anchor_text": "this post"}, {"url": "https://youtu.be/gYpoJMlgyXA?t=14m14s", "anchor_text": "CS231n lecture video"}, {"url": "https://youtu.be/gYpoJMlgyXA?t=20m54s", "anchor_text": "CS231n lecture video"}, {"url": "https://www.youtube.com/watch?v=yCC09vCHzF8", "anchor_text": "CS231n lecture video"}, {"url": "https://github.com/devsisters/DQN-tensorflow/issues/16", "anchor_text": "issue"}, {"url": "https://www.youtube.com/watch?v=i94OvYb6noo", "anchor_text": "CS231n lecture on backprop"}, {"url": "https://cs231n.github.io/", "anchor_text": "CS231n assignments"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----e2f06eab496b---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----e2f06eab496b---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----e2f06eab496b---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----e2f06eab496b---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "More from Andrej Karpathy"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe877d56ca397&operation=register&redirect=https%3A%2F%2Fkarpathy.medium.com%2Fyes-you-should-understand-backprop-e2f06eab496b&newsletterV3=ac9d9a35533e&newsletterV3Id=e877d56ca397&user=Andrej+Karpathy&userId=ac9d9a35533e&source=-----e2f06eab496b---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----e2f06eab496b--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----e2f06eab496b--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----e2f06eab496b--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Andrej Karpathy"}, {"url": "https://medium.com/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "48K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fe877d56ca397&operation=register&redirect=https%3A%2F%2Fkarpathy.medium.com%2Fyes-you-should-understand-backprop-e2f06eab496b&newsletterV3=ac9d9a35533e&newsletterV3Id=e877d56ca397&user=Andrej+Karpathy&userId=ac9d9a35533e&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}