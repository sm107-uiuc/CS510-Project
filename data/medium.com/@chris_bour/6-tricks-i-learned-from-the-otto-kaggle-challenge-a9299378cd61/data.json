{"url": "https://medium.com/@chris_bour/6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61", "time": 1683019476.1443882, "path": "medium.com/@chris_bour/6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61/", "webpage": {"metadata": {"title": "6 Tricks I Learned From The OTTO Kaggle Challenge | by Christophe Bourguignat | Medium", "h1": "6 Tricks I Learned From The OTTO Kaggle Challenge", "description": "Here are a few things I learned from the OTTO Group Kaggle competition. I had the chance to team up with great Kaggle Master Xavier Conort, and the french community as a whole has been very active\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.kaggle.com/users/17379/xavier-conort", "anchor_text": "Xavier Conort", "paragraph_index": 0}, {"url": "http://scikit-learn.org/stable/whats_new.html", "anchor_text": "the last scikit-learn", "paragraph_index": 3}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Calibration.ipynb", "anchor_text": "Here is a mini notebook", "paragraph_index": 4}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "as described in this paper", "paragraph_index": 9}, {"url": "https://www.kaggle.com/bamine", "anchor_text": "Amine", "paragraph_index": 10}, {"url": "https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14334/hyperparameter-optimization-using-hyperopt", "anchor_text": "Hyperopt", "paragraph_index": 10}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost", "paragraph_index": 11}, {"url": "https://github.com/dmlc/xgboost/blob/master/doc/python.md", "anchor_text": "there is a Python API", "paragraph_index": 11}, {"url": "https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets", "anchor_text": "guess this is really the time to try out neural nets", "paragraph_index": 13}, {"url": "http://h2o.ai/", "anchor_text": "H2O", "paragraph_index": 15}, {"url": "https://github.com/fchollet/keras", "anchor_text": "Keras", "paragraph_index": 15}, {"url": "https://github.com/dmlc/cxxnet", "anchor_text": "cxxnet", "paragraph_index": 15}, {"url": "https://github.com/Lasagne/Lasagne", "anchor_text": "Lasagne", "paragraph_index": 15}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Tuning%20Neural%20Networks.ipynb", "anchor_text": "Here is a notebook", "paragraph_index": 15}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html", "anchor_text": "BaggingClassifier meta-estimator", "paragraph_index": 17}], "all_paragraphs": ["Here are a few things I learned from the OTTO Group Kaggle competition. I had the chance to team up with great Kaggle Master Xavier Conort, and the french community as a whole has been very active.", "Teaming with Xavier has been the opportunity to practice some ensembling technics.", "We heavily used stacking. We added to an initial set of 93 features, new features being the predictions of N different classifiers (Random Forest, GBM, Neural Networks, \u2026). And then retrained P classifiers over the 93 + N features. And finally made a weighted average of the P outputs.", "This is one of the great functionalities of the last scikit-learn version (0.16). It allows to rescale the classifier predictions by taking observations predicted within a segments (e.g. 0.3\u201304), and comparing to the actual truth ratio of these observation (e.g. 0.23, with means that a rescaling is needed).", "Here is a mini notebook explaining how to use calibration, and demonstrating how well it worked on the OTTO challenge data.", "At the beginning of the competition, it appeared quickly that \u2014 once again \u2014 Gradient Boosting Trees was one of the best performing algorithm, provided that you find the right hyper parameters.", "On the scikit-learn implementation, most important hyper parameters are learning_rate (the shrinkage parameter), n_estimators (the number of boosting stages), and max_depth (limits the number of nodes in the tree, the best value depends on the interaction of the input variables). min_samples_split, and min_samples_leaf can also be a way to control depth of the trees for optimal performance.", "I also discovered that two other parameters were crucial for this competition. I must admit I never paid attention on it before this challenge : namely subsample (the fraction of samples to be used for fitting the individual base learners), and max_features (the number of features to consider when looking for the best split).", "The problem was to find a way to quickly find the best hyperparameters combination. I first discovered GridSearchCV, that makes an exhaustive search over specified parameter ranges. As always with scikit-learn, it has a convenient programming interface, handling for example smoothly cross-validation and parallel distributing of search.", "However, the number of parameters to tune, and their range, was too large to discover the best ones in the acceptable time frame I had in mind (typically while sleeping, i.e 7 to 10 hours). I had to fall back to an other option : I then used RandomizedSearchCV, that appeared in 0.14 version. With this method, search is done randomly on a subspace of parameters. It gives generally very good results, as described in this paper, and I was able to find a suitable parameter set within a few hours.", "Note that some competitors, like french kaggler Amine, used Hyperopt for hyperparameters optimization.", "XGBoost is a Gradient Boosting implementation heavily used by kagglers, and I now understand why. I never used it before, but it was a hot topic discussed in the forum. I decided to have a look at it, even if its main interface is in R (but there is a Python API, that I didn\u2019t use yet). XGBoost is much faster than scikit-learn, and gave better prediction. It will remain for sure part of my toolblox.", "Someone posted on the forum :", "\u201cguess this is really the time to try out neural nets\u201d.", "He was right. It has been for me the opportunity to play with neural networks for the first time.", "Several implementations have been used by the competitors : H2O, Keras, cxxnet, \u2026 I personally used Lasagne. Main challenges was to fine tune the number of layers, number of neurons, dropout and learning rate. Here is a notebook on what I learned.", "One of the secret of the competition was to run several times the same algorithm, with random selection of observations and features, and take the average of the output.", "To do that easily, I discovered the scikit-learn BaggingClassifier meta-estimator. It hides the tedious complexity of looping over model fits, random subsets selection, and averaging \u2014 and exposes easy fit() / predict_proba() entry points.", "Data enthusiast #BigData #DataScience #MachineLearning #FrenchData #Kaggle", "Data enthusiast #BigData #DataScience #MachineLearning #FrenchData #Kaggle"], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris_bour?source=post_page-----a9299378cd61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris_bour?source=post_page-----a9299378cd61--------------------------------", "anchor_text": "Christophe Bourguignat"}, {"url": "https://www.kaggle.com/users/17379/xavier-conort", "anchor_text": "Xavier Conort"}, {"url": "http://blog.kaggle.com/2015/05/20/hacking-the-otto-group-challenge-in-paris/", "anchor_text": "Hacking The Otto Group Challenge"}, {"url": "http://en.wikipedia.org/wiki/Logit", "anchor_text": "logit"}, {"url": "http://scikit-learn.org/stable/whats_new.html", "anchor_text": "the last scikit-learn"}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Calibration.ipynb", "anchor_text": "Here is a mini notebook"}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Calibration.ipynb", "anchor_text": "Using Scikit-Learn calibration"}, {"url": "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf", "anchor_text": "as described in this paper"}, {"url": "https://www.kaggle.com/bamine", "anchor_text": "Amine"}, {"url": "https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14334/hyperparameter-optimization-using-hyperopt", "anchor_text": "Hyperopt"}, {"url": "https://github.com/dmlc/xgboost", "anchor_text": "XGBoost"}, {"url": "https://github.com/dmlc/xgboost/blob/master/doc/python.md", "anchor_text": "there is a Python API"}, {"url": "https://www.kaggle.com/tqchen/otto-group-product-classification-challenge/understanding-xgboost-model-on-otto-data", "anchor_text": "XGBoost tutorial"}, {"url": "https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/12882/guess-this-is-really-the-time-to-try-out-neural-nets", "anchor_text": "guess this is really the time to try out neural nets"}, {"url": "http://h2o.ai/", "anchor_text": "H2O"}, {"url": "https://github.com/fchollet/keras", "anchor_text": "Keras"}, {"url": "https://github.com/dmlc/cxxnet", "anchor_text": "cxxnet"}, {"url": "https://github.com/Lasagne/Lasagne", "anchor_text": "Lasagne"}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Tuning%20Neural%20Networks.ipynb", "anchor_text": "Here is a notebook"}, {"url": "https://github.com/christophebourguignat/notebooks/blob/master/Tuning%20Neural%20Networks.ipynb", "anchor_text": "Fine Tuning a Lasagne Neural Network"}, {"url": "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html", "anchor_text": "BaggingClassifier meta-estimator"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----a9299378cd61---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/data-science?source=post_page-----a9299378cd61---------------data_science-----------------", "anchor_text": "Data Science"}, {"url": "https://medium.com/tag/python?source=post_page-----a9299378cd61---------------python-----------------", "anchor_text": "Python"}, {"url": "https://medium.com/@chris_bour?source=post_page-----a9299378cd61--------------------------------", "anchor_text": "More from Christophe Bourguignat"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e24679e9e04&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40chris_bour%2F6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61&newsletterV3=90ca8bc640f4&newsletterV3Id=7e24679e9e04&user=Christophe+Bourguignat&userId=90ca8bc640f4&source=-----a9299378cd61---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----a9299378cd61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----a9299378cd61--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----a9299378cd61--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----a9299378cd61--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----a9299378cd61--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----a9299378cd61--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----a9299378cd61--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@chris_bour?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@chris_bour?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Christophe Bourguignat"}, {"url": "https://medium.com/@chris_bour/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "1.5K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F7e24679e9e04&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40chris_bour%2F6-tricks-i-learned-from-the-otto-kaggle-challenge-a9299378cd61&newsletterV3=90ca8bc640f4&newsletterV3Id=7e24679e9e04&user=Christophe+Bourguignat&userId=90ca8bc640f4&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}