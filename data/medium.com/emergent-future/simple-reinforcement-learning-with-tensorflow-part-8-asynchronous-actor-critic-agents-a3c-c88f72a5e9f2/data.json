{"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2", "time": 1683019525.330435, "path": "medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2/", "webpage": {"metadata": {"title": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C) | by Arthur Juliani | Emergent // Future | Medium", "h1": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)", "description": "In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "earlier entries", "paragraph_index": 0}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "A3C algorithm", "paragraph_index": 1}, {"url": "https://openai.com/blog/universe/", "anchor_text": "just released", "paragraph_index": 1}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.ut59y2t80", "anchor_text": "DQN", "paragraph_index": 3}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.nac3dxoyy", "anchor_text": "implementation of Policy Gradient", "paragraph_index": 5}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.snohiu2y2", "anchor_text": "Dueling Q-Network architecture", "paragraph_index": 6}, {"url": "https://arxiv.org/pdf/1506.02438.pdf", "anchor_text": "Generalized Advantage Estimation", "paragraph_index": 9}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "DennyBritz", "paragraph_index": 10}, {"url": "https://github.com/openai/universe-starter-agent", "anchor_text": "OpenAI", "paragraph_index": 10}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb", "anchor_text": "Github repository", "paragraph_index": 10}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb", "anchor_text": "here", "paragraph_index": 21}, {"url": "http://vizdoom.cs.put.edu.pl/", "anchor_text": "VizDoom", "paragraph_index": 22}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/basic.wad", "anchor_text": "Github repository", "paragraph_index": 23}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Arthur Juliani", "paragraph_index": 27}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjuliani", "paragraph_index": 27}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating", "paragraph_index": 28}], "all_paragraphs": ["In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven\u2019t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.", "So what is A3C? The A3C algorithm was released by Google\u2019s DeepMind group earlier this year, and it made a splash by\u2026 essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their \u201cuniversal starter agent\u201d for working with their new (and very diverse) set of Universe environments.", "Asynchronous Advantage Actor-Critic is quite a mouthful. Let\u2019s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.", "Asynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it\u2019s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.", "Actor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy \u03c0(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.", "Advantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were \u201cgood\u201d and which were \u201cbad.\u201d The network was then updated in order to encourage and discourage actions appropriately.", "The insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network\u2019s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:", "Since we won\u2019t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.", "Advantage Estimate: A = R - V(s)", "In this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.", "In the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you\u2019d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won\u2019t run on its own. To view and run the full, functional A3C implementation, see my Github repository.", "The general outline of the code architecture is:", "The A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.", "Next, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.", "~ From here we go asynchronous ~", "Each worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.", "Each worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.", "Once the worker\u2019s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.", "A worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.", "A worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.", "Once a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.", "To view the full and functional code, see the Github repository here.", "The robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:", "Once it is installed, we will be using the basic.wad environment, which is provided in the Github repository, and needs to be placed in the working directory.", "The challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.", "I hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.", "(There are a lot of moving parts in A3C, so if you discover a bug, or find a better way to do something, please don\u2019t hesitate to bring it up here or in the Github. I am more than happy to incorporate changes and feedback to improve the algorithm.)", "If you\u2019d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.", "If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!", "More from my Simple Reinforcement Learning with Tensorflow series:", "Postdoctoral researcher at Microsoft. Interested in artificial intelligence, neuroscience, philosophy, and meditation."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/emergent-future?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18dfe63fa7f0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&user=Arthur+Juliani&userId=18dfe63fa7f0&source=post_page-18dfe63fa7f0----c88f72a5e9f2---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/emergent-future?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "earlier entries"}, {"url": "https://arxiv.org/pdf/1602.01783.pdf", "anchor_text": "A3C algorithm"}, {"url": "https://openai.com/blog/universe/", "anchor_text": "just released"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.ut59y2t80", "anchor_text": "DQN"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724#.nac3dxoyy", "anchor_text": "implementation of Policy Gradient"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.snohiu2y2", "anchor_text": "Dueling Q-Network architecture"}, {"url": "https://arxiv.org/pdf/1506.02438.pdf", "anchor_text": "Generalized Advantage Estimation"}, {"url": "https://github.com/dennybritz/reinforcement-learning", "anchor_text": "DennyBritz"}, {"url": "https://github.com/openai/universe-starter-agent", "anchor_text": "OpenAI"}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb", "anchor_text": "Github repository"}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/A3C-Doom.ipynb", "anchor_text": "here"}, {"url": "http://vizdoom.cs.put.edu.pl/", "anchor_text": "VizDoom"}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/basic.wad", "anchor_text": "Github repository"}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjuliani"}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "Part 0 \u2014 Q-Learning Agents"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "Part 1 \u2014 Two-Armed Bandit"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c#.uzs1axw0s", "anchor_text": "Part 1.5 \u2014 Contextual Bandits"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724", "anchor_text": "Part 2 \u2014 Policy-Based Agents"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99", "anchor_text": "Part 3 \u2014 Model-Based RL"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df#.i2zpbmre8", "anchor_text": "Part 4 \u2014 Deep Q-Networks and Beyond"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a", "anchor_text": "Part 5 \u2014 Visualizing an Agent\u2019s Thoughts and Actions"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.9djtshpqo", "anchor_text": "Part 6 \u2014 Partial Observability and Deep Recurrent Q-Networks"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf#.qfg7lqxpr", "anchor_text": "Part 7 \u2014 Action-Selection Strategies for Exploration"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----c88f72a5e9f2---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----c88f72a5e9f2---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----c88f72a5e9f2---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----c88f72a5e9f2---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/robotics?source=post_page-----c88f72a5e9f2---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/@awjuliani?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/emergent-future?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----c88f72a5e9f2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Written by Arthur Juliani"}, {"url": "https://medium.com/@awjuliani/followers?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "12.6K Followers"}, {"url": "https://medium.com/emergent-future?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----c88f72a5e9f2---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "MAOIs, SSRIs, and PsychedelicsThree mechanisms of action for antidepressant drugs"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "10 min read\u00b7Apr 13"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----c88f72a5e9f2----0---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural NetworksFor this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms\u2026"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "6 min read\u00b7Aug 25, 2016"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----c88f72a5e9f2----1---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "112"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@amangoeliitb?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@amangoeliitb?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Aman Goel"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Spam detection using neural networks in PythonNeural networks are powerful machine learning algorithms. They can be used to transform the features so as to form fairly complex non\u2026"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "3 min read\u00b7Apr 9, 2016"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----c88f72a5e9f2----2---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/better-programming?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Better Programming"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "Large Language Models Don\u2019t \u201cHallucinate\u201dUsing this term attributes properties to the LLMs they don\u2019t have while also ignoring the real dynamics behind their made-up information"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------", "anchor_text": "6 min read\u00b7Mar 10"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----c88f72a5e9f2----3---------------------c89cca15_4be4_400d_b5e5_fe861a2481bc-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "23"}, {"url": "https://medium.com/@awjuliani?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "See all from Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "See all from Emergent // Future"}, {"url": "https://medium.com/towards-data-science/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@wvheeswijk?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@wvheeswijk?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://medium.com/towards-data-science?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/towards-data-science/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://medium.com/towards-data-science/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://medium.com/towards-data-science/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://medium.com/@arshren/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@arshren?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@arshren?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Renu Khandelwal"}, {"url": "https://medium.com/@arshren/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Deep Q Learning: A Deep Reinforcement Learning AlgorithmAn easy-to-understand explanation of Deep Q-Learning with PyTorch code implementation"}, {"url": "https://medium.com/@arshren/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b711 min read\u00b7Jan 12"}, {"url": "https://medium.com/@arshren/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/artificial-corner/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@frank-andrade?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@frank-andrade?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "The PyCoach"}, {"url": "https://medium.com/artificial-corner?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Artificial Corner"}, {"url": "https://medium.com/artificial-corner/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://medium.com/artificial-corner/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://medium.com/artificial-corner/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----c88f72a5e9f2----0---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "276"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----c88f72a5e9f2----1---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/datadriveninvestor/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@byfintech?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@byfintech?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.com/datadriveninvestor?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://medium.com/datadriveninvestor/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://medium.com/datadriveninvestor/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://medium.com/datadriveninvestor/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----c88f72a5e9f2----2---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/towards-data-science/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@wvheeswijk?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": ""}, {"url": "https://medium.com/@wvheeswijk?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://medium.com/towards-data-science?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/towards-data-science/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "Rainbow DQN \u2014 The Best Reinforcement Learning Has to Offer?What happens if the most successful techniques in Deep Q-Learning are combined into a single algorithm?"}, {"url": "https://medium.com/towards-data-science/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------", "anchor_text": "\u00b711 min read\u00b7Dec 8, 2022"}, {"url": "https://medium.com/towards-data-science/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----c88f72a5e9f2----3---------------------17d0b0f6_e3fe_4ba8_b748_0f972e5f9aaa-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----c88f72a5e9f2--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}