{"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "time": 1683019477.5082781, "path": "medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0/", "webpage": {"metadata": {"title": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks | by Arthur Juliani | Emergent // Future | Medium", "h1": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks", "description": "For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "here", "paragraph_index": 0}, {"url": "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "anchor_text": "DeepQ-Networks which can play Atari Games", "paragraph_index": 1}, {"url": "https://gym.openai.com/envs/FrozenLake-v0", "anchor_text": "FrozenLake", "paragraph_index": 2}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI gym", "paragraph_index": 2}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equation", "paragraph_index": 4}, {"url": "https://github.com/PraneetDutta", "anchor_text": "Praneet D", "paragraph_index": 6}, {"url": "http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/", "anchor_text": "great post", "paragraph_index": 10}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating", "paragraph_index": 11}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Arthur Juliani", "paragraph_index": 12}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjliani", "paragraph_index": 12}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": "See all from Emergent // Future", "paragraph_index": 15}], "all_paragraphs": ["For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1\u20133). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).", "Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.", "For this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn\u2019t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.", "In it\u2019s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.", "We make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:", "This says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (\u03b3) future reward expected according to our own table for the next state (s\u2019) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:", "(Thanks to Praneet D for finding the optimal hyperparameters for this approach)", "Now, you may be thinking: tables are great, but they don\u2019t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don\u2019t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.", "In the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the \u201ctarget\u201d value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.", "Below is the Tensorflow walkthrough of implementing our simple Q-Network:", "While the network learns to solve the FrozenLake problem, it turns out it doesn\u2019t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!", "If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!", "If you\u2019d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on Twitter @awjliani.", "More from my Simple Reinforcement Learning with Tensorflow series:", "Postdoctoral researcher at Microsoft. Interested in artificial intelligence, neuroscience, philosophy, and meditation.", "See all from Emergent // Future"], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----d195264329d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18dfe63fa7f0&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0&user=Arthur+Juliani&userId=18dfe63fa7f0&source=post_page-18dfe63fa7f0----d195264329d0---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "here"}, {"url": "http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html", "anchor_text": "DeepQ-Networks which can play Atari Games"}, {"url": "https://gym.openai.com/envs/FrozenLake-v0", "anchor_text": "FrozenLake"}, {"url": "https://gym.openai.com/", "anchor_text": "OpenAI gym"}, {"url": "https://en.wikipedia.org/wiki/Bellman_equation", "anchor_text": "Bellman equation"}, {"url": "https://github.com/PraneetDutta", "anchor_text": "Praneet D"}, {"url": "http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/", "anchor_text": "great post"}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating"}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjliani"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "Part 1 \u2014 Two-Armed Bandit"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c", "anchor_text": "Part 1.5 \u2014 Contextual Bandits"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724", "anchor_text": "Part 2 \u2014 Policy-Based Agents"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99", "anchor_text": "Part 3 \u2014 Model-Based RL"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df", "anchor_text": "Part 4 \u2014 Deep Q-Networks and Beyond"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8", "anchor_text": "Part 5 \u2014 Visualizing an Agent\u2019s Thoughts and Actions"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk", "anchor_text": "Part 6 \u2014 Partial Observability and Deep Recurrent Q-Networks"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf", "anchor_text": "Part 7 \u2014 Action-Selection Strategies for Exploration"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw", "anchor_text": "Part 8 \u2014 Asynchronous Actor-Critic Agents (A3C)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----d195264329d0---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----d195264329d0---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/neural-networks?source=post_page-----d195264329d0---------------neural_networks-----------------", "anchor_text": "Neural Networks"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----d195264329d0---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/robotics?source=post_page-----d195264329d0---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/@awjuliani?source=post_page-----d195264329d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----d195264329d0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Written by Arthur Juliani"}, {"url": "https://medium.com/@awjuliani/followers?source=post_page-----d195264329d0--------------------------------", "anchor_text": "12.6K Followers"}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fmedium.com%2Femergent-future%2Fsimple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----d195264329d0---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "MAOIs, SSRIs, and PsychedelicsThree mechanisms of action for antidepressant drugs"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "10 min read\u00b7Apr 13"}, {"url": "https://medium.com/@awjuliani/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----d195264329d0----0---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will\u2026"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "8 min read\u00b7Dec 17, 2016"}, {"url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----d195264329d0----1---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "86"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@amangoeliitb?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@amangoeliitb?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Aman Goel"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Spam detection using neural networks in PythonNeural networks are powerful machine learning algorithms. They can be used to transform the features so as to form fairly complex non\u2026"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "3 min read\u00b7Apr 9, 2016"}, {"url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272?source=author_recirc-----d195264329d0----2---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "12"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": ""}, {"url": "https://medium.com/@awjuliani?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/better-programming?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Better Programming"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "Large Language Models Don\u2019t \u201cHallucinate\u201dUsing this term attributes properties to the LLMs they don\u2019t have while also ignoring the real dynamics behind their made-up information"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------", "anchor_text": "6 min read\u00b7Mar 10"}, {"url": "https://medium.com/better-programming/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----d195264329d0----3---------------------65cdbf19_15d5_415a_8661_87dcb27fe913-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "23"}, {"url": "https://medium.com/@awjuliani?source=post_page-----d195264329d0--------------------------------", "anchor_text": "See all from Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=post_page-----d195264329d0--------------------------------", "anchor_text": "See all from Emergent // Future"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@anandmishra.kanha?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Anand Mishra"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Deep reinforcement learning \u2014 current state of artCurrent"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "5 min read\u00b7Dec 15, 2022"}, {"url": "https://medium.com/@anandmishra.kanha/deep-reinforcement-learning-current-state-of-art-383190b14464?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@sthanikamsanthosh1994?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Sthanikam Santhosh"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Custom Gym environment(Stock trading) for Reinforcement Learning(Stable baseline3)For designing any Reinforcement Learning(RL) the environment plays an important role. The success of any reinforcement learning model\u2026"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "8 min read\u00b7Dec 22, 2022"}, {"url": "https://medium.com/@sthanikamsanthosh1994/custom-gym-environment-stock-trading-for-reinforcement-learning-stable-baseline3-629a489d462d?source=read_next_recirc-----d195264329d0----0---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@tinkertytonk?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Steve Roberts"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "State Values and Policy Evaluation in 5 minutesAn Introduction to Reinforcement Learning"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "\u00b75 min read\u00b7Jan 11"}, {"url": "https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50?source=read_next_recirc-----d195264329d0----1---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----d195264329d0----2---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://medium.com/towards-data-science/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": ""}, {"url": "https://medium.com/@amaster_37400?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Aaron Master"}, {"url": "https://medium.com/towards-data-science?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Towards Data Science"}, {"url": "https://medium.com/towards-data-science/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "Please Stop Drawing Neural Networks WrongThe Case for GOOD Diagrams"}, {"url": "https://medium.com/towards-data-science/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------", "anchor_text": "12 min read\u00b7Mar 21"}, {"url": "https://medium.com/towards-data-science/please-stop-drawing-neural-networks-wrong-ffd02b67ad77?source=read_next_recirc-----d195264329d0----3---------------------ed39a691_e5f6_4081_88bd_a3a864e6af32-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "33"}, {"url": "https://medium.com/?source=post_page-----d195264329d0--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----d195264329d0--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----d195264329d0--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}