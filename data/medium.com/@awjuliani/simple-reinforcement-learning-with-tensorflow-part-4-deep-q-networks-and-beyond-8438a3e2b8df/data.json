{"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df", "time": 1683019526.9034219, "path": "medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df/", "webpage": {"metadata": {"title": "Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond | by Arthur Juliani | Medium", "h1": "Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond", "description": "Welcome to the latest installment of my Reinforcement Learning series. In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer\u2026"}, "outgoing_paragraph_urls": [{"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.a5pppei4l", "anchor_text": "Part 0", "paragraph_index": 0}, {"url": "http://www.davidqiu.com:8888/research/nature14236.pdf", "anchor_text": "the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent", "paragraph_index": 1}, {"url": "http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963", "anchor_text": "similar to those of the primate visual cortex", "paragraph_index": 2}, {"url": "https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#convolution2d", "anchor_text": "Tensorflow documentation", "paragraph_index": 4}, {"url": "https://arxiv.org/pdf/1509.02971.pdf", "anchor_text": "another DeepMind paper", "paragraph_index": 7}, {"url": "http://www.davidqiu.com:8888/research/nature14236.pdf", "anchor_text": "described by DeepMind", "paragraph_index": 8}, {"url": "http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847", "anchor_text": "Double DQN", "paragraph_index": 9}, {"url": "http://arxiv.org/pdf/1511.06581.pdf", "anchor_text": "Dueling DQN", "paragraph_index": 10}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "Tutorial 0", "paragraph_index": 12}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating", "paragraph_index": 14}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Arthur Juliani", "paragraph_index": 15}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjliani", "paragraph_index": 15}], "all_paragraphs": ["Welcome to the latest installment of my Reinforcement Learning series. In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:", "It was these three innovations that allowed the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent. We will be walking through each individual improvement, and showing how to implement it. We won\u2019t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task.", "Since our agent is going to be learning to play video games, it has to be able to make sense of the game\u2019s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of the primate visual cortex. As such, they are ideal for the first few elements within our network.", "In Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:", "Here num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (\u201cVALID\u201d) or add padding around it (\u201cSAME\u201d) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the Tensorflow documentation.", "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent\u2019s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.", "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network\u2019s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network\u2019s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.", "Instead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.", "With the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture described by DeepMind, have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.", "The main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn\u2019t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.", "In order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:", "The goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn\u2019t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.", "Now that we have learned all the tricks to get the most out of our DQN, let\u2019s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine (I am using a GTX970). In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!", "The game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. The hyperparameters may need some tuning, but it is definitely possible. Good luck!", "If this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!", "If you\u2019d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjliani.", "More from my Simple Reinforcement Learning with Tensorflow series:", "Postdoctoral researcher at Microsoft. Interested in artificial intelligence, neuroscience, philosophy, and meditation."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F18dfe63fa7f0&operation=register&redirect=https%3A%2F%2Fawjuliani.medium.com%2Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df&user=Arthur+Juliani&userId=18dfe63fa7f0&source=post_page-18dfe63fa7f0----8438a3e2b8df---------------------post_header-----------", "anchor_text": "Follow"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.a5pppei4l", "anchor_text": "Part 0"}, {"url": "http://www.davidqiu.com:8888/research/nature14236.pdf", "anchor_text": "the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent"}, {"url": "http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963", "anchor_text": "similar to those of the primate visual cortex"}, {"url": "https://www.tensorflow.org/versions/r0.10/api_docs/python/contrib.layers.html#convolution2d", "anchor_text": "Tensorflow documentation"}, {"url": "https://arxiv.org/pdf/1509.02971.pdf", "anchor_text": "another DeepMind paper"}, {"url": "http://www.davidqiu.com:8888/research/nature14236.pdf", "anchor_text": "described by DeepMind"}, {"url": "http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12389/11847", "anchor_text": "Double DQN"}, {"url": "http://arxiv.org/pdf/1511.06581.pdf", "anchor_text": "Dueling DQN"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "Tutorial 0"}, {"url": "https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb", "anchor_text": "awjuliani/DeepRL-AgentsDeepRL-Agents - A set of Deep Reinforcement Learning Agents implemented in Tensorflow.github.com"}, {"url": "https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=V2R22DV4XSR5Y&lc=US&item_name=Arthur%20Juliani%27s%20Deep%20Learning%20Tutorials&currency_code=USD&bn=PP%2dDonationsBF%3abtn_donateCC_LG%2egif%3aNonHosted", "anchor_text": "donating"}, {"url": "https://medium.com/u/18dfe63fa7f0?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Arthur Juliani"}, {"url": "https://twitter.com/awjuliani", "anchor_text": "@awjliani"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0", "anchor_text": "Part 0 \u2014 Q-Learning Agents"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149", "anchor_text": "Part 1 \u2014 Two-Armed Bandit"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c", "anchor_text": "Part 1.5 \u2014 Contextual Bandits"}, {"url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724", "anchor_text": "Part 2 \u2014 Policy-Based Agents"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99", "anchor_text": "Part 3 \u2014 Model-Based RL"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a#.kdgfgy7k8", "anchor_text": "Part 5 \u2014 Visualizing an Agent\u2019s Thoughts and Actions"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc#.gi4xdq8pk", "anchor_text": "Part 6 \u2014 Partial Observability and Deep Recurrent Q-Networks"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf", "anchor_text": "Part 7 \u2014 Action-Selection Strategies for Exploration"}, {"url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2#.hg13tn9zw", "anchor_text": "Part 8 \u2014 Asynchronous Actor-Critic Agents (A3C)"}, {"url": "https://medium.com/tag/machine-learning?source=post_page-----8438a3e2b8df---------------machine_learning-----------------", "anchor_text": "Machine Learning"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----8438a3e2b8df---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/deep-learning?source=post_page-----8438a3e2b8df---------------deep_learning-----------------", "anchor_text": "Deep Learning"}, {"url": "https://medium.com/tag/computer-science?source=post_page-----8438a3e2b8df---------------computer_science-----------------", "anchor_text": "Computer Science"}, {"url": "https://medium.com/tag/robotics?source=post_page-----8438a3e2b8df---------------robotics-----------------", "anchor_text": "Robotics"}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fawjuliani.medium.com%2Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----8438a3e2b8df---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Written by Arthur Juliani"}, {"url": "https://medium.com/followers?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "12.6K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Fc3760ec725a6&operation=register&redirect=https%3A%2F%2Fawjuliani.medium.com%2Fsimple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df&newsletterV3=18dfe63fa7f0&newsletterV3Id=c3760ec725a6&user=Arthur+Juliani&userId=18dfe63fa7f0&source=-----8438a3e2b8df---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "MAOIs, SSRIs, and PsychedelicsThree mechanisms of action for antidepressant drugs"}, {"url": "https://medium.com/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "10 min read\u00b7Apr 13"}, {"url": "https://medium.com/maois-ssris-and-psychedelics-9175fe1aac3f?source=author_recirc-----8438a3e2b8df----0---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "6"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural NetworksFor this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms\u2026"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "6 min read\u00b7Aug 25, 2016"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0?source=author_recirc-----8438a3e2b8df----1---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "112"}, {"url": "https://medium.com/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Arthur Juliani"}, {"url": "https://betterprogramming.pub/?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Better Programming"}, {"url": "https://medium.com/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Large Language Models Don\u2019t \u201cHallucinate\u201dUsing this term attributes properties to the LLMs they don\u2019t have while also ignoring the real dynamics behind their made-up information"}, {"url": "https://medium.com/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "6 min read\u00b7Mar 10"}, {"url": "https://medium.com/large-language-models-dont-hallucinate-b9bdfa202edf?source=author_recirc-----8438a3e2b8df----2---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "23"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": ""}, {"url": "https://medium.com/?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Arthur Juliani"}, {"url": "https://medium.com/emergent-future?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Emergent // Future"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will\u2026"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------", "anchor_text": "8 min read\u00b7Dec 17, 2016"}, {"url": "https://medium.com/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=author_recirc-----8438a3e2b8df----3---------------------cc268ab3_5c1d_4197_b74b_31082818a556-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "86"}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "See all from Arthur Juliani"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "AI Anyone Can Understand Part 1: Reinforcement LearningReinforcement learning is a way for machines to learn by trying different things and seeing what works best. For example, a robot could\u2026"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-reinforcement-learning-6c3b3d623a2d?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://thepycoach.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://thepycoach.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "The PyCoach"}, {"url": "https://artificialcorner.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Artificial Corner"}, {"url": "https://thepycoach.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "You\u2019re Using ChatGPT Wrong! Here\u2019s How to Be Ahead of 99% of ChatGPT UsersMaster ChatGPT by learning prompt engineering."}, {"url": "https://thepycoach.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b77 min read\u00b7Mar 17"}, {"url": "https://thepycoach.com/youre-using-chatgpt-wrong-here-s-how-to-be-ahead-of-99-of-chatgpt-users-886a50dabc54?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "274"}, {"url": "https://wvheeswijk.medium.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Towards Data Science"}, {"url": "https://wvheeswijk.medium.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Proximal Policy Optimization (PPO) ExplainedThe journey from REINFORCE to the go-to algorithm in continuous control"}, {"url": "https://wvheeswijk.medium.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b713 min read\u00b7Nov 29, 2022"}, {"url": "https://wvheeswijk.medium.com/proximal-policy-optimization-ppo-explained-abed1952457b?source=read_next_recirc-----8438a3e2b8df----0---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "2"}, {"url": "https://byfintech.medium.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://byfintech.medium.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Bruce Yang ByFinTech"}, {"url": "https://medium.datadriveninvestor.com/?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "DataDrivenInvestor"}, {"url": "https://byfintech.medium.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement LearningNeurIPS 2022 Datasets and Benchmarks."}, {"url": "https://byfintech.medium.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b79 min read\u00b7Nov 13, 2022"}, {"url": "https://byfintech.medium.com/finrl-meta-market-environments-and-benchmarks-for-data-driven-financial-reinforcement-learning-7af8e747c4bd?source=read_next_recirc-----8438a3e2b8df----1---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://medium.com/@MoneyAndData?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Andrew Austin"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "AI Anyone Can Understand: Part 2 \u2014 The Bellman EquationMake sure you check out the rest of the AI Anyone Can Understand Series I have written and plan to continue to write on"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b74 min read\u00b7Dec 11, 2022"}, {"url": "https://medium.com/@MoneyAndData/ai-anyone-can-understand-part-1-the-bellman-equation-614846383eb7?source=read_next_recirc-----8438a3e2b8df----2---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": ""}, {"url": "https://wvheeswijk.medium.com/?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Wouter van Heeswijk, PhD"}, {"url": "https://towardsdatascience.com/?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Towards Data Science"}, {"url": "https://wvheeswijk.medium.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "Rainbow DQN \u2014 The Best Reinforcement Learning Has to Offer?What happens if the most successful techniques in Deep Q-Learning are combined into a single algorithm?"}, {"url": "https://wvheeswijk.medium.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------", "anchor_text": "\u00b711 min read\u00b7Dec 8, 2022"}, {"url": "https://wvheeswijk.medium.com/rainbow-dqn-the-best-reinforcement-learning-has-to-offer-166cb8ed2f86?source=read_next_recirc-----8438a3e2b8df----3---------------------568673bf_43b4_4d61_9cb5_d91018ac6bc7-------&responsesOpen=true&sortBy=REVERSE_CHRON", "anchor_text": "1"}, {"url": "https://medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "See more recommendations"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=post_page-----8438a3e2b8df--------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}