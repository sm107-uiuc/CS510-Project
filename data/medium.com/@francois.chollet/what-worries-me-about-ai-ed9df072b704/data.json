{"url": "https://medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704", "time": 1683019513.958373, "path": "medium.com/@francois.chollet/what-worries-me-about-ai-ed9df072b704/", "webpage": {"metadata": {"title": "What worries me about AI. Disclaimer: These are my own personal\u2026 | by Fran\u00e7ois Chollet | Medium", "h1": "What worries me about AI", "description": "Disclaimer: These are my own personal views. I do not speak for my employer. If you quote this article, please have the honesty to present these ideas as what they are: personal, speculative\u2026"}, "outgoing_paragraph_urls": [{"url": "https://www.theatlantic.com/technology/archive/2015/03/when-people-feared-computers/388919/", "anchor_text": "\u201ccomputerphobia\u201d", "paragraph_index": 1}, {"url": "http://environment.yale.edu/ycom/factsheets/MapPage/2017Rev/?est=worried&type=value&geo=national", "anchor_text": "a large fraction (44%) of the American public still chooses to ignore it", "paragraph_index": 4}, {"url": "http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805", "anchor_text": "the harmful biases of machine learning models", "paragraph_index": 6}, {"url": "http://www.pnas.org/content/112/4/1036", "anchor_text": "Facebook \u201clikes\u201d enable algorithms to better assess your personality that your own friends could", "paragraph_index": 10}, {"url": "https://techcrunch.com/2014/02/14/facebook-love-data/", "anchor_text": "predict a few days in advance when you will start a new relationship (and with whom)", "paragraph_index": 10}, {"url": "https://www.scientificamerican.com/article/can-facebooks-machine-learning-algorithms-accurately-predict-suicide/", "anchor_text": "who is at risk of suicide", "paragraph_index": 10}, {"url": "https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds", "anchor_text": "It can influence your mood", "paragraph_index": 13}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "logistic regression", "paragraph_index": 19}, {"url": "https://en.wikipedia.org/wiki/Social_Credit_System", "anchor_text": "\u201csocial credit system\u201d", "paragraph_index": 21}], "all_paragraphs": ["Disclaimer: These are my own personal views. I do not speak for my employer. If you quote this article, please have the honesty to present these ideas as what they are: personal, speculative opinions, to be judged on their own merits.", "If you were around in the 1980s and 1990s, you may remember the now-extinct phenomenon of \u201ccomputerphobia\u201d. I have personally witnessed it a few times as late as the early 2000s \u2014 as personal computers were introduced into our lives, in our workplaces and homes, quite a few people would react with anxiety, fear, or even aggressivity. While some of us were fascinated by computers and awestruck by the potential they could glimpse in them, most people didn\u2019t understand them. They felt alien, abstruse, and in many ways, threatening. People feared getting replaced by technology.", "Most of us react to technological shifts with unease at best, panic at worst. Maybe that is true of any change at all. But remarkably, most of what we worry about ends up never happening.", "Fast-forward a few years, and the computer-haters have learned to live with them and to use them for their own benefit. Computers did not replace us and trigger mass unemployment \u2014 and nowadays we couldn\u2019t imagine life without our laptops, tablets, and smartphones. Threatening change has become comfortable status quo. But at the same time as our fears failed to materialize, computers and the internet have enabled threats that almost no one was warning us about in the 1980s and 1990s. Ubiquitous mass surveillance. Hackers going after our infrastructure or our personal data. Psychological alienation on social media. The loss of our patience and our ability to focus. The political or religious radicalization of easily-influenced minds online. Hostile foreign powers hijacking social networks to disrupt Western democracies.", "If most of our fears turn out to be irrational, inversely, most of the truly worrying developments that have happened in the past as a result of technological change stem from things that most people didn\u2019t worry about until it was already there. A hundred years ago, we couldn\u2019t really forecast that the transportation and manufacturing technologies we were developing would enable a new form of industrial warfare that would wipe out tens of millions in two World Wars. We didn\u2019t recognize early on that the invention of the radio would enable a new form of mass propaganda that would facilitate the rise of fascism in Italy and Germany. The progress of theoretical physics in the 1920s and 1930s wasn\u2019t accompanied by anxious press articles about how these developments would soon enable thermonuclear weapons that would place the world forever under the threat of imminent annihilation. And today, even as alarms have been sounding for decades about the most dire problem of our times, climate, a large fraction (44%) of the American public still chooses to ignore it. As a civilization, we seem to be really bad at correctly identifying future threats and rightfully worrying about them, just as we seem to be extremely prone to panic due to irrational fears.", "Today, like many times in the past, we are faced with a new wave of radical change: cognitive automation, which could be broadly summed up under the keyword \u201cAI\u201d. And like many time in the past, we are worried that this new set of technologies will harm us \u2014 that AI will lead to mass unemployment, or that AI will gain an agency of its own, become superhuman, and choose to destroy us.", "But what if we\u2019re worrying about the wrong thing, like we have almost every single time before? What if the real danger of AI was far remote from the \u201csuperintelligence\u201d and \u201csingularity\u201d narratives that many are panicking about today? In this post, I\u2019d like to raise awareness about what really worries me when it comes to AI: the highly effective, highly scalable manipulation of human behavior that AI enables, and its malicious use by corporations and governments. Of course, this is not the only tangible risk that arises from the development of cognitive technologies \u2014 there are many others, in particular issues related to the harmful biases of machine learning models. Other people are raising awareness of these problems far better than I could. I chose to write about mass population manipulation specifically because I see this risk as pressing and direly under-appreciated.", "This risk is already a reality today, and a number of long-term technological trends are going to considerably amplify it over the next few decades. As our lives become increasingly digitized, social media companies get ever greater visibility into our lives and minds. At the same time, they gain increasing access to behavioral control vectors \u2014 in particular via algorithmic newsfeeds, which control our information consumption. This casts human behavior as an optimization problem, as an AI problem: it becomes possible for social media companies to iteratively tune their control vectors in order to achieve specific behaviors, just like a game AI would iterative refine its play strategy in order to beat a level, driven by score feedback. The only bottleneck to this process is the intelligence of the algorithm in the loop \u2014 and as it happens, the largest social network company is currently investing billions in fundamental AI research.", "In the past 20 years, our private and public lives have moved online. We spend an ever greater fraction of each day staring at screens. Our world is moving to a state where most of what we do consists of digital information consumption, modification, or creation.", "A side effect of this long-term trend is that corporations and governments are now collecting staggering amounts of data about us, in particular through social network services. Who we communicate with. What we say. What content we\u2019ve been consuming \u2014 images, movies, music, news. What mood we are in at specific times. Ultimately, almost everything we perceive and everything we do will end up recorded on some remote server.", "This data, in theory, allows the entities that collect it to build extremely accurate psychological profiles of both individuals and groups. Your opinions and behavior can be cross-correlated with that of thousands of similar people, achieving an uncanny understanding of what makes you tick \u2014 probably more predictive than what yourself could achieve through mere introspection (for instance, Facebook \u201clikes\u201d enable algorithms to better assess your personality that your own friends could). This data makes it possible to predict a few days in advance when you will start a new relationship (and with whom), and when you will end your current one. Or who is at risk of suicide. Or which side you will ultimately vote for in an election, even while you\u2019re still feeling undecided. And it\u2019s not just individual-level profiling power \u2014 large groups can be even more predictable, as aggregating data points erases randomness and individual outliers.", "Passive data collection is not where it ends. Increasingly, social network services are in control of what information we consume. What see in our newsfeeds has become algorithmically \u201ccurated\u201d. Opaque social media algorithms get to decide, to an ever-increasing extent, which political articles we read, which movie trailers we see, who we keep in touch with, whose feedback we receive on the opinions we express.", "Integrated over many years of exposure, the algorithmic curation of the information we consume gives the algorithms in charge considerable power over our lives \u2014 over who we are, who we become. If Facebook gets to decide, over the span of many years, which news you will see (real or fake), whose political status updates you\u2019ll see, and who will see yours, then Facebook is in effect in control of your worldview and your political beliefs.", "Facebook\u2019s business lies in influencing people. That\u2019s what the service it sells to its customers \u2014 advertisers, including political advertisers. As such, Facebook has built a fine-tuned algorithmic engine that does just that. This engine isn\u2019t merely capable of influencing your view of a brand or your next smart-speaker purchase. It can influence your mood, tuning the content it feeds you in order to make you angry or happy, at will. It may even be able to swing elections.", "In short, social network companies can simultaneously measure everything about us, and control the information we consume. And that\u2019s an accelerating trend. When you have access to both perception and action, you\u2019re looking at an AI problem. You can start establishing an optimization loop for human behavior, in which you observe the current state of your targets and keep tuning what information you feed them, until you start observing the opinions and behaviors you wanted to see. A large subset of the field of AI \u2014 in particular \u201creinforcement learning\u201d \u2014 is about developing algorithms to solve such optimization problems as efficiently as possible, to close the loop and achieve full control of the target at hand \u2014 in this case, us. By moving our lives to the digital realm, we become vulnerable to that which rules it \u2014 AI algorithms.", "This is made all the easier by the fact that the human mind is highly vulnerable to simple patterns of social manipulation. Consider, for instance, the following vectors of attack:", "From an information security perspective, you would call these vulnerabilities: known exploits that can be used to take over a system. In the case of the human minds, these vulnerabilities never get patched, they are just the way we work. They\u2019re in our DNA. The human mind is a static, vulnerable system that will come increasingly under attack from ever-smarter AI algorithms that will simultaneously have a complete view of everything we do and believe, and complete control of the information we consume.", "Remarkably, mass population manipulation \u2014 in particular political control \u2014 arising from placing AI algorithms in charge of our information diet does not necessarily require very advanced AI. You don\u2019t need self-aware, superintelligent AI for this to be a dire threat \u2014 current technology may well suffice. Social network companies have been working on it for a few years, with significant results. And while they may only be trying to maximize \u201cengagement\u201d and to influence your purchase decisions, rather than to manipulate your view of the world, the tools they\u2019ve developed are already being hijacked by hostile state actors for political purposes \u2014 as seen in the 2016 Brexit referendum or the 2016 US presidential election. This is already our reality. But if mass population manipulation is already possible today \u2014 in theory \u2014 why hasn\u2019t the world been upended yet?", "In short, I think it\u2019s because we\u2019re really bad at AI. But that may be about to change.", "Until 2015, all ad targeting algorithms across the industry were running on mere logistic regression. In fact, that\u2019s still true to a large extent today \u2014 only the biggest players have switched to more advanced models. Logistic regression, an algorithm that predates the computing era, is one of the most basic techniques you could use for personalization. It is the reason why so many of the ads you see online are desperately irrelevant. Likewise, the social media bots used by hostile state actors to sway public opinion have little to no AI in them. They\u2019re all extremely primitive. For now.", "Machine learning and AI have been making fast progress in recent years, and that progress is only beginning to get deployed in targeting algorithms and social media bots. Deep learning has only started to make its way into newsfeeds and ad networks in 2016. Who knows what will be next. It is quite striking that Facebook has been investing enormous amounts in AI research and development, with the explicit goal of becoming a leader in the field. When your product is a social newsfeed, what use are you going to make of natural language processing and reinforcement learning?", "We\u2019re looking at a company that builds fine-grained psychological profiles of almost two billion humans, that serves as a primary news source for many of them, that runs large-scale behavior manipulation experiments, and that aims at developing the best AI technology the world has ever seen. Personally, it scares me. And consider that Facebook may not even be the most worrying threat here. Ponder, for instance, China\u2019s use of information control to enable unprecedented forms of totalitarianism, such as its \u201csocial credit system\u201d. Many people like to pretend that large corporations are the all-powerful rulers of the modern world, but what power they hold is dwarfed by that of governments. If given algorithmic control over our minds, governments may well turn into far worst actors than corporations.", "Now, what can we do about it? How can we defend ourselves? As technologists, what can we do to avert the risk of mass manipulation via our social newsfeeds?", "Importantly, the existence of this threat doesn\u2019t mean that all algorithmic curation is bad, or that all targeted advertising is bad. Far from it. Both of these can serve a valuable purpose.", "With the rise of the Internet and AI, placing algorithms in charge of our information diet isn\u2019t just an inevitable trend \u2014 it\u2019s a desirable one. As our lives become increasingly digital and connected, and as our world becomes increasingly information-intensive, we will need AI to serve as our interface to the world. In the long-run, education and self-development will be some of the most impactful applications of AI \u2014 and this will happen through dynamics that almost entirely mirror that of a nefarious AI-enabled newsfeed trying to manipulate you. Algorithmic information management has tremendous potential to help us, to empower individuals to realize more of their potential, and to help society better manage itself.", "The issue is not AI itself. The issue is control.", "Instead of letting newsfeed algorithms manipulate the user to achieve opaque goals, such as swaying their political opinions, or maximally wasting their time, we should put the user in charge of the goals that the algorithms optimize for. We are talking, after all, about your news, your worldview, your friends, your life \u2014 the impact that technology has on you should naturally be placed under your own control. Information management algorithms should not be a mysterious force inflicted on us to serve ends that run opposite to our own interests; instead, they should be a tool in our hand. A tool that we can use for our own purposes, say, for education and personal instead of entertainment.", "Here\u2019s an idea \u2014 any algorithmic newsfeed with significant adoption should:", "We should build AI to serve humans, not to manipulate them for profit or political gain. What if newsfeed algorithms didn\u2019t operate like casino operators or propagandists? What if instead, they were closer to a mentor or a good librarian, someone who used their keen understanding of your psychology \u2014 and that of millions of other similar people \u2014 to recommend to you that next book that will most resonate with your objectives and make you grow. A sort of navigation tool for your life \u2014 an AI capable of guiding you through the optimal path in experience space to get where you want to go. Can you imagine looking at your own life through the lens of a system that has seen millions of lives unfold? Or writing a book together with a system that has read every book? Or conducting research in collaboration with a system that sees the full scope of current human knowledge?", "In products where you are fully in control of the AI that interacts with you, a more sophisticated algorithm, instead of being a threat, would be a net positive, letting you achieve your own goals more efficiently.", "In summary, our future is one where AI will be our interface to the world \u2014 a world made of digital information. This can equally lead to empowering individuals to gain greater control over their lives, or to a total loss of agency. Unfortunately, social media is currently engaged on the wrong road. But it\u2019s still early enough that we can reverse course.", "As an industry, we need to develop product categories and markets where the incentives are aligned with placing the user in charge of the algorithms that affect them, instead of using AI to exploit the user\u2019s mind for profit or political gain. We need to strive towards products that are the anti-Facebook.", "In the far future, such products will likely take the form of AI assistants. Digital mentors programmed to help you, that put you in control of the objectives they pursue in their interactions with you. And in the present, search engines could be seen as an early, more primitive example of an AI-driven information interface that serves users instead of seeking to hijack their mental space. Search is a tool that you deliberately use to reach specific goals, rather than a passive always-on feed that elects what to show you. You tell it what to it should do for you. And instead of seeking to maximally waste your time, a search engine attempts to minimize the time it takes to go from question to answer, from problem to solution.", "You may be thinking, since a search engine is still an AI layer between us and the information we consume, could it bias its results to attempt to manipulate us? Yes, that risk is latent in every information-management algorithm. But in stark contrast with social networks, market incentives in this case are actually aligned with users needs, pushing search engines to be as relevant and objective as possible. If they fail to be maximally useful, there\u2019s essentially no friction for users to move to a competing product. And importantly, a search engine would have a considerably smaller psychological attack surface than a social newsfeed. The threat we\u2019ve profiled in this post requires most of the following to be present in a product:", "Most AI-driven information-management products don\u2019t meet these requirements. Social networks, on the other hand, are a frightening combination of risk factors. As technologists, we should gravitate towards products that do not feature these characteristics, and push back against products that combine them all, if only because of their potential for dangerous misuse. Build search engines and digital assistants, not social newsfeeds. Make your recommendation engines transparent, configurable, and constructive, rather than slot-like machines that maximize \u201cengagement\u201d and wasted hours of human time. Invest your UI, UX, and AI expertise into building great configuration panels for your algorithm, to enable your users to use your product on their own terms.", "And importantly, we should educate users about these issues, so that they reject manipulative products, generating enough market pressure to align the incentives of the technology industry with that of consumers.", "Conclusion: the fork in the road ahead", "One path leads to a place that really scares me. The other leads to a more humane future. There\u2019s still time to take the better one. If you work on these technologies, keep this in mind. You may not have evil intentions. You may simply not care. You may simply value your RSUs more than our shared future. But whether or not you care, because you have a hand in shaping the infrastructure of the digital world, your choices affect us all. And you may eventually be held responsible for them."], "all_outgoing_urls": [{"url": "https://medium.com/?source=---two_column_layout_nav----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@francois.chollet?source=post_page-----ed9df072b704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@francois.chollet?source=post_page-----ed9df072b704--------------------------------", "anchor_text": "Fran\u00e7ois Chollet"}, {"url": "https://www.theatlantic.com/technology/archive/2015/03/when-people-feared-computers/388919/", "anchor_text": "\u201ccomputerphobia\u201d"}, {"url": "http://environment.yale.edu/ycom/factsheets/MapPage/2017Rev/?est=worried&type=value&geo=national", "anchor_text": "a large fraction (44%) of the American public still chooses to ignore it"}, {"url": "https://www.facebook.com/zuck", "anchor_text": "acebook.com/zuck"}, {"url": "http://www.nature.com/news/there-is-a-blind-spot-in-ai-research-1.20805", "anchor_text": "the harmful biases of machine learning models"}, {"url": "http://www.pnas.org/content/112/4/1036", "anchor_text": "Facebook \u201clikes\u201d enable algorithms to better assess your personality that your own friends could"}, {"url": "https://techcrunch.com/2014/02/14/facebook-love-data/", "anchor_text": "predict a few days in advance when you will start a new relationship (and with whom)"}, {"url": "https://www.scientificamerican.com/article/can-facebooks-machine-learning-algorithms-accurately-predict-suicide/", "anchor_text": "who is at risk of suicide"}, {"url": "https://www.theguardian.com/technology/2014/jun/29/facebook-users-emotions-news-feeds", "anchor_text": "It can influence your mood"}, {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "anchor_text": "logistic regression"}, {"url": "https://en.wikipedia.org/wiki/Social_Credit_System", "anchor_text": "\u201csocial credit system\u201d"}, {"url": "https://medium.com/tag/artificial-intelligence?source=post_page-----ed9df072b704---------------artificial_intelligence-----------------", "anchor_text": "Artificial Intelligence"}, {"url": "https://medium.com/tag/social-media?source=post_page-----ed9df072b704---------------social_media-----------------", "anchor_text": "Social Media"}, {"url": "https://medium.com/tag/technology?source=post_page-----ed9df072b704---------------technology-----------------", "anchor_text": "Technology"}, {"url": "https://medium.com/@francois.chollet?source=post_page-----ed9df072b704--------------------------------", "anchor_text": "More from Fran\u00e7ois Chollet"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F946edb567ea1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40francois.chollet%2Fwhat-worries-me-about-ai-ed9df072b704&newsletterV3=7462d2319de7&newsletterV3Id=946edb567ea1&user=Fran%C3%A7ois+Chollet&userId=7462d2319de7&source=-----ed9df072b704---------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://medium.com/?source=post_page-----ed9df072b704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/about?autoplay=1&source=post_page-----ed9df072b704--------------------------------", "anchor_text": "About"}, {"url": "https://help.medium.com/hc/en-us?source=post_page-----ed9df072b704--------------------------------", "anchor_text": "Help"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----ed9df072b704--------------------------------", "anchor_text": "Terms"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----ed9df072b704--------------------------------", "anchor_text": "Privacy"}, {"url": "https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&mt=8&ct=post_page&source=post_page-----ed9df072b704--------------------------------", "anchor_text": ""}, {"url": "https://play.google.com/store/apps/details?id=com.medium.reader&source=post_page-----ed9df072b704--------------------------------", "anchor_text": ""}, {"url": "https://medium.com/plans?source=upgrade_membership---two_column_layout_sidebar----------------------------------", "anchor_text": "Get unlimited access"}, {"url": "https://medium.com/@francois.chollet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": ""}, {"url": "https://medium.com/@francois.chollet?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Fran\u00e7ois Chollet"}, {"url": "https://medium.com/@francois.chollet/followers?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "8.7K Followers"}, {"url": "https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F946edb567ea1&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40francois.chollet%2Fwhat-worries-me-about-ai-ed9df072b704&newsletterV3=7462d2319de7&newsletterV3Id=946edb567ea1&user=Fran%C3%A7ois+Chollet&userId=7462d2319de7&source=---two_column_layout_sidebar-----------------------subscribe_user-----------", "anchor_text": ""}, {"url": "https://help.medium.com/hc/en-us?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Help"}, {"url": "https://medium.statuspage.io/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Status"}, {"url": "https://about.medium.com/creators/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Writers"}, {"url": "https://blog.medium.com/?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Blog"}, {"url": "https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Careers"}, {"url": "https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Privacy"}, {"url": "https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Terms"}, {"url": "https://medium.com/about?autoplay=1&source=---two_column_layout_sidebar----------------------------------", "anchor_text": "About"}, {"url": "https://speechify.com/medium?source=---two_column_layout_sidebar----------------------------------", "anchor_text": "Text to speech"}]}, "scrape_status": {"code": "1"}}