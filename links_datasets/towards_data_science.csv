,Title,Links,Description
0,Does Deep Learning always have to Reinvent the Wheel?,https://towardsdatascience.com/does-deep-learning-always-have-to-reinvent-the-wheel-2c526018c5c5?source=collection_category---4------0-----------------------,"Machine learning and in particular deep learning revolutionize the world as we know it today. We have seen tremendous advances in speech and image recognition, followed by the application of deep learning to many other domains. In many of those domains, deep learning is now the state of the art or is even going beyond it. A clear trend is that networks are growing more and more complex and more and more computationally demanding.Today, we are building ever-increasing networks that are built on top of previous generations of network topologies. As neural networks are inherently compatible with other neural networks, we are able to combine and adapt them to new purposes. If you aim to tackle a new problem there are no clear guidelines that define an appropriate network topology. The most common approaches are to have a look at the work of others that attempted to solve similar problems or to design an entirely new topology on your own. This new design is often inspired by classical methods, but it is up to the network and the training data to learn the correct weights such that they converge to a plausible solution. As such they are even networks that learn well-known functions such as the Fourier transform from scratch. With the discrete Fourier transform being a matrix multiplication, it is often modeled as a fully connected layer. With this approach it is immediately clear that two disadvantages cannot be avoided: First, the fully connected layer introduces a lot of free parameters that may model entirely different functions. Second, the computational efficiency of a fast Fourier transform can never be reached with this approach.If we already know that a specific function is required to solve a particular problem, it comes to our mind to ask the question of whether it would not be of advantage to include it into the structure of our network as a kind of prior knowledge. The method of Known Operator Learning investigates exactly this procedure in a new theoretical framework. While this idea seems simple and intuitive, the theoretical analysis identifies also clear advantages: First, the introduction of a known operation into a neural network always results in a lower or equal maximal training error bound. Second, the number of free parameters in the model is reduced and therewith also the size of the required training data is reduced. Another interesting observation is that any operation that allows the computation of a gradient with respect to the inputs may be embedded into a neural network. In fact, even a sub-gradient is already sufficient as we know from, e.g., max-pooling operations.Interestingly, this piece of theory was only published in 2019. It was developed for the theoretical analysis of embedding of physical prior knowledge into neural networks. Yet, the observations also very nicely explain why we see the tremendous success of convolutional neural networks and pooling layers. In analogy to biology, we could argue that convolution and pooling operations are prior knowledge on perception. Recent work goes even further: there exist approaches that even include complicated filter functions such as Vesselness Filter or the Guided Filter into a neural network.The theoretical analysis also shows that modeling errors in earlier layers are amplified by subsequent layers. This observation is also in line with the importance of feature extraction in classical machine learning and pattern analysis. A combination of feature extraction and classification as it is done in deep learning, allows us to synchronize both processes and therewith reduces the expected error after training.As precision learning allows the combination of classical theoretical approaches and deep learning, we are now able to drive these ideas even one step further: A recent publication proposes to derive an entire neural network topography for a specific problem from the underlying physical equations. The beauty of this approach is that many of the operators and building blocks of the topology are well-known and can be implemented efficiently. Yet, they were still operations that are computationally very inefficient. However, we know from other solutions to similar problems that particular matrix inverses or other less tractable operations can be represented by other functions. In this example, an expensive matrix inverse is replaced with a circulant matrix, i.e., a convolutional layer which is the only learnable part of the proposed network. In their experiments, they demonstrate that the proposed architecture indeed tackles the problem that could previously only be approximately solved. Although they only trained on simulated data, the application on real data is also successful. Hence, the inclusion of prior knowledge also supports building network architectures that generalize well towards specific problems.We think that these new approaches are interesting towards the community of deep learning that is going well beyond only modeling perceptual tasks today. To us, it is exciting to see that traditional approaches are inherently compatible with everything that is done today in deep learning. Hence, we believe that there are many more new developments to come in the field of machine and deep learning in the near future and it will be exciting to follow up on them.If you think that these observations are interesting and exciting, we recommend reading our gentle introduction into deep learning as a follow up on this article (link ) or our free Deep Learning resources.Text and images of this article are licensed under Creative Commons License 4.0 Attribution. So feel free to reuse and share any part of this work. This article was published first at MarkTechPost.com.WRITTEN BY"
1,Roadmap to Computer Vision,https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4?source=collection_category---4------1-----------------------,"Computer Vision (CV) is nowadays one of the main application of Artificial Intelligence (eg. Image Recognition, Object Tracking, Multilabel Classification). In this article, I will walk you through some of the main steps which compose a Computer Vision System.A standard representation of the workflow of a Computer Vision system is:We will now briefly walk through some of the main processes our data might go through each of these three different steps.When trying to implement a CV system, we need to take into consideration two main components: the image acquisition hardware and the image processing software. One of the main requirements to meet in order to deploy a CV system is to test its robustness. Our system should, in fact, be able to be invariant to environmental changes (such as changes in illumination, orientation, scaling) and able to perform it’s designed task repeatably. In order to satisfy these requirements, it might be necessary to apply some form of constraints to either the hardware or software of our system (eg. remotely control the lighting environment).Once an image is acquired from a hardware device, there are many possible ways to numerically represents colours (Colour Spaces) within a software system. Two of the most famous colour spaces are RGB (Red, Green, Blue) and HSV (Hue, Saturation, Value). One of the main advantages of using an HSV colour space is that by taking just the HS components we can make our system illumination invariant (Figure 1).Once an image enters a system and is represented by using a colour space, we can then apply different operators on the image in order to improve its representation:Once pre-processed an image, we can then apply more advanced techniques in order to try to extract the edges and shapes within an image by using methods such as First Order Edge Detection (eg. Prewitt Operator, Sobel Operator, Canny Edge Detector) and Hough Transforms.Once pre-processed an image, there are 4 main types of Feature Morphologies which can be extracted from an image by using a Feature Extractor:Once extracted a set of discriminative features, we can then use them in order to train a Machine Learning model to make inference. Feature descriptors can be easily applied in Python using libraries such as OpenCV.One of the main concept used in Computer Vision to classify an image is the Bag of Visual Words (BoVW). In order to construct a Bag of Visual Words, we need first of all to create a vocabulary by extracting all the features from a set of images (eg. using grid-based features or local features). Successively, we can then count the number of times an extracted feature appears in an image and build a frequency histogram from the results. Using the frequency histogram as a basic template, we can finally classify if an image belongs to the same class or not by comparing their histograms (Figure 3).This process can be summarised in the following few steps:New images can then be classified by repeating this same process for each image we want to classify and then using any classification algorithm to find out which image in our vocabulary resembles the most our test image.Nowadays, thanks to the creation of Artificial Neural Networks architectures such as Convolutional Neural Networks (CNNs) and Recurrent Artificial Neural Networks (RCNNs), it has been possible to ideate an alternative workflow for Computer Vision (Figure 4).In this case, the Deep Learning Algorithm incorporates both the Feature Extraction and Classification steps of the Computer Vision workflow. When using Convolutional Neural Networks, each layer of the neural network applies the different feature extraction techniques at his description (eg. Layer 1 detects edges, Layer 2 finds shapes in an image, Layer 3 segments the image, etc…) before providing the feature vectors to the dense layer classifier.Further applications of Machine Learning in Computer Vision include areas such as Multilabel Classification and Object Recognition. In Multilabel Classification, we aim to construct a model able to correctly identify how many objects there are in an image and to what class they do belong to. In Object Recognition instead, we aim to take this concept a step further by identifying also the position of the different objects in the image.If you want to keep updated with my latest articles and projects follow me on Medium and subscribe to my mailing list. These are some of my contacts details:[1] Modular robot used as a beach cleaner, Felippe Roza. Researchgate. Accessed at: https://www.researchgate.net/figure/RGB-left-and-HSV-right-color-spaces_fig1_310474598[2] Bag of visual words in OpenCV, Vision & Graphics Group. Jan Kundrac. Accessed at: https://vgg.fiit.stuba.sk/2015-02/bag-of-visual-words-in-opencv/[3] Deep Learning Vs. Traditional Computer Vision. Haritha Thilakarathne, NaadiSpeaks. Accessed at: https://naadispeaks.wordpress.com/2018/08/12/deep-learning-vs-traditional-computer-vision/WRITTEN BY"
2,Should you really use machine learning for that?,https://towardsdatascience.com/should-you-really-use-machine-learning-for-that-d781a80aa0fb?source=collection_category---4------0-----------------------,"Disclaimer: The following is based on my observations of machine learning teams — not an academic survey of the industry. For context, I’m a contributor to Cortex, an open source platform for deploying models in production.Machine learning is in an awkward phase.Its viability has been thoroughly proven—all of the most popular mobile apps use it in some way—but the ecosystem hasn’t quite matured to the point to where the uninitiated can quickly ramp up.It’s difficult for teams to decide when to introduce machine learning, particularly if no one on the team is a data scientist. Software engineers—while typically having some high level understanding of machine learning—often lack the domain expertise to know whether their problem is suited to ML.The goal of this article is to present a series of informative questions for teams curious about using production machine learning. It isn’t to discuss what problems are theoretically solvable via machine learning, but rather to help teams that don’t have in-house data scientists understand whether or not applied machine learning could be effective.Without any experienced machine learning engineers or data scientists, the hardest question for you to answer will be “Is it even possible to solve this with machine learning?”You have three choices for answering this question:The first two will cost you significant time or money. The last one might take a day of Googling.Taking a look around the field will also have the added benefit of showing you where to begin. It is unlikely—given that you don’t have a data scientist on your team—that you will be designing your own model architecture for this problem. If you want to build a support agent, look at how other companies build theirs (you’ll inevitably come across Rasa and Google’s Meena).Getting a feel for what models and approaches have been used to solve problems like yours will give you a sense of where you should start. For example, Robert Lucian is an engineer who built a popular DIY license plate reader. His solution relied on a few pre-existing models for object detection and text extraction:As you can see from his write up, he began the machine learning portion of his project simply by looking around at what other people were using in similar domains. He eventually found a model that had been fine tuned specifically for license plates, as well as an effective model for text extraction. He was able to put both into production fairly quickly.Unless your problem is solvable using vanilla pre-trained models, you will need some relevant data to train your model.If you are building a recommendation engine, you will need data on user profile attributes as well as viewing habits. If you are building a customer support agent, you will need docs for it to train on. In order for a model to be tailored to your domain, you need data from said domain to train it on.However, this data doesn’t necessarily need to be proprietary. Even if you haven’t collected sophisticated data from your users, there still may be publicly available data sources you can leverage.For example, AI Dungeon is an ML-powered choose-your-own-adventure text game that went viral a few months ago:The game generates results on par with state-of-the-art models, despite the engineer behind it (Nick Walton) fine tuning his model with only 50 MB of text scraped from chooseyourstory.com. This approach worked thanks to transfer learning, a technique in which the “knowledge” of a model—in this case, OpenAI’s GPT-2—is transferred to a new model, fine tuned to a more specific domain (like dungeon crawler fiction) using a smaller data set.In many situations, machine learning is a tool for the job, but not necessarily the best one. If machine learning doesn’t provide tangible performance benefits over other solutions, it is not worth the extra overhead.You can analyze this pretty simply by asking a few questions.First, are there any solutions other than machine learning?For many problems, like speech recognition or some applications of computer vision, machine learning is currently the only viable solution.Second, can you replicate the quality of ML prediction with other solutions?Let’s say you’re building a recommendation system, for example. If you don’t collect much user information, and you only have 100 blog posts to recommend, you can probably use a basic tagging system (i.e. if a user likes Javascript, show them articles tagged “Javascript”):However, if you are curating a massive library of content and you have robust user data, machine learning is uniquely powerful in its ability to provide personalized recommendations.Finally, can your other solution scale as well as machine learning?One of the central promises of ML is that it is dynamic enough to remove the “human in the loop” from processes that traditionally require some manual intervention. For example, inventory management is a messy process. Products often have incomplete information, listed in inconsistent ways. As a result, manual processing is often required.For small quantities of products, manual processing is fine as an alternative to machine learning—but at scale, it can’t compare. To process one million products would require many humans working for many hours each, whereas a product like Glisten, which uses machine learning to parse product data, can do it rapidly:The problem is that oftentimes, machine learning’s cool factor leads to it being applied in situations that don’t entirely make sense, contributing to the general skepticism many have of machine learning as “just another hype cycle.”The reality is that just like any other popular technology, there are use cases where machine learning is the ideal solution, and others where it is not. Knowing whether or not your project calls for machine learning can be the hardest part of getting started—particularly if you aren’t experienced in the field—but hopefully, this is a helpful start.If you want to see some examples of production machine learning projects, check out the Cortex examples repo.WRITTEN BY"
3,How to Get Beautiful Results with Neural Style Transfer,https://towardsdatascience.com/how-to-get-beautiful-results-with-neural-style-transfer-75d0c05d6489?source=collection_category---4------1-----------------------,"I
recently became interested in generating a Medium profile picture with Machine Learning. This pulled me deep into the land of Neural Style Transfer. While NST is conceptually simple to understand, generating high quality images is surprisingly difficult. There are a lot of intricate details and unmentioned tricks that one must implement correctly in order to get great results. In this article, we’ll dive deep into Neural Style Transfer and examine what these tricks are in detail.There are a number of solid introductions to NST, both on Medium and other publications, so I won’t waste any time going over the basics. If you have no idea what NST is (or you want to follow along with the article) a great way to get started is by looking at the official PyTorch tutorial. Unfortunately, like many other introductory articles, the final implementation generates mediocre results at best (Figure 1). We’ll spend the next few sections updating the tutorial code to improve the transfer quality, but first we go on a tangent.All the accompanying code for this article can be found on my GitHub.Most of the Gatys et al. paper which introduced Neural Style Transfer is simple and straightforward to understand. However, one question that does not get addressed is why the Gram matrix is a natural way to represent style (i.e. texture)?At a high level, the Gram matrix measures the correlations between different feature maps in the same layer. A feature map is simply the post-activation output of a convolutional layer. For example, if a convolutional layer has 64 filters, it will output 64 feature maps. The Gram matrix then measures the correlation (similarity) between each feature map and every other feature map in the layer, without necessarily caring about exact pixel positions. To illustrate why this is a reasonable measure of texture, suppose we have two filters, one which detects blue things and one which detects spirals. We can apply these filters to an input image to produce 2 filter maps and measure their correlation. If the filter maps are highly correlated, then any spiral present in the image is almost certain to be blue. This means the texture of the image is composed of blue spirals, instead of red, green, or yellow spirals.Although this explanation still leaves me slightly uneasy, it seems to be a widely accepted fact in the texture synthesis community that the Gram matrix corresponds to style, as this article explains. Furthermore, we can’t deny that the results we get by using the Gram matrix are impressive.The first step to improve transfer quality is to fix the PyTorch tutorial implementation. The tutorial tries to be faithful to the Gatys et al. paper but misses a few things along the way. For one thing, the paper’s authors replace the MaxPool2d with an AvgPool2d as they found it produces higher quality results. Another detail is that the tutorial computes the ContentLoss and StyleLoss on the outputs of the convolutions instead of the the ReLU activations. This is more of a nitpick since I did not notice a big difference between using the convolutions vs. the ReLUs in my experiments.The most egregious difference between the tutorial and paper is that the “wrong” layers are used for the ContentLoss and StyleLoss respectively. I put wrong in quotes as the choice of layers is largely subjective and depends heavily on what produces the most pleasing style. That being said, there are some rules of thumb we can use to guide our decision. When measuring content similarity, the lower layers tend to activate most highly when there is a pixel perfect match between the content_img and the generated input_img. The deeper we go into the network, the less the layers care about an exact match, and instead activate highly when features are generally in the right place. To visualize what each layer is most concerned with, we can set style_weight=0 and run the training process on a random input_img using different layers as the content_layer.The tutorial uses the 4th convolution (conv2_2 in Figure 2) as the content layer. As we can see in Figure 3 above, this is likely too low of a layer to use for content as the network still cares about matching pixels exactly at this depth. Gatys et al. use conv4_2 instead, which is much more concerned with the overall feature arrangement instead of individual pixels.In the case of style, lower layers respond to small repetitive features while higher layers capture more abstract, global features. Therefore, in order to transfer the whole style of the style_img— from low level details to overarching motifs — we should include layers from all depths in the network. The tutorial uses the first 5 convolution layers, but these are all fairly low in the network and are unlikely to capture global features. Gatys et al. use conv1_1, conv2_1, conv3_1, conv4_1, and conv5_1, a nice distribution of layers across the entire network hierarchy. We can use the same method we used for content to visualize the style each choice of layers is optimizing for. To do this, we set content_weight=0, specify which style_layers we want to use, and run the training process on a random input_img.As expected, the style optimized by the tutorial layers captures low level, repetitive features but fails to capture high level, global features.The fixes we’ve implemented so far should get us fairly close to the quality seen in the Gatys et al. paper. From here, we’ll go even deeper and look at what next steps we can take to generate even better images.One of the first things I changed from the paper was to switch the optimizer from L-BFGS to Adam. In the paper, the authors claim that L-BFGS results in higher quality transfer but I did not notice a difference when using Adam in my experiments. Furthermore, Adam seemed to be more stable, especially when training for a large number of steps or with a large style_weight. In these cases, L-BFGS seemed to NaN out, probably due to exploding gradients (though I didn’t look too deeply into it).Another minor tweak was to switch the mse_loss (i.e. L2 loss) to l1_loss. I can’t think of a good reason to use L2 loss for style transfer (besides differentiability at 0) as the square term heavily penalizes outliers. As alluded to in the previous section, we don’t really care about matching pixels exactly and can tolerate a few outliers in the generated image. Indeed, the outliers may even lead to more visually pleasing results as the style and content features are blended together. Finally, the authors of Feature Visualization — a must read article on a related topic — also use l1_loss for their task, likely for similar reasons.Actually, a lot of the tricks used to generate high quality Feature Visualizations elegantly transfer over to Neural Style Transfer. In fact, FV and NST are conceptually very similar and only differ in how they generate the input_img. In NST the input_img is optimized to activate different layers in a network in the same way as the content_img and style_img. FV on the other hand does not use a content_img and style_img but instead generates an input_img which maximally excites neurons in different layers.One trick I borrowed from FV is to use data augmentation on the input_img. This works exactly the same as with regular classification tasks: at each step, we apply some augmentations to the input_img (e.g rotation, cropping, resizing, etc.) before running it through the model and computing the loss. By augmenting the input_img at each step, we’re forcing the input_img to generate features which are robust to minor perturbations. These robust features should contain less high frequency artifacts and generally look more visually appealing.¹ However, I found the augmentations used in the Feature Visualization article to be very aggressive and had to scale them down appropriately. Even so, there are still some rotation artifacts that develop around the edges of the generated image (Figure 5). The simplest way to get rid of these artifacts is to just crop the image down by a few pixels 🙃.Finally, the last modification I made was to switch the content_layer to conv3_2 instead of the conv4_2 Gatys et al. use. Most of the articles I read also recommend conv4_2, though I found that fine details get washed out at conv4_2 and the style overpowers the content in the generated image. On the other hand, conv3_2 still maintains these fine details without over-penalizing for pixel perfectness like lower layers do. Indeed, we can confirm that this is the case by looking at the Figure 3 again.We’ve now discussed all the tricks I’ve implemented in my Neural Style Transfer code. At this point, we’ve substantially improved the transfer quality over the original PyTorch tutorial. Furthermore, the content_weight and style_weight are a lot more robust to the specific choice of images. For example, in the PyTorch tutorial, I found that a good style_weight on one set of images did not readily transfer to another set without proper tuning.That being said, it’s possible to get even better results by trying to remove high frequency noise in the generated image. The most interesting approach to this that I came across was from the article Differentiable Image Parameterization — another must read which touches on similar topics. In the article, the authors generate the input_img by first parameterizing it in (decorrolated) Fourier space instead of (corrolated) pixel space. Since the input_img is generated via gradient descent, decorrelating the input acts as a preconditioner which makes optimization easier by allowing gradient descent to find minima more quickly (similar to removing correlated features in supervised learning tasks). Why that leads to higher quality transfer is not entirely clear to me, besides hand-wavy explanations like the minima found in decorrelated space are wider and more robust.A simpler approach is to dampen high frequency noise by penalizing it either directly or indirectly. The noise can be directly penalize by adding the total variation loss of the input_img to the optimization objective. Conversely, it’s possible to implicitly penalize the noise by either blurring the input_img after each gradient descent step, or by blurring the gradients before applying them to the input_img. One issue with both approaches is that they also adversely penalize true high frequency features. This can be somewhat ameliorated by scaling down either the total variation loss or the amount of blur during training.If you made it this far, you should now know quite a lot about generating beautiful images with Neural Style Transfer. While conceptually simple, getting high quality results requires a lot of care. My original goal was to use Machine Learning to generate a Medium profile picture. After much trial and error, I think I’ve stumbled on something that looks pretty striking. To me, the most exciting part of this whole process is the end-to-end differentiability of Neural Networks. With very little effort, we were able to “invert” a model originally trained to discriminate between cats and dogs, and use it to generate countless images in a myriad different styles. Try doing that with a random forest.If you enjoyed this article, follow me to get notified when I post new stuff. All the code is available on my GitHub.Eugen Hotaj,
March 27, 2020WRITTEN BY"
4,En-Lightning Reinforcement Learning,https://towardsdatascience.com/en-lightning-reinforcement-learning-a155c217c3de?source=collection_category---4------2-----------------------,"This article looks at using PyTorch Lightning for the exciting domain of Reinforcement Learning (RL). Here we are going to build a standard Deep Q Network (DQN) model using the classic CartPole gym environment to illustrate how to start using Lightning to build your RL models.In this article we will cover:If you would like to jump straight to the code you can find the example in the PyTorch Lightning examples page or check out the interactive colab notebook by clicking the colab icon below!Lightning is a recent PyTorch library that cleanly abstracts and automates all the day to day boilerplate code that comes with ML models, allowing you to focus on the actual ML part (the fun part!) . If you haven’t already, I highly recommend you check out some of the great articles published by the Lightning teamAs well as automating boilerplate code, Lightning acts as a type of style guide for building clean and reproducible ML systems.This is very appealing for a few reasons:Before we get into the code, lets do a quick recap of what a DQN does. A DQN learns the best policy for a given environment by learning the value of taking each action A while being in a specific state S. These are known as Q values.Initially the agent has a very poor understanding of its environment as it hasn’t had much experience with it. As such, its Q values will be very inaccurate. However, over time as the agent explores its environment, it learns more accurate Q values and can then make good decisions. This allows it improve even further, until it eventually converges on an optimal policy (ideally).Most environments of interest to us like modern video games and simulations are far too complex and large to store the values for each state/action pair. This is why we use deep neural networks to approximate the values.The general life cycle of the agent is described below:That’s a high level overview of what the DQN does. For more information there are lots of great resources on this popular model out there for free such as the PyTorch example. If you want to learn more about reinforcement learning in general, I highly recommend Maxim Lapan’s latest book Deep Reinforcement Learning Hands On Second EditionLets take a look at the parts that make up our DQNModel: The neural network used to approximate our Q valuesReplay Buffer: This is the memory of our agent and is used to store previous experiencesAgent: The agent itself is what interacts with the environment and the replay bufferLightning Module: Handles all the training of the agentFor this example we can use a very simple Multi Layer Perceptron (MLP). All this means is that we aren’t using anything fancy like Convolutional or Recurrent layers, just normal Linear layers. The reason for this is due to the simplicity of the CartPole environment, anything more complex than this would be overkill.The replay buffer is fairly straight forward. All we need is some type of data structure to store tuples. We need to be able to sample these tuples and also add new tuples. This buffer is based off Lapins replay buffer found here as it is the cleanest and fastest implementation I have found so far. That looks something like thisBut we aren’t done. If you have used Lightning before you know that its structure is based around the idea of DataLoaders being created and then used behind the scenes to pass mini batches to each training step. It is very clear how this works for most ML systems such as supervised models, but how does it work when we are generating our dataset as we go?We need to create our own IterableDataset that uses the continuously updated Replay Buffer to sample previous experiences from. We then have mini batches of experiences passed to the training_step to be used to calculate our loss, just like any other model. Except instead of containing inputs and labels, our mini batch contains (states, actions, rewards, next states, dones)You can see that when the dataset is being created, we pass in our ReplayBuffer which can then be sampled from to allow the DataLoader to pass batches to the Lightning Module.The agent class is going to handle the interaction with the environment. There are 3 main methods that are carried out by the agentget_action: Using the epsilon value passed, the agent decides whether to use a random action, or to take the action with the highest Q value from the network output.play_step: Here the agent carries out a single step through the environment with the action chosen from get action. After getting the feedback from the environment, the experience is stored in the replay buffer. If the environment was finished with that step, the environment resets. Finally, the current reward and done flag is returned.reset: resets the environment and updates the current state stored in the agent.Now that we have our core classes for our DQN set up we can start looking at training the DQN agent. This is where Lightning comes in. We are going lay out all of our training logic in a clean and structured way by building out a Lightning Module.Lightning provides a lot of hooks and override-able functions allowing for maximum flexibility, but there are 4 key methods that we must implement to get our project running. That is the following.With these 4 methods populated we can make pretty train any ML model we would encounter. Anything that requires more than these methods fit in nicely with the remaining hooks and callbacks within Lightning. For a full list of these available hooks, check out the Lightning docs . Now, lets look at populating our Lightning methods.First we need to initialize our environment, networks, agent and the replay buffer. We also call the populate function which will fill the replay buffer with random experiences to begin with (The populate function is shown in the full code example further down).All we are doing here is wrapping the forward function of our primary DQN network.Before we can start training the agent, we need to define our loss function. The loss function used here is based off Lapan’s implementation which can be found here.This is a simple mean squared error (MSE) loss comparing the current state action values of our DQN network with the expected state action values of the next state. In RL we don’t have perfect labels to learn from. Instead, the agent learns from a target value of what it expects the value of the next state to be.However, by using the same network to predict the values of the current state and the values of the next, the results become an unstable moving target. To combat this we use a target network. This network is a copy of the primary network and is synced with the primary network periodically. This provides a temporarily fixed target to allow the agent to calculate a more stable loss function.As you can see, the state action values are calculated using the primary network, while the next state values (the equivalent of our target/labels) uses the target network.This is another simple addition of just telling Lightning what optimizer will be used during backprop. We are going to use a standard Adam optimizer.Next, we need to provide our training dataloader to Lightning. As you would expect, we initialize the IterableDataset we made earlier. Then just pass this to the DataLoader as usual. Lightning will handle providing batches during training and converting these batches to PyTorch Tensors as well as moving them to the correct device.Finally we have the training step. Here we put in all the logic to be carried out for each training iteration.During each training iteration we want our agent to take a step through the environment by calling the agent.play_step() defined earlier and passing in the current device and epsilon value. This will return the reward for that step and whether the episode finished in that step. We add the step reward to the total episode in order to keep track of how successful the agent was during the episode.Next, using the current mini batch provided by Lightning, we calculate our loss.If we have reached the end of an episode, denoted by the done flag, we are going to update the current total_reward variable with episode reward.At the end of the step we check if it is time to sync the main network and target network. Often a soft update is used where only a portion of the weights are updated, but for this simple example it is sufficient to do a full update.Finally, we need to return a Dict containing the loss that Lightning will use for backpropagation, a Dict containing the values that we want to log (note: these must be tensors) and another Dict containing any values that we want displayed on the progress bar.And that’s pretty much it! we now have everything we need to run our DQN agent.All that is left to do now is initialize and fit our Lightning Model. In our main python file we are going to set our seeds and provide an arg parser with any necessary hyper parameters we want to pass to our model.Then in our main method we initialize the DQNLightning model with our specified parameters. Next the Lightning Trainer is setup.Here we set the Trainer to use the GPU. If you don’t have access to a GPU, remove the ‘gpus’ and ‘distributed_backend’ args from the Trainer. This model trains very quickly, even when using a CPU, so in order to see Lightning in action we are going to turn early stopping off.Finally, because we are using an IterableDataset, we need to specify the val_check_interval. Usually this interval is automatically set by being based of the length of the Dataset. However, IterableDatasets don’t have a __len__ function. So instead we need to set this value ourselves, even if we are not carrying out a validation step.The last step is to call trainer.fit() on our model and watch it train. Below you can see the full Lightning codeAfter ~1200 steps you should see the agents total reward hitting the max score of 200. In order to see the reward metrics being plotted spin up tensorboards.On the left you can see the reward for each step. Due to the nature of the environment this will always be 1, as the agent gets +1 for every step that the pole has not fallen (that’s all of the them). On the right we can see the total rewards for each episode. The agent quickly reaches the max reward and then fluctuates between great episodes and not so great.You have now seen how easy and practical it is to utilize the power of PyTorch Lightning in your Reinforcement Learning projects.This a very simple example just to illustrate the use of Lightning in RL, so there is a lot of room for improvement here. If you want to take this code as a template and try and implement your own agent, here are some things I would try.I hope this article was helpful and will help kick start your own projects with Lightning. Happy coding!WRITTEN BY"
5,Where you should drop Deep Learning in favor of Constraint Solvers,https://towardsdatascience.com/where-you-should-drop-deep-learning-in-favor-of-constraint-solvers-eaab9f11ef45?source=collection_category---4------0-----------------------,"Machine Learning and Deep Learning are ongoing buzzwords in the industry. Branding ahead of functionalities led to Deep Learning being overused in many artificial intelligence applications.This post will provide a quick grasp at constraint satisfaction, a powerful yet underused approach which can tackle a large number of problems in AI and other areas of computer science, from logistics and scheduling to temporal reasoning and graph problems.Let’s consider a factual and highly topical problem.A pandemic is rising. Hospitals must organize quickly to treat ill people.The world needs an algorithm which matches infected people and hospitals together given multiple criteria such as the severity of illness, patient age and location, hospital capacity and equipment, etc.Many would say that a neural network would be the perfect fit for it: different configurations from a broad range of parameters that need to be reduced to a unique solution.However, there are downsides which would undermine such an approach:On the other hand, if formulated in terms of a boolean satisfiability problem, the situation wouldn’t have any of the aforementioned downsides while still giving a sub-optimal solution in nondeterministic polynomial time (NP-complete problem), and without the need for any historical data.Disclaimer: the purpose of this post is to deliver a quick look at CSPs. Theory and problem formulation will be much overlooked. For a more rigorous approach, please refer to [2][3][4].This post will provide a gentle introduction to constraint programming, aiming to resolve this case study. This map of the pandemic (1) illustrates the output of our algorithm which will match infected people with hospitals.There are several frameworks for constraint solving. Google Optimization Tools (a.k.a., OR-Tools) is an open-source software suite for solving combinatorial optimization problems. Our problem will be modeled using this framework in Python.For now, let’s simplify the problem to 4 parameters (1):Let’s define those parameters in python:A constraint satisfaction problem consists of a set of variables that must be assigned values in such a way that a set of constraints is satisfied.Let define as our indexed family of variables :If in the hospital i, the bed j is taken by the person k then xᵢⱼₖ = 1. In order to associate each bed of an hospital to an ill person, the goal is to find a set of variables that satisfies all of our constraints.We can add those variables to our model:Hard constraints define a goal for our model. They are essential, if they are not resolved then the problem can’t be tackled:Let’s focus on the first hard constraint. For each bed j in every hospital i:Hence, it can be expressed in the following way:Our solver is a combinatorial optimization solver, it can process integer constraints only. Hence, must be turned into an integer equation:This inequality can then be added to our model.Next, the second hard constraint: for every patient k:In the same way, can be translated into an integer inequality:Finally, this constraint can be added to the model.Next, there are soft constraints. Those are highly desired: our solution must try to satisfy them as much as possible, yet they are not essential to find a solution:While hard constraints are modeled as equalities or inequalities, soft constraints are expressions we want to minimize or maximize.Let Ω be the set of all solutions that satisfy the hard constraints.Every sick person should be placed into a bed means to maximize the number of occupied beds.Every person should be handled by the nearest hospital means to minimize the distance between every patient and his assigned hospital.Sick persons in a severe condition should be handled first when there are not enough beds means to maximize the total severity of all handled patients. By denoting sev(k) the severity of the patient k:Then we can reduce all the soft constraints into a single objective:One needs to be careful then: these soft constraints don’t have the same domain.Given that all of these constraints share the same priority, we must define penalty factors to equilibrate the different constraints.Here is the corresponding code:Now we can launch the solver. It will try to find the optimal solution within a specified time limit. If it can’t manage to find the optimal solution, it will return the closest sub-optimal solution.In our case, the solver returns an optimal solution in 2.5 seconds (2).To create this solution, all it takes is 1 hour of research and 30 minutes of programming.For a Deep Learning counterpart, one can predict a few days of data cleansing, at least a day to test different architectures and another day for training.Moreover, a CP-SAT model is very robust if well modelized. Below are the results with different simulation parameters (3). Results are still coherent in many different cases, and with increased simulation parameters (3000 patients, 1000 beds), solution inference took a little less than 3 minutes.Of course, CSPs hardly apply to topics like computer vision and NLP, where Deep Learning is sometimes the best approach. However, in logistics, scheduling and planning, it is often the way to go.Special thanks to Laurent Perron, and his Operations Research team at Google for their fantastic work, and for the time they take to answer technical questions on StackOverflow, GitHub and Google Groups.Antoine Champion, Apr. 1st 2020[1] Jingchao Chen, Solving Rubik’s Cube Using SAT Solvers, arXiv:1105.1436, 2011.[2] Biere, A., Heule, M., and van Maaren, H. Handbook of satisfiability, volume 185. IOS press, 2009a[3] Knuth, D. E., The art of computer programming, Volume 4, Fascicle 6: Satisfiability. Addison-Wesley Professional, 2015[4] Vipin Kumar, Algorithms for constraint-satisfaction problems: a survey, AI Magazine Volume 13, Issue 1, 1992.WRITTEN BY"
6,Predicting Weekly Hotel Cancellations with ARIMA,https://towardsdatascience.com/predicting-weekly-hotel-cancellations-with-arima-4cb4f1849ef6?source=collection_category---4------1-----------------------,"Data analytics can help to solve this issue in terms of identifying the customers who are most likely to cancel — allowing a hotel chain to adjust its marketing strategy accordingly.An ARIMA model is used to determine whether hotel cancellations can also be predicted in advance. This will be done using the Algarve Hotel dataset in the first instance (H1full.csv). Since we are now seeking to predict the time series trend, all observations are now included in this dataset (cancellations and non-cancellations, irrespective of whether the dataset as a whole is uneven).To do this, cancellations are analysed on a weekly basis (i.e. the number of cancellations for a given week are summed up).Firstly, data manipulation procedures were carried out using pandas to sum up the number of cancellations per week and order them correctly.In configuring the ARIMA model, the first 80 observations are used as training data, with the following 20 then used as validation data.Once the model has been configured, the last 15 observations are then used as test data to gauge the model accuracy on unseen data.Here is a snippet of the output:The time series is visualised, and the autocorrelation and partial autocorrelation plots are generated:Time SeriesAutocorrelationPartial AutocorrelationWhen a Dickey-Fuller test is run, a p-value of less than 0.05 is generated, indicating that the null hypothesis of non-stationarity is rejected (i.e. the data is stationary).An ARIMA model is then run using auto_arima from the pyramid library. This is used to select the optimal (p,d,q) coordinates for the ARIMA model.The following output is generated:Based on the lowest AIC, the SARIMAX(1, 1, 0)x(0, 1, 0, 52) configuration is identified as the most optimal for modelling the time series.Here is the output of the model:With 90% of the series used as the training data to build the ARIMA model, the remaining 10% is now used to test the predictions of the model. Here are the predictions vs the actual data:We can see that while the prediction values were lower than the actual test values, the direction of the two series seem to be following each other.From a business standpoint, a hotel is likely more interested in predicting whether the degree of cancellations will increase/decrease in a particular week — as opposed to the precise number of cancellations — which will no doubt be more subject to error and influenced by extraneous factors.In this regard, the mean directional accuracy is used to determine the degree to which the model accurately forecasts the directional changes in cancellation frequency from week to week.An MDA of 89% is yielded:In this regard, the ARIMA model has shown a reasonably high degree of accuracy in predicting directional changes for hotel cancellations across the test set.The RMSE (root mean square error) is also predicted:The RMSE stands at 77 in this case. Note that the units of RMSE are the same as the response variable, in this case — hotel cancellations. With an average cancellation of 94 for all weeks across the validation data, the RMSE of 77 is technically the standard deviation of the unexplained variance. All else being equal, the lower this value, the better.Even though the ARIMA model has been trained and the accuracy validated across the validation data, it is still unclear how the model would perform against unseen data (or test data).In this regard, the ARIMA model is used to generate predictions for n=15 using the test.index to specify the unseen data.Firstly, the array is reshaped accordingly:Now, the predictions are made, and the RMSE (root mean squared error), MDA (mean directional accuracy) and mean forecast errors are calculated:The RMSE has improved slightly (dropped to 57), while the MDA has dropped to 86% and the mean forecast error stands at -12, meaning that the model has a tendency to slightly underestimate the cancellations and therefore the forecast bias is negative.Here is a plot of the predicted vs actual cancellations:The same procedures were applied — this time using the second dataset.The following is the ARIMA configuration obtained using pyramid-arima:Predicted vs. ValidationPredicted vs. ActualIn this example, the ARIMA model was used to predict the degree of hotel cancellations on a week-by-week basis. The MDA demonstrated 86% accuracy in doing so across the test set with an RMSE of 57 on the H1 dataset, and an 86% MDA was yielded once again for the H2 dataset with an RMSE of 274 (with the mean cancellations across the 15 weeks in the test set coming in at 327).Of course, a limitation of these findings is that both hotels under study are based in Portugal. Testing the model across hotels in other countries would help to validate the accuracy of this model further.The datasets and notebooks for this example are available at the MGCodesandStats GitHub repository, along with further research on this topic.You can also find more of my data science content at michael-grogan.com.WRITTEN BY"
7,A Primer to Neural Networks,https://towardsdatascience.com/what-constitutes-a-neural-network-af6439f0cdd7?source=collection_category---4------2-----------------------,"I’ve been working in many machine learning projects involving neural network models built for different tasks in various contexts. But I always find myself working with high-level frameworks and tools that mostly hide the real structure of the models being used. Mainly due to the variety of existing libraries and frameworks helping developers around the world to quickly start with machine learning code, encouraging them to build endpoint applications that use intelligent models for the purpose of countless tasks, without necessarily helping them to understand what is really happening behind the scenes.Unfortunately, acting in that way, people fail to understand the inner workings of the models they are using, and therefore they will not even consider taking time for digging into the science behind it.But guess what, it is not that difficult to grasp what’s going on inside a neural network, and the goal of this article is to explain in a very simple and accessible way the mechanics underneath, breaking down the main components, explaining the general architecture and hoping to give you a full insight of how a network behaves from the input data to the expected end result.I will break down the topic in two articles so that I can properly focus on each part without overwhelming you with too much information.A perceptron can be conceived as a binary model that basically mimics a biological neuron. It was first introduced in 1905 and developed in 1960 by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. The perceptron schematizes a decision model that can output different results based on a given set of binary inputs. Those inputs can have different weights to illustrate their level of importance, and hence the idea is to compute a weighted sum of binary inputs that happen to be greater or lower than a specific threshold intimately related to the actual perceptron itself.The perceptron can be also represented by a simple algebraic formula:A sigmoid neuron is a slightly more sophisticated model than the perceptron. Unlike the perception which can only hold 0 or 1 values, the sigmoid neuron can vary from a range between 0 and 1 so it can hold infinite numbers. The advantage of it is that we increase the output scale enormously and therefore the model becomes more numerically stable because small changes in the input weights cause small variations in the output. In the previous model, no matter how tiny we tweak the input weights, the output will abruptly switch between 0 and 1, whereas with the sigmoid neuron we reach a certain level of smoothness in the output variations.To be more clear the neuron here plays the role of a sigmoid function that takes the weighted sum of all the inputs, and outputs a number between 0 and 1. A neuron is activated once the weighted sum is greater than its threshold. The inner threshold is commonly referred to as a bias specifically scaled for each neuron.To summarize, a sigmoid neuron computes the weighted sum of its inputs, applies a sigmoid function that squashes the results in a number range between 0 and 1, and finally compares the obtained results with its threshold so it can light up or extinguish.Basically stacking multiple neurons together either in parallel or in series constitutes a layer. But as you might obviously expect, there is much more in that than just simply stacking neurons carelessly. Each network is built for a specific task, and therefore it has a different kind of layers assembled in different ways. But generally, there are three types of layers commonly used in multiple architecture patterns, the input layer, the hidden layers, and the output layer.In a more scientific way, those layers are involved in the process of generating levels of abstraction. Meaning that, the more logical patterns the network is able to detect and learn, the more its “abstraction capability” is engaged and developed, leading the system to generalize from simple raw data to high level concepts.The same generalization process occurs in our brain instinctively, without even noticing. From a dog picture, we immediately think of an animal, because for a dog figure we have learned specific patterns which are related to physiological characteristics that help make the difference between a dog and a cat. And in a sense, the entire process from the image of a dog to a high-level understanding of the idea that a dog represents is built up by assembling multiple layers of abstraction.Each network defines its hidden layers depending on the nature of the problem the network is aiming to solve, and therefore multiple designs are available. One architecture can fit very well for a certain problematic, and not perform either well on another.What is very interesting with these layers, is that a pattern of activation in one layer causes a very specific pattern in the next layer and so forth. Which means that each layer is accountable for detecting singular patterns within the data. In the case of Convolutional Neural Networks which classify images into targeted labels, they implement a mechanism within their hidden layers that could conceivably combine pixels into edges, edges into patterns and finally patterns into digits.You can check an interesting article by Omar M’Haimdat to get hands-on CNNs from a developer perspective.Side note: At this stage, if you are looking to dig into the details of CNNs, I wouldhighly recommend you to read Yann LeCun’s paper: Gradient-based learning applied to document recognition.The most commonly used activation function in the output layer is the Softmax, calculating the probability distribution of each target class over all possible target classes.In mathematics, the softmax function, also known as softargmax or normalized exponential function, is a function that takes as input a vector of N real numbers, and normalizes it into a probability distribution consisting of N probabilities proportional to the exponentials of the input numbers.There is a massive variety of network architectures that can be used. But always keep in mind that the architecture depends on the type of problem and the purpose intended. There are actually a lot of design patterns or architectures which have been proven to better suit a specific set of problems. Network architectures are an active field of research, and every year there are multiple papers published in that sense.Generally speaking, there are 5 big Neural Network Types:I hope this article helped to demystify a little bit the not so hard to understand mathematical concepts behind neural networks and I’m quite sure that with a bit of practice you can easily get quite familiar with those concepts 😉.I would like to mention the different sources which inspired me to write this article:In the second part, I will cover the way neural networks learn, and we will explore a lot of exciting concepts like backpropagation, gradient descent, and many others. So stay tuned for the next part 😉😉.WRITTEN BY"
8,4 free maths courses to do in quarantine and level up your Data Science skills,https://towardsdatascience.com/4-free-maths-courses-to-do-in-quarantine-and-level-up-your-data-science-skills-f815daca56f7?source=collection_category---4------3-----------------------,"When I got into Data Science and Machine Learning, anything related to math and statistics was something I had visited for the last time around 10 years ago. That’s probably why I found it so hard at first. It took me lots of hours of reading and watching videos to get some understanding of how things happen for lots of the tools we daily use in the industry. However, I got to a point where I felt the need of developing a solid understanding of what was happening underneath all those imports and fits I was doing, so I decided to freshen up that dusty math knowledge.Nowadays, I’m still doing it and I reckon it will be never enough. Moreover coming from Business and being in an industry full of professionals from engineering, statistics, physics and other exact sciences. I know there are LOTS of things to learn in the world of Data Science, but you know what? Technologies and languages might come and go, but the mathematical background of the field is going to remain.That’s why today I’m wrapping up a list of 5 courses to level up your math knowledge and take advantage of some of all this spare time we have been given this unfortunate situation we’re going home. Since, you know, you should be staying at home these days 😉.1. Mathematics for Machine LearningWhere: CourseraInvolved institution: Imperial College LondonTime required: 104hs (realistically it will be at least +50%)Prior requirements: NoneAbstract from the course itself:For a lot of higher level courses in Machine Learning and Data Science, you find you need to freshen up on the basics in mathematics — stuff you may have studied before in school or university, but which was taught in another context, or not very intuitively, such that you struggle to relate it to how it’s used in Computer Science. This specialization aims to bridge that gap, getting you up to speed in the underlying mathematics, building an intuitive understanding, and relating it to Machine Learning and Data Science.Topics covered:2. Essential Math for Machine Learning: Python EditionWhere: edXInvolved institution: MicrosoftTime required: 50hsPrior requirements: Python and some ground understanding of mathsAbstract from the course itself:Want to study machine learning or artificial intelligence, but worried that your math skills may not be up to it? Do words like “algebra’ and “calculus” fill you with dread? Has it been so long since you studied math at school that you’ve forgotten much of what you learned in the first place?You’re not alone. machine learning and AI are built on mathematical principles like Calculus, Linear Algebra, Probability, Statistics, and Optimization; and many would-be AI practitioners find this daunting. This course is not designed to make you a mathematician. Rather, it aims to help you learn some essential foundational concepts and the notation used to express them. The course provides a hands-on approach to working with data and applying the techniques you’ve learned.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.3. Probability and Statistics in Data Science using PythonWhere: edXInvolved institution: UC San DiegoTime required: 100–120hsPrior requirements: multivariate calculus and linear algebraAbstract from the course itself:Reasoning about uncertainty is inherent in the analysis of noisy data. Probability and Statistics provide the mathematical foundation for such reasoning.In this course you will learn the foundations of probability and statistics. You will learn both the mathematical theory, and get a hands-on experience of applying this theory to actual data using Jupyter notebooks.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.4. Bayesian Statistics: From Concept to Data AnalysisWhere: CourseraInvolved institution: Santa Cruz, University of CaliforniaTime required: 22hs (realistically no less than 30hs)Prior requirements: some ground understanding of probabilityAbstract from the course itself:This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach.Topics covered:And that’s all for now peeps. I recommend doing these courses in the order presented, but of course, go ahead with any you like if you match the requirements :).And don’t forget to take a look at some of my latest stories:Also, feel free to visit my profile on Medium and check any other of my other stories :). See you around! And thanks for reading!WRITTEN BY"
9,How can you improve your machine learning model quality?,https://towardsdatascience.com/how-can-you-improve-your-machine-learning-model-quality-b22737d4fe5f?source=collection_category---4------4-----------------------,"Sometimes, your machine learning models for classification just don’t work as well as expected. When faced with this situation, what many people do is try different methods more or less at random or follow their guts. It might be adding more data, trying a new model, or tweaking some variables, but it is quite uncommon to have a method for choosing what to try first.What this article presents is exactly that: a well-defined method to choose the best strategy, depending on what type of model error you are facing.This method is based on the Structuring Machine Learning Projects course available on Coursera, but with adaptations in format, methodology and nomenclature. If you like the article, it might be worth taking the course, however, it is not too long and it has some extra useful hints.There are some criteria your model should match to be considered a good model, and usually they are met one at a time, in this order:For each of these steps, there are different strategies that allow you to improve performance, that should be applied accordingly. It is quite unlikely that your model will perform well in the real-world if it doesn’t perform well in your training set, so when you see things start going wrong, you should be able to find the source of the error to fix it first.To be able to measure if your model is doing well or not, you should have a single number evaluation metric, the one metric that will enable you to compare different models. The moment you start trying to optimise for multiple metrics, you are not really optimising, and you won’t get any objective answers.“Ok, but what if I REALLY need more than one metric?”Then, choose one single numeric metric to be your “one and only”, and the others will be your satisficing metrics, the metrics for which you will set a cut-off, but for which you don’t have to optimise.Say, for instance, you have 2 models, A and B. Model A has an accuracy of 95% and it takes 1 second to score a new observation. Model B has an accuracy of 98% and it takes 8 seconds to score a new observation.If you are optimising for accuracy, in theory, you would choose Model B. But imagine runtime is also important for your application, so you can’t have a model that takes 8 seconds to score each observation. However, as long as it takes less than 2 seconds, it’s fine for you. You will then optimise for accuracy, with a constraint of a maximum of 2 seconds for runtime, which will then lead you to choose model A. Note that you can have multiple satisficing metrics.Now that you have your definition of error, to understand where it comes from and optimise our chosen metric, we have to split error into different parts, using a sort of error pipeline.Never send a human to do a machine’s job. — Agent Smith in the film The MatrixFor some tasks, such as image recognition, we usually use humans as a baseline for getting the accuracy of our models: if humans get it wrong 1% of the times, usually we can’t realistically expect to do much better. That’s why we use human performance as a proxy for Bayes error, which is the minimum theoretical error for a task and thus will be the starting point of our pipeline.There’s usually a lot of confusion and debate when it comes to defining train, validation and test sets, but I think industry standard is leaning towards using train for the dataset used for training the model (not much debate here), validation for the dataset you use to first validate your models and fine tune your parameters, and test dataset for the final dataset you use to test your model. It is important that validation and test observations come from the same distribution and that they both reflect data you will encounter in the real world.The default split between train and test is usually 70% / 30%, and between train, validation and test is 60% / 20% / 20%, more or less. However, when you have huge amounts of data, it’s fine to use a 98% / 1% / 1% split, as long as you have at least something around 10 000 observations in each of validation and test sets.Finally, if your train and validation sets come from different distributions, you can split your train set in two parts: train and train-dev, and use the first one to train your model and the second one to test on data that comes from the same distribution than your train set, in order to isolate error that comes from your model being unable to generalise and error that comes from this difference in distributions.In the end, you would have 5 sequential values for your error measure (this being the one metric defined above). The difference between each one of them has a different source, and looking at the greatest sources of error will indicate to you which one you should address first:We’ll now address each of these error sources and the best strategies to deal with each one of them.Avoidable bias should be as close as possible to zero, since the error in the train set will most likely be your smallest error, and you want it to be at least equal to human performance (if not better). Some strategies to address it include:Variance, on the other hand, comes from the fact that your model is overfitting the train set and is not yet capable of generalising its conclusions to data it has not seen yet. You can reduce it by:Sometimes, you can’t have train and test data come from the same distributions: suppose you want to train a face recognition algorithm to be used specifically with the front camera of a cell phone, but you don’t have enough labelled data with these exact same characteristics. Then you resort to publicly available data, that is already labelled, to train your model and test it on the pictures that come from a front camera. In this case, data mismatch error might happen and it can be reduced mainly by making your training data more similar to your validation and test sets.One way of doing this is by conducting manual error analysis. In our previous example, this could mean randomly taking a few (~100) of the mislabelled pictures from the validation set and trying to understand why the algorithm got them wrong by comparing to some pictures from the train set. Some might be distorted or low-resolution due to the front camera’s poor quality, some might be much closer to the face than your train set, and some might be even correct and they were just mislabelled by the human in the first place. Try attributing each of the 100 pictures to an error category and, in the end, you can know what % of errors is generated by each category. You can then take into account the % of errors attributed to each category and how hard/expensive it would be to fix them, helping you decide which categories to address first. In our example, say most of the model’s error happens when pictures are low-resolution. Maybe you could try to artificially reduce resolution in some pictures of your train set to make it more similar to your validation set, or simply take some pictures from your validation set and put them in your train set.This step my seem a bit cumbersome, but it can actually save you a lot of unnecessary work down the road.If your model overfits, then try making the validation set bigger by adding data to it.To sum the workflow up:If you would like to learn more about this workflow, I suggest you take the Structuring Machine Learning Projects course, available on Coursera.WRITTEN BY"
10,TensorFlow Lite Android Support Library: Simplify ML On Android,https://towardsdatascience.com/tensorflow-lite-android-support-library-simply-ml-on-android-561402292c80?source=collection_category---4------5-----------------------,"Everyone loves TensorFlow and even more when you can run a TF model on Android directly. We all use TensorFlow Lite on Android and we have a couple of CodeLabs on it too. Using the Interpreter class on Android, we are currently running our .tflite models in apps.But we have to do a lot before that, right? If we’re performing an image classification task, you’ll probably get a Bitmap or an Image object from the Camera library and then we transform it into a float[][][] or a byte[] . Then we load our model from the assets folder as a MappedByteBuffer . After calling interpreter.run() , we get the class probabilities, on which we perform the argmax() operation and then finally get a label from the labels.txt file.This is the traditional approach which we developers follow and there’s no other way round.The TensorFlow team has released the TensorFlow Lite Android Support Library to solve the tedious tasks of preprocessing. The GitHub page gives an intuition of their aim,Mobile application developers typically interact with typed objects such as bitmaps or primitives such as integers. However, the TensorFlow Lite Interpreter that runs the on-device machine learning model uses tensors in the form of ByteBuffer, which can be difficult to debug and manipulate. The TensorFlow Lite Android Support Library is designed to help process the input and output of TensorFlow Lite models, and make the TensorFlow Lite interpreter easier to use.First, we need to get this right in our Android project. Remembering build.gradle file? Right! We’ll add these dependencies in our app-level build.gradle file,The first step in running a TFLite model is to create some array object which can store the inputs for our model as well the outputs which the model will produce. To make our lives easier and less struggling with float[] objects, TF Support Library includes a TensorBuffer class that takes in the shape of the desired array and its data type.Note: As of 1st pril 2020, only DataType.FLOAT32 and DataType.UINT8 are supported.You can even create a TensorBuffer object from an existing TensorBuffer object by modifying its data type,If you’re working with object detection, image classification or other images -related models, you need to work on Bitmap and resize it or normalize it. We have three ops for this namely, ResizeOp , ResizeWithCropOrPadOp and Rot900p .First, we define our preprocessing pipeline using the ImageProcessor class.Question: What are BILINEAR and NEAREST_NEIGHBOR methods?Answer: Read this.Next, create a TensorImage object and process the image.Normalization of image arrays is necessary for almost all models to be it image classification models or regression models. For processing tensors, we have a TensorProcessor . Along with NormalizeOp we have CastOp , QuantizeOp and DequantizeOp .Question: What is Normalization?Answer: The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1.Also, we have the freedom to build custom ops by implementing TensorOperator class, as shown below.We can easily load our .tflite model using the FileUtil.loadMappedFile() method. Similarly, we can load the labels from a InputStream or from the assets folder.And then perform inference using Interpreter.run() ,I hope you liked the new TensorFlow Lite Android Support Library. This was a quick review of what’s inside but try exploring it yourself too. Thanks for reading!WRITTEN BY"
11,Numbers Can Lie,https://towardsdatascience.com/numbers-can-lie-3474e0efa1e1?source=collection_category---4------0-----------------------,"Analytics and statistics have become integral parts of almost every aspect of business due to advancements in technology. Which is great for Data Scientists & Analysts but can also create some very real problems.The main issue is caused by the pace in which analytics are integrated into business processes, due to the fact that most people don’t have much if any statistics education and those that do often haven’t revisited the topics since their school days. This creates a very real problem for organisations.Blind conviction in decision making that is based on metrics and figures that don’t actually hold up are running rampant.If you are a Data Scientists or Analysts, it’s vital to remember our work does not occur in a void. Our analysis are shared and will impact how decisions are made so it’s important to keep this in mind when building reports, dashboards etc. to ensure they don’t unintentionally mislead the end users.Simply put, if you can’t substantiate your numbers, why would anyone believe your analysis? Or worse, if your analysis is unintentionally misleading, it can cause serious harm.In this article I will explore some of the different factors that can cause decision makers to be misled and how to avoid this in your work.The first thing to think about is: what is meant by the term “average”.There are three: Mean, Median and Mode.When you see the term ‘average’ you could be referring to any one of them. Most commonly, the average being referred to is Mean but not always, in my work I opt for calling it a ‘Mean Average’ unless specifically requested otherwise by a client.I’ll go more in-depth into Means as they can be very misleading due to the fact there are multiple ways different datasets could yield the same Mean. For example, three values ‘1,2,3’ can have the same mean as ‘2,2,2’.The main way Mean can mislead is due to the outlying data where comparatively very large or very small values have a large impact on the Mean.A real life example: a company could have average sales of £25,000 a month.
The executives see this and are satisfied with the company’s position.
In reality, the average sales is skewed due to a £50,000 a month customer that is due to be acquired by a competitor. The imminent loss of this customer actually drops the Mean to say £15,000, a very risky cash position for the company. In this scenario seeing the distribution or another type of average could help the executives foresee these events and prepare for them.I tend towards a ‘more is better’ approach and include other descriptives in any scenario where I am not presenting the results myself, this leaves little room for error. Alway remember to think of the end users needs, not yours, and I can guarantee they haven’t spend hours scanning the raw data like you, so don’t make an assumption they have the knowledge of the dataset.The above analysis show three completely different datasets that all have the same Mean Average, Count and Sum however, they all have very different distributions (I have kept the scales similar to emphasise this point). This situation happens frequently in analytics and is often completely swept under the rug.Descriptive Statistics are a must when it comes to analysis. Simply making nice metrics and graphs without proper context can cause more harm than good. If you’re planning on using averages as a part of reports and dashboards, then I recommend at least showing distributions with them, so the context behind the average is understandable and please, remember to label it “Mean, Mode, Medium”. If you’re unsure of the most appropriate average for your dataset I would recommend researching each in detail beforehand.Statistics are often quoted without any reference to the original data, as well as missing certain figures that could act against the creator’s hypothesis.An important question to ask yourself when being presented findings on their own is ‘whose interest is it in this figure be correct’? This isn’t to say that the figures are incorrect. Just there is no way of knowing definitely.
Therefore, the figures should not be solely relied upon.If you’re presenting data always cite sources, methods and anything else relevant. If you’re leaving something out, ask yourself why and if the answer is defensive in nature, don’t leave it out. There so many fake facts out there we don’t need anymore.A common examples of misrepresentation is survey data where the information on method or sample size is left out. This often include things totals such as total downloads figures, which marketers love, that conveniently leave out other metrics like active users.This is shown clearly in the following two statements, both of which are accurate.China’s GDP is higher than Luxembourg’s.
Luxembourg’s GDP is higher than China’s.They are both true as China’s total GDP is much bigger. However, per capita, Luxembourg is much higher. In this case, you’re able to make an argument that both economies performed better than the other.Statistics are often used to prove a point or solidify an opinion. Though in reality they often don’t succeed at either.It’s easy enough to come up with a poignant statement and back it up quasi sophisticated statistics, but they will fall over when you dig a little beneath the surface.One such statement would be ‘CEOs earn more than they have ever have before’. This statement comes backed up with figures such as ‘on average CEOs now earn £250,000. Compared to 1950, they only earned £50,000’. This on its own seems like resounding proof but in actual fact, £50,000 in 1950 gave the equivalent buying power as £480,000 today. So despite first appearances, this statement is ridiculous. I know this may seem obvious but there are many less obvious one out there.Another common example I’ve personally seen is a large HR analytics software company stating on their website: ‘our clients achieve up to 80% ROI.’There are two major issues with such statements.Firstly, from a mathematics perspective, the phrase ‘Up to’ could effectively mean anything in this scenario, i.e most of the company’s clients may receive zero percent return on investment, and a single client may achieve 80%. Therefore, the statement is true but misleading.The other issue with such statements is the assumptions made in the methodology. Let’s say in this scenario, the product the company sell is sales software. They calculate that the increase in sales in the first year of use versus the previous year leads to the ROI of 80%. They conveniently leave out the fact that sales were growing by 50% a year before the software was introduced. Therefore claiming that the increase was caused by the software is baseless, as the increase was within the normal operating trend of the client already.An easy tip to avoid this is simply avoid generalisation, they leave too much room for error. Remember you’re trying to tell a story with your data, that story needs to include the details.This is probably the most common mistake that is made when looking at statistics and the consequences of doing so can be huge.Let’s say you want to understand the relationship between workplace engagement and employee length of service, so you conduct a survey and gain the results.You see that there is a positive correlation between length of time and engagement level. From this you conclude that the longer someone works at the organisation, the more engaged they will be.
How can we be sure that is really the case though? Firstly, you need to access the distribution of the length of service. Is this close to normally distributed levels, or is there a large skew to the distributions? Remember there is very little value in comparing apples to oranges.Thankfully, there is a helpful method to understand how important the correlation is, which is P Values. This calculation is used to show the probability of having the same results if the correlation coefficient was zero. If a P Value is less than 5%, (0.05), then the correlation is deemed to be statistically significant.This does not mean that the correlation is a causation, just that it is worth looking into further to determine whether the correlation is in fact due to a cause and effect relationship.Due to naivety around the meaning of P values or simply relying on correlation coefficients alone, things that appear to be related are often stated as proven to be a cause and effect relationship. If you find that two things are correlated, and wish to understand the relationship further, then Bayesian statistics would be a helpful tool to understanding the data in more depth.Some simple things to consider regarding correlations:
Usual correlations fall under four categorise (these can be positive or negative relationships):Uncorrelated: Pretty self explanatory.
Correlated, but not really: Sometimes thing are statistically correlated but in reality don’t have a relationship and occur by chance. Two ways to avoid this, is to get more data or conduct experiments to put this relationship to the test under controlled conditions.
Actually correlated: These inputs have a relationship, but if influencing conditions change they could alter or break this relationship. Remember data doesn’t exist in a void, things change, don’t assume ceteris paribus.
Causal: The input directly dictates the output, consider the order of the relationship eg does X cause Y to change, or the opposite, or is the relationship bidirectional? This of course depends on the nature of your data.Correlations are a vital part of data science but it is all too easy to get carried away when you get the result you were looking for. Really dig until you fully understand what's going on you’ll be surprised what you find.Most of these may seem quite obvious, but in a fast paced business environment, it’s easy for them to creep into your analysis without anyone noticing, especially you’re using off the shelf analytics tools that don’t allow you to inspect the raw data or run custom queries and formulas.As a general rule, for better practice, if you don’t understand or know what is happening behind a pretty graph or cutting edge machine learning model then you shouldn’t be relying on their outputs when it comes to decision making, always provide context.Thankfully there are lots of great places online to read about statistics, data science and machine learning, enabling everyone to improve their data-literacy.WRITTEN BY"
12,"A Data Science View of Herd Immunity, What do We Have to Pay to Stop The Virus?",https://towardsdatascience.com/a-data-science-view-of-herd-immunity-what-do-we-have-to-pay-to-stop-the-virus-3a05fc2ce720?source=collection_category---4------1-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.C
ovid-19 spreads like wildfire. this microorganism particle is rapidly changing the world. In less than four months, it has emerged from Wuhan (China) to almost every country in the world.On the 13 March 2020, the prime minister of UK Boris Johnson gave the Nation the latest coronavirus briefing. At the time he mentioned that the UK would use scientific research to model the best strategy based on the current shreds of evidence that are available, this loose control strategy was later indicted by the UK’s chief scientific advisor Patrick Vallance as Herd Immunity.Britain would keep operating as normal without any stringent control or lockdown but at the same time, Boris addressed every Britain family should be prepared to “lose loved ones before their time”.And after 14 days of the Herd Immunity proposal, Boris Johnson was reported to test positive with COVID-19.In the same 14 days, UK has identified almost 20,000 infected cases with more than 1,200 death. (Data from March 30, 2020, and The UK government later imposed more strict legislation to suppress the outbreak)So what exactly is the herd immunity? And Is this the best way for us to combat the virus?Let’s discuss the topic today through the lens of Data Science.The virus transmission process is extremely complicated in its nature. There are various epidemiological models to explain or simulate the spread of infectious disease. But for a basic understanding, we start with a simple model.In this model, we have two fundamental concepts of grounding (basic reproduction number and generation interval)The basic reproduction number (R0), is defined as the expected number of secondary cases produced by a single (typical) infection in a completely susceptible population.It will help us to understand how quickly an infectious disease will spread across the population.If the R0 is less than 1, for example, 0.5. Then one person will produce 0.5 new cases, the secondary 0.5 cases will pass the virus to the next 0.25 cases. Thus the infected population will be getting smaller. and die out without any chance to become a global epidemics.If the R0 is close or equal to 1, meaning that one person will only pass the virus to another, then to the next one. We end up having a flat line of the infectious case. Hence we call it endemic (local spread), for example, chickenpox.The last scenario is when the R0 is larger than 1, for example, 2, then the growth will be exponential, 2 to 4, 4 to 8 and eventually reach the tipping point that it cannot be contained in a local area. We now call it a pandemic.Once a virus outbreak becomes a pandemic, there are normally two endings, the patient is either die (break the circulation) or develop an immune system against the virus. (Stop the spreading)To conclude, the bigger the R0, the more contagious It will be. (some scientist use Rt for notation)A more realistic calculation of R0Take a look at the R0 for some of the well-known diseases. The most contagious one is Measles, it has a basic reproduction rate of 12–18, followed by diphtheria, with an R0 of 6–7.The WHO initially estimated the Covid-19 R0 to be 1.4–2.5 (average 1.95), however a recent review of 12 studies estimated the basic R0 to be 3.28 and the median R0 to be 2.79.What is the generation interval? It is the period of time separating sequential infections.In a simple term, let’s assume a Covid-19 patient can pass the disease to 2 others, so how long does this transmission gonna take? 1 day, 2 days or a week. This is the other critical factor in the mathematical model, the shorter the interval between two-generation, the more contagious it was.“A” is infected by the virus at the time that denoted as a circle. After a while, “A” shows some mild symptoms, denoted as a square.The gap between the circle and the square is the incubation period. ( period between exposure to an infection and the appearance of the first symptoms) The incubation period for Covid-19 varies quite a lot, ranging from 3 days to 20 days. Let’s take a weighted average, 8 days.To point out, patient “A” is contagious throughout the whole incubation period, thus can infect person B anytime between the 8 days. Then it’s gonna take another 8 days for B to show symptoms (marked as square as well). The time gap between the two squares is known as the generation interval. For Covid-19, it’s around 4 days.Take “Ro = 2.5” and “Generation Interval = 4”, how long will it take Covid-19 to spread to the entire Europe if there is no intervention?By January 24, 2020, the first COVID-19 patient was confirmed in France. Unfortunately, the local government didn’t pay enough attention thus gave a chance for the virus to take an outbreak with limited control.The above equation is deduct based on the summation of geometric progression, it calculates the accumulated predicted case of coronavirus in each generation (R0=2.5). The total population in Europe is 741 million then the result of t = 23 generations.Most of the people will be shocked by the number, thinking how come this is so small. That the magic of exponential growth.And when we take 4 days as per generation, the final result corresponds to 92 days.Counting from 24/01/2020, by 25/04/2020 COVID-19 will reach every corner of the European continent without any non-pharmaceutical intervention (NPI).Remember, this is an extremely simple model, the real-world situation is much more complicated and the magnitude of more variables need to be factor into the dynamic epidemiological model.Here are a few Model that can be considered,Finally, we come to Herd Immunity.Economist summaries that the world governments are using three main methods to combat the Covid-19.As mentioned before, Britain is the first country that proposed the Herd Immunity approach to contain the disease( as shown form the graph above, the UK is the last country that enforces a lockdown).This theory is proposed a century ago (same as many Data Science classic models)As you can see in the visualization on the left side.The sicked case grows exponentially at the beginning and quickly infects most of the nodes. (Orange)But after a period of time, more and more nodes develop an immune system (purple).Thus the whole group is recovering back to its healthy state.Let’s say if the British government go ahead and implement this approach, 66.4 M * 60 % ≈ 40 M people will be infected. Furthermore, the current case fatality rate (CFR) of Covid-19 is 4% and 8% in China and Italy prospectively, and let’s assume Britain has the best health care system in the world that could bring down this rate to just 1%, there will still be about 40 M * 1% = 400 K death.Similarly, Neil Ferguson, an Imperial College London professor who leads the research on Covid-19 non-pharmaceutical research [Paper], said his original estimate, which showed the coronavirus would kill 500,000 people (with limited NPI, Non-Parmarcutical Intervention )in the U.K., remains true, while a new model reading reflecting the influence of lockdown measures saw that estimate shrink to 20,000 or fewer.What is this number mean?In 2018, the total death toll in the UK is just over 500K. And with the herd immunity strategy, Covid-19 will take almost the same amount of people’s lives in just a few months. And as a comparison, World War Two only cost over 450K lives in Britain including soldiers, nurses, citizens, etc. This implies Covid-19 will potentially have a larger impact in the UK than WWII.The supporters of the Herd Immunity approach argue that without a vaccine, this epidemic would return within a few weeks of the restrictions being lifted. The government might need to suppress the disease each time it resurfaces. This on and off-cycle must be repeated until either the disease has worked through the population or there is a vaccine which would be 6 months away.The Critics defend that there is no scientific evidence that supports the theory of a seasonal Covid-19 outbreak, and even if the public can build up the herd immunity it won’t be effective when the virus mutates.The best example to look at is Measles.Premises:For all these characters, Measles seems like the best candidate for herd immunity.So did this disease die out soon?Measles’ first appearance was in the 10th Century, it caused more than 2.6 million death each year till we discover vaccine in 1980.Before the age of the MMR vaccine, 90% (or some experts say 99% )of the infants will get Measles. The typical symptoms include rash and pneumonia or even brain damage for small cases.Even after we discovered the MMR. Measles is still hard to prevent and control.Using the previous equation P>1–(1/R0) = 1–(1/15) = 93%. We need to let 93% of the population build up an immune system to contain the virus. (WHO suggest this number should be 95%).But, due to some of the negative comments on the bad reaction after receiving the MMR vaccine, the actual vaccination rate is only 86%.in 1998, The lancet of UK published an article talking about the measles vaccine may cause autism in children [Reference]We already eliminated smallpox, the next in the line for WHO is Measles, Polio, and Covid-19.However, to combat a pandemic with an R0=2.5 and CER of 1%, would people willing to get infected in order to gain public herd immunity?Mitigation cost too many lives and suppression may be economically unsustainable, herd immunity? Are we able to put a price tag on people’s lives?When I saw the speech from the prime minister of the United Kingdom and the Chief Science Officer. I am both shocked and disappointed.The UK has long been the birthplace of great genius and great artists, some of the unformidable heroes of all time. They lighted up the entire world, Newton, Faraday, who brought humanity into the third industrial revolution; Charles Darwin, who tough us our origin; Shakespeare and Lennon, who made us laugh with tears; Churchill, who united the nation to defend its sovereignty against the enemy invasion.A county that first created a vaccine for clinical trials, the first country to conduct epidemiological research, and the country with the world’s top researchers and best medical journal.Some people are using World War III as the metaphor for Covid-19, it will be remembered and recorded in human history as an event bigger than the Spanish flu and Black Death.I hope one day when the next generation looks back in history and sees the world is united regardless of ideologies and religions, rich or poor to fight against the virus.It is not the time for ease and comfort, not the time to shoving and blaming each other and use it as a political weapon.No one knows what we will face in the weeks ahead, but everyone knows enough to understand that COVID-19 will test our capacities to be kind and generous and to see beyond ourselves and our own interests. Our task now is to bring the best of who we are and what we do to a world that is more complex and more confused than any of us would like it to be. May we all proceed with wisdom and grace.ReferenceAbout me, I am a 👧🏻 who is living in Melbourne, Australia. I studied computer science and applied statistics. I am passionate about general-purpose technology. Working in a Consulting firm as an AI Engineer 👩🏻‍🔬, helping an organization to integrate AI solutions and harness its innovation power. See more about me on LinkedIn.WRITTEN BY"
13,Data Science Concepts Explained to a Five-year-old,https://towardsdatascience.com/data-science-concepts-explained-to-a-five-year-old-ad440c7b3cbd?source=collection_category---4------0-----------------------,"I have seen a lot of data science interview questions that ask you to “describe [insert data science concept] to me as though I were a five-year-old. After talking this over with my sister, who is studying to become an elementary school teacher, we decided that this question is a bit hyperbolic, perhaps for emphasis or to be catchy. So I decided to keep the catchy title, but this article will be slightly more focused on explaining these concepts to an adult with a non-technical background. To keep with the theme, however, I have also created graphics to go along with each answer to depict the drawing I would accompany my explanation with if I had access to a whiteboard.The topics follow this order:For hypothesis testing, you first start with a question about a large group. For example, what is the average height of Americans? You guess it is 5 ft. 8 in. (just based on personal experience). You know that you cannot realistically measure the height of every single American, so you randomly pick a smaller group to measure (a random sample).To see if the average height of all Americans is really 5 ft. 8 in., you would conduct a hypothesis test! In this example, you would need to use a one-sample t-test, which has a specific equation that I will not go too deep into. The important thing to be aware of here is that you need to know the number of people you measured, their average height, and the standard deviation of their heights (a.k.a. how far away their heights are from the average).When you get a value (a t-value) from the t-test, you need to use a chart or program to evaluate whether or not you can conclude you are pretty sure — 95% sure — that this is the average height of Americans. If your t-value is larger than 1.96 or less than -1.96, you would say that you reject your hypothesis that the average height of all Americans is 5 ft. 8 in. If it is in between that range, you “fail to reject” (this is statistics lingo) your hypothesis and conclude this is the actual average height of Americans.Sometimes you have a dataset where you have so many columns (variables) that it is hard to work with. Putting all of these variables into a model would cause it to run slowly and it would also be hard to visualize the relationship between your dependent variable (the one you are trying to predict) and all of your other variables at once. This is where PCA comes in!Heads up, I am definitely leaving 5-year-old level explanations for this one. Through some complex math (linear algebra), PCA transforms your variables into “components.” The graph on the left is called a biplot, and it shows that component 1 and component 2 are made up of our original four variables. However, each variable is represented more in one component than another. The green (variable 2) vector, for example, has high values of component 1 but stays under 0.2 for its component 2 values. Thus, variable 2 is more of a significant part of component 1 than component 2.The goal here is to have a smaller number of components than the number of variables you have (so your model can run faster). Also, you want your components to retain as much information about the original variables as possible. To analyze this and choose how many of the components to put in a model, you would look at a metric called the explained variance ratio. This metric tells you the percentage of the total variance in the dataset that each component “explains.” Hopefully, the first illustration in this section makes this idea a little more clear. In an ideal scenario, you would choose the number of components to include in your model by adding up each one’s explained variance ratio until you reach a cumulative total of around 80% or 0.8.Finally, one more point for clarification: you lose some ability to interpret the results of your model when you are using PCA components. The best you can do once you figure out which components are meaningful in your model is to look at biplots, like the one above, to show how each of your variables contributes information to each of your components.Let’s say you have a database, which is really like a collection of Excel sheets or tables. SQL joins are easiest to understand via example, so here is what the example tables look like:For this example, let’s assume that there are some students whose information is in the student table, but they are not in the enrollment table because they are not currently enrolled in classes. Also, there are students enrolled in classes whose information we do not have in the student table. Here is what each type of join would tell you:So, the type of SQL join you want to use in your query (a.k.a. your code you send to the database to get data back) depends on the information you want to know. If you want to know the phone numbers of students who are enrolled in classes, use an inner join. If you want the names and year of every student, along with the classes they are in if they happen to be enrolled in anything, then you would use a left join.We are now venturing into modeling territory.This one is tough to explain to a non-technical audience, so bear with me.The bias/variance trade-off is a classic data science concept which states that there is an inherent trade-off between bias and variance when you are creating models. I will start with a description of both of these terms and then briefly explain why there is a trade-off between the two.Bias: A model that has high bias does not “fit” the data very well. That is to say, its accuracy is low on the data you use to make the model (the training data) and the data you use to test the model (for more on testing, see the cross-validation section). High bias indicates that there is something missing in your data or you are using the wrong model! Example: your data looks like a 2nd order polynomial (x²), but you try to use a linear model (x) for predictions, like in the top graph in the illustration.Variance: A model that has high variance is very sensitive to small changes in the independent variables of a dataset. This issue is associated with overfitting, where your model fits the data you train it with too well because it identifies noise (or randomness) in the dataset as being important when it really isn’t. Thus, when you introduce new data it has not seen before, it has poor performance in prediction. Example: you fit your data that has a 2nd order polynomial (x²) relationship with a 20th order polynomial, x²⁰ (see graphs above).The trade-off between these two issues exists because as you tweak your model to fit your data better, you are decreasing the bias but you necessarily increase the variance. The tension is between under and overfitting the model to the data you have collected.The image on the left is one of the most simple decision trees you could make. You start at the top and ask questions about your observation (a row in your dataset) and follow the tree down until you reach an outcome, which would be your predicted y value.It is easy with decision trees to overfit them to your data. The worst version of this would be if every outcome of your tree represented exactly 1 observation in your dataset.To solve this issue, Random Forest models are used. Essentially, a program will generate a bunch of Decision Trees, and each one will look a little different due to the randomness involved in where the model makes the decision splits. Then, the outcomes of all of these trees are averaged to get a final prediction. This method allows you to make smaller trees and reduce the variance of your model while keeping its accuracy, which is why it is very popular.And finally, a bit on validating your models.Cross-validation is a best practice when evaluating any kind of machine learning model. The image below shows how you would split your dataset if you wanted to perform cross-validation 3 times (3-fold cross-validation).For each split of the data above, you would build your model on the train segments and test your model (generate predictions) on the test set. Then, you can average the accuracy score for each split to get a good picture of how good your model really is. Data scientists do this because then they can use a smaller total amount of data and can test their model on a larger variety of data that the model has not seen before.When employers ask these questions, they seem to want to asses two things:I hope this article was helpful for you to see concepts broken down, whether you are trying to understand them yourself or better explain them to others. If you happen to have a kid who is around 5 years old, it would be amazing if you could explain one of these to them and let me know how it went. (I’m guessing they may be just a tad confused.) Also, if any of these topics are still unclear to you, please let me know!Thanks for reading! Check out my other articles here.All graphics made on www.canva.comWRITTEN BY"
14,Your simplified data science workflows have arrived!,https://towardsdatascience.com/your-simplified-data-science-workflows-have-arrived-208ce20bad88?source=collection_category---4------1-----------------------,"Hey, data scientists using Google Cloud: why aren’t you using machine images yet? Because they’re only two weeks new, that’s why…Machine images on Google Cloud make it easy to create, back up, restore, and share virtual machines you’ve customized. They’re what you’d get if disk images and snapshots and instance templates combined to have a baby. (Er, okay, that’s not how having a baby works.)Machine images make it easy to create, back up, restore, and share virtual machines.Oh, and if you’re on a tight budget, you’ll love them in a way you’d never love a VM copy.Ever lost your smartphone and then used a one-button solution to restore all your favorite apps, contacts, etc. onto a replacement phone? Machine images are like that, but for virtual machines on Google Cloud.Imagine that the virtual computer you’ve rented from Google Cloud is a character in a video game. Before machine images, that video game had no save option — you had to restart the game from scratch every time you did something you regret. Now it’s super-easy to save and load all those items and experience points you worked so hard to get.If you’re like most data scientists, you can’t find it in your heart to love virtual machine (VM) setup and other computing chores. These things take time away from working with your data, which is the part you’re actually passionate about. Sure, you’re discerning enough to want your own special customized setup — just the way you like it — so you’re able to restrain your grumbling the first time through.Weeks go by. You occasionally install shiny new packages, adjust settings, run scripts, and then one day… boom! You broke it.Ideally, you’d rewind the clock to your perfect machine exactly as it was last week, but… you can’t remember what you clicked on to get there. Even if you did, starting from scratch would take hours (if you’re lucky).Sadness.BEFORE: Restoring last week’s setup meant starting from scratch.AFTER: Click, click, done. Less time on chores, more time for data.Machine images to the rescue! A machine image is a single resource that contains all the information — machine type, network tags, labels, etc. — needed to backup, copy, restore, and share an instance (virtual machine).That means you’re able to save copies of configurations you like so that you can restore old versions of your instance easily. It’s as simple as it sounds. Click, click, done.Hang on, why not just save copies of the virtual machine itself? Because you’d probably like to save money while you save backups.Using machine images costs much less than saving multiple copies of a VM.If you make a copy of your VM, you’ll have doubled the billable disk space. If you create a machine image instead, you’ll only be billed for the difference in disk contents, so even if you picked a flashy beast of a VM, every additional machine image might only cost you a few cents.Imagine you’re a leader with new team members to onboard.You already have the perfect setup for your team’s needs, but it lives in a nasty doc detailing which scripts each new hire has to run and which buttons they need to click.Getting new teammates up and running takes forever.BEFORE: Getting new teammates up and running takes forever.AFTER: Much faster onboarding and a team that can keep up with the cutting edge.Duplication of effort doesn’t serve your business well but luckily machine images let skip it. They let you customize once to create a golden image for your team and share it with the whole group. That moves the starting line forward for everyone and speeds up your onboarding.“A machine image is more than just a save backup. It’s a way to accelerate your whole team.” — Ari Liberman, Product Manager, Google Compute EngineAs a bonus, you’ll want to keep updating the golden image to keep everyone’s configuration on the cutting edge. No more keeping track of which updates your group’s stragglers forgot to make.Grading that coding homework you gave your students — image recognition in TensorFlow, perhaps?—quickly becomes a nightmare if you have deal with debugging all the different ways your students might have messed up their machine setup. (Dare you trust them with figuring out how to get hold of GPUs, for example?)Sure, you could tell them to get a ready-made solution on Marketplace Solutions — not a bad idea! — but what if you’re itching to customize that solution? Are you going to waste a week of class time shepherding your 100 students’ teething pains as they try to twiddle the right knobs in the right sequence? Will you give up on a customized starting point?There’s a better way.BEFORE: You wasted valuable teaching time on walking your students through pressing the right setup buttons in the right sequence.AFTER: After you complete your own customization, your students can instantly start (and stay) on the same page.Start wherever you like (from scratch, from the Marketplace, from the previous prof’s machine image) and customize to your heart’s content, then share the final machine image with your students. Skip the in-class clickfest and rest assured that if their code doesn’t work, it’s not because they forgot to check a firewall settings box in week 1.As a bonus, if you have regrets about your students’ setup by the time midterm exams roll around, it’s easy to replace.Option 1 is to create a machine image right from your VM instance in Compute Engine as the first GIF shows.Option 2 is head straight to the machine images section itself and create a new one, indicating the VM instance source in the form, as the next GIF shows.You already saw one way to create a VM from your machine image (first GIF in the article), but here’s another:Almost too easy, right? Almost. As far as we data scientists are concerned, no matter how easy it is, anything that doesn’t involve playing with data should always be easier. But until the telepathic interface comes around, I’m happy to have this one.Learn more about machine images on the Google Cloud Blog.WRITTEN BY"
15,The Gaussian Model,https://towardsdatascience.com/the-gaussian-model-4a94a2b3ff1b?source=collection_category---4------2-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.Disclaimer first: I’m not an epidemiologist. These are not professional projections; these are back-of-the-envelope calculations. I’m a physicist, and you know how we love our “orders of magnitude”, “spherical cow approximations”, and “back-of-envelope calculations”.The New York Times this past week had an article about which countries are “flattening the curve”, so let’s explore a simple model of these “curves”.We’ll use the simplest model: the Normal or Gaussian curve.The infections start out growing exponentially at first, then whatever response the host country enacts, after some time, new infections go back to near zero. At least that’s the back-of-the-envelope theory. Surely there are better models, but we’ll use the Gaussian model as a first shot.The Gaussian model is defined by only three parameters: N, μ, and σ, and looks like this:In this model, at 2σ days before peak infection, on day μ-2σ (day 20), about 2% of the total people have been infected. By 1σ days before peak (day 35), roughly 16% of the total final infected population are already positive. On day μ, 50% of total cases are infected.Let’s first try the model on countries which seem to be past peak infection and are on their way to recovery: China and South Korea.We can get data in the format we need from the European Center for Disease Prevention and Control, the EU equivalent of the US CDC. Kaggle also has good data.Plotting the daily infection rate from China, starting on Jan 1, 2020, shows the large spike in cases around day 43 (Feb 12) when medical teams started to use simpler and faster methods of diagnosis versus the earlier DNA matching tests. Even so, the Gaussian model provides a decent fit.For South Korea, the fit perhaps looks better. But it’s not capturing the stalled falloff on the right side very well. It could even be that this excess represents a separate regional Gaussian event.To fit the model to these data, I used the curve_fit() function from the python scipy.optimize module and my Jupyter notebook is here.One simple check on these models is whether the total area under the model curve is anywhere close to the actual total infections for the country. Standard procedure for back-of-the-envelope calculations is frequent sanity checks.Given N and σ, the total area under the model curve is:Here are the comparisons (as of Mar 24, 2020)Chinasurprisingly, only about 3% off.South Korea∼10% off, mainly because the model is missing that recent stall in the drop in cases in South Korea. But, 10% is not that bad for a back-of-the-envelope estimate.Here’s where the back-of-the-envelope model fails, or rather looses its ∼10% goodness.Below are the Gaussian model fits to the data as of writing (Mar 24, 2020) for a few countries.We can get a sense of “how far into it” the country is at the current moment, by comparing the current cumulative number infected to the expected total cases from the model. This ratio is the area under the model so far, divided by total area of the model.This percentage is shown below the figures (area fractions greater than 100% are possible since the total number of actual cases can differ from the total predicted by the model, e.g. South Korea). Keep in mind that the area fraction is an estimate from the model, and thus is subject to statistical suspicions of the model that will be raised below.There are so many ways that this model is too simple!Real epidemiological models incorporate things like Markov Chain Monte Carlo (MCMC), coupled differential equations, real-time transportation data, and social graph analysis. As you can see by experimenting with games like Plague Inc. on your phone, there are many ways the epidemic can take on a life of its own and deviate from simple models. Really — the only thing the Gaussian model has going for it is the Central Limit Theorem, and that it’s back-of-the-envelope simple.The model captures gross features reasonably well after the event for countries which seem to now have the infections under control, such as China and South Korea (and something non-gaussian seems to be happening now in South Korea, as mentioned above). But the fit parameters, μ, N, and σ, of the model depend very much on where you are along the curve, i.e. your area fraction.As you climb one side of the data, there are many local peaks and valleys. But, if your country is already close to halfway through the infection curve, you can make a pretty good guess about the future using the Gaussian model. From the perspective at the peak, when the area fraction = 50%, the future should look, well, sort of like the past in reverse. Infections should start to lessen, and finally dwindle to zero.The problem is, you don’t know for sure when you are there.If you are only, say, 10% through the exponential uptick of infections, then the error bars on the parameters you get using the Gaussian model are far too sensitive to the latest data. Below are some of the different possible models that we get from statistical fluctuations of the data at 10%, 20%, and 30% of the way through pandemic (area fraction). You can see that the models fluctuate wildly when we only have data far to the left of the center peak, at a small area fraction.In fact the variance of the Gaussian model’s σ parameter, as estimated at different points “into the event” (represented by the area fraction under the curve at that point), blows up as we move earlier to the left.Thus, we can’t really use this simple approximation as a predictive model, at least until we are well into the thick of it, with maybe 30% of total infections already known. And again, we don’t really know when that is until afterward.For example, the current area fraction estimate from the US data is ∼15%. This means there is a lot of variance in the possible models we get using the data to date (Mar 24, 2020). Here are some examples, including the “best” fit: N=21178, μ=91 (March 31st!), and σ=6, as returned by the curve_fit() function. This seems ridiculously optimistic, despite calls from various leaders to get “back to work”. But other model parameters that match the US data similarly well at this moment (day 85) are not so encouraging, and look like this.Looking at the longer term side of these models shows how little we really know at this point. Note that even the “area fraction” estimate (15% for the US) has very large modeling error, as can be seen below. While we seem to be 15% of the way through the optimistic (blue) model, we are less than 1% of our way through the other “reasonable” models (red, green, and orange). It’s just too early to tell.Still this back-of the-envelope model is instructive to play with. It’s very simple, and each week more data comes in, moving us further along the curve so that the Gaussian model has a bit more predictive power. It’s interesting to see that Iran and Italy seem to be near or nearing the peak.The model also points out how bad unsophisticated predictions are at this stage of the epidemic, and does seem to show that we may have a way to go before we are at something like a comfortable 90% area fraction. Note that South Korea has a 111% area fraction and their infections are still not trailing off, so don’t mistake this for real epidemiological modeling.It’s just the musings of someone with easy access to data, suddenly lots of time at home, and a stack of envelopes piling up.WRITTEN BY"
16,COVID-19 Testing. What are your chances?,https://towardsdatascience.com/covid-19-testing-what-are-your-chances-33f0af5d2ae4?source=collection_category---4------0-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.Note: I am not a medical professional and this blog is just my take on what I believe are statistical ideas pertinent to diagnostic testing.Testing for COVID-19
You need no introduction to COVID-19, the coronovirus that’s now spread to every country on Earth and is wreaking havoc on economies, health systems and people’s lives.Beyond social distancing, one of the priories for governments the world over is to test as many of its citizens as possible, both to see who currently has the virus (the PCR test) and to see who has already had it (the antibody test). Note that in this blog post, I’ll be using ‘positive’ to mean that an antibody is detected and the person has had COVID-19.So far, so good. You roll out the tests and assess the population, then certain people can return to work.Evaluating a yes/no diagnostic test
The problem comes when you look down at the test cartridge (typically a point-of-care lateral flow device, similar to how a home pregnancy test works) and see the result. These tests give a ‘positive’ or a ‘negative’ result, and we often hear that such tests are evaluated by their accuracy. We hear it on the news all the time. You run a test that’s 99% accurate, get a positive result, so it’s easy to assume that probability of you having had COVID-19 is 99%, right?Not quite.Accuracy is the number of cases, positive or negative, that a test correctly identified during some sort of evaluation. The problem with this way of quoting how good a positive/negative test is, is that it doesn’t distinguish between the positives that it got right and the negatives that it got right.Imagine you have a test that fundamentally doesn’t work. It just says ‘positive’ every single time you run it, regardless of the patient, the condition, etc. You test it on a random group of medical staff, 98 of whom are known to have had COVID-19 and 2 who haven’t. Your bogus test, diagnosing everyone as positive, would end up with an accuracy of 98%!Thankfully, there are better ways to summarise the effectiveness of such tests, the most common being sensitivity and specificity. These are defined as follows,Notice the denominators. For sensitivity, we have ‘true positives’, which are of course positives, and ‘false negatives’, which are also positives. So, sensitivity only deals with positive cases, and the logic is the same with specificity for negative cases. The result is that sensitivity is a measure of the probability of getting a positive result out of all the positive cases, and that specificity is a measure of the probability of getting a negative result out of all the negative cases.Another way of phrasing this is that sensitivity is the probability of getting a positive result, given that you have had the disease, which is written as P(+ve result|Disease)However, in practice, we’re interested in the opposite of this, namely the probability of having had the disease, given a positive test result, i.e. P(Disease|+ve result). So, in practice, how do we do this flip?Enter Reverend Bayes
Thomas Bayes was born in England in 1701. He spent most of his life focused on the Church and only published a single work on mathematics. In later life he developed an interest in probability, possibly motivated by wishing to refute David Hume’s argument against taking personal testimonials as evidence of miracles.After his death, a friend found and handed in manuscripts on probability to the Royal Society, to which Bayes had been elected a fellow in 1742. One of the essays contained within, titled ‘An Essay towards solving a Problem in the Doctrine of Chances’, led to what is today known as Bayes’ theorem, defined as,Note the first term on the left and the first on the right. Here we can see the ‘flip’ that we’re after, i.e. obtaining P(Disease|+ve result) when we know P(+ve result|Disease), along with a few other terms which we’ll get to soon. This theorem in turn (after refinement by Pierre-Simon Laplace) led to what is today known as Bayesian statistics.The prevailing type of statistics and that used in many applications is known as frequentist statistics. This approach assigns probabilities based upon the frequency of events that occur in a sample. In contrast, in the Bayesian mindset, probabilities are related to our current level of knowledge about an event. This latter approach allows the introduction of something called a prior or pretest probability (explained in more detail below), which is a probability assigned before data is collected on an event. This pre-test probability is combined with new evidence to give a posterior or post-test probability.In the Bayes’ theorem, P(A) is the probability of having had the disease and P(B) is the probability of getting a positive result. What are these values? P(A) is the pre-test probability (mentioned above), for which local disease prevalence is often used. Alternatively, a pre-test probability can also be estimated from a detailed medical examination, a review of a patient’s records, etc. P(B) is the combination of all the ways a positive result could be obtained, i.e. the patient could have had the disease and received a positive result or the patient could not have had the disease and received a positive result. Specifically,In practice, such calculations are usually done using something called likelihood ratios. These are defined as,Where LR+ is the positive likelihood ratio and LR- is the negative likelihood ratio. They allow you to go from a pre-test probability to a post-test probability using a graphical tool called a Fagan nomogram. Alternatively, the calculation can be quickly performed using a computer. They also give you an indication of the effectiveness of a test, as they tell you how much your pre-test probability will be affected by either a positive or negative result (see table below).The exact calculation involves switching from probabilities to odds using,Taking a look at some of the COVID-19 tests that are being used around the world, values for sensitivity seem to be around 91% and specificity around 99%. That gives us a LR+ of 91 and a LR- of 0.09. Pretty impressive.We now have all the pieces of the puzzle to work out, given a positive result, what the probability of having had COVID-19 is for a given individual. First, let’s assume you run the test in an area that has a prevalence of 1% and get a positive result. What is the post-test probability?There we have it. The probability that an individual has had COVID-19 after a positive result from a fantastic test is less than 50%.Now, imagine if Tom Hanks had suspected COVID-19. No, not the real Tom Hanks, who did actually get COVID-19. I mean Chuck Noland, the character Tom Hanks plays in the film Castaway. If, via a highly convoluted turn of events, Chuck Noland, stranded for years on a desert island, found a COVID-19 rapid point-of-care test washed up on the beach and decided to run it because he had had a bit of a cough, what would it mean if it gave him a positive result? Well, the pre-test probability is zero, which means the post-test probability is also zero. Running the test is a complete waste of time! (not that that is much of an issue when you’re stuck on a desert island).Now let’s flip it the other way. You run the test on medical staff who have been working endless hours in an emergency COVID-19 shelter, without sufficient personal protective equipment. They’ve all had symptoms but have since recovered. The patients that they’ve been treating all had severe symptoms. They’re were all struggling to breathe. They all had a fever. They’ve all had contact with family members that were themselves confirmed cases. It’s the middle of summer, with very low rates of the common cold and the flu. It’s hard to say what the prevalence is, so you use the method of using (past) clinical symptoms and patient history. You estimate a pre-test probability for each staff member to be 0.95. Here are the numbers,You’ve gone from a pre-test probability of 0.95 to a post-test probability of, effectively, 1. Not particularly surprising. Again, this test was arguably a waste of time.Triaging Tests
The conclusion from this is that, given you’re likely to have a shortage of tests, does it make sense to test people who have an extremely low or an extremely high pre-test probability? After all, at the end of the day, what you do after you’ve run the test probably won’t change. If you’re pretty sure someone has had COVID-19 and the plan is to allow them to leave isolation, using up a precious test which comes back positive isn’t going to change things. Even if someone has a pre-test probability of 0.95 and they get a negative result, that doesn’t change things dramatically either (based upon the LR- above),We get a post-test probability of 0.63. If in the doctor’s mind they’re thinking “I’ll allow a member of the team return to work if they have a post-test probability over 0.5 or above”, this negative result doesn’t change anything. They would have been allowed to return before the test. They’ll be allowed to return after it, too.This ‘treatment threshold’ is easier to see with a plot. Below is a Python function that works out the post-test probability for a given pre-test probability, with given values for sensitivity and specificity.We can use this function along with sensitivity and specificity values similar to those in the real-world to create a plot of pre-test probability vs post-test probability,Here is what we get for a positive test result,And for a negative test result,As you can see for a positive result, as the pre-test probability increases the post-test probability shoots up rapidly. For a negative result, you need a very high pre-test probability for the post-test probability to also be high.How Confident?
There’s one more aspect to all of this. The numbers typically quoted for COVID-19 tests do not often include confidence intervals.Confidence intervals are a range of values around some number, such as a sensitivity value, that show you the possible range of values that are compatible with your data. They’re a way to indicate where you may find your result if you were to repeat your experiment. Such information is critical, because if you’re basing key decisions on a particular number which might actually be different in reality, you could end up with serious problems.Let’s take an example. Let’s say a COVID-19 test is quoted with a sensitivity and specificity of 88% and 90%, respectively. Given the number of cases which fell into each category during testing (true positives, false positive, true negatives and false negatives), we could calculate the confidence intervals.Let’s say we do this and end up with a sensitivity of 88% (85% to 92%, 95% CI) and a specificity of 90% (84% to 95%, , 95% CI), where the ‘95% CI’ indicates it’s a 95% confidence interval (see more on this in this blog post).In essence, this COVID–19 could have a sensitivity as low as 85% and a specificity as low as 84%. That gives us an LR+ of 5.3 and an LR- of 0.18. What does this mean in practice?Let’s say your estimated pre-test probability is 0.1. A positive test result in this case would give you a post-test probability of around 0.38, which is still low (see image below). Do you remain in isolation with a probability of 0.38? What if you have a critical job? Has the test really achieved anything?Likewise, in a previous example dealing with front-line member of medical staff, their pre-test probability might be estimated to be 0.95. If this test gives you a negative result, the resulting post-test probability is 0.78 (see image below). Still very high.Closing thoughts
We’ve seen that the results of rapid, point-of-care yes/no diagnostic tests, which are never perfect, can be placed into a meaningful context using Bayesian statistics and the concept of a pre-test probability. A positive result does not necessarily mean that the person has had COVID-19, especially if they’re in an isolated hamlet in the Scottish Highlights (for the purpose of this illustration, please ignore the fact that people are flocking to the highlands to self-isolate). Likewise, a negative result does not necessary mean that the person has not had COVID-19, especially if they’re just spent 4 weeks on the front-line treating hundreds of patients with the virus.Context matters, and in these frantic times of testing, such details may be important.As stated at the start, this article is just an example of some statistical ideas and is in no way meant to offer any sort of guidance with regards to COVID-19.WRITTEN BY"
17,Predicting Weekly Hotel Cancellations with ARIMA,https://towardsdatascience.com/predicting-weekly-hotel-cancellations-with-arima-4cb4f1849ef6?source=collection_category---4------1-----------------------,"Data analytics can help to solve this issue in terms of identifying the customers who are most likely to cancel — allowing a hotel chain to adjust its marketing strategy accordingly.An ARIMA model is used to determine whether hotel cancellations can also be predicted in advance. This will be done using the Algarve Hotel dataset in the first instance (H1full.csv). Since we are now seeking to predict the time series trend, all observations are now included in this dataset (cancellations and non-cancellations, irrespective of whether the dataset as a whole is uneven).To do this, cancellations are analysed on a weekly basis (i.e. the number of cancellations for a given week are summed up).Firstly, data manipulation procedures were carried out using pandas to sum up the number of cancellations per week and order them correctly.In configuring the ARIMA model, the first 80 observations are used as training data, with the following 20 then used as validation data.Once the model has been configured, the last 15 observations are then used as test data to gauge the model accuracy on unseen data.Here is a snippet of the output:The time series is visualised, and the autocorrelation and partial autocorrelation plots are generated:Time SeriesAutocorrelationPartial AutocorrelationWhen a Dickey-Fuller test is run, a p-value of less than 0.05 is generated, indicating that the null hypothesis of non-stationarity is rejected (i.e. the data is stationary).An ARIMA model is then run using auto_arima from the pyramid library. This is used to select the optimal (p,d,q) coordinates for the ARIMA model.The following output is generated:Based on the lowest AIC, the SARIMAX(1, 1, 0)x(0, 1, 0, 52) configuration is identified as the most optimal for modelling the time series.Here is the output of the model:With 90% of the series used as the training data to build the ARIMA model, the remaining 10% is now used to test the predictions of the model. Here are the predictions vs the actual data:We can see that while the prediction values were lower than the actual test values, the direction of the two series seem to be following each other.From a business standpoint, a hotel is likely more interested in predicting whether the degree of cancellations will increase/decrease in a particular week — as opposed to the precise number of cancellations — which will no doubt be more subject to error and influenced by extraneous factors.In this regard, the mean directional accuracy is used to determine the degree to which the model accurately forecasts the directional changes in cancellation frequency from week to week.An MDA of 89% is yielded:In this regard, the ARIMA model has shown a reasonably high degree of accuracy in predicting directional changes for hotel cancellations across the test set.The RMSE (root mean square error) is also predicted:The RMSE stands at 77 in this case. Note that the units of RMSE are the same as the response variable, in this case — hotel cancellations. With an average cancellation of 94 for all weeks across the validation data, the RMSE of 77 is technically the standard deviation of the unexplained variance. All else being equal, the lower this value, the better.Even though the ARIMA model has been trained and the accuracy validated across the validation data, it is still unclear how the model would perform against unseen data (or test data).In this regard, the ARIMA model is used to generate predictions for n=15 using the test.index to specify the unseen data.Firstly, the array is reshaped accordingly:Now, the predictions are made, and the RMSE (root mean squared error), MDA (mean directional accuracy) and mean forecast errors are calculated:The RMSE has improved slightly (dropped to 57), while the MDA has dropped to 86% and the mean forecast error stands at -12, meaning that the model has a tendency to slightly underestimate the cancellations and therefore the forecast bias is negative.Here is a plot of the predicted vs actual cancellations:The same procedures were applied — this time using the second dataset.The following is the ARIMA configuration obtained using pyramid-arima:Predicted vs. ValidationPredicted vs. ActualIn this example, the ARIMA model was used to predict the degree of hotel cancellations on a week-by-week basis. The MDA demonstrated 86% accuracy in doing so across the test set with an RMSE of 57 on the H1 dataset, and an 86% MDA was yielded once again for the H2 dataset with an RMSE of 274 (with the mean cancellations across the 15 weeks in the test set coming in at 327).Of course, a limitation of these findings is that both hotels under study are based in Portugal. Testing the model across hotels in other countries would help to validate the accuracy of this model further.The datasets and notebooks for this example are available at the MGCodesandStats GitHub repository, along with further research on this topic.You can also find more of my data science content at michael-grogan.com.WRITTEN BY"
18,Jupyter as a Service on FlashBlade,https://towardsdatascience.com/jupyter-as-a-service-on-flashblade-3c9ec27f8fcf?source=collection_category---4------2-----------------------,"Jupyter notebooks are a great tool for data scientists to explore datasets and experiment with model development. They enable developers to easily supplement code with analysis and visualizations.Rather than the historical practice of having users manage their own notebook servers, JupyterHub can be deployed by an organization to offer a centralized notebook platform. JupyterHub also enables infrastructure teams to give each user access to centralized storage for: shared datasets, scratch space, and a persistent IDE.This blog post presents an example of deploying Jupyter-as-a-Service on Pure Storage FlashBlade. Users are able to create new notebook servers on the fly within a Kubernetes cluster with zero-touch provisioning. IT teams are able to manage efficient use of compute and storage resources across users.JupyterHub is used to manage and proxy multiple instances of the “single-user” Jupyter notebook server. It provides a public HTTP proxy on your network so users can login to a central landing page from their browser. Once a user logs in, JupyterHub spins up a server (pod) for that user. It reconnects to that user’s persistent storage. So, users can have stateful dev environments, but the compute nodes are only used as needed.We’ll deploy JupyterHub as a Kubernetes service so it’s easily manageable as part of a cluster.FlashBlade is an excellent storage backend for JupyterHub for a few reasons.First, it enables access to training datasets in-place, eliminating the need to copy datasets between nodes. Data scientists can perform training and testing of models using shared datasets with minimal data management.Second, FlashBlade supports the Pure Service Orchestrator (PSO), which fully automates creation and management of PersistentVolumes (PV) for applications in a Kubernetes cluster. PSO brings self-service to a JupyterHub deployment by eliminating manual storage administration for new users whose environments need persistent storage.In fact, JupyterHub is just one of the many applications that together, form a complete AI platform for data scientists. All of these applications should be backed by the same, centralized storage for management simplicity and efficient data management.Remove storage silos.Customize: You’ll need a psovalues.yaml file that describes your FlashBlade array. The easiest thing to do is copy our default ./psovalues.yaml and adjust the “arrays” section.Example customization:Install:helm install pure-storage-driver pure/pure-csi — namespace jhub -f ./psovalues.yamlCustomize:The ./datasetpv.yaml file is used create a Persistent Volume Claim named “shared-ai-datasets”. Adjust datasetpv.yaml to use your FlashBlade Data VIP and filesystem name.Install:kubectl create -f datasetpv.yamlCustomize:The only change required for the jupvalues.yaml file is to add a security token. Generate a random hex string:openssl rand -hex 32Copy the output and, in your jupvalues.yaml file, replace the phrase SECRET_TOKEN with your generated string:Install:helm install jhub jupyterhub/jupyterhub — namespace jhub — version 0.8.2 -f jupyterhub/values.yamlJupyterHub is now ready for use.Installing JupyterHub creates a proxy service that serves traffic for end users. The public address (proxy-public) can be found via:When a user navigates to proxy-public’s external-IP address, they’ll get the JupyterHub login screen:When Victor logs in, he has access to shared datasets (like cifar10 and openimages) as well as his home directory of personal notebooks, plots, and files.Running JupyterHub as a service within a Kubernetes cluster is easy to deploy and manage. Data scientists not only have persistent storage backing their personal environments, but they also have access to all shared datasets without time-consuming data copying or complex data management.Our code is on github: https://github.com/PureStorage-OpenConnect/ai-platformTry out these quick installation steps and let us know how it goes! #PureStorageWRITTEN BY"
19,Roadmap to Computer Vision,https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4?source=collection_category---4------3-----------------------,"Computer Vision (CV) is nowadays one of the main application of Artificial Intelligence (eg. Image Recognition, Object Tracking, Multilabel Classification). In this article, I will walk you through some of the main steps which compose a Computer Vision System.A standard representation of the workflow of a Computer Vision system is:We will now briefly walk through some of the main processes our data might go through each of these three different steps.When trying to implement a CV system, we need to take into consideration two main components: the image acquisition hardware and the image processing software. One of the main requirements to meet in order to deploy a CV system is to test its robustness. Our system should, in fact, be able to be invariant to environmental changes (such as changes in illumination, orientation, scaling) and able to perform it’s designed task repeatably. In order to satisfy these requirements, it might be necessary to apply some form of constraints to either the hardware or software of our system (eg. remotely control the lighting environment).Once an image is acquired from a hardware device, there are many possible ways to numerically represents colours (Colour Spaces) within a software system. Two of the most famous colour spaces are RGB (Red, Green, Blue) and HSV (Hue, Saturation, Value). One of the main advantages of using an HSV colour space is that by taking just the HS components we can make our system illumination invariant (Figure 1).Once an image enters a system and is represented by using a colour space, we can then apply different operators on the image in order to improve its representation:Once pre-processed an image, we can then apply more advanced techniques in order to try to extract the edges and shapes within an image by using methods such as First Order Edge Detection (eg. Prewitt Operator, Sobel Operator, Canny Edge Detector) and Hough Transforms.Once pre-processed an image, there are 4 main types of Feature Morphologies which can be extracted from an image by using a Feature Extractor:Once extracted a set of discriminative features, we can then use them in order to train a Machine Learning model to make inference. Feature descriptors can be easily applied in Python using libraries such as OpenCV.One of the main concept used in Computer Vision to classify an image is the Bag of Visual Words (BoVW). In order to construct a Bag of Visual Words, we need first of all to create a vocabulary by extracting all the features from a set of images (eg. using grid-based features or local features). Successively, we can then count the number of times an extracted feature appears in an image and build a frequency histogram from the results. Using the frequency histogram as a basic template, we can finally classify if an image belongs to the same class or not by comparing their histograms (Figure 3).This process can be summarised in the following few steps:New images can then be classified by repeating this same process for each image we want to classify and then using any classification algorithm to find out which image in our vocabulary resembles the most our test image.Nowadays, thanks to the creation of Artificial Neural Networks architectures such as Convolutional Neural Networks (CNNs) and Recurrent Artificial Neural Networks (RCNNs), it has been possible to ideate an alternative workflow for Computer Vision (Figure 4).In this case, the Deep Learning Algorithm incorporates both the Feature Extraction and Classification steps of the Computer Vision workflow. When using Convolutional Neural Networks, each layer of the neural network applies the different feature extraction techniques at his description (eg. Layer 1 detects edges, Layer 2 finds shapes in an image, Layer 3 segments the image, etc…) before providing the feature vectors to the dense layer classifier.Further applications of Machine Learning in Computer Vision include areas such as Multilabel Classification and Object Recognition. In Multilabel Classification, we aim to construct a model able to correctly identify how many objects there are in an image and to what class they do belong to. In Object Recognition instead, we aim to take this concept a step further by identifying also the position of the different objects in the image.If you want to keep updated with my latest articles and projects follow me on Medium and subscribe to my mailing list. These are some of my contacts details:[1] Modular robot used as a beach cleaner, Felippe Roza. Researchgate. Accessed at: https://www.researchgate.net/figure/RGB-left-and-HSV-right-color-spaces_fig1_310474598[2] Bag of visual words in OpenCV, Vision & Graphics Group. Jan Kundrac. Accessed at: https://vgg.fiit.stuba.sk/2015-02/bag-of-visual-words-in-opencv/[3] Deep Learning Vs. Traditional Computer Vision. Haritha Thilakarathne, NaadiSpeaks. Accessed at: https://naadispeaks.wordpress.com/2018/08/12/deep-learning-vs-traditional-computer-vision/WRITTEN BY"
20,Speeding Up Pandas DataFrame Concatenation,https://towardsdatascience.com/speeding-up-pandas-dataframe-concatenation-748fe237244e?source=collection_category---4------4-----------------------,"DataFrame concatenations is an expensive action, especially in terms of processing time. Imagine having 12 Pandas DataFrames of varying sizes that you want to concatenate on the column axis, as seen in the following box.In order to speed up your pd.concate(), there are two things you need to remember.That’s all you need to know :)Dr. Ori Cohen has a Ph.D. in Computer Science with a focus on machine-learning. He is a lead data-scientist at New Relic TLV, doing machine and deep learning research in the field of AIOps.WRITTEN BY"
21,4 free maths courses to do in quarantine and level up your Data Science skills,https://towardsdatascience.com/4-free-maths-courses-to-do-in-quarantine-and-level-up-your-data-science-skills-f815daca56f7?source=collection_category---4------5-----------------------,"When I got into Data Science and Machine Learning, anything related to math and statistics was something I had visited for the last time around 10 years ago. That’s probably why I found it so hard at first. It took me lots of hours of reading and watching videos to get some understanding of how things happen for lots of the tools we daily use in the industry. However, I got to a point where I felt the need of developing a solid understanding of what was happening underneath all those imports and fits I was doing, so I decided to freshen up that dusty math knowledge.Nowadays, I’m still doing it and I reckon it will be never enough. Moreover coming from Business and being in an industry full of professionals from engineering, statistics, physics and other exact sciences. I know there are LOTS of things to learn in the world of Data Science, but you know what? Technologies and languages might come and go, but the mathematical background of the field is going to remain.That’s why today I’m wrapping up a list of 5 courses to level up your math knowledge and take advantage of some of all this spare time we have been given this unfortunate situation we’re going home. Since, you know, you should be staying at home these days 😉.1. Mathematics for Machine LearningWhere: CourseraInvolved institution: Imperial College LondonTime required: 104hs (realistically it will be at least +50%)Prior requirements: NoneAbstract from the course itself:For a lot of higher level courses in Machine Learning and Data Science, you find you need to freshen up on the basics in mathematics — stuff you may have studied before in school or university, but which was taught in another context, or not very intuitively, such that you struggle to relate it to how it’s used in Computer Science. This specialization aims to bridge that gap, getting you up to speed in the underlying mathematics, building an intuitive understanding, and relating it to Machine Learning and Data Science.Topics covered:2. Essential Math for Machine Learning: Python EditionWhere: edXInvolved institution: MicrosoftTime required: 50hsPrior requirements: Python and some ground understanding of mathsAbstract from the course itself:Want to study machine learning or artificial intelligence, but worried that your math skills may not be up to it? Do words like “algebra’ and “calculus” fill you with dread? Has it been so long since you studied math at school that you’ve forgotten much of what you learned in the first place?You’re not alone. machine learning and AI are built on mathematical principles like Calculus, Linear Algebra, Probability, Statistics, and Optimization; and many would-be AI practitioners find this daunting. This course is not designed to make you a mathematician. Rather, it aims to help you learn some essential foundational concepts and the notation used to express them. The course provides a hands-on approach to working with data and applying the techniques you’ve learned.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.3. Probability and Statistics in Data Science using PythonWhere: edXInvolved institution: UC San DiegoTime required: 100–120hsPrior requirements: multivariate calculus and linear algebraAbstract from the course itself:Reasoning about uncertainty is inherent in the analysis of noisy data. Probability and Statistics provide the mathematical foundation for such reasoning.In this course you will learn the foundations of probability and statistics. You will learn both the mathematical theory, and get a hands-on experience of applying this theory to actual data using Jupyter notebooks.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.4. Bayesian Statistics: From Concept to Data AnalysisWhere: CourseraInvolved institution: Santa Cruz, University of CaliforniaTime required: 22hs (realistically no less than 30hs)Prior requirements: some ground understanding of probabilityAbstract from the course itself:This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach.Topics covered:And that’s all for now peeps. I recommend doing these courses in the order presented, but of course, go ahead with any you like if you match the requirements :).And don’t forget to take a look at some of my latest stories:Also, feel free to visit my profile on Medium and check any other of my other stories :). See you around! And thanks for reading!WRITTEN BY"
22,End-to-end AWS Quantitative Analysis: Moving from SKLearn to PySpark,https://towardsdatascience.com/end-to-end-aws-quantitative-analysis-moving-from-sklearn-to-pyspark-f20f883bec90?source=collection_category---4------0-----------------------,"In a previous post I described how to set up an automated workflow using Amazon Web Services (AWS). We’ve explored how to spin up Elastic Map Reduce (EMR) clusters using AWS’s command line tool, awscli.
In that post we’ve seen how to use sklearn’s DecisionTreeClassifier to classify price movement as “going up” or “going down” (1 or 0, binary classification problem).One drawback of using sklearn is that it uses a single machine to perform all calculations. This has the drawback of increasing run-time as data increases, even when our EMR clusters can contain multiple machines which could perform work in parallel. This is where Spark comes in with its version of a DecisionTreeClassifier. We will compare the use of sklearn and Spark in this tutorial.Spark is an Apache framework designed to do parallel and distributed processing across multiple machines. The idea is to break up the work into independent chunks that can all be computed (partition the data), then pool the results of the independent calculations. Spark is very powerful, but is written in Java. This is where PySpark comes into play. It’s a python wrapper around the Spark framework.Spark combines several abstractions from pandas, such as dataframes, as well as from sklearn, such as transformations and machine learning techniques.The first thing we have to do is convert our pandas dataframe into a spark dataframe. This is a very common operation so PySpark has a built-in function to do exactly that.Notice we have used Spark’s built-in types to specify a schema for the conversion. This step is highly recommended and saves tons of processing time. If this is not done, Spark tries to infer the schema on its own.
Also notice we set the nullable Attribute to False in the schema. This also saves processing time since Spark doesn’t have to worry about some columns containing nulls when doing the conversion.
Once we do that, you notice we can print the spark dataframe’s schema to see its structure:Scikit-learn models are employed on tabular data; either numpy arrays or pandas dataframes. However, Spark machine learning models require a slightly different architecture.While pandas and numpy leverage efficient memory use on a single machine, Spark is more concerned with easily spreading the data across different machines. As such, it needs every datapoint encapsulated in a single vector class. We need to employ a VectorAssembler to convert our feature columns into a single column which contains a vector. We have to tell the VectorAssembler which columns to insert into this new column.Now, when we check the schema, we see we added a single column of type vector which stores our data. Under this method, each data point is an encapsulated item which can be sent around our cluster.Spark has two main machine learning libraries; MLlib and ML. The preferred library is ML since MLlib is slowly being faded away. We will employ the decision tree classifier from ML.This part is fairly similar to our use of sklearn; We initialize our model, fit it, and then ask for predictions. There are two main differences:
1) We have to specify the features and target columns in the same dataframe. The decision tree expects all the features to reside in a single column (of type vector).
2) Unlike the sklearn decision tree, there are no separate predict and predict_proba methods. A very detailed dataframe is returned as our predictions which contains hard class predictions, probability predictions, and many more items.We will run the following code to employ the decision tree:We’ve taken a few steps there so let’s break it down:We’re going to need to use a new command with slightly more information hereI won’t explore every new parameter since I explored the create-cluster command in my earlier post. However I will point out you will have to set the s3 buckets to be your own buckets.
With that said, let’s take a look at a few new parameters we’ve added:For comparison’s sake, let’s take a look at the code I previously used for the sklearn tree as well as for Spark’s tree.>>> run-time: 9.19>>> run-time: 3.55Employing Spark’s decision tree ends up being more than twice as far as processing the data is concerned. This is along the lines of what we expected since Spark can cut the work up between the task and core nodes. However, this particular example becomes slightly more complex.While Spark’s decision tree shows clearly superior performance, employing Spark was not a much faster operation in this case.The cluster employing the sklearn decision tree had a total run time of 14 minutes, while the Spark cluster ran for 13 minutes. The close run times come from the fact that converting the dataframe into a Spark dataframe can be a costly operation on its own.However, the data we were exploring in this instance was only ~4GB in size. This difference will certainly increase as both the data size, and the cluster size, increase. Spark was designed to run on terabytes of data and more. So this example only illustrates the process of deploying a Spark based machine learning workflow.The entire script which was used for the processing with PySpark can be found below:Spark can be used to accelerate your machine learning needs, and contains many of the same algorithms employed in sklearn. The PySpark interface allows for an almost seamless transition, once you are familiar with the Spark framework. However, to reach that point of familiarity, one must venture past the usual knowledge of machine learning algorithms and also dive into configuration and memory management knowledge to allow Spark applications to run to their full potential.I had a lot of fun writing this particular post, and I hope you had fun reading it. I have a few more things to learn about Spark but I hope to make your learning experience easier.While experimenting with clusters of size 3–4 nodes, as well as taking quite some time to realize specifying an explicit schema saves quite a lot of processing time, I had to spend a bit more money on learning how to use Spark. I hope this tutorial will indeed save time and money for whoever reads it, but the overall cost for me has been around $60.
I advice you to be careful when prototyping with Spark and EMR, since it is easy to rack up charges, but you can’t learn how to use these tools without trying and failing.WRITTEN BY"
23,Function Estimation with Tensorflow,https://towardsdatascience.com/function-estimation-with-tensorflow-2b2beea142b?source=collection_category---4------1-----------------------,"Gradient descent is a mathematical approach in Calculus that considers derivates of variables in space to achieve an optimal combination of values to minimize a function. In machine learning usually, we use a loss function defined as predicted — actual to minimize thus giving a set of variables for a prediction problem. You can read here if interested.In this article, we will see how we can use Gradient Descent in Tensorflow to estimate function variables that we happen to see in our day-to-day research life. Furthermore, I will show an example as to how one might use TensorFlow optimizers to find the minimum value of a function and its parameters at the minima.Let us consider the following 3 scenarios. First two scenarios are for function estimation and the last will be to demonstrate the capability of evaluating a function minima. I will be using the following code to generate the data.In this scenario, we will try to estimate the values of X and Y when the function is minimum.As per the documentation, there are several optimizers for us to use. Let us have a look at how we can estimate variables for Set 1.In the above code, I will try to estimate M and C parameters of the function Y = Mx + C. Therefore, as the first step, I create the initializer function to return M and C as TensorFlow variables. This means the optimizer will compute the GRAD (multivariate gradient) and apply those to the Tensorflow variables with the learning rate provided.Note the loss function is lambda: sum(abs(y — M*x — C)). This translates to the mean absolute error, which is quite similar to the regression model which uses mean squared error. After 1000 iterations we get the values 4.4131455 and 0.29607454 for M and C. Both plots on the same plane would look like below.Note that we have an estimation that closely resembles the original set of data. Next, we’ll see how we tackle the second problem.We use the estimator Y = Ax^3 + Bx^2 + Cx + D as the target function. Therefore, we will have four tensor variables. Similar to the previous scenario we will be using the mean absolute error for the function estimation. We get the coefficients 1.0356606, 1.1082488, -2.7969947 and 8.258981 for A, B, C and D respectively. Our plot would look like below.Here we straightaway use the function value for the optimizer and ask it to minimize. After 1000 iterations we get values 0.5938998 and 1.5066066 for X and Y respectively. Plotting again gives us the below plot with the marked minima.Note that the minimum of this plot is actually a plane. However, we only compute points thus, we cannot expect to find a plane. A more mathematically sound approach would model a better approximation for this task.I hope you enjoyed reading this article about the practical use of gradient descent. Here, we use the functions to get data for demonstration. However, this is equally applicable for situations where only data exists with multiple parameters.Cheers!WRITTEN BY"
24,How to Use DBSCAN Effectively,https://towardsdatascience.com/how-to-use-dbscan-effectively-ed212c02e62?source=collection_category---4------0-----------------------,"DBSCAN is an extremely powerful clustering algorithm. The acronym stands for Density-based Spatial Clustering of Applications with Noise. As the name suggests, the algorithm uses density to gather points in space to form clusters. The algorithm can be very fast once it is properly implemented. However, in this article, we would rather be talking about tuning the parameters of DBSCAN for a better utility than the algorithm implementation itself. (Implementation of DBSCAN is very simple. The harder part, if any, would be structuring data for neighbourhood lookups.)Before we start, make sure you have these packages in hand.Let us first create a set of data that could replicate a suitable set of data points for our analysis. Python is very generous with its set of libraries. For the purpose of data generation, we will be using sci-kit learn library’s make blobs function.Here, I have created 3 blobs of data. We can see the visualization of these data blobs as illustrated in Figure 1. For this example, I intentionally created 3 clusters with different densities to make clustering harder.DBSCAN has a few parameters and out of them, two are crucial. First is the eps parameter, and the other one is min_points (min_samples). Latter refers to the number of neighbouring points required for a point to be considered as a dense region, or a valid cluster. Usually, we set this to a value that makes sense for the dataset and the number of dimensions present in the data. This will determine the number of outliers identified. However, this parameter is not as crucial as eps.The most important parameter of DBSCAN can be identified as eps. It is the furthest distance at which a point will pick its neighbours. Therefore, intuitively this will decide how many neighbours a point will discover. Although for the min_points/min_samples we can give a default value, we cannot do so for eps. This will depend on the distribution of the data itself. Let us do DBSCAN with some guessed values for our dataset. The code and visualisation (in Figure 2) would be as shown below.We can clearly see from our last figure, two clusters have been merged together. This is bad. Such situations can reduce recall in a real-world clustering application. Let’s try to vary eps and cluster again. The code and visualization (in Figure 3) would look like below.We can see that we hit a sweet spot between eps=0.1 and eps=0.3. eps values smaller than that have too much noise or outliers (shown in green colour). Note that in the image, I decrease eps by increasing my denominator in the code from 10 to 1. How can we do this automatically?Since the eps figure is proportional to the expected number of neighbours discovered, we can use the nearest neighbours to reach a fair estimation for eps. Let us compute the nearest neighbours.Note that in the nearest neighbour calculation, the point itself will appear as the first nearest neighbour. So we seek the 11 nearest neighbours. We sort the distance to the 10th nearest neighbour and plot the distance variation. As we can see, the elbow point appears somewhere in between 0.1 and 0.3. Quite what we were expecting isn’t it? I choose the 10th neighbour considering the fact that I pick 10 as min_samples value for clustering. I hope it makes sense so far.Ville Satopaa et al. presented the paper “Finding a “Kneedle” in a Haystack: Detecting Knee Points in System Behavior” in the year 2011. In this article, for the purpose of detecting the elbow point (or knee point), I will be using their python library kneed. We can use the following code to find and plot the knee point.We can see that the detected knee point by this method is at distance 0.178. Now we can use this value as our eps to see how our new clustering would look like.We can see that we have a reasonable estimate of the actual clustering. This is usually good enough for research work. If non-existence of out-liers is an intuitive assumption for the scenario, one can simply use the computed nearest neighbours to re-assign the outlier points (named as cluster--1) to detected clusters.There are a few implicit assumptions in this approach.These assumptions are implied when we consider the same neighbour level for knee computation. However, in the original data, we can clearly see that the densities are not the same. This is the main reason why we observe a few outliers even though the points are distributed using a fixed standard deviation when we create blobs. Moreover, fixing these is beyond the scope of this article.I have attached a jupyter notebook with the complete code used for the examples in this article. You can access the notebook from the link below.I hope you enjoy reading my article as much as I did so writing. Feel free to try these out in your research work. It helped me a lot.Thanks for reading!Cheers! 😊WRITTEN BY"
25,Anaconda: Start here for data science in Python!,https://towardsdatascience.com/anaconda-start-here-for-data-science-in-python-475045a9627?source=collection_category---4------1-----------------------,"If you’ve been following me or have read a few of my articles, you must know that I am a big fan of Python Virtual Environments. I’ve written about this before as well which you can read here and since then almost all of my projects include requirements.txt file for anyone else to easily replicate my work.I’ve worked with several virtual environment managers but never really gave much attention to Anaconda. However, now I need to work with the conda environments for my daily college work. Here, the recommended way to work is with virtual environments and my fellow colleagues use Anaconda for them. So, I decided to give it a try!Anaconda is just like any other virtual environment manager but it packs a lot more than just managing environments. It has its own Python package manager called conda and it supports pip as well. It is not limited to Python and can also work with other languages as well. But there are some caveats as well such as some packages can only be installed using pip and not conda. So, in this post, we’ll explore a bit about Anaconda and then go over some of the most useful commands you’ll need to become an expert in working with virtual environments.Sounds absurd to name a project after a snake, but hey, we know that Python is a snake too **wink wink**Anaconda is all about Data Science. It focuses on encompassing features and packages that aid a data scientist to have a workbench where he/she can do it all. It hosts several data science packages from the very start to enable data scientists to start quickly. With the added advantage of environments, it becomes an ideal starting ground for anyone who is driving towards their Data Science journey.Sounds interesting! But why would you prefer it over other virtual environment managers? I’ve been using it for a while and here is what I found really intriguing about it:However, I still feel that conda is incomplete without pip. If you would try to install some packages via conda, it might give an error but the same package can be easily installed using pip.Use both conda and pip while working with Python environments.Python2 is no longer being developed and all of the development efforts are being targeted towards Python3. However, several packages and projects still rely on Python2, so it’s a good practice not to lose touch with it. Check the project below which is based on Python 2.X and creates some phenomenal artwork.This article works for both Python2 and Python3 as it would only essentially differ in selecting the Python version while setting up your environment but we’ll work with Python3 distribution for now.Start any new project in Python3 to be future proof.If you’ve installed any software before, you’ll feel right at home. Go here, select your operating system (I’m using a Mac) and then select the Python 3.x version Download button which will download the installer to your machine.Then, open the installer on your machine and follow along with all the steps, while reading and agreeing with the agreement. You do not need to change anything as everything gets set up all by itself and once it finishes, you’re all set to use Anaconda on your machine.As I mentioned before, the Anaconda package comes with conda manager which shall allow you to install packages, create and activate virtual environments and a lot more.I use iTerm2 as my main terminal which I have designed based on how I like. You can create a similar (or even better) view of the terminal by following a guide I posted on Medium.You will notice that the terminal now has (base) written in front of the computer name. It means that your base conda environment is set (meaning you’re working globally for the whole user and not a specific environment).I’ll first describe a few commands in this base environment but they work inside any particular environment as well. We will see how to set up and work with environments later down the article.One of the most basic commands is to know the list of all installed packages inside your environment. Note that Anaconda comes with many packages built in, so you’ll see a lot more packages than you expect. The command is as follows:conda is very intuitive because what you want to do is exactly what the command will probably look like. Let’s say we want to install numpy but we do not know the version. We can use the following command to search for its versions:Let’s say we plan to install numpy with version 1.18.1, we’ll use the following command to do so:Suppose in a certain case, you no longer need a particular package, or you installed a wrong version, you can simply use the following command to remove it. Let’s do it with numpy.conda also enables you to create, activate and deactivate virtual environments as needed. All these environments are isolated from one other and can host very different combination of packages and package versions without interfering with one another.A virtual environment can be created with the following command with an option to directly install a few packages while creating the environment:The word after -n becomes the name of the environment. In our case, it is env_name. We then specify the version of python as defined above. We can then specify a list of packages that we want to install while creating the environment. This means the packages will be listed in place of <list_of_packages>. For example, to create the environment and install numpy and pandas we will use the following command:It’s very easy to activate the environment. Simply use the following command:If you happen to use a prior version of conda, you will need to use the command source activate on Linux/MacOS or activate on Windows.As we can see in the image above, the name of the environment precedes the command line indicating that we are inside the environment.Now, we can install packages as we need using conda install or pip install and they will not interfere with any packages outside this environment.Deactivating is equally simple. Use the commandAgain, if you are on a prior version of conda use source deactivate on Linux/MacOS or deactivate on Windows.You’ll notice that the text in the front changes back to (base) indicating that we are no longer in our environment.One of the prime reasons (apart from managing isolated environments) for using environments is the ability to share the exact Python packages with their exact version with others. conda uses YAML files to share the environment information in contrast to requirements.txt generally used with pip. To export all packages, whether installed via conda or pip, we use the following command inside our environment:The file will look something like the image below:I recommend that we also create the requirements.txt file so users who do not use conda can still use our environment configuration. For this, use the command inside the environment:You can simply share the files environment.yml and requirements.txt with your project for easy replication.Once you have a environment.yml file with you, you can simply use it while creating the environment and set it up all ready to be used like below:When you work on multiple projects, you create many environments. You might forget the environments that you might already have created. There is a quick command to view all available environments:As you can see, I have quite a few environments with a special base which we normally have.Once you no longer work on a project, you might want to remove its environment as well. Use the command to do so:Here, env_name should be replaced by the name of the environment being deleted. I’m deleting env_name so I’ll keep the same.In this article, we explored what is Anaconda and how it is better than other virtual environment managers. We explored ways to use conda to work with packages as well as create and run Python virtual environments.You can also refer to the conda cheatsheet once you get familiar with the basics in this article.I hope you liked my work. If you have any suggestions, ideas or you face any problems, please let me know in the comments. You can also connect with me on LinkedIn.WRITTEN BY"
26,Top Programming Languages for AI Engineers in 2020,https://towardsdatascience.com/top-programming-languages-for-ai-engineers-in-2020-33a9f16a80b0?source=collection_category---4------2-----------------------,"Artificial Intelligence has now become an integral part of our daily lives with all the benefits it provides over hundreds of unique use cases and situations, not to mention how simple and easy it has made things for us.With the boost in recent years, AI has come a long way to help businesses grow and achieve their full potential. These advancements in AI would not have been possible without the core improvements in the underlying programming languages.With the boom in AI, the need for efficient and skilled programmers and engineers skyrocketed along with improvements in programming languages. While there are plenty of programming languages to get you started with developing on AI, no single programming language is a one-stop-solution for AI programming as various objectives require a specific approach for every project.We will discuss some of the most popular ones listed below and leave the decision making up to you —Python is the most powerful language you can still read.
- Pau DuboisDeveloped in 1991, Python has been A poll that suggests over 57% of developers are more likely to pick Python over C++ as their programming language of choice for developing AI solutions. Being easy-to-learn, Python offers an easier entry into the world of AI development for programmers and data scientists alike.Python is an experiment in how much freedom programmers need. Too much freedom and nobody can read another’s code; too little and expressiveness is endangered.- Guido van RossumWith Python, you not only get excellent community support and an extensive set of libraries but also enjoy the flexibility provided by the programming language. Some of the features that you may benefit the most from Python are platform independence and extensive frameworks for Deep Learning and Machine Learning.The joy of coding Python should be in seeing short, concise, readable classes that express a lot of action in a small amount of clear code — not in reams of trivial code that bores the reader to death.- Guido van RossumPython Code Snippet Example:● TensorFlow, for machine learning workloads and working with datasets● scikit-learn, for training machine learning models● PyTorch, for computer vision and natural language processing● Keras, as the code interface for highly complex mathematical calculations and operations● SparkMLlib, like Apache Spark’s Machine Learning library, making machine learning easy for everyone with tools like algorithms and utilities● MXNet, as another one of Apache’s library for easing deep learning workflows● Theano, as the library for defining, optimizing and evaluating mathematical expressions● Pybrain, for powerful machine learning algorithmsAlso, Python has surpassed Java and became the 2nd most popular language according to GitHub repositories contributions. In fact, Stack Overflow calls it the “fastest growing” major programming language.”Python Courses for Beginners —Write once, run anywhereJava is considered one of the best programming languages in the world and the last 20 years of its use is proof of that.With its high user-friendliness, flexible nature and platform independence, Java has been used for developing for AI in various ways, read on to know about some of them:● TensorFlow
TensorFlow’s list of supported programming languages also includes Java with an API. The support isn’t as feature-rich as other fully supported languages, but it’s there and is being improved at a rapid pace.● Deep Java Library
Built by Amazon to create and deploy deep learning abilities using Java.● Kubeflow
Kubeflow facilitates easy deployment and management of Machine Learning stacks on Kubernetes, providing ready to use ML solutions.● OpenNLP
Apache’s OpenNLP is a machine learning tool for natural language processing.● Java Machine Learning Library
Java-ML provides developers with several machine learning algorithms.● Neuroph
Neuroph makes designing neural networks using the open-source framework of Java possible with the help of Neuroph GUI.If Java had true garbage collection, most programs would delete themselves upon execution.
- Robert SewellJava Code Snippet Example:Java Courses for Beginners —R was created by Ross Ihaka and Robert Gentleman with the first version being launched in 1995. Currently being maintained by the R Development Core Team, R is the implementation of S programming language and aids in developing statistical software and data analysis.The qualities that are making R a good fit for AI programming among developers are:● The fundamental feature of R being good at crunching huge numbers puts it in a better position than Python with its comparatively unrefined NumPy package.● With R, you can work on various paradigms of programming such as functional programming, vectorial computation and object-oriented programming.Some of the AI programming packages available for R are:● Gmodels provides a collection of several tools for model fitting● Tm, as a framework for text mining applications

● RODBC as an ODBC interface for R● OneR, for implementing One Rule Machine Learning classification algorithm, useful for machine learning modelsUsed widely among Data Miners and Statisticians, features provided by R are:● Wide variety of libraries and packages to extend its functionalities● Active and supportive community● Able to work in tandem with C, C++ and Fortran● Several packages help extend the functionalities● Support for producing high-quality graphsSomething Interesting —
Covid-19 Interactive Map made using RShort for Logic Programming, Prolog first showed up in 1972. It makes for an exciting tool for developing Artificial Intelligence, specifically Natural Language Processing. Prolog works best for creating chatbots, ELIZA was the first-ever chatbot created with Prolog to have ever existed.To understand Prolog, you must familiarize yourself with some of the fundamental terms of Prolog’s that guide it’s working, they are explained in brief below:● Facts define the true statements● Rules define the statement but with additional conditions● Goals define where the submitted statements stand according to the knowledgebase● Queries define the how of making your statement true and the final analysis of facts and rulesProlog offers two approaches for implementing AI that has been in practice for a long time and is well-known among data scientists and researchers:● The Symbolic Approach includes rule-based expert systems, theorem provers, constraint-based approaches.● The Statistical approach includes neural nets, data mining, machine learning and several others.Short for List Processing, it is the second oldest programming language next to Fortran. Called as one of the Founding Fathers of AI, Lisp was created by John McCarthy in 1958.Lisp is a language for doing what you’ve been told is impossible.-Kent PitmanBuilt as a practical mathematical notation for programs, Lisp soon became the choice of AI programming language for developers very quickly. Below are some of the Lisp features that make it one of the best options for AI projects on Machine Learning:● Rapid Prototyping● Dynamic Object Creation● Garbage Collection● FlexibilityWith major improvements in other competing programming languages, several features specific to Lisp have made their way into other languages. Some of the notable projects that involved Lisp at some point in time are Reddit and HackerNews.Take Lisp, you know its the most beautiful language in the world — at least up until Haskell came along.
-Larry WallDefined in 1990 and named after the famous mathematician Haskell Brooks Curry, Haskell is a purely functional and statically typed programming language, paired with lazy evaluation and shorter code.It is considered a very safe programming language as it tends to offer more flexibility in terms of handling errors as they happen so rarely in Haskell compared to other programming languages. Even if they do occur, a majority of the non-syntactical errors are caught at compile-time instead of runtime. Some of the features offered by Haskell are:● Strong abstraction capabilities● Built-in memory management● Code reusability● Easy to understandSQL, Lisp , and Haskell are the only programming languages that I’ve seen where one spends more time thinking than typing.
-Philip GreenspunIts features help improve the productivity of the programmer. Haskell is a lot like the other programming languages, just used by a niche group of developers. Putting the challenges aside, Haskell can prove to be just as good as other competing languages for AI with increased adoption by the developer community.Julia is a high-performance and general-purpose dynamic programming language tailored to create almost any application but is highly suited for numerical analysis and computational science. Various tools available for working with Julia are:● Popular editors such as Vim and Emacs● IDEs such as Juno and Visual StudioSome of the several features offered by Julia that make it a noteworthy option for AI programming, Machine Learning, statistics, and data modeling are:● Dynamic type system● Built-in package manager● Able to work for parallel and distributed computing● Macros and metaprogramming abilities● Support for Multiple dispatches● Direct support for C functionsBuilt to eliminate the weaknesses of other programming languages, Julia can also be used for Machine Learning applications with integrations with tools such as TensorFlow.jl, MLBase.jl, MXNet.jl and many more that utilize the scalability provided by Julia.Google Trend — Julia Interest Over TimeJuliaCon 2019 Highlights —With several AI programming languages to choose from, AI engineers and scientists can pick the right one that suits the needs of their project. Every AI programming language comes with its fair share of pros and cons. With improvements made to these languages regularly, it won’t be long when developing for AI would become more comfortable than how it is today so that more people could join this wave of innovation. Outstanding community support has made things even better for new people, and the community contributions towards several packages and extensions make life easier for everyone.I hope you’ve found this article useful! Below are additional resources if you’re interested in learning more: —About AuthorClaire D. is a Content Crafter and Marketer at Digitalogy — a tech sourcing and custom matchmaking marketplace that connects people with pre-screened & top-notch developers and designers based on their specific needs across the globe. Connect with Digitalogy on Linkedin, Twitter, Instagram.WRITTEN BY"
27,Speeding Up Pandas DataFrame Concatenation,https://towardsdatascience.com/speeding-up-pandas-dataframe-concatenation-748fe237244e?source=collection_category---4------0-----------------------,"DataFrame concatenations is an expensive action, especially in terms of processing time. Imagine having 12 Pandas DataFrames of varying sizes that you want to concatenate on the column axis, as seen in the following box.In order to speed up your pd.concate(), there are two things you need to remember.That’s all you need to know :)Dr. Ori Cohen has a Ph.D. in Computer Science with a focus on machine-learning. He is a lead data-scientist at New Relic TLV, doing machine and deep learning research in the field of AIOps.WRITTEN BY"
28,4 free maths courses to do in quarantine and level up your Data Science skills,https://towardsdatascience.com/4-free-maths-courses-to-do-in-quarantine-and-level-up-your-data-science-skills-f815daca56f7?source=collection_category---4------1-----------------------,"When I got into Data Science and Machine Learning, anything related to math and statistics was something I had visited for the last time around 10 years ago. That’s probably why I found it so hard at first. It took me lots of hours of reading and watching videos to get some understanding of how things happen for lots of the tools we daily use in the industry. However, I got to a point where I felt the need of developing a solid understanding of what was happening underneath all those imports and fits I was doing, so I decided to freshen up that dusty math knowledge.Nowadays, I’m still doing it and I reckon it will be never enough. Moreover coming from Business and being in an industry full of professionals from engineering, statistics, physics and other exact sciences. I know there are LOTS of things to learn in the world of Data Science, but you know what? Technologies and languages might come and go, but the mathematical background of the field is going to remain.That’s why today I’m wrapping up a list of 5 courses to level up your math knowledge and take advantage of some of all this spare time we have been given this unfortunate situation we’re going home. Since, you know, you should be staying at home these days 😉.1. Mathematics for Machine LearningWhere: CourseraInvolved institution: Imperial College LondonTime required: 104hs (realistically it will be at least +50%)Prior requirements: NoneAbstract from the course itself:For a lot of higher level courses in Machine Learning and Data Science, you find you need to freshen up on the basics in mathematics — stuff you may have studied before in school or university, but which was taught in another context, or not very intuitively, such that you struggle to relate it to how it’s used in Computer Science. This specialization aims to bridge that gap, getting you up to speed in the underlying mathematics, building an intuitive understanding, and relating it to Machine Learning and Data Science.Topics covered:2. Essential Math for Machine Learning: Python EditionWhere: edXInvolved institution: MicrosoftTime required: 50hsPrior requirements: Python and some ground understanding of mathsAbstract from the course itself:Want to study machine learning or artificial intelligence, but worried that your math skills may not be up to it? Do words like “algebra’ and “calculus” fill you with dread? Has it been so long since you studied math at school that you’ve forgotten much of what you learned in the first place?You’re not alone. machine learning and AI are built on mathematical principles like Calculus, Linear Algebra, Probability, Statistics, and Optimization; and many would-be AI practitioners find this daunting. This course is not designed to make you a mathematician. Rather, it aims to help you learn some essential foundational concepts and the notation used to express them. The course provides a hands-on approach to working with data and applying the techniques you’ve learned.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.3. Probability and Statistics in Data Science using PythonWhere: edXInvolved institution: UC San DiegoTime required: 100–120hsPrior requirements: multivariate calculus and linear algebraAbstract from the course itself:Reasoning about uncertainty is inherent in the analysis of noisy data. Probability and Statistics provide the mathematical foundation for such reasoning.In this course you will learn the foundations of probability and statistics. You will learn both the mathematical theory, and get a hands-on experience of applying this theory to actual data using Jupyter notebooks.Topics covered:TIP: this course has starting dates, but you can select a prior starting date and see all the content from that cohort for free.4. Bayesian Statistics: From Concept to Data AnalysisWhere: CourseraInvolved institution: Santa Cruz, University of CaliforniaTime required: 22hs (realistically no less than 30hs)Prior requirements: some ground understanding of probabilityAbstract from the course itself:This course introduces the Bayesian approach to statistics, starting with the concept of probability and moving to the analysis of data. We will learn about the philosophy of the Bayesian approach as well as how to implement it for common types of data. We will compare the Bayesian approach to the more commonly-taught Frequentist approach, and see some of the benefits of the Bayesian approach.Topics covered:And that’s all for now peeps. I recommend doing these courses in the order presented, but of course, go ahead with any you like if you match the requirements :).And don’t forget to take a look at some of my latest stories:Also, feel free to visit my profile on Medium and check any other of my other stories :). See you around! And thanks for reading!WRITTEN BY"
29,10 Interesting Python Tricks to knock your socks off,https://towardsdatascience.com/10-interesting-python-tricks-to-knock-your-socks-off-1dd4d8e82101?source=collection_category---4------2-----------------------,"Important list of 10 python snippets to make your code efficientPython is considered to be the most versatile programming language till date with wide range of application across different domains. If you’ve grasped the basics of Python , it is time to explore the unique snippets which can help you in your daily work.Here are the 10 features or snippets which can probably impress you and help in making your code more efficient.Sometimes you get a column with unequal distribution of elements. Few categories are present merely. You usually wish to combine these categories into one.Here we wish to combine Coldplay and Weekend into one category as they have a mere impact on the dataset.First we need to find the elements we don’t want to change which are Eminem, Taylor Swift and Bruno MarsWe will use the where() function to replace the other elementsAnd there we have our updated column with required modifications.When we are given 2 different list and we are required to find elements which are present in a list but not present in another listConsider 2 listsIn order to find new elements of list A, we take the set difference of A with BValues 1, 3 and 9 are present only in list A but not in B.Map function takes in 2 parameters function and iterable and returns a map of the resultfunc is a function to which map passes each element of given iterable.
itr is a iterable which is to be mapped.Let’s break down the codeThe product function takes in two list and returns the product of both the list.list1 and list2 are the two lists which will act as an iterable for our map functionmap() takes the product function and the iterable → list1 and list2 and finally returns the product of both the list as a result.The above code can be modified using a lambda function to replace the product functionLambda function helps in avoiding the cost to write a function separately.slice(start:stop[:step]) is an object usually containing a portion of a sequence.From the above code, 1 is our start index, 6 is our stop index and 2 is our step index. It implies that we start from index 1 to index 6 with step size of 2.You can also flip the list using [::-1] operationYes, it is that easy to reverse a whole list with start, stop and step operation.You might have heard of both zip and enumerate functions. They are mainly used with for loops. But it is even cool to use them together. It not only allows you to iterate multiple values in a single loop but also get the index at the same time.Zip function helps you to bring all the list together as one so that you can iterate through each of them simultaneously whereas Enumerate helps you in getting index as well as an element attached to that index.Sometimes when you encounter a very large dataset, you decide to work on random subset of the data. The sample function of pandas dataframe helps you in achieving more than that.Let’s consider the artist dataframe which we have already created aboveThis helps in getting 10 random rows of your dataset.Let’s breakdown the above code,frac parameter takes in values between 0 and 1 including 1. It takes the fraction of the dataframe assigned to it. In the above snippet we have specified 0.5 , thus it will return random subset of size → 0.5 * original sizeYou can notice the reset_index function ahead of it. This helps in reseting the index properly as the index also gets shuffled when we take the random subset.You often get a lot of warning while running your code. After a point it starts irritating us. For example, you might get a FutureWarning message whenever you import erasYou can hide all your warning with the following code. Make sure you write this at the top of your code.This will help in hiding all your warnings in your entire code.As you go deep into coding, you start realizing the importance of memory efficient codes. A generator is a function that returns an object which we can iterate over. This helps in utilizing the memory efficiently and thus it is primarily used when we are iterating over an infinitely long sequence.Yield statement pauses the function saving all its states and later continues from there on successive calls.As you can see, yield saves the previous state and whenever we call the next function, it moves on to the next yield returning its new output.You can iterate through a single yield by adding a while loop that runs infinitely inside your generator function.The while statement helped us in iterating the same yield statement over and over again.Finally saving the best for my last. Ever encountered reading a csv file so big that it never fits in your memory? Skiprows helps you deal with it easily.It allows you to specify the number of rows to be skipped in your dataframe.Consider a dataset of 1 million rows which is not fitting in your memory. If you assign skiprows = 0.5 million, it skips 0.5 million rows from your dataset while reading it thus making you read a subset of the dataframe easily.From the above snippet, df represents a dataset of 112 rows. After adding skiprows=50, it skipped 50 rows from your dataset thus reading 62 rows as our new dataset.Thank you for reading the article. Hope you like it!WRITTEN BY"
30,Here Is How You Can Apply Software Development Best Practices to Analytics Pipelines,https://towardsdatascience.com/here-is-how-you-can-apply-software-development-best-practices-to-analytics-pipelines-8d65ba43bc9c?source=collection_category---4------3-----------------------,"I
have been closely working with the Data & Analytics domain for almost a decade and have seen a lot of interesting trends around analytics, big data, data engineering in general. Being a hardcore software engineer, I always wondered how to bring the principle of software engineering best practices to the analytics world. Recently I came across a very interesting open-source tool — debt ( Data Build Tool).dbt applies the following principles of software engineering to analytics code —dbt is an open-source project under Apache 2.0 license.One thing we need to note is, dbt does NOT help you in ingesting data. dbt’s magic comes live when your data is already sitting in your data warehouse.dbt currently supports ( via core and community contributions) the following databases/data warehouses —The focus of this article is to showcase how dbt is built inline with software development best practices.dbt allows you to transform your data using a simple command-line option. Installation instructions are available on the dbt site based on your OS. Once dbt CLI is installed you can verify by running a simple command —You will be able to see options like —dbt was first created by a company called Fishtown Analytics. They also offer a cloud-based dbt service — https://cloud.getdbt.com/You can either use CLI or the cloud service as per your choice.In this tutorial, I am going to transform a very simple data which I loaded into the Big Query. The dataset I am using is available on my GitHub repo. You can download and upload it to Big Query.The table scheme looks like thisSample data looks like this —You can configure the Big Query credentials using the methods described on the dbt site. Here I am using the service account JSON key method. You need to make sure the service account has following permission —Then you need to make sure dbt profile is setup $HOME/.dbt/profile.yml and details for the service account are updated as shown below —Here you can notice, we can set up profiles for dev and prod separately without affecting any code change. This one of the features of dbt which allows us to do the sandboxing and environment management easily.You can create a dbt project using a simple CLI commandThis command creates a sample dbt project with all the required files and folders. The typical structure looks like as shown below -Looking at the folder structure, you can see the sense of software development principles where you see folders for tests, logs, modules, data separately. This allows the overall manageability and readability of the code structure. This is very much version control friendly and easy to follow.You can read about the details of the folder structure and their meaning on the dbt site.In dbt_project.yml , you need to set up the profile you configured in the earlier section.I am creating two simple models to manipulate the data and get some insights.The models/example/athletes.sql looks like this —and model/example/players_by_country.sql looks like this —Here you can see the dbt feature where you can define a model in one file and refer it in other models. This helps in re-usability. Also if we need to change anything in base model, we can do it in one place. Just like in any typical software engineering language.Models have other features like —You can create models inside the specific folders. This allows you to achieve modularity. This is similar to creating packages/namespaces in certain programming languages.dbt allows you to auto-generate documentation for the models. Making it easier for the team to understand and collaborate betterYou can generate docs by running a commandYou will be able to look at the documentation by running a command —You can even look at the lineage information in the documentation website as shown below —dbt allows you to do some testing of the models generated. By default, you can configure tests like a unique check, null check, referential integrity etc. for a model. In this example, I am running sample schema tests to validate the results of the model for null checks. I can configure these tests in schema.yml as shown below —You can run tests for this project by running a simple command likeand see output like this —You can even run some complex tests as described on the dbt site.Every time you compile and run a dbt project, it will generate the detailed logs for you. In case, you face any issues, these logs are quite useful to trace back the errors.All dbt commands give proper exit codes like 0, 1, 2. You can use this feature in a CI/CD pipelines so that you know if a particular step was successful or failed.Like any other programming language, dbt allows you to write a reusable package and make it available for others ( within an organization or outside the organization) to use. There some interesting packages like dbt-utils which provide a reusable set of utilities for a common purpose.It also has some data source specific packages like MailChimp which provides a set of models for anyone who is using MailChimp services.You can even create a reusable model and contribute to the open-source.Overall the work done on the tool is very impressive and there is a rising trend of new adopters. For a hardcore software engineer like me, this is bliss 😊 And I am hopeful that the supported list of data warehouses gets growing in the near future.WRITTEN BY"
31,Adding Jupyter Notebook Extensions to a Docker Image,https://towardsdatascience.com/adding-jupyter-notebook-extensions-to-a-docker-image-851bc2601ca3?source=collection_category---4------4-----------------------,"The Docker image for our Jupyter Python and R users required them to set their Nbextensions preferences after every launch. We were able to increase Jupyter notebook user productivity by setting commonly used Nbextensions preferences in the Docker image.We have a shared disk where anybody can copy the template files to their local directory structure.Each member of Dev and Test copies Release 1.3.0dockerSeasons directory over to their PROJECTS directory.Note: Dev 1.0.3 is able to use the 0.0.3 PROJECTS directory structure unchanged. Release 3.0.1dockerSeasons overwrites docker 0.0.3 directory.Test modifies <project-name>/requirements.txt for the difference in Python packages they do and don't require. Then the Test symbolically links <project-name>/requirements.txt(in the <project-name>/dockerSeasons/test directory) with the command:The only differences in the Test directory are the files requirements.txt, the files in directory dockerSeasons/test/ . Test moves this directory structure to stage server and 3 redundant production servers. The server farm is internal.1.0.3 released a dockerfile template that enabled Dev and Test to customize each Docker image by requirements.txt.In a previous article, I showed the Dockerfile used to create a Docker image with Jupyter notebook. We are now on DES 1.3.0 (Docker Enterprise Solution) with a different specification of the Dockerfile for Jupyter notebook users.The meaningful change is that jupyterthemes (nbextentsions) stage after requirements.txt. Through trial and feedback, we found that Dev and Test changed requirements.txt less often than jupyterthemes.Changing the order of staging from less frequent changes to more frequent changes allowed us to take advantage of Docker buffering. The result is faster Dockerfile builds than prior.Depending on what is in requirements.txt, we experience full builds up to 10 minutes. By putting jupyterthemes last, we experience builds (no requirements.txt change) of approximately 30 seconds.You might consider the frequency of changes and order of builds in your Dockerfile builds.The DES 1.3.0 release of the partial Dockerfile is:Note: Do not execute the following commands from your terminal shell. If you do, you install jupyterthemes in your local copy of jupyter. The bigger problem is that you reset your local nbextentsions settings in your local copy of jupyter.We had to build the Dockerfile in two steps because we need to find what jupyterthemes are available and what their names are. First, we executed the partial Dockerfile listed above.We launched juptyer using command updev (described later in this article). We then ran the following in a Jupyter notebook.output->>Now we had the Nbextensions names to add to the 1.3.0 release dockerfile!The dockerfile for Dev enabled the following Nbextensions shown after the comment #enable the Nbextensions.Docker-Compose is used to manage several containers at the same time for the same application. This tool offers the same features as Docker but allows you to have more complex applications.Docker-Compose can merge more than one Docker container into one runtime image. Currently, we use Docker-Compose for only one Docker image (dockerfile.yaml).note: When you use command updev (described later in this article) the docker-compose command volume: - ./../../. causes <path-to-projects> to be mapped to /docker , the internal directory of the docker image. The subsequent launch of jupyter uses <path-to-projects> as its top level directory. Please use the example directory structure shown above or else substitute your own local directory structure for - ./../../. .The Docker commands are setup by adding the following commands to your ˜/.bashrc_profile or ˜/bashrc.txt.If you can not find the file ˜/bashrc.txt create it with touch˜/bashrc.txt.(MacOs or one of various Linux, or Unix operating systems.)Note: Remember to source ˜/.bashrc_profile and/or ˜/bashrc.txt when you done editing them.The updev command resulted in the following stream of messages on my terminal console. (Yours will have different timestamps and other slight differences.)Note: The updev command outputs:Because we map port 8888 to port 8889(Dev ) and port 8890(Test ) in the dockerfile.yaml. Users of DET 1.3.0 release must cut&paste the following into their web browser.Does that seem a lot of detail to ""Adding Jupyter Notebook Extensions to a Docker Image."" Perhaps it is. I recap:It took a couple of days of twist, turns, and wrong paths to get to DET 1.3.0 release. I hope you take less time to implement your Docker solution and, this article helps in your effort.If you have a suggestion or improvements, please comment.There is more detail on the Docker implementation I use in:Note: You can adapt the Docker code to your project from a clone-able GitHub repo.WRITTEN BY"
32,Designing Intelligent Python Dictionaries,https://towardsdatascience.com/designing-intelligent-python-dictionaries-cc138ac3f197?source=collection_category---4------5-----------------------,"Last week while working on a hobby project, I encountered a very interesting design problem:How do you deal with wrong user input?Let me explain.Dictionaries in Python represent pairs of keys and values. For example:What happens when you try to access a key which is not present?You receive a KeyError.KeyError occurs whenever a dict() object is requested value for a key that is not present in the dictionary.This error becomes extremely common when you take user input. For example:This tutorial provides several ways in which we can deal with key errors in Python Dictionaries.We will work our way towards building an intelligent python dictionary that can deal with a variety of typos in user input.A very lazy method would be to return a default value whenever the requested key is not present. This can be done using the get() method:You can read more about the get() method here.Let’s suppose you have a dictionary containing country-specific population data. The code will ask the user for a country name and would print its population.But, let’s say the user types input as ‘france’. Currently, in our dictionary all keys have first letter in Capital. What will be the output?As ‘france’ is not a key in the dictionary, we receive an error.A simple workaround: store all country names in lower-case letters.Also, convert whatever input the user types to lower-case.But, now let’s say the user enters ‘Frrance’ instead of ‘France’. How can we deal with this?One way would be to use conditional statements.We check if the given user_input is available as a key. If it is not available, then we print a message.It’s best to put this in a loop and break on a special flag input like exit.The loop will run in continuation until the user enters exit .While the above method ‘works’, it’s not the ‘intelligent method’ that we promised in the intro.We want our program to be robust, and to detect simple typos like frrance and chhina (very similar to google search).After some research, I was able to find a couple of libraries that could suit our purpose. My favorite is the standard python library: difflib.difflib can be used to compare files, strings, lists etc and produce difference information in various formats.The module provides a variety of classes and functions for comparing sequences.We will use two features from difflib: SequenceMatcher and get_close_matches.Let’s take a brief look at both of them. You can skip to the next section if you are only curious about the application.SequenceMatcher class is used to compare two sequences. We define its object as follows:Let’s use SequenceMatcher to compare two strings chinna and china:In the code above, we used the ratio() method.ratio returns a measure of the sequences’ similarity as a float in the range [0, 1].Now, we have a way of comparing two strings based on similarity.But, what happens if we wish to find all the strings(stored in a database) that are similar to a particular string.get_close_matches() returns a list containing the best matches from a list of possibilities.The best n matches among the possibilities are returned in a list, sorted by similarity score, most similar first.Let’s take a look at an example:Now that we have the difflib at our disposal, let’s bring everything together and build a typo-proof python dictionary.We have to focus on the case when the Country_name given by the user is not present in population_dict.keys() . In this case, we try to find a country with a similar name to user input and output its population.The final code will need to account for some other cases. For example, if there is no similar string or confirming from user if this is the string that they require. Take a look:Output:The goal of this tutorial was to provide you with a guide towards building dictionaries that are robust to user input.We looked at ways to deal with a variety of errors like type-case errors and small typos.We can build further on this and look at a variety of other applications. Example: Using NLPs to better understand user input and bring nearby results in search engines.Hope you found this tutorial useful!WRITTEN BY"
33,"AI safety, AI ethics and the AGI debate",https://towardsdatascience.com/ai-safety-ai-ethics-and-the-agi-debate-d5ffaaca2c8c?source=collection_category---4------0-----------------------,"Editor’s note: The Towards Data Science podcast’s “Climbing the Data Science Ladder” series is hosted by Jeremie Harris. Jeremie helps run a data science mentorship startup called SharpestMinds. You can listen to the podcast below:Most of us believe that decisions that affect us should be reached by following a reasoning process that combines data we trust with a logic that we find acceptable.As long as human beings are making these decisions, we can probe at that reasoning to find out whether we agree with it. We can ask why we were denied that bank loan, or why a judge handed down a particular sentence, for example.But today, machine learning is automating away more and more of these important decisions. Our lives are increasingly governed by decision-making processes that we can’t interrogate or understand. Worse, machine learning algorithms can exhibit bias or make serious mistakes, so a world run by algorithms risks becoming a dystopian black-box-ocracy, potentially a worse outcome than even the most imperfect human-designed systems we have today.That’s why AI ethics and AI safety have drawn so much attention in recent years, and why I was so excited to talk to Alayna Kennedy, a data scientist at IBM whose work is focused on the ethics of machine learning, and the risks associated with ML-based decision-making. Alayna has consulted with key players in the US government’s AI effort, and has expertise applying machine learning in industry as well, through previous work on neural network modelling and fraud detection.Here were some of my biggest take-homes from the conversation:You can follow Alayna on Twitter here and you can follow me on Twitter here.We are looking for guests who have something valuable to share with our audience. If you happen to know someone who would be a good fit, please let us know here: publication@towardsdatascience.com.WRITTEN BY"
34,Painting Pixel Art With Machine Learning,https://towardsdatascience.com/painting-pixel-art-with-machine-learning-5d21b260486?source=collection_category---4------1-----------------------,"T
he above sprites come from the Trajes Fatais: Suits of Fate game, which I work on as the lead developer. Long story short, each sprite takes about one hour to be drawn, and each character takes up to five hundred sprites, on average. In “Towards Machine-Learning Assisted Asset Generation for Games: A Study on Pixel Art Sprite Sheets,” we explore the Pix2Pix architecture to automate the sprite production pipeline, reducing the average time taken per sprite by 15 minutes (~25%). This is our first published work on sprite generation, and we expect to improve it further in the future.This paper received the Best Paper Award from the 2019 Brazilian Symposium on Games & Digital Entertainment (SBGames 2019)P
ixel art is one of the most popular aesthetics in video games. It strives to recreate the look and feel of old Nintendo and Arcade titles. In the ’90s, pixel-art was the only option on most consoles. Screens had limited resolution, and most devices could not perform advanced techniques in real-time. Today, pixel art is a choice — a costly one.To achieve the look-and-feel of arcade games, artists must embrace the limitations of the time. The original Game Boy had only four greenish colors. Its successor, Game Boy Color, could simultaneously display up to 56 different colors. Later devices, known as the 16-bit generation, allowed up to 256 colors per character, a significant change in aesthetics. In our game, we constrain ourselves to the 256 color limit per character.Commonly, a character is a blend of “index sprites” with a “color palette.” When drawing, artists shade each pixel with an “index” that relates to one of the 256 colors of the palette. In-game, each index sprite is replaced with its associated color, composing the final image. This procedure allows designers to create different “skins” for each character, allowing users to customize their experience and to create “evil” versions for the characters. The following picture depicts an indexed sprite, a color palette, and the rendered mix.Limiting artists to 256 colors is not natural. Choosing shades is difficult. To ease this task, the work is divided semantically. In our pipeline, two intermediate sprites are generated: the “shading” and the “regions” sprites. The former uses up to 6 tones, which indicate the “light,” and the latter uses up to 42 tones, which state “regions ”of the sprite, such as the arms, the hair, the legs, etc. Multiplying both sprites pixel-wise, we obtain the index sprite, which allows up to 252 colors (6 * 42). The following picture shows an example of the shading, regions, and index sprites. This procedure turns the 256 colors problem into two straight-forward sub-problems with 6 and 42 colors each.Finally, each character is designed by a single person, which produces the concept-art for all of its animations. These are presented in a “sketch” sprite and are later refined into a “line-art” sprite. The former is used to quickly prototype new animations in-game, and the latter is used to communicate with the other artists how the final sprite should look like. This way, the designer can conceptualize an entire character in a matter of days and out-source the remainder of the work to the drawing team. Here are examples of the sketch and line-art sprites:Putting it all together, the designer creates the character by sketching each of its animations and then producing their respective line-arts. In sequence, these line-art sprites are handled to the drawing team, which will draw the respective shading and region sprites. Finally, a script combines both to generate the game-ready index sprites.In total, this process takes about one hour. The sketch, line-art, and region sprites production take, on average, ten minutes, while the shading takes the remainder of the hour to be completed. Tracking the exact time spent per drawing is almost impossible. To compute them, we inspected our production logs, interviewed the team, and measured the steps in a controlled manner for a dozen sprites.T
his work hypothesizes that producing the shading and color sprites is doable using modern generative models. To be considered useful, a generated sprite must be good enough that a human artist could perfect it in less time than it would take to do it from scratch.In this work, we tackle two image mapping problems: line-art to shading and line-art to regions. Formally, we have to create a generator G(x), that receives inputs from the line-art domain and produces outputs in the shading/region domain. This problem is also known as image translation.To guarantee that G(x) is a useful mapping, we shall create a discriminator D(x, y) that looks at x and y and says if y is a quality sprite. In other words, G is our “virtual artist,” and D is our virtual “quality control.” If we can get G to make D happy, we have a useful mapping.In more detail, consider we have several line-art sprites (x) and the already drawn shading and region sprites (y), made by human artists. We know these passed quality control, so D(x, y) will be happy. Our task now is to train G to, given x, produce ŷ (an imitation of the real y). If the reproduction is good, D will approve ŷ; else, it will reprove it. In the end, we spoiler D if it was right or wrong, and we ask it to give constructive feedback to G.The procedure I just described is known as Adversarial Training. Two models “compete” in the sense that one is trying to beat the other. In our case, G is trying to beat D into thinking that ŷ is y, and D is trying desperately to tell what is real and what is fake. With time, G will become a successful artist 😄 , and D might be fired from quality control 💩.By using Neural Networks to implement G and D, we get what is known as a Conditional Generative Adversarial Network. Breaking the title up, “Conditional” because G takes an x as input instead of random noise, “Generative Adversarial” because it trains against an adversary to be a sound generator, and “Network” because it is (surprise!) a neural network.Algorithmically, for each line-art x and shading/region sprite y:Repeating this procedure to the entire dataset multiple times will eventually converge into a G network that creates realistically looking sprites and a D network that is unable to tell which images are real or fake.The Pix2Pix architecture is based on a U-Net Generator and a Patch-based Discriminator. The combined architecture is shown in the following picture. The discriminator is trained to classify each 32x32 patch of ŷ as real or fake and is trained with the binary cross-entropy loss. In turn, the generator is trained to minimize the L1 loss between y and ŷ and to maximize the discriminator loss.The U-Net model is a fully convolutional neural network based on the encoder-decoder idea. For each encoder layer, a skip-connection is added to the equivalent decoder layer. This allows the network to leverage the “original” information from the encoding layers and “processed” information through the decoder layers. A comprehensive overview of this architecture and its respective publication is given here.The Patch-Based discriminator is a truncated network, outputting its judgment for several image patches instead of a single judgment for the entire image. Thus, the discriminator provides detailed feedback to the generator, pointing which regions look real and which look fake. A complete overview of the inner details of the architecture can be found here.In contrast to the original network, we made the following changes:From the Trajes Fatais game, we selected the Sarah and Lucy characters as datasets to evaluate the usefulness of the Pix2Pix architecture. The Sarah character has only 87 finished sprites and has 207 ones left to be drawn. It is also a moderately complex character, with several smooth and complex regions. Lucy, on the other hand, is finished, so it has 530 fully drawn sprites, and is considerably easy to draw, having mostly smooth features.In a sense, Lucy is our upper-bound. It has all the data we could hope for, and it is an easy character to be drawn. If the algorithm can’t handle Lucy, then it will likely fail for any other character. In contrast, Sarah is our ideal scenario: a moderately complex figure with only a couple dozen sprites ready for training. If it is useful for Sarah, then it may have production value for us.As can be seen, the algorithm had pretty good results for the shading problem and issues on region sprites. Namely, the colors are shifted, and there is a bit of noise around the girl. For shading sprites, only minor problems can be detected, such as the shoulder and legs in the second row.In this second batch, more issues can be found. In the generated shading column, many artifacts can be seen in shadowed regions, such as on the girl (row one), the platypus back (row 2), and the platypus beak (row 3). For color sprites, a lot of noise is present, rendering these sprites unusable, as fixing noise is hard for humans to do by hand.This third batch comes from the 207 sprites that only the line-art is available. Thus, these require a subjective analysis. The rows are composed of sprites similar to the ones used during training, sprites with previously unseen poses, and sprites in radically different poses, respectively.While the first row is mostly useful, color sprites deteriorate quickly on the second and third rows. The quality of shading sprites is mostly consistent. However, the third column of shading sprites shows it is not always consistent. The front-facing sprite in the second row should have its face brighter, and the sprite right bellow it has incoherent lighting.For now, we can safely assume that shading sprites can be useful, but region sprites are not, as they are too noisy and have color shift issues. Let’s shift our attention to Lucy.With five times more data, Lucy sprites show a significant improvement over Sarah’s. Shading sprites are near perfect, with only minor issues in shadowed regions and tolerable differences in the hair. Region sprites, however, are still far from optimal. The color shift issues and the noise are still relevant. This shows that increasing the data set size did not improve on these matters substantially.This second batch has sprites we manually selected to be in the validation set, as they are considerably different from most others. Despite this fact, shading sprites are still near identical to their human drawn counterparts. Concerning color images, the quality did not deteriorate much, as was the case for Sarah. However, it is still far from ideal.Considering these results, it can be said that increasing the dataset size considerably improves shading, but not regions. Since Lucy is our best-case scenario, it is safe to assume that we need another problem formulation/architecture to solve the region sprites problem.To more objectively quantify the quality of the generated content, we computed the MSE, MAE, and SSIM scores for both datasets.As can be seen in the table, the gray sprites have a better mean (μ) and variance (σ²) than color sprites on all three metrics. Also, the difference between the 75% quartile and the max seen values is huge, which indicates a highly skewed distribution.Additionally, Lucy’s results are consistently better than Sarah’s, having a much lower variance and significantly less skew.The SSIM score ranges from 0 (totally dissimilar) to 1 (identical) and measures the perceived similarity of two images. While MSE and MAE are purely mathematical notions, the SSIM score tries to be more correlated with human perception. In the table, gray sprites have near one score, which indicates that they are near identical to the average observer, which is not the case for color sprites.As a third and final evaluation, we asked the design team to comment on the 207 generated sprites for the Sarah character. Their feedback was mostly positive, praising the quality of shading sprites and discarding the color ones. In sum, they made four comments:The following figures illustrate points 2, 3, and 4.In this work, we evaluated the use of modern generative models to tackle the pixel-art generation problem. Namely, we employed a modified Pix2Pix architecture, achieving moderate success. In more detail, shading sprites were judged useful by the artistic team, and color sprites were discarded as unuseful.For the accepted shading sprites, the team stipulated that an average of 20 to 30 minutes would be needed to perfect each one, which is from 10 to 30 minutes less than what it takes to draw one from scratch. A conservative estimate is that each useful sprite would save 10 minutes of labor, which translates to about 15% more productivity.Despite having more colors, region sprites do not take as much time as shading sprites for the design team. As explained by the lead artist, regions are more predictable within animations and can be easily copied from one sprite to another. Thus, not having them generated is not a big issue.From a technical perspective, this work demonstrates that current models can be effectively used as assistants to creative tasks. Other works have found similar conclusions on the anime domain, which is made of mostly flat and ample surfaces and has fewer restrictions than pixel art. Moreover, the Pix2Pix model was conceptualized for real-world pictures but worked for pixel-art and anime data, which attests to its generality.Our current system is based on per-pixel regression, as the Pix2Pix model. However, our problems can be formulated as per-pixel classification, relying on the image segmentation literature. This might improve our results significantly.Sometimes, simplifying the problem might make it more tractable. Region sprites have a total of 42 colors, but only about a dozen appear on every sprite and occupy a significant portion of it. Downsizing the problem to a more selective set of shades might ease our generator’s job.Pix2Pix is from 2017. Since then, several advancements have been made in the GAN literature, which includes better loss functions, attention mechanisms, and improved formulations. Using more modern techniques might improve the obtained results significantly.O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation” 2016P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks” 2017If
you enjoyed this review, you might like some of my other published articles here on Medium. Here are a couple of suggestions:Thanks for reading 😃WRITTEN BY"
35,Building a Bot That Plays Videos for My Toddler,https://towardsdatascience.com/building-a-bot-that-plays-videos-for-my-toddler-597330d0005e?source=collection_category---4------0-----------------------,"My wife and I have a super curious 21-month-old boy name Dexie. Although he does not fully speak yet, he really loves pointing at things asking us to tell him what they are. It can be a picture of an animal in his favourite book, a picture of a car on his flash card or just a toy. I enjoy doing this activity with him, and recently I have been showing him videos that depict tigers, dolphins, trains and other interesting things. He really loves seeing how an actual tiger walks, roars and socialises, which I think is good for his cognitive development.One day, I came up with the idea of building him a bot that can play this pointing game with him. Don’t get me wrong, the aim wasn’t to replace us but to complement us and to expose him to technology as early as possible.After a bit of a brainstorming session … with myself, I knew exactly what I wanted to build. It was to be a chat bot with the appearance of a dog, which is Dexie’s favourite animal. Her name was to be Qrio, a blend of the two words question and curiosity. In my one and a half years of buying toys for him and observing which ones he keeps playing with and which ones he doesn’t, I found out that the more closely the toy is able to mimic a living thing (a dog in this case) with which he can bond, the higher the chance for it to be successful.With the aim of maximising the bonding factor, Qrio is modelled after our late dog Pepsi, with whom Dexie had built a relationship in his first year.Qrio will be able to see Dexie walking by and say to him, ‘Hi, Dexie! Do you want to come and show me your toy?’ Next, when Dexie picks up and shows her an airplane toy, she will continue with ‘Hey, that is an airplane. Let me play you a video about an airplane,’ and then look for an airplane video for him to play.In order to achieve the above, Qrio needs to have the following modules:After some serious research, I came up with a list of the hardware required to run the system.With a solid plan in hand, I began to fulfil my mission.Building the SightFirst, an object detection component needed to be developed and trained to identify specific human faces and toys. Bear in mind that the NVIDIA Jetson Nano’s GPU is way less powerful than a desktop class GPU card like 1080Ti, so choosing an object detection model architecture which has a good balance between accuracy and performance is crucial. First, I decided the minimum rate of frames per second (FPS) I can live with. I then worked backwards and searched for the model that can deliver such FPS on a Jetson Nano. I chose 8 FPS which, in practice, will go down to about 5 FPS when it has video processing, text to speech, virtual puppetry rendering and so forth running simultaneously. Having less than 5 detections per second would significantly decrease the chance of achieving a good quality capture in which Dexie’s face and his toy are clearly visible. The winning model architecture was SSDLiteMobileNetV2, which runs on TensorFlow 1.8 object detection API.I trained the model using just four classes: a human face and three of Dexie’s toys (airplane, train and panda). All the training set images (150 images per class) had been generated from video files recorded using the same Sony IMX219 camera. In order to maximise detection accuracy and make sure that the lighting and the background were consistent, they were taken in the same living room where I will run the system. The three toys were manually and painstakingly labelled from the videos. However, to save my sanity, I used Amazon Rekognition, an off-the-shelf object detection cloud service to automatically label all faces.The video recording was done using GStreamer, which is easily done by executing the command below. Recording with a low FPS causes a significant motion blur in the final video and yields a low-quality training set. Hence, I set the recording frame rate to be 120 FPS and down sampled them later using a video editing tool. The recording dimension was set to 720x540, which was more than enough as our object detection model only runs on 300x300 pixels with any larger image to be automatically resized to 300x300 pixels during training and inference.I used EVA, a great and free object detection labelling tool which you can install locally and can import a video file as an image source.The training was completed in five hours, using AWS EC2 Deep Learning AMI running on P3.2XLarge (Pascal V100) which achieves mAP=0.8. Mean Average Precision (mAP) is a metric used to evaluate the performance of an object detection by calculating an area under the curve of precision/recall which are averaged over various iOU threshold. It takes an entire blog posting to explain this properly, so I will simply refer you to mAP for Object Detection blog to read. Otherwise, you’ll just have to trust me that an mAP of 0.8 is very good, and it can still be further improved by aggregating detection from several frames.Deploying and running this model on NVIDIA Jetson Nano is pretty straightforward once you have flashed the device (follow the steps here), as it is running a fully functional Ubuntu 18.04. What I mean is you can install Tensorflow, Object Detection API and all the Python dependencies just as you would on your laptop or PC. I used Tensorflow 1.8 because at the time this blog was written, object detection API was not supported in Tensorflow 2.0 due to missing a contrib dependency.GStreamer and OpenCV framework were used to connect to and obtain the video feed from the camera. From there, we simply needed to pass the captured image directly to our Object Detection model. See the self-explanatory code sample here.I managed to get the object detection running at 10 FPS, which was more than my minimum requirement of 8 FPS — and with a pretty good detection accuracy!Building the Visual PresenceIt is critical to get the visual presence component right. Qrio must be attractive and, more importantly, look enough like a real, living dog for Dexie to want to play with her. Her eyes need to be able to look directly at Dexie’s face, wherever he is. And like a normal dog, she needs fidget by wagging her tail, moving her head around and looking in random directions when she is not interacting with Dexie. I feel so grateful that I learned such valuable skills in my good five years as a 3D animation programmer working at a movie special effects company that specialised in facial animation and virtual puppetry. All I needed now was a game engine!A few hours of digging led me to this awesome Python framework called arcade which had everything I needed. Well… almost, I will get to this later. It supports a game animation loop and is able to render/display a sprite (PNG image with transparency) with rotation and scaling. Since this framework is based on OpenGL, the speed performance on NVIDIA Jetson Nano should be excellent as it will be GPU accelerated.In order for individual parts of Qrio’s body (ears, eyeballs, eyebrows, head and tail) to move both independently and as a group, separate sprites needed to be assembled (e.g. moving the head will also move the ears, eyebrows and eyeballs). I needed to build a skeletal animation system (SAS) which allows you to join several objects together in a hierarchical relationship (e.g. the head is a child to the body while the ears, eyes and eyebrows are children to the head). Thus, when you apply a transformation (rotation, translation or scale) to an object, it will also affect all of its children.Most game characters, humans, animals, monsters like the one in the animation below are built using an SAS.Most game engines support SAS natively; however, Arcade does not. Since I couldn’t find an alternative framework, I decided to implement the SAS capability from scratch, which is actually not that hard. The first thing I needed to do was to build a tree data structure which stores all the sprites (ear, head, eyebrows, etc.) and connects their joints according to their relationships. Next was to build an SAS hierarchical transformation function. This involved a little bit of trigonometry and matrices. If you are keen to know the mathematical details, you can read them in this blog.Qrio’s SAS is demonstrated in the image below. The first image shows how sprites are used to define each of the body parts, which are assembled hierarchically as shown in the next image. Each body part will also have a joint defined to be the centre of rotation. The rightmost image shows transformation (rotation) being applied to the right ear around its joint, and no other body parts are affected since the right ear does not have any children. The bottom image depicts rotation being applied to the head around its joint, and it affects the eyebrows, eyeballs and ears. Note the marking showing that the right ear is also rotated accordingly around the head joint.In order to complete the visual presence module, the next thing I needed to build was a fidget animation system which is based on a simple keyframe animation. A keyframe animation lets you animate an object such as a head by supplying its initial and final transform (position, scale and rotation) and the duration of the animation. The system interpolates the transform from the initial to the final value. Next, I defined a few fidget animations such as the up-and-down movement of the ears, rotation of the head and tail, and movement of the eyeballs and eyebrows. Each fidget animates the respective objects from one transform to the next with its value (rotation/translation), duration and frequency picked randomly so as to look more natural.I spent a couple of hours tweaking the fidget animation parameters to finally get the result I wanted.Lastly, I added a way to overwrite the eyeballs and eyebrows’ position on demand by manually supplying their position which is needed for the head tracking logic to follow where Dexie’s face is at a later stage.Building the SpeechWith her sight and her visual presence module completed, what she needed next was speech capability. The best free offline text to speech application I could find with a good couple of hours research was pyttsx3. The engine supports a few drivers but the only one available on Ubuntu is espeak and it has the most horrendous voice quality. It sounds like the late Stephen Hawking’s wheelchair (with no intent to disrespect). Dexie is currently at the stage where he repeats every single thing we say, and the last thing I want is for him to learn to speak like that. Check out the below and be your own judge.After giving up on an offline offering, I started to look for an online counterpart. I landed on Amazon Polly. After a few minutes of playing with it I was totally sold. The voice quality is 100 times better and there is no noticeable delay, even though it needs to make an API call via the internet to generate and download the resulting audio file from the cloud. This was initially was my main concern. It only takes 200ms to generate seven seconds of audio file. I know this is not a free solution. However, it can be heavily cached given that Qrio will only need to utter 50 different sentences at most, and we will only ever need to pay for 50 Amazon Polly calls (0.08 cents). Yay!!!Building the Video Search and PlayAs we discussed previously, Qrio needs to be able to search for and play a specific video on YouTube. The best way to do this is by using an automation test suite, which can control a web browser to perform a search in YouTube and to play the video from the search result. Here Selenium automation framework comes to the rescue!It is a tool which is normally used by a QA to test a website. It allows you to write scripts that automate things such as typing into a text field, pressing a button, etc. As you can guess, I will be using it to navigate to a YouTube site, entering a search term like panda and then automatically clicking on the first video in the search result and pressing the full screen button to play it full screen. First, you need to install a chromium-chromedriver for Selenium to be able to control the Chromium web browser (the native browser that comes with Ubuntu 18.04) by executing the apt-get install command below.You can then programmatically execute the Selenium script from your Python code using python binding for Selenium.
In order to make sure that the video played is kid safe, I use YouTubeKids.com instead of the normal YouTube. This introduces a slight complication, as it does have a series of steps you have to go through to prove that you are a parent every time Selenium starts. However, I managed to write a Selenium script which automatically completes these steps and it just needs to be executed once.You can see the code here.Building the CoordinatorThis module serves as a coordinator which glues all other modules together. One critical part of the coordinator is a state machine which keeps track of the current state of the game. Why do we need a state machine? So that we can make different decisions upon receiving the same event, depending on which state we are in at the moment. For example, seeing an airplane toy by itself should not trigger a call to play the YouTube video if previously Qrio has not yet seen Dexie, as it may be a situation where the toy plane was just there on the couch. Seeing an airplane toy after playing an airplane video should make Qrio say ‘Hey, we have played with an airplane before. Why don’t you bring me something else?’ That way, we avoid having Qrio play the same video again and again if Dexie continues to hold the plane after it was previously recognised and video playback had been triggered.There are four major states — Idle, Engaged, ObectRecognised and PlayingVideo — as you can see in the state diagram below. When the system is in any state except PlayingVideo, it periodically calls Fidget Animation System to animate Qrio fidgeting and checks with the sight module to get the location of all recognisable objects. The system starts from an Idle state and if Dexie is detected for at least 0.5 seconds (to minimise false detection), it will call the speech module to say something like ‘Hi Dexie, do you want to come and play?’ and set the game state to an Engaged state.Furthermore, if a panda toy is visible while we are in Engaged mode, Qrio will say ‘Hi Dexie. I think that is a panda,’ and will enter ObjectRecognised mode. If the panda toy still remains visible for another two seconds, Qrio will switch into PlayingVideo state, will say ‘Let me play you a video about a panda,’ and will call the Video Search and Play module to search for a panda video and play it. However, if we have just recently played a video about a panda, instead it will say ‘Hey, we have played with a panda before. Why don’t you bring me something else? The video will only play full screen for 45 seconds while the sight and fidget animation system are paused to focus the CPU resources on playing a smooth video. When the video playback is completed, the browser window is hidden, and the sight and fidget animation systems are resumed. When Dexie is not visible for 10 seconds while in engaged mode, Coordinator will reset the state to Idle.You can also see that there is a call to the Head Tracking module in any state except PlayingVideo when a face is visible in order to make Qrio’s eyeballs follow the centre point of the face bounding box.With everything ready to go, I set up the system in the living room for a final calibration and testing.At initialisation, the system goes through and passes the YouTubeKids parent authorisation without any issue. I saw Qrio’s eyes following my face swiftly wherever it goes, a sign that the object detection and the head tracking logic work very well. I noticed NVIDIA Jetson Nano has been pushed to the limit, with the RAM running super low and the device becoming very hot. It is totally understandable given that it is running a heavy-duty AI model on the fly and still needs to render the game engine in real-time, controlling the Selenium browser and decoding videos. However, the whole system seemed to function pretty well with the game engine showing the benchmark of 5 FPS.All I need now is to find the right time to show Qrio to Dexie!It was a priceless moment watching Dexie when he saw Qrio for the first time. He rushed towards her curiously while Qrio was calling him and he stood still, just staring at her for a few seconds in disbelief. Suddenly, he burst into giggles and laughs… I just knew that I had passed the first test with flying colours — a successful bonding between boy and bot!In the interest of making make him more familiar and comfortable, I let him do whatever he wanted without any guidance. He walked towards her, watched her fidgeting, touched the TV (thinking that he is actually touching her) and even called her ‘Doggie’.Then came the moment of truth — to test the actual game play. As an example, I let him watch me show Qrio his airplane toy which she flawlessly recognised it and said, ‘Hey, I think it is an airplane. Let me play you a video about an airplane’. Dexie was super excited when he saw the airplane video start playing!Once the video playback is finished, I passed a panda toy to Dexie. Copying me, he showed the panda toy to Qrio, and she again searched for and played an appropriate video! You can watch that whole experience on the video below.The system is not perfect yet. Although it manages to recognise toys and to play the right video about 80% of the time, it still fails from time to time — which is fine. The most important thing is that I have learned a lot about what works and what doesn’t work for the next time a similar project comes around.Camera’s FOVMost of the failures happens when Dexie is standing too close to the TV, which is outside the camera visibility range. As seen in the photo below, the camera has a 77-degree FOV and is positioned to cover a medium-to-close distance from the TV — but not very close (area marked in red circle). The same problem happens with the vertical coverage where the camera cannot see a toy he is holding very low while sitting on the floor. Lowering the focus of the camera would solve this but would introduce the opposite problem of not being able to see his face when he is standing and close to the camera.The solution to this may be to get a new camera with a wider (120-degree) FOV so that it can cover more areas. However, the accuracy of the detection around the edge of the FOV might drop due to lens distortion.ReplayabilitySo far Qrio sight module has only been trained with three toys and always plays the same video for the same toy. This is only enough to entertain Dexie for five minutes until he gets bored. Hence, it has a low replayability factor. I plan to add support for more toys in the future — if I can muster the courage to manually label thousands of additional images. A randomisation could also be added to pick randomly from the top five videos found, rather than always picking the first one.Faster FPSAnother area for improvement is the game’s FPS. The game runs at around 5 FPS with occasional short freezes and longer ones when the Video Search and Play module executes Selenium scripts to control the chromium browser. You have probably already spotted this in the demo video above. A frozen game engine means Qrio stops moving, which is not that pleasant to watch. An idea would be to move the object detection into a separate thread so that it can run concurrently and not block the game engine. The same treatment might also be applied to the video search and play module. However, we need to test whether OpenCV and Selenium are happy to run on a separate thread. Aside from that, I would also like to test a more powerful device such as NVIDIA Jetson NX which would probably be more appropriate for a project of this scale.That’s it, folks! I hope you have enjoyed reading about my exciting weekend project as much as I have enjoyed sharing it with you.The full source code is available here.WRITTEN BY"
36,What is Natural Language Processing?,https://towardsdatascience.com/what-is-natural-language-processing-86a7123a076b?source=collection_category---4------1-----------------------,"There’s something remarkable about humans.We’re capable of doing unbelievably complex tasks. Even more amazing is that most of the things easiest for us are incredibly difficult for machines to learn.Have you ever caught a baseball? Or picked out your favorite shirt in your closet? Or made a witty comment to a friend that gets a few laughs? These are second nature to us.Sure, a computer can hold near-infinite calculations on its hard drive while we have trouble estimating a tip. But if you ask a computer to tell a joke, it’s usually a bit cringey, if not disturbing!Understandably, the fields of mathematics and machine learning quickly made progress on problems concerning hard numbers:But this ease hits a stopping point when we want to dip into problems facing our main methods of communication: talking and writing.We created coding programs to help us communicate on the same level as a computer. But can you imagine a world where humans talked to each other in the equivalent of code? It would be very dry…A programmer is going to the grocery store and his wife tells him, “Buy a gallon of milk, and if there are eggs, buy a dozen.”So the programmer goes, buys everything, and drives back to his house.Upon arrival, his wife angrily asks him, “Why did you get 13 gallons of milk?” The programmer says, “There were eggs!”Fortunately for us, there’s little chance that we’ll adopt Python as a spoken language. We can keep the beauty and complexity of the languages we speak and write, with their vast vocabulary, double-meanings, sarcasm, slang, abbreviations, and idiosyncrasies!Natural language simply refers to the way we communicate with each other: speech and text.Processing refers to making natural language usable for computational tasks.So, Natural Language Processing (NLP) is concerned with finding, digesting, and understanding human speech and text.For NLP practitioners, the subtleties of natural language make NLP a very challenging and very exciting field to be a part of!NLP wasn’t always conducted with artificial intelligence and machine learning.The original approach to solving natural language problems was to “hard code” hand-written, rules-based methods.For example, “dog” in English should always translate to “perro” in Spanish.But, what if the dog is female? Well, then this should translate to “perra”.What about “dog-days” referring to the hottest days of the year? Would “dias de perro” make any sense to a Spanish speaker? No — in Spanish this would be “la canícula”.We can see how hand-written rules quickly become problematic — this was actually a reason for a brief halt in NLP progress in the 20th century.But, with the aid of massive computational resources and some math know-how, NLP researchers have devised “statistical NLP” approaches to these problems, which have served a huge boost in accuracy and practicality in the field.We’ll leave it there as to the history for now, but know that this is why NLP is often considered a subfield of artificial intelligence/machine learning.If a computer can aid us in understanding natural language, there are a whole world of tasks we can do more accurately and cheaply, saving people thousands of hours of tedious work.At one point I was helping a large transportation company with its marketing efforts. Millions of people go on trips with them every year.The company collects surveys from a sampling of these travelers — often asking questions on a scale from 1–10 (How smelly were the bathrooms?). In addition, they ask for free-text responses (How was your trip? Is there anything else we should know?).We got hundreds of thousands of survey responses. Can you imagine being the analyst assigned with reading through each response, trying to glean some useful information? Maybe you’re sitting with an Excel spreadsheet slowly ticking through rows of responses, marking how happy or angry the customer was.This sucks!Instead, we used a very simple but powerful method for automatically inferring how happy or angry the customer survey response was — a practice called “sentiment analysis”.Previously, a worker would spend valuable hours on this task. Or, more realistically, the company would just throw out the text data — wasting the time the customers spent filling it out, and any helpful insights waiting to be revealed.Now, in minutes, this analysis meant we could rate from 0% to 100% how happy or angry thousands of customers were on their trips. Then we can start tackling the 20% of issues that are causing 80% of customer complaints.This is one simple example — but there are so many times where a little sprinkle of NLP can save countless hours and provide a whole lot of valuable insight.So, we have some idea of the value that NLP can provide. What are the major use cases? How else might we employ NLP to make the world a better place?Natural language processing is a constantly growing, evolving field, with new applications and breakthroughs happening all the time.This certainly makes it difficult to break the field into neat categories, but below is one breakdown to help get our heads around the many different NLP methods:Syntax is something we take for granted. As children, we mostly learned the rules for our language through hearing others follow them, and trying to follow them ourselves. Syntax is usually invisible to us unless it is incorrect.Syntax is the set of rules around how to structure our sentences, use the correct tense, punctuation, etc. In other words, the rules your English teacher would mark you down for getting wrong in a paper.Notice how one of these sentences seems quite wrong:Much of NLP is devoted to learning, dissecting, and reusing the rules of syntax.Most syntax tasks are low-level, and are not an end in themselves, but produce information used in higher-level NLP tasks that we will discuss below.One example is part-of-speech (POS) tagging. Given a word, we want to infer whether it’s a noun, verb, etc:While we may not particularly care that “cat” is a noun, this knowledge is very helpful for downstream NLP applications, like finding proper nouns or summarizing text.Other examples of NLP methods concerned with syntax include:Allowing our machines to work with syntax forms the basis of (in my opinion) much cooler applications of NLP!Let’s continue.“I am the egg man. They are the egg men. I am the walrus.”Although the above snippet contains perfect syntax and follows all the rules, what does it mean? (Maybe a Beatles fan out there can explain it to me.)The meaning of the text is the concern of semantics.One example of an NLP method concerned with semantics is the ability to pick out proper nouns from text. For example, in the following sentence:“The next door neighbor Victor works for DreamWorks. Melissa, his wife, works for the Department of Defense.”The bolded entities are picked out by a Named Entity Recognition system which is designed to capture proper nouns.While easy for our eyes to pick out, this would be very difficult to hand-code rules for, especially since the system will not only find the proper nouns, it will tag them:At one point I worked for a company that wanted to extract the names of people mentioned in hundreds of thousands of contracts.Trying to find a dictionary with all possible names, or having somebody manually read through and highlight, would be error prone and take ages. Using an NER solved this quickly and accurately.Some other NLP methods concerned with semantics include:Semantics is a flourishing field — needless to say there is a lot of progress being made in helping our computers find meaning in text, and in turn helping us perform much more powerful analytics.What does it take for somebody to persuade you to do something?Let’s say your friend wants to go to Burger King. It would be in their favor to mention things like:Excellent — your friend makes a compelling argument!Then another friend wants to go to Wendy’s. They mention that:Your second friend fell a bit short. Their ideas and suggestions had little to do with each other, or with the topic at hand for that matter. There is no coherence in what they are saying, so it’s really not very persuasive.In the first example, your friend talks directly to your desire to eat, and everything they say relates to Burger King. Great!Above all, discourse in NLP is concerned with groups of coherent sentences, and what makes for high-quality, human-like communication.One important method for discourse in NLP is text summarization.With text summarization, machines can automatically summarize any length of text to a length that you prefer, whether it’s an article summary in a sentence, a scientific paper summary in a paragraph, or a book summary in a page.For instance, there are often situations where professionals have heaps of papers they need to read through, but it’s not important that they read every word. They just need the gist of it.Most importantly, the returned summarization must be internally coherent — meaning the sentences in the summary follow one another logically and succinctly — and that the summary accurately condenses the information from the original text.Further methods of NLP that concern themselves with discourse include:Much of discourse is used when trying to train chatbots to interact well with humans and be easily understandable. If you tell a chatbot on a cosmetics website that you’re looking for a good moisturizer, it’s unhelpful for the bot to ask what your favorite book is.Whereas most of what we’ve discussed so far concerns written text data, natural language includes the spoken word, too.Because the written word is easier for machines to work with, a main task in this category is speech-to-text translation.When you talk to a smart speaker like Alexa, software converts your speech to written words so that machines can perform the previous tasks we’ve described on it.In the other direction, given a piece of text, an NLP practitioner can try to make a machine read the text convincingly in a human voice.It’s amazing how in tune we are with other people’s speech — we can usually quickly detect a robot speaking to us.But text to speech is getting much better. Google Duplex robot voices have been shown to speak convincingly to the unsuspecting person on the other end. Turing would be proud!When we talk to each other, we usually have a point we’re driving at (if we don’t we wouldn’t be talking). Maybe we’re trying to make the other person laugh, or figure out how to get to the grocery store, or ask them for money, or vent our feelings.In short, we have some purpose.Dialogue tries to explain the purpose of natural language in the context of people (or robots) talking to one another. Another term for this would be conversational knowledge.For example, the Switchboard Dialogue Act Corpus contains data from thousands of phone calls. The dataset tags the many kinds of intent that the speaker’s utterances may have.“Why don’t you go first” is classified as an “Action Directive” — you’re telling somebody to do something.“How about you?” is an “Open Question”.“ I’m sorry.” is an “Apology”.And one of my favorites: “Uh-huh.” is an “Acknowledgement”.You can imagine how a keen understanding of intent can help a chatbot become much more natural and convincing. That way it can say just the right thing to make you buy that moisturizer!There has never been a better time to start learning Natural Language Processing. We’re generating more unstructured text than ever before, and the questions we need to answer are complex and important.You now have the foundational knowledge to recognize which problems fall under the umbrella of NLP.Not only that — you also know about the different branches of NLP which will allow you to brainstorm different solutions to your problems, and different approaches to mining value from your data.If you’re very new to the field, a great way to start is to pick a methodology that interests you and go deep — see what problems you can solve with it.Most importantly, stay reading and practicing. I’ll be doing the same!WRITTEN BY"
37,Stylistic differences between R and Python in modelling data through neural networks,https://towardsdatascience.com/stylistic-differences-between-r-and-python-in-modelling-data-through-neural-networks-1156627ed07e?source=collection_category---4------2-----------------------,"A core step in a data science methodology js to model data to produce accurate predictions. At that end, there is a wide array of methods and algorithms to explore. This blog gives an introduction in R and Python to an evolving methodology when modelling data, in addition to previous approaches (Decision Trees, Bayes Theorem), which simulates how the brain functions.Neural networks in data science represent an attempt to reproduce a non-linear learning that occurs in the network of neurons found in nature. A neuron consists of dendrites to gather inputs from other neurons and combines the input information in order to generate a response when some threshold is reached which is sent to other neurons. The inputs (Xi) are collected from upstream neurons through a combination function which is then transferred into an activation function to produce a final output response with predictive power (y).The combination function is often a summation producing a linear combination of the node inputs and the connection weights into a single scalar value. As per the equation below, X represents the ith input to node J, W represents the weight associated to each input to node j and comb the final result of the combination function.Once the combination function leads to a set of numerical results, the activation function progresses the pathway forward. There could be a range of choices but the most common one is a sigmoid function. The reason being that it combines linear, curvilinear and constant behaviour while using the base of natural logarithms (e= 2.718..). The generic formula shown below.Once an activation function is defined, the values from the combination function are applied to it in order to produce the output from the neural network pathway. These values would represent the prediction for the target variable under consideration.The neural structure is composed by a layered, feedforward and connected network of artificial neurons that are called nodes.
1. Layered: At a minimum there is an input, an hidden and an output layer
2. Feedforward: One direction of flow without loops and cycles
3. Connected: Every node is connected to another one in adjoining layers
The layered nature of a neural network is its strength and its weakness since having more nodes in a hidden layer increases its power for complex patterns but it might also lead to overfitting if at the expense of generalisation.A neural network is not a static model, it continues to learn. For each observation from a training dataset, an output value is produced and then compared to the actual value of the target variable for a set of training observations and the error is calculated (actual-output). To measure how well the output predictions fit with the actual target values, most neural network models use the sum of squared errors (SSE).The key benefit of using neural networks is that they are quite robust for noisy and complicated data thanks to its non-linear structure yet, the underpinning inner workings remain difficult to human interpretation.The starting point in Python to start building a neural network is to upload the relevant libraries, among which the keras.models, keras.layers and keras.utils. These are necessary preconditions to gather and define the various components of the neural network model.Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible.Secondly, uploading or creating a relevant dataframe is the basis to carry out the analysis and in the case of neural network it is sensible to choose non linear datasets, especially if they relate to visual records. To start simply, we can randomly generate a dataset to simulate the code. In this example, the prediction of how much assistance is received (Y) is explored as a function of the type of shock and location (X1, X2). We can transform a numeric variable into a categorical one by specifying the number of attributes and the typology of variable to be inserted by combining the commands insert and tolist .For a neural network, it is necessary to convert all categorical variables into numerical ones. The command to achieve this conversion in Python would beastype(‘category’).cat.codes. In addition, we can split the dataframe beforehand by using the train_test_split command.Once the relevant variables are transformed, the next critical step is to define the predictors and target variables in a way that can be processed through the neural network algorithm. In Python, this can be achieved by subsetting the variables and by ensuring that each variable is under the form of a np.array.Then, the most defining moment to create the neural network takes place through the definition of a model to test. In Python, the first step is to define whether we wish to explore a sequential or functional application program interface (API). The former allows you to create models layer-by-layer for most problems but does not allow models that share layers or have multiple simultaneous inputs or outputs. The latter enables you to create models that have a lot more flexibility as you can easily define models that may have multiple different input sources, produce multiple output destinations or that re-use layers. In this case, a simpler sequential model is used.Secondly, we start building fully connected layers by using the dense class. This is a specification of how many neurons or nodes in the layer we wish to test, the input dimension expressed asinput_dim along with the activation function using a specific argument. In the model, we can express various layer types. The most common is dense but it could also be convolutional, pooling or recurrent. Within the dense layer, we can define several items such as:1) The number of units, which represent the dimensionality of the output space. There is not perfect formula to define the optimal number of units, a starting point is the average of inputs and outputs in the model and their attributes. In our case, the dataset has 2 inputs with 5 attributes overall and 1 output. These values inform a series of iterations to create an optimal model.2) The activation function: If you don’t specify anything, no activation is applied (ie. “linear” activation: a(x) = x). Otherwise An activation function is generally applied to the output of a node to limit or bound its value. Activation functions can help in bounding the value and are useful for deciding whether the node should be “fired” or not. They can be a linear, sigmoid, hyperbolic tangent, or Rectified Linear Unit (ReLU) function.3) Other elements can be included. For example, the kernel_initializer describes the statistical distribution to be used for initialising the weights. The kernel_regularizer applies penalties on layer parameters or layer activity during optimization to be incorporated in the loss function that the network optimizes. The activity_regularizer works as a function of the output and is mostly used to regularize hidden units. Other components can be found here.In addition to dense layer, other elements can be added to the model. For example, a Dropout function works by “dropping out” inputs variable from previous activations from subsequent layer based on their probability. It has the effect of simulating a large number of networks with very different network structure and, in turn, making the nodes in the network generally more robust to the inputs.To produce the predictions in Python from the model created so far a compilation is necessary. In this critical step, we need to define the loss function by taking into account the nature of the target variable we can use the following options:1) mean_squared_error is a loss function for regression problems that calculates the square difference between real and predicted target values for each example in the dataset and then returns their average.
2) mean_absolute_error is a loss function for regression problems that calculates the different between real and predicted target values for each example in the dataset and then returns their average.
3) mean_absolute_percentage_error is a loss function for regression problems that calculates the average percentage difference between real and predicted target values for each example in the dataset.
4) binary_crossentropy is a loss function for two-class/binary classification problems. In general, cross-entropy loss is for calculating the loss for models where the output is a probability number between 0 and 1.
5) categorical_crossentropy is a loss function for multi-class (more than two classes) classification problems.Within the model function, we are also compelled to choose an optimizer. Every time a neural network finishes passing a batch of inputs through the network, it must decide how to use the difference between the predicted values and the actual values to adjust the weights on the nodes so that the network optimises its solution. The algorithm that determines that step is known as the optimization algorithm. The most recurrent ones are:1) Stochastic gradient descent (SGD), which computes the gradient of the network loss function with respect to each individual weight in the network. Nesterov accelerated gradient (NAG) can also be used to accelerate the gradient descent.
2) Adaptive Gradient (Adagrad) is a more advanced machine learning technique which performs gradient descent with a variable learning rate. The historical weights of nodes gradient is retained and strengthened through big updates for infrequent parameters and small updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.
3) Root Mean Square Propagation (RMSprop) is a correction to Adagrad, the learning rate is further divided by an exponentially decaying average of all squared gradients which leads to a global tuning value.
4) Adaptive Moment Estimation (Adam), in addition to storing an exponentially decaying average of past squared gradients it also retains an exponentially decaying average of past gradients.Lastly, we can also rely on a “balanced” mode applied to the weights utilised through the neural pathway. A class_weight.compute_sample_weight utility can transform the values of y to automatically adjust weights inversely proportional to class frequencies in the input data through the following formula n_samples / (n_classes * np.bincount(y))Once the model is defined and compiled in Python, then we can fit the model according to a set of parameters. In the fit function, we would specify: 1) the key variables in the model, 2) the number of epochs to run the model for which means the number of iterations over the whole dataset, 3) the batch_sizes which is the number of training data samples to use per gradient update and the validation_data that refers to the equivalent variables in the test dataset. Once this is defined, we can run the model and evaluate its results through the model.evaluate command. The final outputs as shown below indicate the loss function and accuracy coefficient for the training and test dataset. We would aim for an accuracy as close as possible to 1 and a loss function producing a result approach zero. The presented output indicates the model still needs fixing and more iterations to parametrize it correctly.In a similar way to the previous example in Python also in R we will be using Keras as the key package to create the neural network. We would also need to install tensorflow and reticulate to run our model. The use of Keras allows using a very similar style in coding between both coding languages.The first step remains the creation or the upload of a relevant dataframe as the basis to carry out the analysis. In this example we will randomly generate a dataset to simulate the code in R. Like in the previous example, the prediction of assistance received (Y) is a function of type of shock and location (X1, X2). We can also transform a numeric target variable into a categorical one by specifying attributes through the commands ifelse .In order to have suitable variables for the analysis, we would need to transform categorical variables into numerical ones. In R the transformation of our predictor and target variables requires a double transformation of each categorical into factor and integer through the as.factor and as.numeric commands. To transform the target variable into a zero and one value, we can rely on ifelse to express the relevant condition. Subsequently, we can split the dataframe into a test and training one through the commandrunif.After all of the variables are in the numerical format, the creation of a suitable target and predictors variables in R also requires further manipulation to ensure the target becomes an array and the predictors a matrix. Compared to Python, this requires an additional step since the selection of the variables cannot be combined with the array and data.matrix command. This needs to be done separately, whilst numpy in Python allows for a combined transformation.Once the variables are transformed, we would define the model. This step in R is absolutely similar in stylistic terms with Python with some minor differences. After having defined a sequential model, we would use the dollar sign to add layers and dropout functions. Within the first layer, the only differences in the parameters are semantic. For example, instead of input_dim we would use input_shape to express the number of predictors. The rest remains very similar, including the name of other parameters.Once the model is defined the model is ready to be compiled. In R this is done through the keras_compile command whilst keeping most of the parameters defined in the same way as per the previous example in Python for loss function and optimization. Once the compilation criteria are defined, the model can be fitted to the training variables according to the same wordings of the parameters as expressed in Python. No significant differences to report, with the exception of a few changes in wording like the need to add list before the validation data specification and the insertion of our model inside the function. To show the results, it is easier in R because we can just call the result of the function to see them without a further evaluation command.The final results of this simulation are acceptable in terms of loss function, but the accuracy remains too low.Once the model has generated the relevant predictions, we can draw the results to see the progression of the loss and accuracy results throughout the various iterations (epoch). In Python, this is a lot more time consuming since we need to define the various parameters of the plot to draw how the model performed in terms of accuracy and loss. The results below show that after the 50th iteration, there has been a marked improvement in accuracy and a drop in the loss function.On the other hand, it is a lot easier to plot the progression of loss and accuracy of our model in R. By simply calling the plot function we can access the trend of our loss and accuracy function. This is another proof of how much R is a visually-focused language and can generate relevant plots very easily. Again we can see a remarkable improvement of our model around the 50th epoch (a.k.a iteration over the dataset).Finally, we can add the predicted values to the test dataset to further verify their consistency. In Python, the combination of predict and a rounded function can generate a series of predicted values that we can use in the test dataset for further analysis.Similarly, in R we can predict the values applied to the test dataset, round them and create a new variable through the use of the dollar sign to advance in our comparison. Besides a few changes in the semantics, the approach remains the command predict generates the output predictions for the input samples in the test sample.Although the outputs of these models can be significantly improved, the objective of this blog was to illustrate how similar R and Python seem from a stylistic point of view when creating a neural network in Keras. The truth is that a neural network can be a lot more complex than what illustrated so far. It can be a multilayer perceptron, a convolutional network or a recurrent network. It can have shared feature extraction layers, multiple inputs or multiple outputs. Although this is a growing field, a basic understanding of the underpinning concepts can be sufficient to simulate a neural network. This can serve us as a basis to keep improving and building upon what we know by testing a wide range of parameters and optimization functions.WRITTEN BY"
38,Where you should drop Deep Learning in favor of Constraint Solvers,https://towardsdatascience.com/where-you-should-drop-deep-learning-in-favor-of-constraint-solvers-eaab9f11ef45?source=collection_category---4------0-----------------------,"Machine Learning and Deep Learning are ongoing buzzwords in the industry. Branding ahead of functionalities led to Deep Learning being overused in many artificial intelligence applications.This post will provide a quick grasp at constraint satisfaction, a powerful yet underused approach which can tackle a large number of problems in AI and other areas of computer science, from logistics and scheduling to temporal reasoning and graph problems.Let’s consider a factual and highly topical problem.A pandemic is rising. Hospitals must organize quickly to treat ill people.The world needs an algorithm which matches infected people and hospitals together given multiple criteria such as the severity of illness, patient age and location, hospital capacity and equipment, etc.Many would say that a neural network would be the perfect fit for it: different configurations from a broad range of parameters that need to be reduced to a unique solution.However, there are downsides which would undermine such an approach:On the other hand, if formulated in terms of a boolean satisfiability problem, the situation wouldn’t have any of the aforementioned downsides while still giving a sub-optimal solution in nondeterministic polynomial time (NP-complete problem), and without the need for any historical data.Disclaimer: the purpose of this post is to deliver a quick look at CSPs. Theory and problem formulation will be much overlooked. For a more rigorous approach, please refer to [2][3][4].This post will provide a gentle introduction to constraint programming, aiming to resolve this case study. This map of the pandemic (1) illustrates the output of our algorithm which will match infected people with hospitals.There are several frameworks for constraint solving. Google Optimization Tools (a.k.a., OR-Tools) is an open-source software suite for solving combinatorial optimization problems. Our problem will be modeled using this framework in Python.For now, let’s simplify the problem to 4 parameters (1):Let’s define those parameters in python:A constraint satisfaction problem consists of a set of variables that must be assigned values in such a way that a set of constraints is satisfied.Let define as our indexed family of variables :If in the hospital i, the bed j is taken by the person k then xᵢⱼₖ = 1. In order to associate each bed of an hospital to an ill person, the goal is to find a set of variables that satisfies all of our constraints.We can add those variables to our model:Hard constraints define a goal for our model. They are essential, if they are not resolved then the problem can’t be tackled:Let’s focus on the first hard constraint. For each bed j in every hospital i:Hence, it can be expressed in the following way:Our solver is a combinatorial optimization solver, it can process integer constraints only. Hence, must be turned into an integer equation:This inequality can then be added to our model.Next, the second hard constraint: for every patient k:In the same way, can be translated into an integer inequality:Finally, this constraint can be added to the model.Next, there are soft constraints. Those are highly desired: our solution must try to satisfy them as much as possible, yet they are not essential to find a solution:While hard constraints are modeled as equalities or inequalities, soft constraints are expressions we want to minimize or maximize.Let Ω be the set of all solutions that satisfy the hard constraints.Every sick person should be placed into a bed means to maximize the number of occupied beds.Every person should be handled by the nearest hospital means to minimize the distance between every patient and his assigned hospital.Sick persons in a severe condition should be handled first when there are not enough beds means to maximize the total severity of all handled patients. By denoting sev(k) the severity of the patient k:Then we can reduce all the soft constraints into a single objective:One needs to be careful then: these soft constraints don’t have the same domain.Given that all of these constraints share the same priority, we must define penalty factors to equilibrate the different constraints.Here is the corresponding code:Now we can launch the solver. It will try to find the optimal solution within a specified time limit. If it can’t manage to find the optimal solution, it will return the closest sub-optimal solution.In our case, the solver returns an optimal solution in 2.5 seconds (2).To create this solution, all it takes is 1 hour of research and 30 minutes of programming.For a Deep Learning counterpart, one can predict a few days of data cleansing, at least a day to test different architectures and another day for training.Moreover, a CP-SAT model is very robust if well modelized. Below are the results with different simulation parameters (3). Results are still coherent in many different cases, and with increased simulation parameters (3000 patients, 1000 beds), solution inference took a little less than 3 minutes.Of course, CSPs hardly apply to topics like computer vision and NLP, where Deep Learning is sometimes the best approach. However, in logistics, scheduling and planning, it is often the way to go.Special thanks to Laurent Perron, and his Operations Research team at Google for their fantastic work, and for the time they take to answer technical questions on StackOverflow, GitHub and Google Groups.Antoine Champion, Apr. 1st 2020[1] Jingchao Chen, Solving Rubik’s Cube Using SAT Solvers, arXiv:1105.1436, 2011.[2] Biere, A., Heule, M., and van Maaren, H. Handbook of satisfiability, volume 185. IOS press, 2009a[3] Knuth, D. E., The art of computer programming, Volume 4, Fascicle 6: Satisfiability. Addison-Wesley Professional, 2015[4] Vipin Kumar, Algorithms for constraint-satisfaction problems: a survey, AI Magazine Volume 13, Issue 1, 1992.WRITTEN BY"
39,Roadmap to Computer Vision,https://towardsdatascience.com/roadmap-to-computer-vision-79106beb8be4?source=collection_category---4------1-----------------------,"Computer Vision (CV) is nowadays one of the main application of Artificial Intelligence (eg. Image Recognition, Object Tracking, Multilabel Classification). In this article, I will walk you through some of the main steps which compose a Computer Vision System.A standard representation of the workflow of a Computer Vision system is:We will now briefly walk through some of the main processes our data might go through each of these three different steps.When trying to implement a CV system, we need to take into consideration two main components: the image acquisition hardware and the image processing software. One of the main requirements to meet in order to deploy a CV system is to test its robustness. Our system should, in fact, be able to be invariant to environmental changes (such as changes in illumination, orientation, scaling) and able to perform it’s designed task repeatably. In order to satisfy these requirements, it might be necessary to apply some form of constraints to either the hardware or software of our system (eg. remotely control the lighting environment).Once an image is acquired from a hardware device, there are many possible ways to numerically represents colours (Colour Spaces) within a software system. Two of the most famous colour spaces are RGB (Red, Green, Blue) and HSV (Hue, Saturation, Value). One of the main advantages of using an HSV colour space is that by taking just the HS components we can make our system illumination invariant (Figure 1).Once an image enters a system and is represented by using a colour space, we can then apply different operators on the image in order to improve its representation:Once pre-processed an image, we can then apply more advanced techniques in order to try to extract the edges and shapes within an image by using methods such as First Order Edge Detection (eg. Prewitt Operator, Sobel Operator, Canny Edge Detector) and Hough Transforms.Once pre-processed an image, there are 4 main types of Feature Morphologies which can be extracted from an image by using a Feature Extractor:Once extracted a set of discriminative features, we can then use them in order to train a Machine Learning model to make inference. Feature descriptors can be easily applied in Python using libraries such as OpenCV.One of the main concept used in Computer Vision to classify an image is the Bag of Visual Words (BoVW). In order to construct a Bag of Visual Words, we need first of all to create a vocabulary by extracting all the features from a set of images (eg. using grid-based features or local features). Successively, we can then count the number of times an extracted feature appears in an image and build a frequency histogram from the results. Using the frequency histogram as a basic template, we can finally classify if an image belongs to the same class or not by comparing their histograms (Figure 3).This process can be summarised in the following few steps:New images can then be classified by repeating this same process for each image we want to classify and then using any classification algorithm to find out which image in our vocabulary resembles the most our test image.Nowadays, thanks to the creation of Artificial Neural Networks architectures such as Convolutional Neural Networks (CNNs) and Recurrent Artificial Neural Networks (RCNNs), it has been possible to ideate an alternative workflow for Computer Vision (Figure 4).In this case, the Deep Learning Algorithm incorporates both the Feature Extraction and Classification steps of the Computer Vision workflow. When using Convolutional Neural Networks, each layer of the neural network applies the different feature extraction techniques at his description (eg. Layer 1 detects edges, Layer 2 finds shapes in an image, Layer 3 segments the image, etc…) before providing the feature vectors to the dense layer classifier.Further applications of Machine Learning in Computer Vision include areas such as Multilabel Classification and Object Recognition. In Multilabel Classification, we aim to construct a model able to correctly identify how many objects there are in an image and to what class they do belong to. In Object Recognition instead, we aim to take this concept a step further by identifying also the position of the different objects in the image.If you want to keep updated with my latest articles and projects follow me on Medium and subscribe to my mailing list. These are some of my contacts details:[1] Modular robot used as a beach cleaner, Felippe Roza. Researchgate. Accessed at: https://www.researchgate.net/figure/RGB-left-and-HSV-right-color-spaces_fig1_310474598[2] Bag of visual words in OpenCV, Vision & Graphics Group. Jan Kundrac. Accessed at: https://vgg.fiit.stuba.sk/2015-02/bag-of-visual-words-in-opencv/[3] Deep Learning Vs. Traditional Computer Vision. Haritha Thilakarathne, NaadiSpeaks. Accessed at: https://naadispeaks.wordpress.com/2018/08/12/deep-learning-vs-traditional-computer-vision/WRITTEN BY"
40,TensorFlow Lite Android Support Library: Simplify ML On Android,https://towardsdatascience.com/tensorflow-lite-android-support-library-simply-ml-on-android-561402292c80?source=collection_category---4------2-----------------------,"Everyone loves TensorFlow and even more when you can run a TF model on Android directly. We all use TensorFlow Lite on Android and we have a couple of CodeLabs on it too. Using the Interpreter class on Android, we are currently running our .tflite models in apps.But we have to do a lot before that, right? If we’re performing an image classification task, you’ll probably get a Bitmap or an Image object from the Camera library and then we transform it into a float[][][] or a byte[] . Then we load our model from the assets folder as a MappedByteBuffer . After calling interpreter.run() , we get the class probabilities, on which we perform the argmax() operation and then finally get a label from the labels.txt file.This is the traditional approach which we developers follow and there’s no other way round.The TensorFlow team has released the TensorFlow Lite Android Support Library to solve the tedious tasks of preprocessing. The GitHub page gives an intuition of their aim,Mobile application developers typically interact with typed objects such as bitmaps or primitives such as integers. However, the TensorFlow Lite Interpreter that runs the on-device machine learning model uses tensors in the form of ByteBuffer, which can be difficult to debug and manipulate. The TensorFlow Lite Android Support Library is designed to help process the input and output of TensorFlow Lite models, and make the TensorFlow Lite interpreter easier to use.First, we need to get this right in our Android project. Remembering build.gradle file? Right! We’ll add these dependencies in our app-level build.gradle file,The first step in running a TFLite model is to create some array object which can store the inputs for our model as well the outputs which the model will produce. To make our lives easier and less struggling with float[] objects, TF Support Library includes a TensorBuffer class that takes in the shape of the desired array and its data type.Note: As of 1st pril 2020, only DataType.FLOAT32 and DataType.UINT8 are supported.You can even create a TensorBuffer object from an existing TensorBuffer object by modifying its data type,If you’re working with object detection, image classification or other images -related models, you need to work on Bitmap and resize it or normalize it. We have three ops for this namely, ResizeOp , ResizeWithCropOrPadOp and Rot900p .First, we define our preprocessing pipeline using the ImageProcessor class.Question: What are BILINEAR and NEAREST_NEIGHBOR methods?Answer: Read this.Next, create a TensorImage object and process the image.Normalization of image arrays is necessary for almost all models to be it image classification models or regression models. For processing tensors, we have a TensorProcessor . Along with NormalizeOp we have CastOp , QuantizeOp and DequantizeOp .Question: What is Normalization?Answer: The process of converting an actual range of values into a standard range of values, typically -1 to +1 or 0 to 1. For example, suppose the natural range of a certain feature is 800 to 6,000. Through subtraction and division, you can normalize those values into the range -1 to +1.Also, we have the freedom to build custom ops by implementing TensorOperator class, as shown below.We can easily load our .tflite model using the FileUtil.loadMappedFile() method. Similarly, we can load the labels from a InputStream or from the assets folder.And then perform inference using Interpreter.run() ,I hope you liked the new TensorFlow Lite Android Support Library. This was a quick review of what’s inside but try exploring it yourself too. Thanks for reading!WRITTEN BY"
41,An Introduction to Nine Essential Machine Learning Algorithms,https://towardsdatascience.com/an-introduction-to-nine-essential-machine-learning-algorithms-ee0efbb61e0?source=collection_category---4------3-----------------------,"If this is the kind of stuff that you like, be one of the FIRST to subscribe to my new YouTube channel here! While there aren’t any videos yet, I’ll be sharing lots of amazing content like this but in video form. Thanks for your support :)In my previous article, I explained what regression was and showed how it could be used in application. This week, I’m going to go over the majority of common machine learning models used in practice, so that I can spend more time building and improving models rather than explaining the theory behind it. Let’s dive into it.All machine learning models are categorized as either supervised or unsupervised. If the model is a supervised model, it’s then sub-categorized as either a regression or classification model. We’ll go over what these terms mean and the corresponding models that fall into each category below.Supervised learning involves learning a function that maps an input to an output based on example input-output pairs [1].For example, if I had a dataset with two variables, age (input) and height (output), I could implement a supervised learning model to predict the height of a person based on their age.To re-iterate, within supervised learning, there are two sub-categories: regression and classification.In regression models, the output is continuous. Below are some of the most common types of regression models.The idea of linear regression is simply finding a line that best fits the data. Extensions of linear regression include multiple linear regression (eg. finding a plane of best fit) and polynomial regression (eg. finding a curve of best fit). You can learn more about linear regression in my previous article.Decision trees are a popular model, used in operations research, strategic planning, and machine learning. Each square above is called a node, and the more nodes you have, the more accurate your decision tree will be (generally). The last nodes of the decision tree, where a decision is made, are called the leaves of the tree. Decision trees are intuitive and easy to build but fall short when it comes to accuracy.Random forests are an ensemble learning technique that builds off of decision trees. Random forests involve creating multiple decision trees using bootstrapped datasets of the original data and randomly selecting a subset of variables at each step of the decision tree. The model then selects the mode of all of the predictions of each decision tree. What’s the point of this? By relying on a “majority wins” model, it reduces the risk of error from an individual tree.For example, if we created one decision tree, the third one, it would predict 0. But if we relied on the mode of all 4 decision trees, the predicted value would be 1. This is the power of random forests.StatQuest does an amazing job walking through this in greater detail. See here.A neural network is a multi-layered model inspired by the human brain. Like the neurons in our brain, the circles above represent a node. The blue circles represent the input layer, the black circles represent the hidden layers, and the green circles represent the output layer. Each node in the hidden layers represents a function that the inputs go through, ultimately leading to an output in the green circles.Neural networks are actually very complex and very mathematical, so I won’t get into the details of it but…Tony Yiu’s article gives an intuitive explanation of the process behind neural networks (see here).If you want to take it a step further and understand the math behind neural networks, check out this free online book here.If you’re a visual/audio learner, 3Blue1Brown has an amazing series on neural networks and deep learning on YouTube here.In classification models, the output is discrete. Below are some of the most common types of classification models.Logistic regression is similar to linear regression but is used to model the probability of a finite number of outcomes, typically two. There are a number of reasons why logistic regression is used over linear regression when modeling probabilities of outcomes (see here). In essence, a logistic equation is created in such a way that the output values can only be between 0 and 1 (see below).A Support Vector Machine is a supervised classification technique that can actually get pretty complicated but is pretty intuitive at the most fundamental level.Let’s assume that there are two classes of data. A support vector machine will find a hyperplane or a boundary between the two classes of data that maximizes the margin between the two classes (see below). There are many planes that can separate the two classes, but only one plane can maximize the margin or distance between the classes.If you want to get into greater detail, Savan wrote a great article on Support Vector Machines here.Naive Bayes is another popular classifier used in Data Science. The idea behind it is driven by Bayes Theorem:While there are a number of unrealistic assumptions made in regards to Naive Bayes (hence why it’s called ‘Naive’), it has proven to perform quite most of the time and it is also relatively fast to build.See here if you want to learn more about them.These models follow the same logic as previously explained. The only difference is that that output is discrete rather than continuous.Unlike supervised learning, unsupervised learning is used to draw inferences and find patterns from input data without references to labeled outcomes. Two main methods used in unsupervised learning include clustering and dimensionality reduction.Clustering is an unsupervised technique that involves the grouping, or clustering, of data points. It’s frequently used for customer segmentation, fraud detection, and document classification.Common clustering techniques include k-means clustering, hierarchical clustering, mean shift clustering, and density-based clustering. While each technique has a different method in finding clusters, they all aim to achieve the same thing.Dimensionality reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables [2]. In simpler terms, its the process of reducing the dimension of your feature set (in even simpler terms, reducing the number of features). Most dimensionality reduction techniques can be categorized as either feature elimination or feature extraction.A popular method of dimensionality reduction is called principal component analysis.In the simplest sense, PCA involves project higher dimensional data (eg. 3 dimensions) to a smaller space (eg. 2 dimensions). This results in a lower dimension of data, (2 dimensions instead of 3 dimensions) while keeping all original variables in the model.There is quite a bit of math involved with this. If you want to learn more about it…Check out this awesome article on PCA here.If you’d rather watch a video, StatQuest explains PCA in 5 minutes here.Obviously, there is a ton of complexity if you dive into any particular model, but this should give you a fundamental understanding of how each machine learning algorithm works!Check out this link if you want to learn all basic statistics for data science.Check out this link if you want to learn a step by step process for exploratory data analysis (EDA).[1] Stuart J. Russell, Peter Norvig, Artificial Intelligence: A Modern Approach (2010), Prentice Hall[2] Roweis, S. T., Saul, L. K., Nonlinear Dimensionality Reduction by Locally Linear Embedding (2000), ScienceIf you like my work and want to support me…WRITTEN BY"
42,End to End Machine Learning Tutorial — From Data Collection to Deployment 🚀,https://towardsdatascience.com/end-to-end-machine-learning-from-data-collection-to-deployment-ce74f51ca203?source=collection_category---4------4-----------------------,"Disclaimer: code and demo at the end of the article.This started out as a challenge. I wanted with a friend of mine to see if it was possible to build something from scratch and push it to production. In 3 weeks. This is our story. Here for more posts like this.In this post, we’ll go through the necessary steps to build and deploy a machine learning application. This starts from data collection to deployment and the journey, as you’ll see it, is exciting and fun 😀.Before we begin, let’s have a look at the app we’ll be building:As you see, this web app allows a user to evaluate random brands by writing reviews. While writing, the user will see the sentiment score of his input updating in real-time along with a proposed rating from 1 to 5.The user can then change the rating in case the suggested one does not reflect his views, and submit.You can think of this as a crowdsourcing app of brand reviews with a sentiment analysis model that suggests ratings that the user can tweak and adapt afterward.To build this application we’ll follow these steps:All the code is available in our Github repository and organized in independent directories, so you can check it, run it and improve it.Let’s get started! 💻⚠️ Disclaimer: The scripts below are meant for educational purposes only: scrape responsibly.In order to train a sentiment classifier, we need data. We can sure download open-source datasets for sentiment analysis tasks such as Amazon Polarity or IMDB movie reviews but for the purpose of this tutorial, we’ll build our own dataset. We’ll scrape customer reviews from Trustpilot.Trustpilot.com is a consumer review website founded in Denmark in 2007. It hosts reviews of businesses worldwide and nearly 1 million new reviews are posted each month.Trustpilot is an interesting source because each customer review is associated with a number of stars.By leveraging this data, we are able to map each review to a sentiment class.In fact, we defined reviews with:In order to scrape customer reviews from Trustpilot, we first have to understand the structure of the website.Trustpilot is organized by categories of businesses.Each category is divided into sub-categories.Each sub-category is divided into companies.And then each company has its own set of reviews, usually spread over many pages.As you see, this is a top-down tree structure. In order to scrape the reviews out of it, we’ll proceed in two steps.All the Selenium code is available and runnable from this notebook 📓We first use Selenium because the content of the website that renders the URLs of each company is dynamic which means that it cannot be directly accessed from the page source. It’s rather rendered on the front end of the website through Ajax calls.Selenium does a good job extracting this type of data: it simulates a browser that interprets javascript rendered content. When launched, it clicks on each category, narrows down to each sub-category and goes through all the companies one by one and extracts their URLs. When it’s done, the script saves these URLs to a CSV file.Let’s see how this is done:We’ll first import Selenium dependencies along with other utility packages.We start by fetching the sub-category URLs nested inside each category.If you open up your browser and inspect the source code, you’ll find out 22 category blocks (on the right) located in div objects that have a class attribute equal to category-objectEach category has its own set of sub-categories. Those are located in div objects that have class attributes equal to child-category. We are interested in finding the URLs of these subcategories.Let’s first loop over categories and for each one of them collect the URLs of the sub-categories. This can be achieved using Beautifulsoup and requests.Now comes the selenium part: we’ll need to loop over the companies of each sub-category and fetch their URLs.Remember, companies are presented inside each sub-category like this:We first define a function to fetch company URLs of a given subcategory:and another function to check if a next page button exists:Now we initialize Selenium with a headless Chromedriver. This prevents Selenium from opening up a Chrome window thus accelerating the scraping.PS: You’ll have to download Chromedriver from this link and choose the one that matches your operating system. It’s basically a binary of a Chrome browser that Selenium uses to start.The timeout variable is the time (in seconds) Selenium waits for a page to completely load.Now we launch the scraping. This approximatively takes 50 minutes with a good internet connexion.Once the scraping is over, we save the company URLs to a CSV file.And here’s what the data looks like:Pretty neat right? Now we’ll have to go through the reviews listed in each one of those URLs.All the Scrapy code can be found in this folder 📁Ok, now we’re ready to use Scrapy.First, you need to install it either using:orThen, you’ll need to start a project:This command creates the structure of a Scrapy project. Here’s what it looks like:Using Scrapy for the first time can be overwhelming, so to learn more about it, you can visit the official tutorials.To build our scraper, we’ll have to create a spider inside the spiders folder. We'll call it scraper.py and change some parameters in settings.py. We won't change the other files.What the scraper will do is the following:Here’s the full script.To fully understand it, you should inspect the source code. It’s really easy to get.In any case, if you have a question don’t hesitate to post it in the comment section ⬇Before launching the scraper, you have to change a couple of things in the settings.py:Here are the changes we made:This indicates to the scraper to ignore robots.txt, to use 32 concurrent requests and to export the data into a: format under the filename: comments_trustpilot_en.csv.Now time to launch the scraper:We’ll let it run for a little bit of time.Note that we can interrupt it at any moment since it saves the data on the fly on this output folder is src/scraping/scrapy.The code and the model we’ll be using here are inspired by this Github repo so go check it for additional information. If you want to stick to this project’s repo you can look at this link.Now that the data is collected, we’re ready to train a sentiment classifier to predict the labels we defined earlier.There is a wide range of possible models to use. The one we’ll be training is a character-based convolutional neural network. It’s based on this paper and it proved to be really good on text classification tasks such as binary classification of Amazon Reviews datasets.The question you’d be asking up-front though is the following: how would you use CNNs for text classification? Aren’t these architectures specifically designed for image data?Well, the truth is, CNNs are way more versatile and their application can extend the scope of image classification. In fact, they are also able to capture sequential information that is inherent to text data. The only trick here is to efficiently represent the input text.To see how this is done, imagine the following tweet:Assuming an alphabet of size 70 containing the English letters and the special characters and an arbitrary maximum length of 140, one possible representation of this sentence is a (70, 140) matrix where each column is a one-hot vector indicating the position of a given character in the alphabet and 140 being the maximum length of tweets. This process is called quantization.Note that if a sentence is too long, the representation truncates up to the first 140 characters. On the other hand, if the sentence is too short 0 column vectors are padded until the (70, 140) shape is reached.So what to do now with this representation?Feed it to a CNN for classification, obviously 😁But there’s a small trick though. Convolutions are usually performed using 2D-shaped kernels because these structures capture the 2D spatial information lying in the pixels. Text is however not suited to this type of convolutions because letters follow each other sequentially, in one dimension only, to form a meaning. To capture this 1-dimensional dependency, we’ll use 1D convolutions.So how does a 1-D convolution work?Unlike 2D-convolutions that make a 2D kernel slide horizontally and vertically over the pixels, 1D-convolutions use 1D kernels that slide horizontally only over the columns (i.e. the characters) to capture the dependency between characters and their compositions. You could think for example about a 1D kernel of size 3 as a character 3-gram detector that fires when it detects a composition of three successive letters that is relevant to the prediction.The diagram below shows the architecture we’ll be using:It has 6 convolutional layers:and 2 fully connected layers:On the raw data, i.e. the matrix representation of a sentence, convolutions with a kernel of size 7 are applied. Then the output of this layer is fed to a second convolution layer with a kernel of size 7 as well, etc, until the last Conv layer that has a kernel of size 3.After the last convolution layer, the output is flattened and passed through two successive fully connected layers that act as a classifier.To learn more about character level CNN and how they work, you can watch this videoCharacter CNNs are interesting for various reasons since they have nice properties 💡That’s all about the theory now!In order to train a character level CNN, you’ll find all the files you need under the src/training/ folder.Here’s the structure of the code inside this folder:train.py: used for training a model
predict.py: used for the testing and inferencesrc: a folder that contains:To train our classifier, run the following commands:When it’s done, you can find the trained models in src/training/models directory.On training setOn the training set we report the following metrics for the best model (epoch 5):Here are the corresponding Tensorboard training logs:On validation setOn the validation set we report the following metrics for the best model (epoch 5):Here are the corresponding validation Tensorboard logs:Few remarks according to these figures:To learn more about the training arguments and options, please check out the original repo.From now on, we’ll use the trained model that is saved as a release here. When running the app for the first time, it’ll get downloaded from that link and locally saved (in the container) for the inference.The dash code can be found here and the API code hereNow that we have trained the sentiment classifier, let’s build our application so that end-users can interact with the model and evaluate new brands.Here is a schema of our app architecture:As you can see, there are four building blocks in our app:The Dash app will make HTTP requests to the Flask API, which will, in turn, interact with either the PostgreSQL database by writing or reading records to it or the ML model by serving it for real-time inference.If you are already familiar with Dash, you know that it is built on top of Flask. So we could basically get rid of the API and put everything within the dash code.We chose not to for a very simple reason: it makes the logic and the visualization parts independent. Indeed, because we have a separated API, we can with very little effort to replace the Dash app with any other frontend technology or add a mobile or desktop app.Now, let’s have a closer look at how those blocks are built.Nothing fancy or original regarding the database part. We chose to use one of the most widely used relational databases: PostgreSQL.To run a PostgreSQL database for local development, you can either download PostgreSQL from the official website or, more simply, launch a Postgres container using Docker:If you are not familiar with docker yet, don’t worry, we’ll talk about it very soon.The two following commands allow to:The RESTful API is the most important part of our app. It is responsible for the interactions with both the machine learning model and the database.Let’s have a look at the routes needed for our API:Sentiment Classification RoutePOST /api/predictThis route used to predict the sentiment based on the review’s text.Body:Response:As you can see, this route gets a text field called review and returns a sentiment score based on that text.It starts by downloading the trained model from Github and saving it to disk. Then it loads it and passes it to GPU or CPU.When the API receives an input review it passes it to the predict_sentiment function. This function is responsible for representing the raw text in a matrix format and feeding it to the model.Create ReviewPOST /api/reviewThis route is used to save a review of the database (with associated ratings and user information).Body:Response:In order to interact with the database, we will use the Object Relational Mapping (ORM) peewee. It lets us define the database tables using python objects, and takes care of connecting to the database and querying it.This is done in the src/api/db.py file:Having done all this using peewee makes it super easy to define the API routes to save and get reviews:Get ReviewsGET /api/reviewsThis route is used to get reviews from the database.Response:Similar to what we have done for the route POST /api/review, we use peewee to query the database. This makes the route's code quite simple:Dash is a visualization library that allows you to write HTML elements such as divs, paragraphs, and headers in a python syntax that gets later rendered into React components. This allows great freedom to those who want to quickly craft a little web app but don’t have front-end expertise.Dash is easy to grasp. Here’s a small hello world example:As you see, components are imported from dash_core_components and dash_html_components and inserted into lists and dictionaries, then affected to the layout attribute of the dash app. If you're experienced with Flask, you'll notice some similarities here. In fact, Dash is built on top of Flask.Here’s what the app looks like in the browser when you visit: localhost:8050Pretty neat right?Dash allows you to add many other UI components very easily such as buttons, sliders, multi selectors, etc. You can learn more about dash-core-components and dash-html-components from the official documentation.In our app, we also used dash bootstrap components to make the UI mobile responsive.CallbacksTo make these components interact with each other, dash introduced the concept of callback. Callbacks are functions that get called to affect the appearance of an HTML element (the Output) every time the value of another element (the Input) changes. Imagine the following situation: you have an HTML input field of id=""A"" and you want when every time it gets input to copy it inside a paragraph element of id=""B"", dynamically, without reloading the page.Here’s how you’d do it with a callback:This callback listens to any change of input value inside the element of id A to affect it to the input value of the element of id B. This is done, again, dynamically.Now I’ll let you imagine what you can do with callbacks when you can handle many inputs to outputs and interact with other attributes than value. You can learn more about callbacks here or here.Now back to our app!Here’s what it looks like. Each red arrow indicates the id of each HTML element.These elements obviously interact with each other. To materialize this, we defined two callback functions which can be visualized in the following graph.Callback 1At every change of the input value of the text area of id review, the whole text review is sent through an HTTP post request to the API route POST /api/predict/ to receive a sentiment score. Then this score is used by the callback to update the value (in percentage) inside the progress bar (proba), the length and the color of the progress bar again, the rating from 1 to 5 on the slider, as well as the state of the submit button (which is disabled by default when no text is present inside the text area.) What you'll have out of all this is a dynamic progress bar that fluctuates (with color code) at every change of input as well as a suggested rating from 1 to 5 that follows the progress bar.Here’s how you’d do it in code:Callback 2This callback does two things.Here’s the code:We’ll skip the definition of the dash app layout. You can check it directly from the source code on the repo.If you have any question you can ask it, as always, in the comment section below ⬇.Now that we have built our app, we’re going to deploy it. But it’s actually easier said than done. Why?Well, installing all our dependencies (Flask, Peewee, PyTorch, and so on…) can be tedious, and this process can differ based on the host’s OS (yours or any other cloud instance’s). We also need to install a PostgreSQL database, which can be laborious as well. Not to mention the services that you have to manually create to run all the processes.Wouldn’t it be nice to have a tool that takes care of all this? Here is where Docker comes in.Docker is a popular tool to make it easier to build, deploy and run applications using containers. Containers allow us to package all the things that our application needs like such as libraries and other dependencies and ship it all as a single package. In this way, our application can be run on any machine and have the same behavior.Docker also provides a great tool to manage multi-containers applications: docker-compose. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.Here is an example of a simple Docker Compose that runs two services ( web and redis):To learn more about Docker and Docker Compose, have a look at this great tutorialLet’s see now how we dockerized our app.First of all, we separated our project into three containers, each one is responsible for one of the app’s servicesTo manage these containers we’ll use, as you expect, Docker Compose.Here’s our docker-compose.yml file, located at the root of our project:Let’s have a closer look at our services.To manage the database service, docker-compose first pulls an official image from the Postgres dockerhub repositoryIt then passes connection information to the container as environment variables and maps the /var/lib/postgresql/data directory of the container to the ~/pgdata directory of the host. This allows data persistence.You can also notice the restart: always policy, which ensures that our service will restart if it fails or if the host reboots.To manage the API service, docker-compose first launches a build of a custom image based on the Dockerfile located at src/api. Then it passes environment variables for the database connection.This service depends on the database service, that has to start before the API. This is ensured by the depends_on clause:Now here’s the Dockerfile to build the API docker image.When running this file, docker will pull an official python image from Dockerhub, copy a requirements.txt to /app/, install the dependencies using pip, expose a port and run a web server.Notice that we are using gunicorn instead of just launching the flask app using the python app.py command.Indeed, Flask's built-in server is a development only server, and should not be used in production.From the official deployment documentation:When running publicly rather than in development, you should not use the built-in development server (flask run). The development server is provided by Werkzeug for convenience but is not designed to be particularly efficient, stable, or secure.You can use any python production web server (tornado, gunicorn, …) instead.For the dash service, similar to what has been done for the API, docker-compose launches a build of a custom image based on the Dockerfile located at src/dash. Then it passes two environment variables. One of them is API_URL. Notice that the hostname API_URL is the name of the api service.To build the image, Docker will be running this file, which is basically the same as the previous one, except for the port.Let’s first have a look at the global deployment architecture we designed:Here’s the workflow:When a user goes to reviews.ai2prod.com from his browser, a request is sent to the DNS server which in turn redirects it to a load balancer. The load balancer redirects its request to an EC2 instance inside a target group. Finally, when docker-compose receives the request on port 8050, it redirects it to the Dash container.So how did we build this workflow? Below are the main steps.Deploy the app on an EC2 instanceThe very first step to our deployment journey is launching an instance to deploy our app on.To do this, go to the EC2 page of the AWS Console, and click on the “Launch Instance”.You will need to select an AMI. We used Amazon Linux 2, but you can choose any Linux based instance.You will then need to choose an instance type. We went for a t3a.large but you could probably select a smaller one.You will also need to configure a security group so that you can ssh into your instance, and access the 8050 port on which our dash app runs. This is done as follows:You can finally launch the instance. If you need more explanations on how to launch an EC2 instance you can read this tutorial.Now that we have our instance, let’s ssh into it:We can then install docker. Below are the installation instructions for Amazon Linux 2 instances. Please refer to the official Docker installation instructions for other OS.⚠️ You will need to log out and log back in. ⚠️Then install docker-compose:And test the installThen clone the git repository:And finally, run the app:Once it’s running, you can access the dashboard from the browser by typing the following address:We could stop here, but we wanted to use a cooler domain name, a subdomain for this app, and an SSL certificate. These are optional configuration steps, but they’re recommended if you want a polished product.Buy your own domain nameFirst, you will need to buy a cool domain name. You can choose any domain registrar, but using AWS Route53 will make things easier as we are deploying the app on AWS.Go the Route53 page of the AWS console, and click on “Domain registration”.Then, follow the domain purchase process which is quite straightforward. The hardest step is finding an available domain name that you like.Request an SSL certificate using ACMOnce you have purchased your own domain name on Route53, you can easily request an SSL certificate using AWS Certificate Manager.You will need to enter the list of subdomains that you wish to protect with the certificate (for example mycooldomain.com and *.mycooldomain.com).Then, if you registered your domain on Route53, the remainder of the process is quite simple:According to the documentation, it can then take a few hours for the certificate to be issued. Although from our own experience, it usually doesn’t take longer than 30 minutes.Put the app behind an Application Load BalancerLoad balancers are, as their names suggest, usually used to balance the load between several instances. However, in our case, we deployed our app to one instance only, so we didn’t need any load balancing. In fact, we used an AWS ALB (Application Load Balancer) as a reverse proxy, to route the traffic from HTTPS and HTTP ports (443 and 80 respectively) to our Dash app port (8050).To create and configure your Application Load Balancer go to the Load Balancing tab of the EC2 page in the AWS console and click on the “Create Load Balancer” button:Then you will need to select the type of load balancer you want. We won’t go into too many details here, but for most use-cases, you will need an Application Load Balancer.Then you will have to:As you added an HTTPS listener, you will be asked to select or import a certificate. Select the one you requested using ACM:Then you will need to configure the security groups for your ALB. Create a new security group for your load balancer, with ports 80 (HTTP) and 443 (HTTPS) opened.Once this is done, remains the final step: creating your target group for your load balancer.To do that you will need to specify the port on which the traffic from the load balancer should be routed. In our case this is our Dash app’s port, 8050:Now you can add the EC2 instance on which we deployed the app as a registered target for the group:And, here it is, you can finally create your load balancer.You can test it by going to your-load-balancer-dns-name-amazonaws.com. you should see your app!BUT, despite the fact that we have used the certificate issued by ACM, it still says that the connection is not secure!Don’t worry, that’s perfectly fine. If you remember correctly, the certificate we requested protects mycooldomain.com, not your-load-balancer-dns-name-amazonaws.com.So we need to create a record set in Route53 to map our domain name to our load balancer. We will see how to do that very soon.But before that, we will already put in place a redirection from HTTP to HTTPS in our load balancer. This will ensure that all the traffic is secured when we will finally use our domain.To do that, you need to edit the HTTP rule of your Application Load Balancer:Click on the pen symbols:Delete the previous action ( Forward to) and then add a new Redirect to action:Finally, select the HTTPS protocol with port 443, and update your rule.You should now be automatically redirected to https://your-load-balancer-dns-name-amazonaws.com when accessing http://your-load-balancer-dns-name-amazonaws.com Create a record set in Route53A Route53 record set is basically a mapping between a domain (or subdomain) and either an IP address or an AWS asset. In our case, our Application Load Balancer. This record will then be propagated in the Domain Name System so that a user can access our app by typing the URL.To create a record set go to your hosted zone’s page in Route53 and click on the Create Record Set button:You will have to:And you will soon be able to access the app using your custom domain address (DNS propagation might usually take about an hour).One last thing that you might want to do is to either redirect traffic from yourcooldomain.com to www.yourcooldomain.com, or the other way around. We chose to redirect reviews.ai2prod.com to www.reviews.ai2prod.comWe won’t go into many details here but here is how to do that:Here is a schema representing how everything works in the end:When building this application, we thought of many improvements that we hadn’t the time to successfully add.In particular, we wanted to:Throughout this tutorial, you learned how to build a machine learning application from scratch by going through the data collection and scraping, model training, web app development, docker and deployment.Every block of this app is independently packaged and easily reusable for other similar use cases.We’re aware that many improvements could be added to this project and this is one of the reasons we’re releasing it. So if you think of any feature that could be added don’t hesitate to fork the repo and create a pull request. If you’re also facing an issue running the app do not hesitate to report it. We’ll try to fix the problem as soon as possible.That’s all folks! 🎉Originally published here. For more posts of this kind, visit my blogWRITTEN BY"
43,How to Produce a DeepFake Video in 5 Minutes,https://towardsdatascience.com/how-to-produce-a-deepfake-video-in-5-minutes-513984fd24b6?source=collection_category---4------5-----------------------,"Do you dance? Do you have a favourite dancer or performer that you want to see yourself copying their moves? Well, now you can!Imagine having a full-body picture of yourself. Just a still image. Then all you need is a solo video of your favourite dancer performing some moves. Not that hard now that TikTok is taking over the world…Image animation uses a video sequence to drive the motion of an object in a picture. In this story, we see how image animation technology is now ridiculously easy to use, and how you can animate almost anything you can think of. To this end, I transformed the source code of a relevant publication into a simple script, creating a thin wrapper that anyone can use to produce DeepFakes. With a source image and the right driving video, everything is possible.In this article, we talk about a new publication (2019), part of Advances in Neural Information Processing Systems 32 (NIPS 2019), called “First Order Motion Model for Image Animation” [1]. In this paper, the authors, Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci and Nicu Sebe, present a novel way to animate a source image given a driving video, without any additional information or annotation about the object to animate.Under the hood, they use a neural network trained to reconstruct a video, given a source frame (still image) and a latent representation of the motion in the video, which is learned during training. At test time, the model takes as input a new source image and a driving video (e.g. a sequence of frames) and predicts how the object in the source image moves according to the motion depicted in these frames.The model tracks everything that is interesting in an animation: head movements, talking, eye tracking and even body action. For example, let us look at the GIF below: president Trump drives the cast of Game of Thrones to talk and move like him.Before creating our own sequences, let us explore this approach a bit further. First, the training data set is a large collection of videos. During training, the authors extract frame pairs from the same video and feed them to the model. The model tries to reconstruct the video by somehow learning what are the key points in the pairs and how to represent the motion between them.To this end, the framework consists of two models: the motion estimator and the video generator. Initially, the motion estimator tries to learn a latent representation of the motion in the video. This is encoded as motion-specific key point displacements (where key points can be the position of eyes or mouth) and local affine transformations. This combination can model a larger family of transformations instead of only using the key point displacements. The output of the model is two-fold: a dense motion field and an occlusion mask. This mask defines which parts of the driving video can be reconstructed by warping the source image, and which parts should be inferred by the context because they are not present in the source image (e.g. the back of the head). For instance, consider the fashion GIF below. The back of each model is not present in the source picture, thus, it should be inferred by the model.Next, the video generator takes as input the output of the motion detector and the source image and animates it according to the driving video; it warps that source image in ways that resemble the driving video and inpatient the parts that are occluded. Figure 1 depicts the framework architecture.The source code of this paper is on GitHub. What I did is create a simple shell script, a thin wrapper, that utilizes the source code and can be used easily by everyone for quick experimentation.To use it, first, you need to install the module. Run pip install deep-animator to install the library in your environment. Then, we need four items:To get some results quickly and test the performance of the algorithm you can use this source image and this driving video. The model weights can be found here. A simple YAML configuration file is given below. Open a text editor, copy and paste the following lines and save it as conf.yml.Now, we are ready to have a statue mimic Leonardo DiCaprio! To get your results just run the following command.For example, if you have downloaded everything in the same folder, cd to that folder and run:On my CPU, it takes around five minutes to get the generated video. This will be saved into the same folder unless specified otherwise by the --dest option. Also, you can use GPU acceleration with the --device cuda option. Finally, we are ready to see the result. Pretty awesome!I this story, we presented the work done by A. Siarohin et al. and how to use it to obtain great results with no effort. Finally, we used deep-animator, a thin wrapper, to animate a statue.Although there are some concerns about such technologies, it can have various applications and also show how easy it is nowadays to generate fake stories, raising awareness about it.My name is Dimitris Poulopoulos and I’m a machine learning researcher at BigDataStack and PhD(c) at the University of Piraeus, Greece. I have worked on designing and implementing AI and software solutions for major clients such as the European Commission, Eurostat, IMF, the European Central Bank, OECD, and IKEA. If you are interested in reading more posts about Machine Learning, Deep Learning and Data Science, follow me on Medium, LinkedIn or @james2pl on twitter.[1] A. Siarohin, S. Lathuilière, S. Tulyakov, E. Ricci, and N. Sebe, “First-order motion model for image animation,” in Conference on Neural Information Processing Systems (NeurIPS), December 2019.WRITTEN BY"
44,Plotly Front to Back: Scatter Charts and Bubble Charts,https://towardsdatascience.com/plotly-front-to-back-scatter-charts-and-bubble-charts-8e6dc5f77b54?source=collection_category---4------0-----------------------,"It’s been a couple of days since my last post on the Plotly library, with the last article covering bar charts and line charts. While I recommend reading that one before this, it’s by no means mandatory, as each article is written to tell the full story.In case you want to follow up with the series, here are the links:Okay, without much ado let’s take a minute to talk about what this article covers. The main idea is to take a dataset (a well-known one, but more on that in a bit) and to explore how it can be visualized — with scatter and bubble charts.The dataset used is quite small, but will be enough for our needs, and can be downloaded from this URL. You don’t need to download it, as we’ll import it straight from the web.Okay, without any ado let’s get started!Imports-wise we’ll need two libraries — Pandas and Plotly — here’s how to import them:As for the dataset, we’ll grab it directly from GitHub. It’s well-known car dataset, perfectly suited for the type of visualization we will do:Okay, that’s it, let’s dive into the good stuff now.Now, the basic idea behind the scatter plot is to visualize the relationship between a set of variables. By relationship, we mean the dependence between variables — or movement — what happens to the second variable if the first one moves by some amount — or correlation to say it in the simplest way.Scatter plots are utterly simple to make with Plotly — well almost anything is (except maps, those can be a pain in the bottom, but more on that in a couple of articles) — as we need to define well known data and layout options.Just like with the line plots, we’ll use go.Scatter() for our data, but we’ll need to specify mode='markers' for everything to work properly.Here’s how you’d go about visualizing attributes hp (horsepower) on the x-axis, and mpg (miles per gallon) on the y-axis:Not too many lines of code, but let’s see what it results in:I know what you’re thinking about right now:Well, let’s fix those issues:Every issue we had previously can be fixed by tweaking the marker params, via the marker argument. Here’s how our chart looks now:Still, not the prettiest, but will do for now. Let’s now explore the bigger brother of scatter charts — bubble charts — to see how easy it is to display additional information and set the sizing to be dynamic.Just in case you haven’t read the entire article, but instead skipped to this section, here’s what you should know: We typically use bubble charts to display additional information, either through size or color — or both.For example, let’s say that we still want to put hp (horsepower) on the x-axis and mpg (miles per gallon) on the y-axis — but we’d also want to set the size to be dynamic — let’s say according to the wt (weight) attribute, and also set the color to be dynamic — by the number of cylinders cyl.It sounds like too much information for a single chart, but it’s really not. Here’s how to implement above-said in code:In case you’re wondering, we multiplied the value of wt attribute because otherwise, it would be too small. Feel free to tweak this value as you like. Here’s how the chart looks:Nice. We have the same information on both axes as we had with a simple scatter chart, but here two additional pieces of information are displayed.If you ask me, this shade of yellow is just a bit too much, so let’s see how to alter the color palette:Here’s how the chart looks like:As you can see, this visualization is a bit easier on the eyes. You can reference this page to get a full list of available palettes.I think this will be enough for today.I hope this wasn’t too much for you — I’m aware that the Plotly syntax might take some time to get used to — but it’s worth the effort if you ask me.The following article will cover pie charts and treemaps — so stay tuned if that’s something you’d find interesting.Thanks for reading.WRITTEN BY"
45,Makeover Monday: Game of Thrones Edition,https://towardsdatascience.com/makeover-monday-game-of-thrones-edition-65432e684318?source=collection_category---4------1-----------------------,"David Murphy’s Game of Thrones-themed dashboard (pictured below) is an incredible accomplishment. Murphy painstakingly collected all on-screen death data and created a dashboard that had bits of his personality and brand tied in with design elements from the Game of Thrones franchise.Note: This dataset and original dashboard is from Makeover Monday’s Week 27, 2019.In this blog post, I’m going to walk you through the design decisions I made when I re-designed David Murphy’s dashboard. But first, let’s go over some context behind the source data.As I mentioned earlier, this data was collected by the original author. This is obviously prone to minor mistakes and has subjective elements, such as Allegiance (e.g. Does Jorah Mormont have Allegiance to the Mormont house or the Targaryen house since he was in Daenerys Targaryen’s service for the whole series?). Regardless, this is a treasure trove for data-enthusiast-GoT-fans like myself.Here are some useful tidbits you should be aware of regarding the data…Now that that’s out of the way, let’s talk about the dashboard makeover.Murphy’s dashboard does a lot of things very well. I actually kept quite a few visualizations that he created. Specifically, I liked his use of Bar Charts, Running Kill Counts, and Episode Breakdown (all highlighted below).I love that his dashboard has GoT-themed elements, like the headers as scrolls, the font, and the pictures of the characters. In my dashboard, I chose to remove these elements because I wanted my dashboard to feel a bit more modern and minimalist.I also strongly feel that treemaps and bar charts do a far better job of showing categorical data to the user than packed bubbles (which were used in the “Top Methods” section). Additionally, I wanted my dashboard to be able to answer more questions about the series, like how Houses compare to one another, which methods were used most, and which House was killed the most.The main goal of my redesign can be summed up with the following problem statement: How can I better understand differences and trends between characters and heroes in a cleaner and crisper dashboard?The dataset is very limited in terms of number of features captured, but a lot of information can still be provided. Let’s go through all the questions the redesigned dashboard can answer…My dashboard is organized into three parts, and each part answers different sorts of questions…Part 1: Character SpotlightPart 2: Overall TotalsPart 3: House SpotlightOverall, the new dashboard does away with all the extra GoT-themed design elements, and it should look a lot cleaner. There are a lot more visualizations, so it’s definitely more complicated and dense, but it can answer many questions.There are a few more tuneable parameters that a viewer can modify (season/episode views and particular houses), and some of the visualizations better facilitate comparisons (line chart for house vs. house comparisons or bullet chart for character vs house comparisons).I’m also a big fan of summary tiles (Part 2: Overall Totals) because they can deliver important information very quickly and succinctly.Finally, formatting-wise, I love color-coordinated tooltips and titles that highlight the dynamic aspects of each visualization. For example, all text in maroon changes depending on your parameter selections. Additionally, if you hover over each visualization, the same maroon highlighting method is used.I don’t claim my dashboard to be better or worse than the original. Makeover Monday is all about personal preference, inspiration from others, and creativity.Finally, a big shoutout and thank you to David Murphy for the inspiration and data source.WRITTEN BY"
46,Spot the Curve: Visualization of Cases Data on Coronavirus,https://towardsdatascience.com/spot-the-curve-visualization-of-cases-data-on-coronavirus-8ec7cc1968d1?source=collection_category---4------0-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.At the time of the writing (26th March 2020), Coronavirus (COVID-19) has wreaked havoc on our society. Though there was already some news about a new kind of virus spotted in Wuhan, China at the end of 2019, The outbreak in western countries just started. During the self-quarantine, it came to my mind that it would be interesting and possibly inspiring if we use the data to compare each country regarding the epidemic, instead of being bombarded with random information and number by mainstream media. As a result, I took the time of my spring break to get into this small project.Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.When I was thinking of visualizing the condition of each country, the first thing that came to my mind is “the curve.” About the start of the March, social networks and medias all suddenly filled with the idea of “Flattening the Curve.” The concept promotes the recognition of the virus for the general public, and it works like a charm in western societies.In this popular and excellent medium post (by Tomas Pueyo), the author explained it pretty well. In general, the curve means the distribution of active patients that the society needs to take care of. The logic behind the graph is that we shouldn’t expect to stop all the spread, but to delay and let the nation has the capacity to resolve the cases.So our goal is not to eliminate coronavirus contagions. It’s to postpone them. — Tomas PueyoAccordingly, I decided to create plots based on this concept, showing the growth of confirmed cases in each country with little variations.At the time of writing this article, one of the most referencing data sources is John Hopkins University. It gathers data from WHO and CDC in different countries, and some of the open-sourced developers also build APIs on top of it. So I chose to use its datasets.I first found the confirmed case data of each country to do the data cleaning, making sure the data is in the time-series format. Then I used the Python visualization package (Matplotlib) to make the graph.Here is a little disclaimer: Since the data is in a very incohesive and under-developing condition, I have made compromises and personal decisions at a granular level to make this possible. If you are interested in the details of ETL, please see the GitHub page or just leave the message to me if I haven’t mentioned anything. This is a project of reporting and EDA without any prediction model.First of all, to get the first baseline graph, let’s just plot the confirmed cases in China, the first happening place, and in other Asian countries.However, I noticed something tricky right away. First of all, the dataset only records the case data starting from 22nd January. This makes it impossible to see the complete trend of the spread (China had its first cases way back in 2019; The earliest record in the data is 548 confirmed cases). Secondly, the data is the cumulative ones, which means that it suppose to only go up. So it is hard to get a sense of the current epidemic status (How fast the spread is) of the country.To deal with the representativeness issue, I decided to overhaul the data. First, I converted the value of cumulative cases into the “daily increase” fashion. In terms of identifying the loading of a country, I consider the total number of cases an inaccurate metric, given that different levels of the population might influence countries’ capacity of public health (e.g. nurse-patient ratios).About the population, I acknowledged that the speed of spread, in most cases, is not correlated with the level of population (It should be the population density). So the main point here is to compare the capacity of dealing confirmed cases, not the “growth” of COVID-19 in each country.Accordingly, I converted it into a new one called “Infection Rate,” the number of confirmed cases per 1 million people (by the population of the country; Source data ), to have a fair comparison with different kinds of countries. Additionally, on the plot, I aggregated the number of daily increased cases into a 2-day format and included records only after the 1st case to smooth the trend (also to have a fair start for each country). With the change, the visualization would also be more apparent since it gets messy when the numbers are big and have too many comparables.And then I pick other Asian countries since they are also the first few countries that got affected and have possibly “flattened the curve” in these days.From the plot above, we could tell that most of the Asian countries are rather steady regarding the Coronavirus spread. South Korea once had a major infection cluster before, but it has stopped the exponential growth successfully after the incident. It is interesting that we could spot a few “curve” already on the graph, but that doesn’t mean they won’t suffer from any outbreak in the future. From the plot below (removed South Korea for better viewing), Singapore and Taiwan, which have been praised for appropriately dealing with the epidemic, are still facing potential growth in spread lately.p.s. India wasn’t included in this chart yet because it just started to have more data on the confirmed case and it has a vastly different population-level from others. I would study it later and separately.Hypothetically speaking, strict travel bans and mandatory quarantine can mitigate the spread of the virus. Most of the countries (especially in Asia) have already issued related policies. Nevertheless, because of the 2-week incubation period and the returned citizens from aboard, we should expect that the effect takes another week or two to happen (To Be Updated).After the first exploration of Asia, we knew that most of the Asian countries have a more mild spread for now. Next, I pivoted to visualizing what is going on with some of the more serious countries now.To identify the most severely-spread countries, I kept using the same metric (infection rate) and ranked the top 10 countries with the latest infection rate (23rd March).I kept the same setting in terms of plotting, but I included records only after the 100th case to smooth the trend (Some countries had peak values due to the irregular testing routine, and some also started actively testing potential cases late.)As it turned out that all 10 countries in the most serious situation are from Europe (only Iran from the Middle East). Surprisingly, Switzerland actually is in a dire situation among other nations (The exceeding growth rate made it surpass Italy in these two days.). In contrast, although the United States has received lots of attention from the media, it is currently at the 15th place in this standard. Despite that, it is still in its earlier stage of virus spread compared to the European countries, so we should expect more change from it (See the graph below).Updated on 3/27/2020: the US is now experiencing a high volume of testing practice so the rank might considerably change in the future.Although we have factored in the population, currently the number of confirmed cases still heavily associated with the amount of the testing. Ideally, we would like to have as much detail about COVID-19 tests in each country as possible. However, there is not yet an official database about this.p.s. Our World in Data (a group of researchers from the University of Oxford) has gathered a provisional and incomplete version of this information and makes it possible to use this in the visualization (Please see one of their charts below).In the future, I will keep adding new stuff based on the work here and build an interactive and self-controllable dashboard on a web app to generate any of these charts for comparison.In this article, we used a more considerate approach to examine the current status of Coronavirus in specific countries. We spot a few potential curves along the way and recognize the seriousness of COVID-19 in some countries. Yet, the information also showed that we should beware of any significant change in terms of the spread and also the data.Though the content seems little, it was actually a lot of work for a novice to epidemiology and public health like me to analyze and make assumptions on this matter. Besides domain knowledge, the open data of COVID-19 is considerably underdeveloped at this point. The more I look into the data, even from highly-referred sources, the more frustrated I am because the most reliable and credible information, for now, is only the confirmed cases (e.g. JHU’s GitHub has some annoying inconsistency in the data; Multiple APIs keep going on and off from time to time).That being said, there is still a group of professionals keeping on making the data complete and actually distill actionable insights. If anyone interested in doing a project on this, you could check out some useful articles and interesting discussions.For now, let’s give it more time and doing our part!Congrats and Thanks for your reading! Feel free to check up my Github for the full codes and drop a message at cl3883@columbia.eduJeffWRITTEN BY"
47,Can Python dataviz libraries repeat the Tableau worksheet?,https://towardsdatascience.com/can-python-dataviz-libraries-repeat-the-tableau-worksheet-e38ef2876f04?source=collection_category---4------1-----------------------,"Well, the short answer is yes. Though, there are some cool things I have noticed during my Tableau exploration.Tableau was on the list of my mandatory stops on a journey into data science and data visualization. It is a very cool and convenient instrument for quick dataset acquainting and initial exploratory visualizations. It delivers a lot of tools necessary for the primary data analysis and further exploration. So, it is not a surprise that I am interested in it as a beginner data scientist. Though, first of all, I am a software developer, so I want not just to get a new fancy tool, but to understand, how it works. The main goal is to recreate any visualization from Tableau with Jupyter notebook and any Python library like plotly or matplotlib.Our task requires some exploratory analysis upon the data, so I have chosen a dataset which promised a lot of interesting things to be shown. It is the table of Kickstarter projects for 2018 (both failed and succeed) from Kaggle. It contains enough categorical variables for grouping (like project type and country), some dates and date ranges (well, they all have start dates and deadlines) and numerical variables for aggregating (money amounts).Just to be aware of data:Although my first impulse was to use pandas, I now have to look at the data with the help of Tableau.Tableau can open the dataset in a few clicks equivalent to the one code line with pandas. Though it brings additional features like automatic column type recognition: date/time, geodata, number, etc.. So, Tableau approach looks more attractive, so I took my dataset …… and I got trouble with data types and my data was not parsed. After a brief look, I found, that the problem was in local Windows language settings. I live in Ukraine, so I have month names in my native language, but dataset contains English format, which Tableau could not recognize with the current language settings. I was a bit disappointed, that the program takes local settings and I cannot tune them within the program. Anyway, the problem was solved and I continue the work.Now we can see all the pros of auto type recognition:As we see, it correctly recognized currency, dates, and geodata. Except for the minor mistake: the “state” column was recognized as geodata instead of a raw text. Nevertheless, it is a great feature, which will be helpful in analysis.Dataset cleaning is the most time-consuming step in the data analysis process. So we need to use any utilities to keep our time. Tableau is an excellent assistant in this process with its aliases, calculation columns, and filter systems. For example, we have the column “state” in our Kickstarter dataset. It keeps several values: “successful”, “failed”, “live”, “suspended” and “canceled”. Though we want to work only with “successful” and “failed” projects. Let’s proceed with the next steps:First of all, all these steps may be performed in a few clicks and it is an obvious advantage of a visual interface. Also, calculation fields allow performing conditions of any complexity. And filters give us quick access to all values of the column.Since we work with comparison, here is the Python code for the same job:Of course, a lot of cleaning jobs left, though we have already performed our example, so let’s move to the charts, the main goal of an article.It is the best practice I have met in data analysis: Tableau turns every column (feature or measure or filter or calculation field) into the drag-and-drop object. So you can create a fully informational visualization in a few mouse moves. For example, I just added the “Main category” feature to the columns and automatically generated “Number of records” to the rows and the bar chart is ready. One more move to add total count as a bar label and here is the result with additional visual tips on each bar:The same action will take a bot more time but a few lines of code in the Jupyter notebook:And what about more complex graphics? Not a problem: drag the “Country” column together with “Number of records” row, change visualization type and we have the treemap. Even more: drag “Country” feature into the filters section to remove the USA row from the visualization (so it becomes cleaner since the USA has the greatest number of records). The result is still interactive:Though, visualization is still reproducible with Python code:So we learned, that we can create simple visualization in a few clicks. Though we also need a few lines of code to have the same result in Python. Let’s create something more complicated.Simple graphics show us the connection between two variables. If we need three of more features to be displayed we use aggregation technics. For example, what if we need to show the number of successful and failed projects for each category? Well, it is an easy task for Tableau. We already have a bar chart of categories, so, let’s just add the “State” variable to the worksheet as color property:One move and we gathered more information. What about the code? Well, we need a lot of it for this graphic:Let’s now create something like AB-test for the “Countries” feature and show the proportion of successful and failed projects. We perform the same steps for “Country” as for “Category” before and change the property of aggregation to show the percentage:Again, here is the code. Though it is even more complex this time:Also, Tableau has different settings for coloring schemes, labels fonts, and custom lines, as we see on our visualizations.Let’s now move to another approach.As the title shows, we will create a map. Python has a lot of additional packages to work with geodata like Folium. Though, Tableau has built-in abilities to recognize geodata (like country codes) and build maps with it. Like in the previous parts, just drag-and-drop your variable into the worksheet and the Tableau will generate the map automatically:Few more clicks and we have a beautiful interactive map with colored signs for the number of Kickstarter records. And we still have full control for marks, colors, labels and other settings.As mentioned above, we use Folium and GeoPy (to convert country codes into the geodata) to create the same map:It is not so easy as in Tableau but still is reproducible.It is a reasonable question. Well, we already know, that matplotlib, seaborn, plotly, and other Python libraries are very functional and capable of almost any visualization. On the other hand, Tableau is a famous and cool instrument with very similar capabilities. We could predict the answer to the article’s title from the beginning. Nevertheless, I believe that this comparison is very important to show some not obvious thoughts:butDespite my conclusions, I believe that everyone should use the tools he likes and used to. I have found Tableau easy in usage, good in quick primary dataset analysis and capable to create grouping and save ready images in a few clicks. Although Python libraries can do the same staff, Tableau can do it faster and do it pretty. So, I will surely use it in my work, especially if I need an art-like picture. Nevertheless, I will still prefer Jupyter notebook for complex analysis, for displaying several images in time, for quick notes, and for the great freedom it gives during the project.You can find the Jupyter notebook with all working examples and built-in graphics on my Github together with Tableau worksheets:Also, you are free to share some interesting tricks to be used in Tableau.WRITTEN BY"
48,Beeralytics — A Guide to Analyzing Beer Prices from Web Data,https://towardsdatascience.com/beeralytics-a-guide-to-analyzing-beer-prices-from-web-data-37d4ba206071?source=collection_category---4------2-----------------------,"A few weeks ago my girlfriend and I were having drinks at a bar and started debating what the cheapest beer on the menu was. The challenge itself was that there were multiple sizes and alcohol content across 20-or-so beers which made simple comparisons a challenge. Bets were made, and we started doing cell phone math (I swear we’re this exciting all the time) using a basic formula to normalize them:‘Oz. of alcohol per $’ = ((‘Total Oz.’ * ‘Alcohol %’) / 100) / PriceThis revealed that the high ABV IPAs were the most value for your money by a decent margin! Now that I had lost yet another betting game to my girlfriend I was curious as to whether this was a local choice (maybe the bar had trouble moving those kinds of drinks) or was this kind of pricing consistent at scale? This seemed like a fun data science problem, so I started brainstorming ways in which to answer this question.A decade ago, this probably wouldn’t be possible without proprietary datasets or visiting each individual store and writing down all the pricing (I’m sure store owners love that). Instead we now have a plethora of delivery services like Drizly and Minibar that act as an aggregator by offering beer delivery via local stores.The rest of the article will go into the method of how to pull data from a website using the Python Requests library, chart and library selection based on the kind of data you are pulling, and medium- to advanced-techniques of visualization using the Bokeh library. The final files and code explained below can be found and forked here.Like I said in the intro, the first challenge was capturing the data so that I could actually perform some analysis on it. Neither has an API, so I started looking at the structure of their website and network calls to see which/either could be viable. Drizly has a very odd way of injecting data directly into react components so that there’s never a simple JSON payload in the network calls (maybe for obfuscation purposes?), and it was clearly going to take a lot of time to morph it in a usable form. Switching focus to Minibar, I quickly found they not only had a standard JSON payload that was easy to grab, but had a bunch of metadata too, yeehaw!Once I found this, it was just a matter of using the Requests library to pull the JSON…Or so I thought. After creating a simple request, I was met with an error message:Hm, well this is awkward. Running item_count.status_code also confirms a 401 unauthorized error, which means our response is correctly making it to the server but is getting denied due to missing some kind of credential alongside the request. Since tokens aren’t created out of thin air and the site requires no sign-in, the only logical conclusion was that it must be created for the session once a user visits the site. This prompted me to check the request headers section of XHR request referenced above, and lo and behold I came across a bearer token:Once I added in the headers parameter I was able to confirm this worked and returned data! The next challenge was to figure out how to programmatically grab and and add the bearer token to the request, since the the token would likely auto-expire in some set amount of time. My plan roughly became:The bearer turned out to be a pain to find on the main page since it wasn’t in any XHR requests, but instead included as one of the meta name tags:So how are we going to grab something if it’s not in json? Simple, we’ll treat it like a normal web page and use Beautiful Soup! This can be done by writing a really simple function:As you can see, all we did was specify to Beautiful Soup that the text should be treated like HTML, and then told it to return the contents of the access-token attribute. This saves us from doing any kind of convoluted or error-prone regex to find the token. Now that we can dynamically grab the data at any time it’s just a matter of flattening it into a DataFrame.Looking at the JSON, many beers had ‘variants’ (different prices due to pack size/store-specific sales/container types/etc.) which I also wanted to capture and contrast. Using the following code I was able to loop through all the beers based on a count variable in the initial call, add each variant as an individual line item, and then output it into a single DataFrameOnce I had ~2,000 beers in the DataFrame I used the missingno library to get a sense of how complete the data was:Turns out alcohol %, the crux of my whole experiment, was missing for a LOT of beers, about 3/5 of the dataset. Blinded by stubbornness and zeal, I did what any rational person would do in this situation — I grabbed a 6-pack from the store and spent the next several hours manually plugging in values I found searching sites like https://untappd.com/. Eventually I completed both the beer and the data entry, and was ready to move on to the next phase.So now that we have a nice dataset, we need to think about how we want to effectively visualize the data. The data has both quantitative and categorical dimensions, and we’re going to want to compare things like type of beer, container type, etc. against different attributes. Considering the amount of potential data points at any given point as well, scatterplots stuck out as a good choice given their ability to easily display pattern and correlation trends. I also knew there were a few different advanced interactions I wanted to perform with the visualization:After experimenting with a few familiar simple graphing libraries I realized that this was not going to work without a highly flexible and customizable library. I ended up settling on Bokeh which not only has a huge library of pre-made charts, but also came with some great examples — particularly this move ratings one. I decided this one would suit my needs with only minor tweaks, so with that it was full-steam ahead!I’ll warn you right now, if you want to do anything beyond basic graphs with Bokeh, read the docs. I resisted at first and it resulted in me filtering through Stack Overflow and Google for hours like a lost child. Understanding the structure of where to access and update objects like glyphs and how ColumnDataSource updates data are key concepts you should try and understand without diving too much further.My second disclaimer is that some of this code is likely not efficient, but if I tried to tune/understand it anymore I would have never published this.Bokeh for the most part is pretty easy to understand after looking through a few examples on their site, plus I’m basically using the movie example. The idea in bullet points is:It seems like a lot of code but it is mainly making unique lists of columns you want for filters, assigning those lists to widgets, and setting default values for the filter.We’ll also add in a section to create a figure, glyphs, and all the styling needed for our final chart. We have both a bar and legend in here because we’ll need to toggle their visibility in order to switch between quantitative and categorical selections:Perfect! Now that the basics are out of the way we can move to the more advanced concepts.One of the things I wanted to do was to map either categorical OR quantitative data to the points on the plot. I wanted the color bar to rescale on quantitative data as well, or else outliers (like the price of a keg) would mess with the color distribution of filtered data. Without going deep on the fundamental issues, my logic ended up looking like this:Now that we have the framework for updating our color scales/legends, we can move onto how to update everything once a user interacts with our widget filters.We’re going to heavily mirror the movie graph example for this, but I’ll walk through the concepts briefly. We want to create a function called select_beer() which will find all the current values for our widget filters, and then filter the dataset based on those selections:The next thing we want to write is an update() function which will update the graph elements (such as name of axis and total # of beers selected), as well as update the ColumnDataSource which is how the data on the graph is updated. It will finally force updates on the legend/color scales by calling cat_linear_toggle and updating the legend. The final points applies to jitter to the newly filtered data:After that’s done all we need to do is setup callbacks for all our controls. The final output is very similar to the generic bokeh in the movie example. I added a os.system(...) at the end to make it convenient to run on my local:Running this all together gave me the first visual of what we put together, not bad!At this point I felt pretty happy with the output and thought I was done, but once I started examining the data I tried to highlight areas of points on the chart to filter the data table. Many data points were on top of each other, and it was difficult to examine without some way of seeing underlying selection areas. Bokeh graphs naturally handle selecting data on graphs through their default tools such as box_select, but there’s no default way to reflect this on the data table.After reading and googling more about it,I came to the unfortunate conclusion that this could not be done using the default library…ughhhhh. Luckily, Bokeh allows for use of JavaScript through the CustomJS model, and what I needed to accomplish required very little knowledge of JS. What we want to do is the following:And there we have it! The final result should only then show the selected points reflected in the data table, similar to what you see below when I select a few data points on the graph:I know what you may be thinking — what about the actual beer?! So turns out there were very few surprises, the malt beverage brand 4Loko (aka battery acid in a can) was by far and away the cheapest in my area with a ‘Oz of Alcohol per $’ of $0.72, about $0.10 per oz cheaper than the next best challenger. “Alcohol %” (ABV) turns out to be the best baseline predictor for value, which you can confirm through Exploratory Data Analysis or other means.Ultimately you could probably do more like including additional zip code areas or sites into this analysis, but the excessive missing data definitely affects what you can do without some manual data cleaning. Bokeh was also really interesting to work with, but I’m not sure I’ll start with it as my first choice for graphs in the future. Maybe if I was doing data visualization for a job I would consider it, but the learning curve is pretty steep for most things people need to show with graphs.Hope this was helpful to anyone out there and feel free to reach out or comment below with questions/comments. Thanks for reading!WRITTEN BY"
49,How Fast Is the Corona Virus Spreading in Your Country Compared to the Rest of the World?,https://towardsdatascience.com/how-fast-is-the-corona-virus-spreading-in-your-country-compared-to-the-rest-of-the-world-3d22bc79c284?source=collection_category---4------0-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.“ How Fast Is the COVID-19 Spreading in Your Country Compared to the Rest of the World? “T
his was the question I asked myself a couple of days ago and I couldn’t find relevant information presented in a nice and visually appealing manner. It was really important for me to be able to compare a particular list of countries with other countries. Therefore, I decided to build this dashboard*.*I may improve the dashboard after writing this article but the steps of this tutorial will be still valid and reproducible. If not, please don’t hesitate to contact me.IMPORTANT: Before we jump into the step-by-step guide, please familiarize yourself with these 10 considerations before you create another chart about COVID-19 by Tableau.You can access this data via various channels. The quickest and easiest way for me was via the GitHub app and then connect to the John Hopkins GitHub repository.You can explore other ways at the Tableau Covid-10 Data Hub page. For instance via a *.Hyper, *.CSV file, Web Data Connector, Google Sheet and others. Step-by-Step instructions for all data sources can be found here.Another quick and dirty way is to open the John Hopkins daily reports repo and directly to download several CSV files from there. For the purpose of this tutorial, this is also absolutely enough.In case you have established a direct connection to a Data Source, you can skip this step. Once we have all of the files needed, we can import them in Tableau. In order to do so:Once we have all of the CSV files available in Tableau, then we can proceed with merging/combining the files into a dedicated Tableau Data Source. In other words we merge all rows from all files into one file. In Tableau this operation is called a Union, which is absolutely identical to UNION ALL with SQL. Usually, this operation is performed when we know that all of the columns from the files have the same names/structure.The first thing we need to do know is to familiarize ourselves with the dataset. You can see a sample of the data with the respective columns in the table overview pane at the Data Source tab.What is worth noticing are two particular fields where you can see date data:In order to create a date field based on the text field Path, we need to use the function Create Calculated Field, using the following calculation:Now, having a date field is great and it is time to create our anchor point. Since we want to compare different countries it makes sense to create a common starting point for each one of them. In our case, we will use the 100th registered case at any row from the data set as a starting point.At this step, we just do a simple grouping of some different naming convention of a couple countries, e.g.China (China; Mainland of China), Iran (Iran; Islamic Republic of Iran), South Korea (South Korea; Korea, South), United Kingdom (UK; United Kingdom) ,United States (US; United States) etc.On the abscissa, we would like to display the days since the 100th registered case in each country. In order to do that, I used the following calculation:Here, we take the minimum date in the dataset for each country, which is actually the date of the 100th case (since we filtered already for this). Then we estimate the difference between this date and the day represented at this row level. I have named this field: days_passedNOTE: Be sure to create this field as a Dimension, NOT as a Measure.One measure, that is extremely useful to have is the number of active cases. We can easily get this using the following calculate and name the measure Active cases:We use the ZN() function to be on the safe side in case there any null values in the dataset.And now is time for the fun part.Now, you can play around by adding labels, tooltips, additional filters, etc.If you found this article useful, you disagree with some of the points, have questions or suggestions, please don’t hesitate to leave me a comment below. Your feedback is highly appreciated!LinkedIn: Kiril YunakovWRITTEN BY"
50,The art of map visualization: Coloring Singapore island with datapoints,https://towardsdatascience.com/the-art-of-map-visualization-coloring-singapore-island-with-datapoints-41c7414adfed?source=collection_category---4------1-----------------------,"Singapore is easily one of the top travel destinations in Asia. Even though the city is smaller than NYC, it has just everything you need or wants to experience. Vast selection of food, street (and luxury) shopping, green city that embraces biodiversity, you name it!In this article, we will use R to visualize spatial data on top of the map of Singapore.First, we need to get a map background using the get_map function.This article focuses only on Singapore island. The other smaller islands are not included in the visualization.All datasets used in this article are obtained from data.gov.sg. The Singapore government provides a great ton of data available for public use. In this article, I downloaded the KML (Keyhole markup language) files from the source and put them onto a map using R.After downloading the KML files, load the files into R for processing. The KML datasets need to be transformed into the EPSG 3857 format, which is the Pseudo-Mercator projection coordinate system used by Google.After getting the Singapore map we want and transforming the datasets into the right format, we need to combine the dataset attributes (dots and polygons) features with the base map. The following codes will allow us to accurately overlay the latitude and longitude points of the dataset and the Singapore map together.In this article, I used ten datasets obtained from data.gov.sg to visualize the food establishments, transportation systems, and recreational activities locations on Singapore islandAfter executing the R codes above, you should be able to get a Singapore map, colored with the selected datasets!Most of the institutes are located close to the Singapore Strait, the Downtown Core Area. So, if you are visiting Singapore soon, you know where to go!The ggmap package in R is a powerful package for spatial data visualization. There are many map types from Google, Stamen, and Openstreetmap that you can try out yourself!WRITTEN BY"
51,Visualizing COVID-19: Isolation measures are working. We need more.,https://towardsdatascience.com/visualizing-covid-19-isolation-measures-are-working-we-need-more-ca1012ff269e?source=collection_category---4------2-----------------------,"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.With the United States recently claiming the undesirable distinction of most Coronavirus cases worldwide (by the official count), the New York Times has made available a dataset of all COVID-19 cases in the nation, tracked at the state and county levels and updated daily.While acknowledging that the data will never be perfect, the true number of cases is likely much larger, and testing frequency & effectiveness vary from area to area, here are my findings thus far.To keep this brief and actionable, I will limit to just two key points, outlined below (please note that I am not an epidemiologist, doctor, or healthcare professional):On to the analysis…The graph below visualizes COVID-19 Cases per 100,000 people in the ten US states which currently have the most cases at the time of writing.Besides the fact that New York & New Jersey very much need our help, the more subtle observation and silver lining from this chart is that cases in California have grown relatively slowly compared to other states, despite California being home to an early epicenter. The state took decisive action, ordering “shelter in place” 5–7 days before New York, and it appears to have made a huge difference, though obviously not the only factor in play.Given the success of countries like Singapore and Taiwan in controlling the spread of the virus using aggressive testing and isolation measures, it is reasonable to think that taking aggressive isolation measures early has helped California control the spread and will help other states across the nation.While it may seem obvious to many that aggressive isolation measures would be effective, a number of states have still not implemented any shelter in place/stay at home orders, or any other form of strong isolation.Implementing those isolation measures may represent our best opportunity at fighting the virus’ spread in a world where the United States currently has the most cases worldwide.The second important point: all states appear to be following a similar pattern of exponential growth, as you’ll see below — even those with smaller numbers of cases.Given that many of them appear to be earlier in the curve, it is imperative that seize the opportunity to take action and implement isolation measures while their situations may still be more controllable.The charts below show the growth in total number of cases per state for states outside of the “top 10”. You’ll notice all of the curves share a similar shape. I’ve limited each graph to no more than 10 states for clarity of viewing.Social distancing and isolation may be the best tools we currently have in our global fight against the novel coronavirus, and according to the data, when implemented correctly they are working.I hope those in leadership positions will take the necessary actions swiftly, and that each of us will take this seriously, do our parts, and stay home to the best of our ability.And of course, all of this isolation does not come without cost. We will also need social solutions, government intervention, new innovation, and a whole lot of empathy to help each other through what will be an extremely challenging and devastating experience, mentally, spiritually, and financially, especially for the most vulnerable segments of our population and economy.With cautious optimism, I believe we are capable of rising to the occasion, but I will not attempt to explore those issues here. From inside my apartment where I will remain with my family, wishing us all the best.For further details behind my analysis, or to see the Python code behind it, you can visit my Github here.Just understand that this is a work in progress, and don’t expect me to clean anything up 😆Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.WRITTEN BY"
52,Forecasting COVID-19 cases in India,https://towardsdatascience.com/forecasting-covid-19-cases-in-india-c1c410cfc730?source=collection_category---4------3-----------------------,"COVID-19 has taken the world by storm after first being reported in Wuhan, China in December-2019. Since then, there has been an exponential growth in the number of such cases around the globe. As of 28th March 2020, the total reported cases reached 668,352 out of which 31,027 have died. The below figure shows the outbreak of the virus in various countriesIt can very well be observed that most countries have reported cases of new Coronavirus. As of now, it has affected 192 countries and 1 international conveyance (the Diamond Princess cruise ship harbored in Yokohama, Japan).Considering India, the cases emerged towards the end of January in the state of Kerala, when three students returned from Wuhan, China. However, things escalated in March, after several cases were reported all over the country, most of whom had a travel history to other countries. The below figure shows the number of cases detected between 30th January and 28th March.After first emerging in late January, the number remained constant until the beginning of March, post which, it grew exponentially. As of 28nd March, the number of total cases has reached 987 cases with 24 deaths. Given the current rate of growth, where can the cases expect to reach in the next 10 days, if no specific precaution is taken?We can do a time series analysis to create a model that helps in the forecast. The dataset is available on Kaggle. We use R-Programming for our analysis.We load the necessary packages and attach the datasetLet’s have a look at the first 6 observations. The columns include:Date: The date on the observations are recorded.Total Confirmed Cases: Number of confirmed cases as of the given date.Cured: Patients recovered as of the given dateDeaths: Patients died as of the given dateSince, we are to analyze only the Total Confirmed Cases, we create a new data frame including only that.The next step is to convert the data frame to a time series object.We get the time series plot with each observation recorded on a daily basis. The next step is to check if the time series is stationary. Putting it simply, a stationary time series is one whose statistical properties such as mean, variance, auto-correlation are constant over time. It is important for a time series observation to be stationary as it then becomes easy to get accurate predictions. We use the Augmented Dickey Fuller Test to test the stationarity of the time series observations. The null hypothesis (Ho) for the test is that the data is not stationary whereas the alternate hypothesis is that the data is stationary. The Level of Significance (LOS) is taken to be 0.05.The p-value turns out be 0.99. We thus fail to reject our Ho and conclude that the data is not stationary. We now have to work on the stationarity of the data. There are various methods to make our time series stationary depending on its behavior. The most popular is the method of Differencing. Differencing helps stabilize the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. Mathematically it is given as:In the above expression, B is the Backshift operator and d represents the differencing order. The code used in R is as followsAfter two rounds of differencing, we again perform the Augmented Dickey Fuller Test (ADF Test). The p-value is smaller than 0.01. We can thus reject our null hypothesis and conclude that the data is stationary. Since the order of differencing is 2, d is 2.The next step is to plot the ACF and PACF graphs.Complete Auto-Correlation Function (ACF) gives us the auto correlation of any series with its lagged values. In other words, ACF is a chart of coefficients of correlation between a time-series and its lagged values.Partial Auto-Correlation Function (PACF) gives us the amount of correlation between two variables which is not explained by their mutual correlations, but with the residuals. Hence, we observe if the residuals can help us generate more information. A more comprehensive guide on the topic can be found here.The ACF plot above shows a significant correlation at lag 3(q) while the PACF plot below shows significant correlation till lag 1(p).Based on the ACF and PACF, we choose an ARIMA (1,2,3) model. We take d as 2 since it took two differencing to make our time series stationary. We will now fit the model based on the above chosen parametersWe get the following resultsWe forecast the Confirmed Cases for the next 10 days, that is, until 7th April 2020. The Point Forecast reaches 2278 on the 69th day, that is, by 7th April 2020, total confirmed cases is likely to reach 2278. Also, it is important to note that the cases double almost every 10 days. The figure plot shows the predicted cases. The blue line represents the forecast and the silver shade around it represents the confidence interval.R also has at auto.arima() function that automatically selects the optimum (p,d,q) values for ARIMA model. Let us fit using that.We get the following resultsThe model using auto.arima(1,2,0) is pretty similar to our previous model, however, the q value is 0. We get an Akaike Information Criterion (AIC) of 486.42 which is less than the AIC for the previous model (491.62). From the point estimates, we can see that the Total Number of Cases reach 2264 on the 10th day, that is, by 7th April 2020, marginally less than 2278 predicted by the previous model. Cases more of less double every 10 days. Plotting the predictions, we get:To estimate Model Adequacy, we do Residual Analysis. For a model to be adequate, the residuals should be Independent and Identically Distributed (I.I.D.) and should be uncorrelated. To test for correlation, we use the ACF plot and look for significant correlations.There is no significant correlation observed at any lag except 0. We can deduce that the residuals are not correlated. To test if the residuals are normally distributed, we use Shapiro-Wilk Test of Normality. The results show that the residuals are normally distributed.We come to the end our Time Series modelling on the Total Number of Cases in India. The number is increasing every passing day, however, the rate at which it grows will slow down in the next few days. The lock down in India implemented on 24th March will prevent community spread of the virus if the steps are taken seriously by the general public. Right now, it’s a major issue troubling every one in every corner of the world. We as a community have the potential to stop it and we can stop it by preventing the transmission of virus.Thank you for the read. I sincerely hope you found it helpful and as always I am open to constructive feedback.Drop me a mail at: icy.algorithms@gmail.comYou can find me on LinkedIn.Note from the editors: Towards Data Science is a Medium publication primarily based on the study of data science and machine learning. We are not health professionals or epidemiologists, and the opinions of this article should not be interpreted as professional advice. To learn more about the coronavirus pandemic, you can click here.WRITTEN BY"
53,Using Google Location History to Analyze Gym Visits,https://towardsdatascience.com/using-google-location-history-to-analyze-gym-visits-a8c7c18e0871?source=collection_category---4------4-----------------------,"One of my new year resolutions for 2019 was to hit the gym more regularly. Being an avid enthusiast of analyzing datasets, I decided to investigate how consistent and fruitful I was in this endeavour last year, using the Location History that I had been collecting. For all those not aware, Google keeps track of all the places you have visited (provided you give permissions, of course!). Since I had assigned Google permissions to store my location history since 2013, when I had my first Android phone, it was not difficult to retrieve the data in JSON. Using Python and Matplotlib to analyze the data, I decided to see how successful I was in keeping my resolutions last year.The data, at first, did not make much sense to me. But a little research brought me up to speed with the intricacies of the data. The data contained information regarding latitudes, longitudes, and DateTime of all the places that I have visited since 2013. Additionally, it also included several columns describing the ‘accuracy’, ‘velocity’, ‘altitude’, ‘heading’, ‘Vertical Accuracy’, which were not useful to me in the current context. The first task, therefore, was to wrangle the data into something which I could interpret.In addition to converting ‘timestampMs’ to the corresponding date and time, I also removed the ‘accuracy’, ‘velocity,’ ‘altitude’, ‘heading’ and ‘verticalAccuracy’ columns to create a processed dataframe. The second challenge was to zero in on the coordinates of my gym to filter the dataframe.I have to admit that this was one of the most challenging parts of the analysis. Latitudes and Longitudes form part of a geographic coordinate system to determine the position or location of any place on Earth.My gym is described by 50° 06'53.5"" N and 8° 41'12.9 “E. However, using them, I was unable to filter out the data points corresponding to my gym from my location history. To find out a rough range of latitude and longitude positions for filtering, I zeroed in on the coordinates that were recorded by Google for my workout on the 6th of December.Using the range of the coordinates corresponding to my gym,along with the corresponding date and time informaton,I could finally get started. Using a little data pre-processing tricks, I added a few columns for my convenience that would make the analysis a little easier.Using the ‘date’ column I added the day and month information to the dataframe.I also added the time duration that I spent at the co-ordinate pair in minutes.Before studying the data for further insights, I had to tackle certain hindrances:Now that I had the necessary information that I needed, i.e., date and time, I could delve deeper into the data.Using unique() from the pandas library, I determined that out of 365 days last year, I visited the gym 119 times.Going deeper, I decided to investigate how long every workout would take and study the trends with respect to the time of the day,day of the week and month.So what were the average times for my gym workouts? For most of the day, I would either be in university,language school or at work, which probably explains why the peak times of my workout would be around 17:00 or 18:00. During holidays, there were also a few days where I would head to the gym a little early. There also exists an outlier where I was at the gym at 00:00.Which day of the week was I more likely to hit the gym?Tuesdays!.Surprisingly the days I was most free are the days where I have rarely hit the gym. However, in defense, most of the weekends were also the days where I would have trips planned. In hindsight, though, one can conclude that my motivation would taper off as the week progressed.Viewing my visits per month also threw interesting facts. October tops the month with the most amount of visits.The table completely turns however, when they are plotted with respect to the average time spent per visit, where July tops the list. July was the time when summer set in, and the pleasant weather coupled with the impending summer vacations added to the motivation to put in the hours at the gym. While I did hit the gym more frequently in October, I was too busy juggling work and school to put in long hours at the gym(also dipping temperatures!). The months February and March are missing due to the stress that examination brings.The gym subscription cost me €15 per month and with 119 visits last year each visit cost me an approximate of €2 . Business sense or not, the benefits far outweighed the cost, so for me, it was worth it.Coming from just 20 days in 2018 to 119 in 2019,I would consider last year quite an improvement.I regret not tracking my growth through an app, which would have given me a holistic information about how my habits have influenced my overall fitness. However, I have begun taking measures to incorporate them and hopefully will be able to give you a better picture in 2020.Thank you for reading.You can surely reach out to me if you have any questions.WRITTEN BY"
54,Who’s The MVP of NBA This Season?,https://towardsdatascience.com/whos-the-mvp-of-nba-this-season-3e347c66a40a?source=collection_category---4------5-----------------------,"Don’t tell me it will be Giannis. Let’s find the answer from the data.I will frame this problem to a machine learning project and finish the project using the general working flow of machine learning similar to that introduced by DEEP LEARNING with Python [1].Let’s get started.The question we want to ask is “Who is the MVP of this season in NBA?”Who is or is not, which seems like a binary classification problem. So, my first attempt is to build a classifier to differentiate MVP players and non-MVP players.However, I found building a classifier is not practical because I will face the problem of sample bias. Specifically, the number of non-MVP players is much larger than that of MVP players, which will result in the difficulties of training and evaluating the model.Therefore, I frame it to a regression problem and the output is defined as the MVP voting share each year.To note, I use my domain knowledge (I am very familiar with the NBA as a big fan) here to choose the correct direction for the project. To make full use of your domain knowledge is very important in conceiving a doable project.Then, let’s look at the data we have in terms of the input X and output y. The X part of the data is the statistics of the players who have got votes for MVP from the season of 1989–1990 to the season of 2018–2019. The y part of the data is the voting share.Then the data is separated to train and test dataset. And the test dataset will never be touched until we have our final model.This is an important step of a project, which defines the success of the model. In this regression problem, I use the mean squared error (MSE) as the evaluation metric. Specifically, the model is trying to minimize the MSE in the training process.Since the sample size is small, I use the K-fold cross-validation instead of a one-time train-validation-test split as my evaluation process. When the sample size is small, the splitting of data can actually affect the performance of the model. K-fold cross-validation is usually used to solve the problem.The preparation of the data includes missing value imputation and feature normalization.The feature normalization is essential in regression problems, the different scale of features will be a disaster for the model training if pooled raw to the engine.In the modeling part, it’s easy to forget to set a baseline model no matter you are a beginner or experienced data scientist.Here we use a simple linear model as our baseline model and pack it together with the preprocessing of the data into a pipeline. For those who are interested in the pipeline usage, please refer to the Pipeline function in sklearn.The baseline model now is ready to be compared with.Next, we are going to develop a model that should perform better than the simple linear regression. I select three candidates, Elastic Net, Random Forest Regressor, and Deep Learning regression.Elastic Net is the combination of LASSO and Ridge, which penalizes the complexity of the model. If interested, please refer to one of my previous posts below.For the hyperparameter tuning process of these three candidates, I write a function that combines preprocessing, hyperparameter space definition and cross-validation process.where the deep learning model my_DL is defined as below:The classification and regression model in a neural network are usually different in the last layer. Here, for the regression modeling, we don’t use any activation function in the last layer: model.add(Dense(1)).However, if it’s a classification problem, you need to add an activation function, such as sigmoid in the last layer. I have written some similar function in one of my other posts as below:We then run the function of the hyperparameter tuning.Now we’ve got the best set of hyperparameters for each of the three models. We then need to refit on the entire training dataset using the trained hyperparameters.We cannot say a model is better than the baseline model (linear regression) based on the training data performance. So, we need one more step evaluation on the test data that we have never touched before.Now, we have the conclusion that model3 (the deep learning regressor) outperforms the other models by achieving the lowest MSE on the test dataset. So, model3 is our final model.Usually, a machine learning model project ends here by having a model with pretty good performance.However, in our project, this step is not sufficient yet to answer the question, “Who is the MVP?” That’s why we are going further.We are going to apply our trained model to the target question, predicting the MVP. So, I extract the same feature space as our training data from the players’ stats in this season (before the NBA suspension due to COVID19).We are only interested in the top players who have the chance to win the MVP. They are Giannis, Anthony, Luka, James (Harden), Lebron, and Kawhi.If you are a basketball fan, you must know these people. If not, you don’t need to know them to understand this machine learning project. Either way, I will not waste time introducing them.This is the prediction of their MVP vote share.We got a different answer from the media, our predicted MVP is James Harden! No like, no hate, but all from data. (But personally I do agree with this result. 😀)After predicting the result, I need to dig deeper into the data to interpret the result or to support the result. So, I compare these players’ stats (our feature space of the model) using the CoxComb Chart.We can see clearly that James Harden’s stats are all among the best (Game played, points per game, field goal percentage, steal, and so on). That’s why the model predicts him as the MVP for this season.For those who are interested in generating the CoxComb Chart, please refer to the following post.That’s it. A question-driven machine learning project from the beginning to the end.François Chollet. Deep Learning with Python.WRITTEN BY"
